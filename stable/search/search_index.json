{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to TerraTorch","text":"The Geospatial Foundation Models Toolkit <ul> <li> <p> Fine-tuning made easy</p> <p>Run <code>pip install terratorch</code> and get started in minutes. </p> <p> Quick start</p> </li> <li> <p> Overview</p> <p>Get an overview of the functionality and architecture with in-depth explanations.</p> <p> User Guide</p> </li> <li> <p> The big picture</p> <p>We outline the ideas and reasons why we build TerraTorch in the paper. </p> <p> arXiv</p> </li> <li> <p> Open Source</p> <p>TerraTorch is distributed under the terms of the Apache 2.0 license.</p> <p> License</p> </li> </ul> <p>The purpose of this package is to build a flexible fine-tuning framework for Geospatial Foundation Models (GFMs) based on TorchGeo and Lightning which can be employed at different abstraction levels.  It supports models from the Prithvi, TerraMind, and Granite series as well as models from TorchGeo and timm. </p> <p>This library provides:</p> <ul> <li>Ease-of-use and all the functionality from Lightning and TorchGeo.</li> <li>A modular model factory that combines any backbone with different decoders for full flexibility.</li> <li>Ready-to-use tasks for image segmentation, pixelwise regression, classification, and more.</li> <li>Multiple abstraction levels and inference pipelines to power enterprise applications.</li> </ul> <p>A good starting place is familiarization with PyTorch Lightning, which this project is built on.  TorchGeo is also an important complementary reference with many available models and specific datasets. If you have any open questions, please check the FAQs or open an issue in GitHub.</p>"},{"location":"about/changelog/","title":"Changelog","text":""},{"location":"about/changelog/#102","title":"1.0.2","text":"<p>https://github.com/IBM/terratorch/releases/tag/1.0.2</p>"},{"location":"about/changelog/#101","title":"1.0.1","text":"<p>https://github.com/IBM/terratorch/releases/tag/1.0.1</p>"},{"location":"about/changelog/#10","title":"1.0","text":"<p>https://github.com/IBM/terratorch/releases/tag/1.0</p>"},{"location":"about/changelog/#0999post1","title":"0.99.9post1","text":"<p>https://github.com/IBM/terratorch/releases/tag/0.99.9post1</p>"},{"location":"about/changelog/#0999","title":"0.99.9","text":"<p>https://github.com/IBM/terratorch/releases/tag/0.99.9</p>"},{"location":"about/changelog/#0998","title":"0.99.8","text":"<p>https://github.com/IBM/terratorch/releases/tag/0.99.8</p>"},{"location":"about/changelog/#0997","title":"0.99.7","text":"<p>https://github.com/IBM/terratorch/releases/tag/0.99.8</p>"},{"location":"about/citation/","title":"Citation","text":"<p>We outlined the idea of TerraTorch and the overall architecture in our paper.</p> <p>If you are using TerraTorch, consider citing it with: <pre><code>@article{gomes2025terratorch,\n  title={TerraTorch: The Geospatial Foundation Models Toolkit},\n  author={Gomes, Carlos and Blumenstiel, Benedikt and Almeida, Joao Lucas de Sousa and de Oliveira, Pedro Henrique and Fraccaro, Paolo and Escofet, Francesc Marti and Szwarcman, Daniela and Simumba, Naomi and Kienzler, Romeo and Zadrozny, Bianca},\n  journal={arXiv preprint arXiv:2503.20563},\n  year={2025}\n}\n</code></pre></p>"},{"location":"about/contributing/","title":"Contributing to TerraTorch","text":"<p>Contributions to an open source project can came in different ways, but we could summarize them in three main components: adding code (as new models, tasks and auxiliary algorithms or even addressing the solution of a bug), examples using the software (scripts, yaml files and notebooks showcasing the package) and documentation. All these ways are valid for TerraTorch and the users are welcome to contribute in any of these fronts. However, some recommendations and rules are necessary in order to facilitate and organize his process. And this is the matter of the next paragraphs. </p>"},{"location":"about/contributing/#contributing-with-code","title":"Contributing with code","text":"<p>It is not a trivial task to determine how a modification in the source code will impact already implemented and established features, in this way, for any modification in the core source code (<code>terratorch/</code>) we automatically execute a pipeline with hundreds of unit and integration tests to verify that the package have not broken after the modification be merged to <code>main</code>. In this way, when an user wants to modify <code>terratorch</code> for adding new features or bufixes, this are the best practices.  * This repository uses <code>pre-commit</code>, a tool which automatically runs basic     steps before sending modifications to the remote (as linting, for example).     See how to configure it here.  * If you are an user outside the IBM org, create a fork to add your modifications. If you are inside the IBM     org or have received writing provileges, create a branch for it.  * If you are adding new features, we ask you to also add tests for it. These tests are defined in the     directory <code>tests/</code> and are fundamental to check if your feature is working as expected and not breaking     anything. If your feature is something more complex, as a new model or auxiliary algorithm, you can also     (optionally) to add a complete example, as a notebook, demonstrating how the feature works. * After finishing your modifications, we recommend you to test locally using <code>pytest</code>, for example:     <pre><code>pytest -s -v tests/\n</code></pre> * If all the tests are passing, you can open a PR to <code>terratorch:main</code> describing what you are adding and why     that is important to be merged. You     do not need to choose a reviewer, since the maintainers will check the new open PR and request review for it by themselves. * The PR will pass through the tests in GitHub Actions and if the reviewer approve it, it will soon be merged.  * It is recommended to add a label to your PR. For example <code>bug</code>, when it solves some issue or <code>enhancement</code>     when it adds new features. </p> <p>Caution</p> <p>The PR will not be merged if the automatic tests are failing and the user which has sent the PR is responsible for fixing it. </p>"},{"location":"about/contributing/#contributing-with-documentation","title":"Contributing with documentation","text":"<p>Documentation is core for any project, however, most part of the time, the developers do not have the time (or patience) to carefully document all the codebase, in this way, contributions from interested users are always welcome.  To add documentation to TerraTorch, you need to be familiar with Markdown, a clean markup language, and MkDocs, a framework which relies on Markdown in order to create webpages as this you are reading. </p> <ul> <li>Install the MkDocs dependencies. Install as a developer <code>pip install terratorch[dev]</code> to include them or manually using this list.</li> <li>Clone the branch dedicated to documentation to a local branch: <pre><code>    git fetch origin improve/docs\n</code></pre></li> <li>Add your modifications and open a PR to <code>improve/docs</code>. It is recommended to add the label <code>documentation</code> to your PR. </li> <li>The PR will be reviewed and approved if it is considered relevant by the maintainers. </li> </ul>"},{"location":"about/contributing/#contributing-by-reporting-issues","title":"Contributing by reporting issues","text":"<p>The users also can contribute by reporting issues they observed during their experiments. When reporting an issue, provide the more details as possible about the problem, as the configuration of your system, the terminal output and even the files required to reproduce it. To illustrate it, take a look on an example of a good issue. </p>"},{"location":"about/license/","title":"Project License","text":""},{"location":"about/license/#apache-license-20","title":"Apache License 2.0","text":"<p>All code in this repository is licensed under the Apache License, Version 2.0.</p> <p>You may obtain a copy of the license at:</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"about/license/#mit-licensed-code-blocks","title":"MIT-Licensed Code Blocks","text":"<p>Certain files in this repository contain code that is licensed under the MIT License. These files are explicitly listed in <code>MIT_FILES.txt</code>.</p> <p>The MIT License is as follows:</p> <p>MIT License</p> <p>Copyright (c) [YEAR][AUTHOR]</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"about/related_packages/","title":"Related packages","text":"<p>TerraTorch is part of a larger open-source ecosystem for geospatial AI.</p>"},{"location":"about/related_packages/#frameworks","title":"Frameworks","text":"<p>TerraTorch uses Lightning as a training and inference engine.  Lightning handles many basic tasks like GPU allocation, logging and more. It uses PyTorch as the machine learning framework.</p> <p>The tasks and data modules in TerraTorch are based on TorchGeo.  Therefore, TorchGeo datasets are directly compatible with TerraTorch. </p>"},{"location":"about/related_packages/#data-tools","title":"Data Tools","text":"<p>Xarray is a well suited tool for handling multidimensional data and supports lazy loading and more. It's extension rioxarray supports reading or writing tif files and handles geospatial functionalities like the CRS.  </p> <p>For geospatial table data like polygons, GeoPandas is a well suited tool.</p>"},{"location":"about/related_packages/#model-libraries","title":"Model Libraries","text":"<p>TerraTorch includes meta registries for backbones, decoders and more.  Models from Pytorch Image Models (timm) and Segmentation Models PyTorch (SMP) are directly available in TerraTorch.</p>"},{"location":"guide/architecture/","title":"Architecture Overview","text":"<p>The main goal of TerraTorch is to provide a flexible way to fine-tune geospatial foundation models by making the backbone, decoder, and head modular. At the same time, we wish to keep the existing Lightning and TorchGeo functionality intact so it can be leveraged with datasets and pretrained models that are already included.</p> <p>We achieve this by making new tasks that accept model factory classes, containing a <code>build_model</code> method. This strategy in principle allows arbitrary models to be trained for these tasks, given they respect some reasonable minimal interface. Together with this, we provide the EncoderDecoderFactory, which enables users to plug together different encoders and decoders, with the aid of necks for intermediate operations.</p> <p>Additionally, we extend TorchGeo with generic datasets and datamodules which can be defined at runtime, rather than requiring classes to be defined beforehand.</p> <p>The glue that holds everything together is LightningCLI, allowing the model, datamodule and Lightning Trainer to be instantiated from a config file or from the CLI. We make extensive use of for training and inference.</p> <p>Initial reading for a full understanding of the platform includes:</p> <ul> <li>Familiarity with PyTorch Lightning</li> <li>Familiarity with TorchGeo</li> <li>Familiarity with LightningCLI</li> </ul> <p>The scheme below illustrates the general TerraTorch's workflow for a CLI job.   </p>"},{"location":"guide/architecture/#tasks","title":"Tasks","text":"<p>Tasks are the main coordinators for training and inference for specific tasks. They are <code>LightningModules</code> that contain a model and abstract away all the logic for training steps, metric computation and inference.</p> <p>One of the most important design decisions is to delegate the model construction to a model factory. This has a few advantages:</p> <ul> <li>Avoids code repetition among tasks - different tasks can use the same factory</li> <li>Prefers composition over inheritance</li> <li>Allows new models to be easily added by introducing new factories</li> </ul> <p>Models are expected to be <code>torch.nn.Module</code> and implement the Model interface, providing:</p> <ul> <li><code>freeze_encoder()</code></li> <li><code>freeze_decoder()</code></li> <li><code>forward()</code></li> </ul> <p>Additionally, the <code>forward()</code> method is expected to return an object of type ModelOutput, containing the main head's output, as well as any additional auxiliary outputs. The names of these auxiliary heads are matched with the names of the provided auxiliary losses. The tasks currently deployed in TerraTorch are described here. In general, these models are constructed using the <code>EncoderDecoderFactory</code>, which automatically returns a model with the expected functionality.</p>"},{"location":"guide/architecture/#models","title":"Models","text":"<p>Models constructed by the EncoderDecoderFactory have an internal structure explicitly divided into backbones, necks, decoders and heads. This structure is provided by the PixelWiseModel and ScalarOutputModel classes.</p> <p>However, as long as models implement the Model interface, and return ModelOutput in their forward method, they can take on any structure.</p> <p>See the models documentation for more details about the core models <code>ScalarOutputModel</code> and <code>PixelWiseModel</code>. For details about backbones (encoders) see the backbones documentation, the same for necks, decoders and heads.  </p>"},{"location":"guide/architecture/#model-factories","title":"Model Factories","text":"<p>A model factory is a class desgined to search a model in the register and properly instantiate it. TerraTorch has a few types of model factories for different situations, as models which require specific wrappers and processing.</p> <p>See the models factories documentation for a better explanation about it. </p>"},{"location":"guide/architecture/#encoderdecoderfactory","title":"EncoderDecoderFactory","text":"<p>However, as we have tried as much as possible to avoid the limitless replication of model factories dedicate to very specific models by concentrating efforts on the <code>EncoderDecoderFactory</code>, which intends to be more general-purpose. With that in mind, we dive deeper into it here.</p>"},{"location":"guide/architecture/#loss","title":"Loss","text":"<p>For convenience, we provide a loss handler that can be used to compute the full loss (from the main head and auxiliary heads as well).</p>"},{"location":"guide/architecture/#generic-datamodules","title":"Generic datamodules","text":"<p>Refer to the section on data.</p>"},{"location":"guide/architecture/#exporting-models","title":"Exporting models","text":"<p>Models are saved using the PyTorch format, which basically serializes the model weights using pickle and store them into a binary file. </p> <p>"},{"location":"guide/custom_datasets/","title":"Dataset comparison table","text":""},{"location":"guide/custom_datasets/#object-detection","title":"Object Detection","text":""},{"location":"guide/data/","title":"Data Processing","text":"<p>In our workflow, we leverage TorchGeo to implement datasets and data modules, ensuring robust and flexible data handling. For a deeper dive into working with datasets using TorchGeo, please refer to the TorchGeo tutorials on datasets.</p> <p>In most cases, it\u2019s best to create a custom TorchGeo dataset tailored to your specific data. Doing so gives you complete control over: - Data Loading: Customize how your data is read and organized. - Transforms: Decide which preprocessing or augmentation steps to apply. - Visualization: Define custom plotting methods (for example, when logging with TensorBoard).</p> <p>TorchGeo offers two primary classes to suit different data formats: - <code>NonGeoDataset</code>:   Use this if your dataset is already split into neatly tiled pieces ready for neural network consumption. Essentially, <code>NonGeoDataset</code> is a wrapper around a standard PyTorch dataset, making it straightforward to integrate into your pipeline. - <code>GeoDataset</code>:   Opt for this class if your data comes in the form of large GeoTiff files from which you need to sample during training. <code>GeoDataset</code> automatically aligns your input data with corresponding labels and supports a range of geo-aware sampling techniques.</p> <p>In addition to these specialized TorchGeo datasets, TerraTorch offers generic datasets and data modules designed to work with directory-based data structures, similar to those used in MMLab libraries. These generic tools simplify data loading when your data is organized in conventional file directories: - The Generic Pixel-wise Dataset is ideal for tasks where each pixel represents a sample (e.g., segmentation or dense prediction problems). - The Generic Scalar Label Dataset is best suited for classification tasks where each sample is associated with a single label. - The Generic Multimodal Dataset can handle multimodal data for classification, segmentation, and regression tasks.</p> <p>TerraTorch also provides corresponding generic data modules that bundle the dataset with training, validation, and testing splits, integrating seamlessly with PyTorch Lightning. This arrangement makes it easy to manage data loading, batching, and preprocessing with minimal configuration.</p> <p>While generic datasets offer a quick start for common data structures, many projects require more tailored solutions. Custom datasets and data modules give you complete control over the entire data handling process\u2014from fine-tuned data loading and specific transformations to enhanced visualization. By developing your own dataset and data module classes, you ensure that every step\u2014from data ingestion to final model input\u2014is optimized for your particular use case. TerraTorch\u2019s examples provide an excellent starting point to build these custom components and integrate them seamlessly into your training pipeline.</p> <p>For additional examples on fine-tuning a TerraTorch model using these components, please refer to the Prithvi EO Examples repository.</p>"},{"location":"guide/data/#data-curation","title":"Data curation","text":"<p>Generally speaking, all the datamodules work by collecting sets of files and concatenating them into batches with a size determined by the user. TerraTorch automatically checks the dimensionality of the files in order to guarantee that they are stackable, otherwise a stackability error will be raised. If you are sure that your data files are in the proper format and do not want to check for stackability, define <code>check_stackability: false</code> in the field <code>data</code> of your yaml file. If you are using the script interface, you just need to pass it as argument to your dataloader class. Alternatively, if you want to fix discrepancies related to dimensionality in your input files at the data loading stage, you can add a pad correction pipeline, as seen in the example <code>tests/resources/configs/manufactured-finetune_prithvi_eo_v2_300_pad_transform.yaml</code>. </p>"},{"location":"guide/data/#using-datasets-already-implemented-in-torchgeo","title":"Using Datasets already implemented in TorchGeo","text":"<p>Using existing TorchGeo DataModules is very easy! Just plug them in! For instance, to use the <code>EuroSATDataModule</code>, in your config file, set the data as: <pre><code>data:\n  class_path: torchgeo.datamodules.EuroSATDataModule\n  init_args:\n    batch_size: 32\n    num_workers: 8\n  dict_kwargs:\n    root: /dccstor/geofm-pre/EuroSat\n    download: True\n    bands:\n      - B02\n      - B03\n      - B04\n      - B08A\n      - B09\n      - B10\n</code></pre> Modifying each parameter as you see fit.</p> <p>You can also do this outside of config files! Simply instantiate the data module as normal and plug it in.</p> <p>Warning</p> <p>To define <code>transforms</code> to be passed to DataModules from TorchGeo from config files, you must use the following format: <pre><code>data:\nclass_path: terratorch.datamodules.TorchNonGeoDataModule\ninit_args:\n  cls: torchgeo.datamodules.EuroSATDataModule\n  transforms:\n    - class_path: albumentations.augmentations.geometric.resize.Resize\n      init_args:\n        height: 224\n        width: 224\n    - class_path: ToTensorV2\n</code></pre> Note the class_path is <code>TorchNonGeoDataModule</code> and the class to be used is passed through <code>cls</code> (there is also a <code>TorchGeoDataModule</code> for geo modules). This has to be done as the <code>transforms</code> argument is passed through <code>**kwargs</code> in TorchGeo, making it difficult to instantiate with LightningCLI. See more details below.</p>"},{"location":"guide/data/#generic-datasets-and-data-modules","title":"Generic datasets and data modules","text":"<p>For the <code>NonGeoDataset</code> case, we also provide \"generic\" datasets and datamodules. These can be used when you would like to load data from given directories, in a style similar to the MMLab libraries.</p>"},{"location":"guide/data/#custom-datasets-and-data-modules","title":"Custom datasets and data modules","text":"<p>Our custom datasets and data modules are crafted to handle specific data, offering enhanced control and flexibility throughout the workflow.  In case you want to use TerraTorch on your specific data, we invite you to develop your own dataset and data module classes by following the examples below. </p>"},{"location":"guide/data/#transforms","title":"Transforms","text":"<p>The transforms module provides a set of specialized image transformations designed to manipulate spatial, temporal, and multimodal data efficiently.  These transformations allow for greater flexibility when working with multi-temporal, multi-channel, and multi-modal datasets, ensuring that data can be formatted appropriately for different model architectures.</p>"},{"location":"guide/encoder_decoder_factory/","title":"EncoderDecoderFactory","text":"<p>Check the Glossary for more information about the terms used in this page.</p> <p>The EncoderDecoderFactory is the main class used to instantiate and compose models for general tasks. </p> <p>This factory leverages the <code>BACKBONE_REGISTRY</code>, <code>DECODER_REGISTRY</code> and <code>NECK_REGISTRY</code> to compose models formed as encoder + decoder, with some optional glue in between provided by the necks. As most current models work this way, this is a particularly important factory, allowing for great flexibility in combining encoders and decoders from different sources.</p> <p>The factory allows arguments to be passed to the encoder, decoder and head. Arguments with the prefix <code>backbone_</code> will be routed to the backbone constructor, with <code>decoder_</code> and <code>head_</code> working the same way. These are accepted dynamically and not checked. Any unused arguments will raise a <code>ValueError</code>.</p> <p>Both encoder and decoder may be passed as strings, in which case they will be looked in the respective registry, or as <code>nn.Modules</code>, in which case they will be used as is. In the second case, the factory assumes in good faith that the encoder or decoder which is passed conforms to the expected contract.</p> <p>Not all decoders will readily accept the raw output of the given encoder. This is where necks come in.  Necks are a sequence of operations which are applied to the output of the encoder before it is passed to the decoder. They must be instances of Neck, which is a subclass of <code>nn.Module</code>, meaning they can even define new trainable parameters.</p> <p>The EncoderDecoderFactory returns a PixelWiseModel or a ScalarOutputModel depending on the task.</p>"},{"location":"guide/encoder_decoder_factory/#encoders","title":"Encoders","text":"<p>To be a valid encoder, an object must be an <code>nn.Module</code> and contain an attribute <code>out_channels</code>, basically a list of the channel dimensions corresponding to the features it returns. The forward method of any encoder should return a list of <code>torch.Tensor</code>.</p> <pre><code>In [19]: backbone = BACKBONE_REGISTRY.build(\"prithvi_eo_v2_300\", pretrained=True)\n\nIn [20]: import numpy as np\n\nIn [21]: import torch\n\nIn [22]: input_image = torch.tensor(np.random.rand(1,6,224,224).astype(\"float32\"))\n\nIn [23]: output = backbone.forward(input_image)\n\nIn [24]: [item.shape for item in output]\n\nOut[24]: \n\n[torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024])]\n</code></pre>"},{"location":"guide/encoder_decoder_factory/#necks","title":"Necks","text":"<p>Necks are the connectors between encoder and decoder. They can perform operations such as selecting elements from the output of the encoder (SelectIndices), reshaping the outputs of ViTs so they are compatible with CNNs (ReshapeTokensToImage), amongst others. Necks are <code>nn.Modules</code>, with an additional method <code>process_channel_list</code> which informs the EncoderDecoderFactory about how it will alter the channel list provided by <code>encoder.out_channels</code>. See a better description about necks here.</p>"},{"location":"guide/encoder_decoder_factory/#decoders","title":"Decoders","text":"<p>To be a valid decoder, an object must be an <code>nn.Module</code> with an attribute <code>out_channels</code>, an <code>int</code> representing the channel dimension of the output. The first argument to its constructor will be a list of channel dimensions it should expect as input. It's forward method should accept a list of embeddings. To see a list of built-in decoders check the related documentation. </p>"},{"location":"guide/encoder_decoder_factory/#heads","title":"Heads","text":"<p>Most decoders require a final head to be added for a specific task (e.g. semantic segmentation vs pixel wise regression). Those registries producing decoders that dont require a head must expose the attribute <code>includes_head=True</code> so that a head is not added. Decoders passed as <code>nn.Modules</code> which do not require a head must expose the same attribute themselves. More about heads can be seen in its documentation. </p>"},{"location":"guide/encoder_decoder_factory/#decoder-compatibilities","title":"Decoder compatibilities","text":"<p>Not all encoders and decoders are compatible. Below we include some caveats. Some decoders expect pyramidal outputs, but some encoders do not produce such outputs (e.g. vanilla ViT models). In this case, the InterpolateToPyramidal, MaxpoolToPyramidal and LearnedInterpolateToPyramidal necks may be particularly useful.</p>"},{"location":"guide/encoder_decoder_factory/#smp-decoders","title":"SMP decoders","text":"<p>Not all decoders are guaranteed to work with all encoders without additional necks. Please check smp documentation to understand the embedding spatial dimensions expected by each decoder and add suitable necks to reshape the encoder output.</p> <p>In particular, smp seems to assume the first feature in the passed feature list has the same spatial resolution as the input, which may not always be true, and may break some decoders.</p> <p>In addition, for some decoders, the final 2 features have the same spatial resolution. Adding the AddBottleneckLayer neck will make this compatible.</p> <p>Some smp decoders require additional parameters, such as <code>decoder_channels</code>. These must be passed through the factory. In the case of <code>decoder_channels</code>, it would be passed as <code>decoder_decoder_channels</code> (the first <code>decoder_</code> routes the parameter to the decoder, where it is passed as <code>decoder_channels</code>).</p>"},{"location":"guide/encoder_decoder_factory/#mmsegmentation-decoders","title":"MMSegmentation decoders","text":"<p>MMSegmentation decoders are available through the BACKBONE_REGISTRY. </p> <p>Warning</p> <p>MMSegmentation currently requires <code>mmcv==2.1.0</code>. Pre-built wheels for this only exist for <code>torch==2.1.0</code>. In order to use mmseg without building from source, you must downgrade your <code>torch</code> to this version. Install mmseg with: <pre><code>pip install -U openmim\nmim install mmengine\nmim install mmcv==2.1.0\npip install regex ftfy mmsegmentation\n</code></pre></p> <p>We provide access to mmseg decoders as an external source of decoders, but are not directly responsible for the maintainence of that library.</p> <p>Some mmseg decoders require the parameter <code>in_index</code>, which performs the same function as the <code>SelectIndices</code> neck. For use for pixel wise regression, mmseg decoders should take <code>num_classes=1</code>.</p>"},{"location":"guide/faqs/","title":"Frequently Asked Questions","text":"<p>If you don't find your question in the FAQs or the user guide, feel free to open an issue in GitHub.</p> What is TerraTorch and what does it do? <p>TerraTorch is a fine-tuning framework for Geospatial Foundation Models (GFMs) built on PyTorch Lightning and TorchGeo. It provides flexible tools for training models on Earth observation tasks.</p> How do I install TerraTorch? <p>For stable releases, use <code>pip install terratorch</code>. For the latest development version, install with <code>pip install git+https://github.com/IBM/terratorch.git</code>.</p> How do I use the CLI for training and inference? <p>You need to define a data module and model in a yaml config file.   Pass this config files in commands like <code>terratorch fit --config config.yaml</code> for training, <code>terratorch test --config config.yaml --ckpt_path model.ckpt</code> for testing, and <code>terratorch predict --config config.yaml --ckpt_path model.ckpt --predict_output_dir output_dir</code> for inference. See <code>terratorch --help</code> for details.</p> How do I configure a YAML file for my experiment? <p>YAML files define the trainer, data, model, optimizer, and lr_scheduler sections.  The model section specifies the task type (like SemanticSegmentationTask) and model architecture parameters, while the data section configures the DataModule for loading your datasets. Please check this tutorial for details.</p> What data formats does TerraTorch support? <p>TerraTorch supports both single-modal and multi-modal geospatial data through generic datasets and datamodules. These generic classes are using <code>rioxarray</code> to load data. You can check if your data is working with: <pre><code>import rioxarray as rxr\nrxr.open_rasterio('&lt;testfile&gt;')\n</code></pre> You can implement a custom datamodule if the generic ones are not working.</p> What foundation models are supported? <p>TerraTorch provides access to multiple pre-trained geospatial foundation models including Prithvi, TerraMind, SatMAE, ScaleMAE, Clay, and models from TorchGeo like DOFA and SSL4EO. It also supports standard computer vision models from the timm library. You can list or filter all available models with: <pre><code>from terratorch import BACKBONE_REGISTRY    \n\nlist(BACKBONE_REGISTRY)\n\n# Filter\nprint([model_name for model_name in BACKBONE_REGISTRY if \"prithvi\" in model_name])\n</code></pre></p> What image size do I have to use with TerraTorch? <p>TerraTorch supports every image size, so you are mainly limited by your GPU memory. For training, we recommend using a chip size between 224 and 512. Ideally, your chips are a multiple of the patch size if you use ViT backbone (16 for most models) or 32 if you use a CNN. However, TerraTorch models automatically apply padding to make it work with any input size.  If the automatic padding does not work, pass <code>patch_size: 16</code> or similar to the model.</p> <p>During inference, you can use tiled inference that can process full satellite tiles if needed.</p> How does the model architecture work? <p>TerraTorch uses an encoder-decoder architecture where models are constructed by combining backbones, necks, decoders, and heads through model factories like <code>EncoderDecoderFactory</code>. Tasks coordinate training and inference while delegating model construction to these factories.</p> How do I perform inference on new data? <p>Use the <code>terratorch predict ...</code> command with your trained model checkpoint and specify the input data directory.  For programmatic inference, you can use the <code>LightningInferenceModel</code> class to load models and perform inference on directories or individual files. See the inference guide for details.</p> How can I on-board new datasets? <p>You can either use one of the generic datamodules and bring your dataset in a supported format, or create a custom datamodule and dataset class.</p> How do I add custom models or components? <p>You can extend TerraTorch by creating custom modules and registering them with the appropriate registries.  For example, register a custom backbone with <code>@TERRATORCH_BACKBONE_REGISTRY.register</code>. Place your code in a folder and adding the following to your yaml: <pre><code>custom_modules_path: &lt;your/folder&gt;\n</code></pre></p> <p>See this tutorial for details.</p> Can I customize a model or task? E.g., I want to use another loss or another decoder. <p>If you want to use a custom module which can be registered in TerraTorch (backbones, necks, and decoders), you can just add the <code>.register</code> function as a decorator and add your code to <code>custom_modules</code>. For edits in tasks like changing the loss or metric, you can copy the class from TerraTorch or inherit from one, change the relevant function, and it to <code>custom_modules</code> and select your new task in the yaml config.  </p> I get an error during inference that is related to some size mismatches? <p>This can happen when the padding is not working correctly.  An easy fix is to add tiled inference to your config or pipeline as described in Inference. </p> My training just stops after one epoch without a stack trace. What is wrong? <p>Lightning automatically stops if only NaN losses are produced. Please check the logs (e.g. in Tensorboard).</p> How can I fix NaN losses? <p>There can be multiple reasons for NaN losses. If you are training with mixed precision, try a run with full precision. Otherwise, your data or labels could include NaN values, or you are using a wrong <code>ignore_label</code>.</p> Why is my dataset length 0? <p>Please check all paths and image/label grep patterns in your config. Likely, TerraTorch does not find the data. If the paths are correct, you can either debug the generic dataset class constructor or open an issue to get support from the developers. </p>"},{"location":"guide/glossary/","title":"Glossary of terms used in this Documentation and in the Geospatial AI area","text":""},{"location":"guide/glossary/#encoder","title":"Encoder","text":"<p>The neural network used to map between the inputs and the intermdiary stage (usually referred as embedding or sometimes as latent space) of the forward step. The encoder is also frequently called backbone and, for  finetuning tasks, it is usually the part of the model which is not updated/trained. </p>"},{"location":"guide/glossary/#decoder","title":"Decoder","text":"<p>The neural network employed to map between the intermediary stage (embedding/latent space) and the target output. For finetuning tasks, the decoder is the most essential part, since it is trained to map the embedding produced by a previoulsy trained encoder to a new task. </p>"},{"location":"guide/glossary/#head","title":"Head","text":"<p>A network, usually very small when compared to the encoder and decoder, which is used as final step to adapt the decoder output to a specific task, for example, by applying a determined activation to it. </p>"},{"location":"guide/glossary/#neck","title":"Neck","text":"<p>Necks are operations placed between the encoder and the decoder stages aimed at adjusting possible discrepancies, as incompatible shapes, or applying some specific transform, as a normalization required for the task being executed. </p>"},{"location":"guide/glossary/#factory","title":"Factory","text":"<p>A Factory is a class which organizes the instantiation of a complete model, as a backbone-neck-decoder-head architecture. A class is intended to receive lists and dictionaries containing the required arguments used to build the model and returns a new instance already ready to be used. </p>"},{"location":"guide/inference/","title":"Inference","text":"<p>You can run inference with TerraTorch by providing the path to an input folder and output directory.  You can do this directly via the CLI with: <pre><code>terratorch predict -c config.yaml --ckpt_path path/to/model/checkpoint.ckpt --data.init_args.predict_data_root input/folder/ --predict_output_dir output/folder/ \n</code></pre></p> <p>This approach works only for supported data modules like the TerraTorch <code>GenericNonGeoSegmentationDataModule</code>.  E.g., the generic multimodal datamodule expects a dictionary for <code>predict_data_root</code>. Therefore, one can define the parameters in the config file as well:</p> <pre><code>data:\n  class_path: terratorch.datamodules.GenericNonGeoSegmentationDataModule\n  init_args:\n    ...\n    predict_data_root: path/to/input/files/\n</code></pre> <p>We provide two tutorials for basic inferrence and a simplified inference.</p>"},{"location":"guide/inference/#tiled-inference-via-cli","title":"Tiled inference via CLI","text":"<p>TerraTorch supports a tiled inference that splits up a tile into smaller chips. With this approach, you can run a model on very large tiles like a 10k x 10k pixel Sentinel-2 tile. </p> <p>Define the tiled inference parameters in the yaml config like the following: <pre><code>model:\n  class_path: terratorch.tasks.SemanticSegmentationTask\n  init_args:\n    ...\n    tiled_inference_parameters:\n      crop: 224\n      stride: 192\n</code></pre></p> <p>Next, you can run: <pre><code>terratorch predict -c config.yaml --ckpt_path path/to/model/checkpoint.ckpt --data.init_args.predict_data_root input/folder/ --predict_output_dir output/folder/\n</code></pre></p> <p>Warning</p> <p>The Lightning CLI load each input tile automatically to the GPU before passing it to <code>tiled_inference</code>.  This can result in CUDA out-of-memory errors for very large tiles like 100k x 100k pixels.  In this case, run the tiled inference via a python script and do not load the full tile into the GPU.</p> <p>By default, tiled inference adds some padding around the tile and removes the edge pixels of each chip before merging which are both defined by the parameter <code>delta</code>. The predictions of overlapping patches are using blend masks to edges in the predictions. This can be deactivated with <code>blend_overlaps=False</code>.  Here is a comparison between both with their respective predictions counts per pixel.  Pixels along the right side and bottom can have more predictions if the tile is not fully divisible by the define <code>crop</code> and <code>stride</code> size. TerraTorch maximises the overlap to fully leverage the compute while generating at least one prediction per pixel.</p> <ul> <li> <p>Without blending, \"patchy\" predictions with visible lines along the chip edges can appear.   </p> </li> <li> <p>By default, a cosine-based blend mask is applied to each chip which smooths the generations. </p> </li> </ul>"},{"location":"guide/inference/#tiled-inference-via-python","title":"Tiled inference via Python","text":"<p>You can use TerraTorch to run tiled inference in a python script like the following:</p> <pre><code>import torch\nimport rioxarray as rxr\nfrom terratorch.tasks import SemanticSegmentationTask\nfrom terratorch.tasks.tiled_inference import tiled_inference\nfrom terratorch.cli_tools import LightningInferenceModel\n\n# Init an TerraTorch task, e.g. for semantic segmentation\nmodel = SemanticSegmentationTask.load_from_checkpoint(\n    ckpt_path,  # Pass the checkpoint path\n    model_factory=\"EncoderDecoderFactory\",\n    model_args=model_args,  # Pass your model args\n)\n\n# Alternatively build the model from a config file\nmodel = LightningInferenceModel.from_config(\n    config_file, \n    ckpt_path, \n    # additional params, e.g. predict_dataset_bands\n)\n\n# Load your data\ninput = rxr.open_rasterio(\"input.tif\")\n\n# Apply your standardization values to the input tile\ninput = (input - means[:, None, None]) / stds[:, None, None]\n# Create input tensor with shape [B, C, H, W] on CPU\ninput = torch.tensor(input, dtype=torch.float, device='cpu').unsqueeze(0)\n\n# Inference wrapper for TerraTorch task model\ndef model_forward(x,  **kwargs):\n    # Retuns a torch Tensor\n    return model(x, **kwargs).output\n\n# Run tiled inference (data is loaded automatically to GPU)\npred = tiled_inference(\n    model_forward, \n    input, \n    crop=256, \n    stride=240, \n    batch_size=16, \n    verbose=True\n)\n\n# Remove batch dim and compute segmentation map\npred = pred.squeeze(0).argmax(dim=0)\n</code></pre> <p>Tip</p> <p>You can easily modify the script by adjusting the parameters or using a custom PyTorch model instead of <code>model_forward</code> for <code>tiled_inference</code>. It is just important, that the output of the passed forward function returns a torch tensor.</p>"},{"location":"guide/inference/#function-reference","title":"Function reference","text":""},{"location":"guide/inference/#terratorch.tasks.tiled_inference.tiled_inference","title":"<code>terratorch.tasks.tiled_inference.tiled_inference(model_forward, input_batch, out_channels, inference_parameters, **kwargs)</code>","text":"<p>Divide an image into (potentially) overlapping tiles and perform inference on them. Additionally, rebatch for increased GPU utilization.</p> <p>Parameters:</p> Name Type Description Default <code>model_forward</code> <code>Callable</code> <p>Callable that return the output of the model.</p> required <code>input_batch</code> <code>Tensor</code> <p>Input batch to be processed</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels</p> required <code>inference_parameters</code> <code>TiledInferenceParameters</code> <p>Parameters to be used for the process.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The result of the inference</p>"},{"location":"guide/models/","title":"Models","text":"<p>To interface with TerraTorch tasks correctly, models must inherit from the Model parent class and have a forward method which returns an object ModelOutput:</p>"},{"location":"guide/models/#model-factories","title":"Model Factories","text":"<p>In order to be used by tasks, models must have a Model Factory which builds them. Factories must conform to the [ModelFactory][terratorch.models.model.ModelFactory] parent class. </p> <p>You most likely do not need to implement your own model factory, unless you are wrapping another library which generates full models.</p> <p>For most cases, the EncoderDecoderFactory can be used to combine a backbone with a decoder.</p> <p>To add new backbones or decoders, to be used with the EncoderDecoderFactory they should be registered. </p> <p>To add a new model factory, it should be registered in the <code>MODEL_FACTORY_REGISTRY</code>.</p>"},{"location":"guide/models/#adding-a-new-model","title":"Adding a new model","text":"<p>To add a new backbone, simply create a class and annotate it (or a constructor function that instantiates it) with <code>@TERRATORCH_BACKBONE_FACTORY.register</code>. </p> <p>The model will be registered with the same name as the function. To create many model variants from the same class, the recommended approach is to annotate a constructor function from each with a fully descriptive name.</p> <p>Your backbone is required to have an attribute <code>self.out_channels</code> that defines a list of the model outputs with the embedding dimension. The output of the <code>forward</code> function should return a list of tensors (ideally the outputs from all layers of your model). See Encoders in EncoderDecoderFactory for an example.</p> <pre><code>from terratorch.registry import TERRATORCH_BACKBONE_REGISTRY, BACKBONE_REGISTRY\n\nfrom torch import nn\n\n# make sure this is in the import path for terratorch\n@TERRATORCH_BACKBONE_REGISTRY.register\nclass BasicBackbone(nn.Module):\n    def __init__(self, out_channels=64):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.layer = nn.Linear(224*224, out_channels)\n        self.out_channels = [out_channels]\n\n    def forward(self, x):\n        return [self.layer(self.flatten(x))]\n\n# you can build directly with the TERRATORCH_BACKBONE_REGISTRY\n# but typically this will be accessed from the BACKBONE_REGISTRY\n&gt;&gt;&gt; BACKBONE_REGISTRY.build(\"BasicBackbone\", out_channels=64)\nBasicBackbone(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (layer): Linear(in_features=50176, out_features=64, bias=True)\n)\n\n@TERRATORCH_BACKBONE_REGISTRY.register\ndef basic_backbone_128():\n    return BasicBackbone(out_channels=128)\n\n&gt;&gt;&gt; BACKBONE_REGISTRY.build(\"basic_backbone_128\")\nBasicBackbone(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (layer): Linear(in_features=50176, out_features=128, bias=True)\n)\n</code></pre> <p>Adding a new decoder can be done in the same way with the <code>TERRATORCH_DECODER_REGISTRY</code>.</p> <p>Info</p> <p>All decoders will be passed the <code>channel_list</code> as the first argument for initialization (<code>self.out_channels</code> of the backbone).</p>"},{"location":"guide/models/#adding-new-model-types","title":"Adding new model types","text":"<p>Adding new model types is as simple as creating a new factory that produces models. See for instance the example below for a potential <code>SMPModelFactory</code></p> <pre><code>from terratorch.models.model import register_factory\n\n@register_factory\nclass SMPModelFactory(ModelFactory):\n    def build_model(\n        self,\n        task: str,\n        backbone: str | nn.Module,\n        decoder: str | nn.Module,\n        in_channels: int,\n        **kwargs,\n    ) -&gt; Model:\n\n        model = smp.Unet(encoder_name=\"resnet34\", encoder_weights=None, in_channels=in_channels, classes=1)\n        return SMPModelWrapper(model)\n\n\nclass SMPModelWrapper(Model, nn.Module):\n    def __init__(self, smp_model) -&gt; None:\n        super().__init__()\n        self.smp_model = smp_model\n\n    def forward(self, *args, **kwargs):\n        return ModelOutput(self.smp_model(*args, **kwargs).squeeze(1))\n\n    def freeze_encoder(self):\n        pass\n\n    def freeze_decoder(self):\n        pass\n</code></pre>"},{"location":"guide/models/#custom-modules-with-cli","title":"Custom modules with CLI","text":"<p>Custom modules must be in the import path in order to be registered in the appropriate registries. </p> <p>In order to do this without modifying the code when using the CLI, you may place your modules under a <code>custom_modules</code> directory. This must be in the directory from which you execute TerraTorch.</p> <p>You can also define a <code>custom_modules_path</code> to a directory in your <code>yaml</code> config or provide it as a argument <code>--custom_modules_path your/directory/</code>.</p>"},{"location":"guide/quick_start/","title":"Quick start","text":""},{"location":"guide/quick_start/#setup","title":"Setup","text":"<p>Let's start by setting up an environment and installing TerraTorch.</p> <p>Tip</p> <p>You can quickly setup a new virtual environment by running: <pre><code>python -m venv venv  # using python 3.10 or newer\npip install --upgrade pip\npip install terratorch\n</code></pre></p>"},{"location":"guide/quick_start/#configuring-the-environment","title":"Configuring the environment","text":"<p>TerraTorch is currently tested for Python in <code>3.10 &lt;= Python &lt;= 3.13</code>. </p> <p>GDAL is required  to read and write TIFF images. It is usually easy to install in Unix/Linux systems, but if it is not your case  we recommend using a conda environment and installing it with <code>conda install -c conda-forge gdal</code>. </p>"},{"location":"guide/quick_start/#installing-terratorch","title":"Installing TerraTorch","text":"<p>For a stable point-release, use <code>pip install terratorch</code>. If you prefer to get the most recent version of the main branch, install the library with <code>pip install git+https://github.com/IBM/terratorch.git</code>.</p> <p>To install as a developer (e.g., to extend the library), fork the repo and clone your repo with <code>git clone https://github.com/&lt;your_username&gt;/terratorch.git</code>. Then run <code>pip install -e .</code>. We welcome contributions from the community and provide some guidelines for you.</p>"},{"location":"guide/quick_start/#creating-backbones","title":"Creating Backbones","text":"<p>You can interact with the library at several levels of abstraction. Each deeper level of abstraction trades off some amount of flexibility for ease of use and configuration. In the simplest case, we might only want access a backbone and code all the rest ourselves. In this case, we can simply use the library as a backbone factory:</p> Instantiating a Prithvi backbone<pre><code>from terratorch import BACKBONE_REGISTRY\n\n# Find available Prithvi models\nprint([model_name for model_name in BACKBONE_REGISTRY if \"terratorch_prithvi\" in model_name])\n&gt;&gt;&gt; ['terratorch_prithvi_eo_tiny', 'terratorch_prithvi_eo_v1_100', 'terratorch_prithvi_eo_v2_300', 'terratorch_prithvi_eo_v2_600', 'terratorch_prithvi_eo_v2_300_tl', 'terratorch_prithvi_eo_v2_600_tl']\n\n# Show all models with list(BACKBONE_REGISTRY)\n\n# check a model is in the registry\n\"terratorch_prithvi_eo_v2_300\" in BACKBONE_REGISTRY\n&gt;&gt;&gt; True\n\n# without the prefix, all internal registries will be searched until the first match is found\n\"prithvi_eo_v1_100\" in BACKBONE_REGISTRY\n&gt;&gt;&gt; True\n\n# instantiate your desired model\n# the backbone registry prefix (e.g., `terratorch` or `timm`) is optional\n# in this case, the underlying registry is terratorch.\nmodel = BACKBONE_REGISTRY.build(\"prithvi_eo_v1_100\", pretrained=True)\n\n# instantiate your model with more options, for instance, passing input bands or weights from your own file\nmodel = BACKBONE_REGISTRY.build(\n    \"prithvi_eo_v2_300\", bands=[\"RED\", \"GREEN\", \"BLUE\"], num_frames=1, pretrained=True, ckpt_path='path/to/model.pt'\n)\n\n# Rest of your PyTorch / PyTorchLightning code\n...\n</code></pre> <p>Internally, TerraTorch maintains several registries for components such as backbones or decoders. The top-level <code>BACKBONE_REGISTRY</code> collects all of them.</p> <p>The name passed to <code>build</code> is used to find the appropriate model constructor, which will be the first model from the first registry found with that name.</p> <p>To explicitly determine the registry that will build the model, you may prepend a prefix such as <code>timm_</code> to the model name. In this case, the <code>timm</code> model registry will be exclusively searched for the model.</p>"},{"location":"guide/quick_start/#creating-a-full-model","title":"Creating a full model","text":"<p>We also provide a model factory for a task-specific model that combines a backbone with a decoder:</p> Building a full model, with task-specific decoder<pre><code>from terratorch.models import EncoderDecoderFactory\n\nmodel_factory = EncoderDecoderFactory()\n\n# Let's build a segmentation model\n# Parameters prefixed with backbone_ get passed to the backbone\n# Parameters prefixed with decoder_ get passed to the decoder\n# Parameters prefixed with head_ get passed to the head\n\nmodel = model_factory.build_model(\n    task=\"segmentation\",\n    backbone=\"prithvi_eo_v2_300\",\n    backbone_pretrained=True,\n    backbone_bands=[\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"NIR_NARROW\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    ],\n    necks=[\n        {\"name\": \"SelectIndices\", \"indices\": [5, 11, 17, 23]},\n        {\"name\": \"ReshapeTokensToImage\"},\n        {\"name\": \"LearnedInterpolateToPyramidal\"}\n    ],\n    decoder=\"UNetDecoder\",\n    decoder_channels=[512, 256, 128, 64],\n    head_dropout=0.1,\n    num_classes=4,\n)\n\n# Rest of your PyTorch / PyTorchLightning code\n...\n</code></pre> <p>You might wonder what <code>necks</code> are. Different model architectures like CNNs or ViTs return outputs in different formats while different decoders expect other input shapes.  Therefore, we use necks to reshape the backbone output into the correct format for the decoder input. In this case, we select intermediate model outputs for the UNet, reshape the 1D tokens of Prithvi into a 2D grid, and upscale the intermediate outputs as the UNet decoder expects hierarchical inputs. You can simply use a backbone-neck-decoder combination from one of the provided examples or check the necks page in the user guide for more details.</p>"},{"location":"guide/quick_start/#training-with-lightning-tasks","title":"Training with Lightning Tasks","text":"<p>At the highest level of abstraction, you can directly obtain a LightningModule ready to be trained. We simply need to pass the model factory and the model arguments to the task. Passed to a Lightning Trainer, the task executes the training, validation, and testing steps. </p> Building a full pixel-wise regression task<pre><code>from terratorch.tasks import PixelwiseRegressionTask\n\nmodel_args = dict(\n  backbone=\"prithvi_eo_v2_300\",\n  backbone_pretrained=True,\n  backbone_num_frames=1,\n  backbone_bands=[\n      \"BLUE\",\n      \"GREEN\",\n      \"RED\",\n      \"NIR_NARROW\",\n      \"SWIR_1\",\n      \"SWIR_2\",\n  ],\n    necks=[\n        {\"name\": \"SelectIndices\", \"indices\": [5, 11, 17, 23]},\n        {\"name\": \"ReshapeTokensToImage\"},\n        {\"name\": \"LearnedInterpolateToPyramidal\"}\n    ],\n    decoder=\"UNetDecoder\",\n    decoder_channels=[512, 256, 128, 64],\n    head_dropout=0.1,\n)\n\ntask = PixelwiseRegressionTask(\n    model_factory=\"EncoderDecoderFactory\",\n    model_args=model_args,\n    loss=\"rmse\",\n    lr=1e-4,\n    ignore_index=-1,\n    optimizer=\"AdamW\",\n    optimizer_hparams={\"weight_decay\": 0.05},\n)\n\n# Pass this LightningModule to a Lightning Trainer, together with some LightningDataModule\n...\n</code></pre> <p>Alternatively, all the process can be summarized in configuration files written in YAML format, as seen below.</p> Configuration file for a semantic segmentation task<pre><code># lightning.pytorch==2.1.1\nseed_everything: 0\ntrainer:\n  accelerator: auto  # Lightning automatically selects all available GPUs\n  strategy: auto\n  devices: auto\n  num_nodes: 1\n  precision: 16-mixed  # Using half precision speeds up the training\n  logger: True  # Lightning uses a Tensorboard logger by default\n  callbacks:  # Callbacks are additional steps executed by lightning. \n    - class_path: RichProgressBar\n    - class_path: LearningRateMonitor\n      init_args:\n        logging_interval: epoch\n  max_epochs: 100\n  log_every_n_steps: 5\n  enable_checkpointing: true  # Defaults to true. TerraTorch automatically adds a Checkpoint callback to save the model\n  default_root_dir: output/prithvi/experiment  # Define your output folder\n\ndata:\n  # Define your data module. You can also use one of TerraTorch's generic data modules \n  class_path: terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule\n  init_args:\n    batch_size: 16\n    num_workers: 8\n  dict_kwargs:\n    data_root: &lt;path_to_data_root&gt;\n    bands:\n      - 1\n      - 2\n      - 3\n      - 8\n      - 11\n      - 12\n\nmodel:\n  class_path: terratorch.tasks.SemanticSegmentationTask\n  init_args:\n    model_factory: EncoderDecoderFactory\n    model_args:\n      backbone: prithvi_eo_v2_300\n      backbone_img_size: 512\n      backbone_pretrained: True\n      backbone_bands:\n        - BLUE\n        - GREEN\n        - RED\n        - NIR_NARROW\n        - SWIR_1\n        - SWIR_2\n      necks:\n        - name: SelectIndices\n          indices: [5, 11, 17, 23]\n        - name: ReshapeTokensToImage\n        - name: LearnedInterpolateToPyramidal\n      decoder: UNetDecoder\n      decoder_channels: [512, 256, 128, 64]\n      head_channel_list: [256]  # Pass a list for an MLP head\n      head_dropout: 0.1\n      num_classes: 2\n    loss: dice\n    ignore_index: -1\n    freeze_backbone: false  # Full fine-tuning\n\noptimizer:\n  class_path: torch.optim.AdamW\n  init_args:\n    lr: 1.e-4\nlr_scheduler:\n  class_path: ReduceLROnPlateau\n  init_args:\n    monitor: val/loss\n</code></pre> <p>To run this training task using the YAML, simply execute: <pre><code>terratorch fit --config &lt;path_to_config_file&gt;\n</code></pre></p> <p>To test your model on the test set, execute: <pre><code>terratorch test --config  &lt;path_to_config_file&gt; --ckpt_path &lt;path_to_checkpoint_file&gt;\n</code></pre></p> <p>For inference, execute: <pre><code>terratorch predict -c &lt;path_to_config_file&gt; --ckpt_path &lt;path_to_checkpoint&gt; --predict_output_dir &lt;path_to_output_dir&gt; --data.init_args.predict_data_root &lt;path_to_input_dir&gt; --data.init_args.predict_dataset_bands &lt;all bands in the predicted dataset, e.g. [BLUE,GREEN,RED,NIR_NARROW,SWIR_1,SWIR_2,0]&gt;\n</code></pre></p> <p>Experimental feature: Users that want to optimize hyperparameters or repeat best experiment might be interested in the plugin <code>terratorch-iterate</code>. For instance, to run TerraTorch Iterate to optimize hyperparameters, one can run:  <pre><code>terratorch iterate --hpo --config &lt;path_to_config_file&gt; \n</code></pre> You can install the package with <code>pip install terratorch-iterate</code> and check the usage description for more information.</p>"},{"location":"guide/registry/","title":"Registries","text":"<p>TerraTorch keeps a set of registries which map strings to instances of those strings. They can be imported from <code>terratorch.registry</code>.</p> <p>Info</p> <p>If you are using tasks with existing models, you may never have to interact with registries directly. The model factory will handle interactions with registries.</p> <p>Registries behave like python sets, exposing the usual <code>contains</code> and <code>iter</code> operations. This means you can easily operate on them in a pythonic way, such as  <code>\"model\" in registry</code> or <code>list(registry)</code>.</p> <p>To create the desired instance, registries expose a <code>build</code> method, which accepts the name and the arguments to be passed to the constructor.</p> Using registries<pre><code>from terratorch import BACKBONE_REGISTRY\n\n# find available prithvi models\nprint([model_name for model_name in BACKBONE_REGISTRY if \"terratorch_prithvi\" in model_name])\n&gt;&gt;&gt; ['terratorch_prithvi_eo_tiny', 'terratorch_prithvi_eo_v1_100', 'terratorch_prithvi_eo_v2_300', 'terratorch_prithvi_eo_v2_600', 'terratorch_prithvi_eo_v2_300_tl', 'terratorch_prithvi_eo_v2_600_tl']\n\n# show all models with list(BACKBONE_REGISTRY)\n\n# check a model is in the registry\n\"terratorch_prithvi_eo_v2_300\" in BACKBONE_REGISTRY\n&gt;&gt;&gt; True\n\n# without the prefix, all internal registries will be searched until the first match is found\n\"prithvi_eo_v1_100\" in BACKBONE_REGISTRY\n&gt;&gt;&gt; True\n\n# instantiate your desired model\n# the backbone registry prefix (e.g. `terratorch` or `timm`) is optional\n# in this case, the underlying registry is terratorch.\nmodel = BACKBONE_REGISTRY.build(\"prithvi_eo_v1_100\", pretrained=True)\n\n# instantiate your model with more options, for instance, passing weights from your own file\nmodel = BACKBONE_REGISTRY.build(\n    \"prithvi_eo_v2_300\", num_frames=1, ckpt_path='path/to/model.pt'\n)\n# Rest of your PyTorch / PyTorchLightning code\n</code></pre>"},{"location":"guide/registry/#multisourceregistries","title":"MultiSourceRegistries","text":"<p><code>BACKBONE_REGISTRY</code>, <code>DECODER_REGISTRY</code>, and <code>FULL_MODEL_REGISTRY</code> are special registries which dynamically aggregate multiple registries. They behave as if they were a single large registry by searching over multiple registries.</p> <p>For instance, the <code>DECODER_REGISTRY</code> holds the <code>TERRATORCH_DECODER_REGISTRY</code>, which is responsible for decoders implemented in terratorch, as well as the <code>SMP_DECODER_REGISTRY</code> and the <code>MMSEG_DECODER_REGISTRY</code> (if mmseg is installed).</p> <p>To make sure you access the object from a particular registry, you may prepend your string with the prefix from that registry.</p> <pre><code>from terratorch import DECODER_REGISTRY\n\n# decoder registries always take at least one extra argument, the channel list with the channel dimension of each embedding passed to it\nDECODER_REGISTRY.build(\"FCNDecoder\", [32, 64, 128])\n\nDECODER_REGISTRY.build(\"terratorch_FCNDecoder\", [32, 64, 128])\n\n# Find all prefixes\nDECODER_REGISTRY.keys()\n&gt;&gt;&gt; odict_keys(['terratorch', 'smp', 'mmseg'])\n</code></pre> <p>If a prefix is not added, the <code>MultiSourceRegistry</code> will search each registry in the order it was added (starting with the <code>TERRATORCH_</code> registry) until it finds the first match.</p> <p>For all of these registries, only <code>TERRATORCH_X_REGISTRY</code> is mutable. To register backbones, decoders, or standalone models to TerraTorch, you should decorate the constructor function (or the model class itself) with <code>@TERRATORCH_DECODER_REGISTRY.register</code>, <code>@TERRATORCH_BACKBONE_REGISTRY.register</code>, or <code>@TERRATORCH_FULL_MODEL_REGISTRY.register</code>.</p> <p>To add a new registry to these top level registries, you should use the <code>.register</code> method, taking the register and the prefix that will be used for it.</p>"},{"location":"guide/registry/#other-registries","title":"Other Registries","text":"<p>Additionally, terratorch has the <code>NECK_REGISTRY</code>, where all necks must be registered, and the <code>MODEL_FACTORY_REGISTRY</code>, where all model factories must be registered.</p>"},{"location":"package/backbones/","title":"Backbones","text":""},{"location":"package/backbones/#built-in-backbones","title":"Built-in Backbones","text":""},{"location":"package/backbones/#terratorch.models.backbones.terramind.model.terramind_vit.TerraMindViT","title":"<code>terratorch.models.backbones.terramind.model.terramind_vit.TerraMindViT</code>","text":"<p>               Bases: <code>Module</code></p> <p>Modified TerraMind model, adapted to behave as a raw data-only ViT.</p> <p>Parameters:</p> Name Type Description Default <code>img_size</code> <code>int</code> <p>Input image size.</p> <code>224</code> <code>modalities</code> <code>(list, dict)</code> <p>List of modality keys and dicts, or dict with modality keys and values being ints (num_channels of modality) or nn.Module (patch embedding layer).</p> <code>None</code> <code>merge_method</code> <code>str</code> <p>Specify how the output is merged for further processing. One of 'mean', 'max', 'concat', 'dict', or None. 'mean', 'max', and 'concat' are dropping all sequence modality tokens, split all image modality tokens and reduce the by applying the appropriate method. 'dict' splits all tokens into a dictionary {'modality': torch.Tensor}. Defaults to 'mean'.</p> <code>'mean'</code> <code>patch_size</code> <code>int</code> <p>Patch size.</p> <code>16</code> <code>in_chans</code> <code>int</code> <p>Number of input image channels.</p> <code>3</code> <code>dim</code> <code>int</code> <p>Patch embedding dimension.</p> <code>768</code> <code>encoder_depth</code> <code>int</code> <p>Depth of ViT / number of encoder blocks.</p> <code>12</code> <code>num_heads</code> <code>int</code> <p>Number of attention heads in each ViT block.</p> <code>12</code> <code>mlp_ratio</code> <code>float</code> <p>Ratio of mlp hidden dim to embedding dim.</p> <code>4.0</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to query, key, value.</p> <code>True</code> <code>proj_bias</code> <code>bool</code> <p>If True, adds a bias to the attention out proj layer.</p> <code>True</code> <code>mlp_bias</code> <code>bool</code> <p>If True, adds a learnable bias for the feedforward.</p> <code>True</code> <code>drop_path_rate</code> <code>float</code> <p>Stochastic depth rate.</p> <code>0.0</code> <code>drop_rate</code> <code>float</code> <p>Dropout rate.</p> <code>0.0</code> <code>attn_drop_rate</code> <code>float</code> <p>Attention dropout rate.</p> <code>0.0</code> <code>modality_drop_rate</code> <code>float</code> <p>Drop modality inputs during training.</p> <code>0.0</code> <code>act_layer</code> <code>Module</code> <p>Activation layer.</p> <code>GELU</code> <code>norm_layer</code> <code>Module</code> <p>Normalization layer.</p> <code>partial(LayerNorm, eps=1e-06)</code> <code>gated_mlp</code> <code>bool</code> <p>If True, makes the feedforward gated (e.g., for SwiGLU)</p> <code>False</code> <code>qk_norm</code> <code>bool</code> <p>If True, normalizes the query and keys (as in ViT-22B)</p> <code>False</code> <code>use_act_checkpoint</code> <code>bool</code> <p>If True, use activation checkpointing.</p> required <code>encoder_norm</code> <code>bool</code> <p>If True, adds a norm layer after the last encoder block.</p> <code>True</code> Source code in <code>terratorch/models/backbones/terramind/model/terramind_vit.py</code> <pre><code>class TerraMindViT(nn.Module):\n    \"\"\"Modified TerraMind model, adapted to behave as a raw data-only ViT.\n\n    Args:\n        img_size (int): Input image size.\n        modalities (list, dict, optional): List of modality keys and dicts, or dict with modality keys and values being\n            ints (num_channels of modality) or nn.Module (patch embedding layer).\n        merge_method (str, optional): Specify how the output is merged for further processing. One of 'mean', 'max',\n            'concat', 'dict', or None. 'mean', 'max', and 'concat' are dropping all sequence modality tokens, split all\n            image modality tokens and reduce the by applying the appropriate method. 'dict' splits all tokens into a\n            dictionary {'modality': torch.Tensor}. Defaults to 'mean'.\n        patch_size (int): Patch size.\n        in_chans (int): Number of input image channels.\n        dim (int): Patch embedding dimension.\n        encoder_depth (int): Depth of ViT / number of encoder blocks.\n        num_heads (int): Number of attention heads in each ViT block.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool): If True, add a learnable bias to query, key, value.\n        proj_bias (bool): If True, adds a bias to the attention out proj layer.\n        mlp_bias (bool): If True, adds a learnable bias for the feedforward.\n        drop_path_rate (float): Stochastic depth rate.\n        drop_rate (float): Dropout rate.\n        attn_drop_rate (float): Attention dropout rate.\n        modality_drop_rate (float): Drop modality inputs during training.\n        act_layer (nn.Module): Activation layer.\n        norm_layer (nn.Module): Normalization layer.\n        gated_mlp (bool): If True, makes the feedforward gated (e.g., for SwiGLU)\n        qk_norm (bool): If True, normalizes the query and keys (as in ViT-22B)\n        use_act_checkpoint (bool): If True, use activation checkpointing.\n        encoder_norm (bool): If True, adds a norm layer after the last encoder block.\n    \"\"\"\n    def __init__(\n        self,\n        img_size: int = 224,\n        modalities: list | dict[str, int | nn.Module] | None = None,\n        merge_method: str | None = 'mean',\n        patch_size: int = 16,\n        in_chans: int = 3,\n        dim: int = 768,\n        encoder_depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = True,\n        proj_bias: bool = True,\n        mlp_bias: bool = True,\n        drop_path_rate: float = 0.0,\n        drop_rate: float = 0.0,\n        attn_drop_rate: float = 0.0,\n        modality_drop_rate: float = 0.0,\n        act_layer: torch.Tensor = nn.GELU,\n        norm_layer: partial | nn.Module = partial(LayerNorm, eps=1e-6),\n        gated_mlp: bool = False,  # Make the feedforward gated for e.g. SwiGLU\n        qk_norm: bool = False,\n        encoder_norm: bool = True,\n    ):\n        super().__init__()\n\n        if modalities is None or len(modalities) == 0:\n            # Init new image modality\n            modalities = [{'image': in_chans}]\n        elif isinstance(modalities, dict):\n            modalities = [modalities]\n        elif not isinstance(modalities, list):\n            raise ValueError(f'Modalities must be None, a list of modality keys or a dict with ints/embedding layers.')\n\n        # Build embedding layers for all defined modalities\n        mod_embeddings, mod_name_mapping = build_modality_embeddings(modalities, img_size=img_size, dim=dim,\n                                                                     patch_size=patch_size)\n        self.encoder_embeddings = nn.ModuleDict(mod_embeddings)\n        self.mod_name_mapping = mod_name_mapping\n        self.modalities = list(mod_name_mapping.keys())  # Further code expects list\n        self.output_mod_name_mapping = {v: k for k, v in mod_name_mapping.items()}\n\n        self.img_size = img_size\n        self.merge_method = merge_method\n        self.image_modalities = [key for key, value in self.encoder_embeddings.items()\n                                 if isinstance(value, ImageEncoderEmbedding)]\n        self.modality_drop_rate = modality_drop_rate\n        assert 0 &lt;= self.modality_drop_rate &lt;= 1, \"modality_drop_rate must be in [0, 1]\"\n        # New learned parameter for handling missing modalities\n        if self.merge_method == 'concat':\n            self.missing_mod_token = nn.Parameter(torch.Tensor(dim))\n\n        # stochastic depth decay rule\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, encoder_depth)]\n\n        self.encoder = nn.ModuleList([\n            Block(dim=dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, proj_bias=proj_bias,\n                  mlp_bias=mlp_bias, drop_path=dpr[i], drop=drop_rate, attn_drop=attn_drop_rate, act_layer=act_layer,\n                  norm_layer=norm_layer, gated_mlp=gated_mlp, qk_norm=qk_norm)\n            for i in range(encoder_depth)\n        ])\n\n        # Needed for terratorch decoders\n        if merge_method == 'concat':\n            self.out_channels = [dim * len(self.image_modalities) for i in range(encoder_depth)]\n        else:\n            self.out_channels = [dim for i in range(encoder_depth)]\n\n        self.encoder_norm = norm_layer(dim) if encoder_norm else nn.Identity()\n\n        # Weight init\n        self.init_weights()\n\n    def init_weights(self):\n        \"\"\"Weight initialization following MAE's initialization scheme\"\"\"\n\n        for name, m in self.named_modules():\n            # Skipping tokenizers to avoid reinitializing them\n            if \"tokenizer\" in name:\n                continue\n            # Linear\n            elif isinstance(m, nn.Linear):\n                if 'qkv' in name:\n                    # treat the weights of Q, K, V separately\n                    val = math.sqrt(6. / float(m.weight.shape[0] // 3 + m.weight.shape[1]))\n                    nn.init.uniform_(m.weight, -val, val)\n                elif 'kv' in name:\n                    # treat the weights of K, V separately\n                    val = math.sqrt(6. / float(m.weight.shape[0] // 2 + m.weight.shape[1]))\n                    nn.init.uniform_(m.weight, -val, val)\n                else:\n                    nn.init.xavier_uniform_(m.weight)\n                if isinstance(m, nn.Linear) and m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            # LayerNorm\n            elif isinstance(m, nn.LayerNorm) or isinstance(m, LayerNorm):\n                nn.init.constant_(m.weight, 1.0)\n                nn.init.constant_(m.bias, 0)\n\n            # Embedding\n            elif isinstance(m, nn.Embedding):\n                nn.init.normal_(m.weight, std=0.02)\n            # Conv2d\n            elif isinstance(m, nn.Conv2d):\n                if '.proj' in name:\n                    # From MAE, initialize projection like nn.Linear (instead of nn.Conv2d)\n                    w = m.weight.data\n                    nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        no_wd_set = set()\n\n        for mod, emb_module in self.encoder_embeddings.items():\n            if hasattr(emb_module, 'no_weight_decay'):\n                to_skip = emb_module.no_weight_decay()\n                to_skip = set([f'encoder_embeddings.{mod}.{name}' for name in to_skip])\n                no_wd_set = no_wd_set | to_skip\n\n        return no_wd_set\n\n    def forward(self, d: dict[str, torch.Tensor] | torch.Tensor | None = None, **kwargs) -&gt; list[torch.Tensor]:\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            d (dict, torch.Tensor): Dict of inputs or input tensor with shape (B, C, H, W)\n\n            Alternatively, keyword arguments with modality=tensor.\n\n        Returns:\n            list[torch.Tensor]: List of transformer layer outputs. Shape (B, L, D).\n        \"\"\"\n        # Handle single image modality\n        if isinstance(d, torch.Tensor):\n            # Assuming first modality\n            d = {self.modalities[0]: d}\n        elif d is None:\n            d = {}\n            assert len(kwargs), \"No input provided.\"\n\n        # Add additional keyword args to input dict\n        for key, value in kwargs.items():\n            d[key] = value\n\n        if self.training and self.modality_drop_rate:\n            # Drop random modalities during training\n            for key in random.sample(list(d.keys()), k=len(d) - 1):\n                if random.random() &lt; self.modality_drop_rate:\n                    _ = d.pop(key)\n\n        x = []\n        num_tokens = []\n        image_mod = []\n        for mod, tensor in d.items():\n            assert mod in self.mod_name_mapping.keys(), \\\n                f'No patch embedding layer found for modality {mod}.'\n\n            mod_dict = self.encoder_embeddings[self.mod_name_mapping[mod]](tensor)\n            # Add embeddings to patchified data\n            x.append(mod_dict['x'] + mod_dict['emb'])\n            num_tokens.append(mod_dict['x'].shape[-2])\n            image_mod.append(self.mod_name_mapping[mod] in self.image_modalities)\n\n        # Concatenate along token dim\n        x = torch.cat(x, dim=1)  # Shape: (B, N, D)\n\n        out = []\n        for block in self.encoder:\n            x = block(x)\n            out.append(x.clone())\n\n        out[-1] = self.encoder_norm(x)  # Shape: (B, N, D)\n\n        def _unstack_image_modalities(x):\n            x = torch.split(x, num_tokens, dim=1)  # Split tokens by modality\n            x = [m for m, keep in zip(x, image_mod) if keep]  # Drop sequence modalities\n            x = torch.stack(x, dim=1)  # (B, M, N, D)\n            return x\n\n        # Merge tokens from different modalities\n        if self.merge_method == 'mean':\n            out = [_unstack_image_modalities(x) for x in out]\n            out = [x.mean(dim=1) for x in out]\n\n        elif self.merge_method == 'max':\n            out = [_unstack_image_modalities(x) for x in out]\n            out = [x.max(dim=1)[0] for x in out]\n\n        elif self.merge_method == 'concat':\n            out = [_unstack_image_modalities(x) for x in out]\n            if len(d) &lt; len(self.image_modalities):\n                # Handle missing modalities with missing_mod_token\n                num_missing = len(self.image_modalities) - len(d)\n                missing_tokens = self.missing_mod_token.repeat(out[-1].shape[0], num_missing, out[-1].shape[2], 1)\n                out = [torch.cat([x, missing_tokens], dim=1) for x in out]\n            # Concat along embedding dim\n            out = [torch.cat(x.unbind(dim=1), dim=-1) for x in out]\n\n        elif self.merge_method == 'dict':\n            out = [torch.split(x, num_tokens, dim=1) for x in out]\n            out = [{self.output_mod_name_mapping[mod]: x[i] for i, mod in enumerate(d.keys())} for x in out]\n\n        elif self.merge_method is None:\n            pass  # Do nothing\n        else:\n            raise NotImplementedError(f'Merging method {self.merge_method} is not implemented. '\n                                      f'Select one of mean, max or concat.')\n\n        return out\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.terramind.model.terramind_vit.TerraMindViT.forward","title":"<code>forward(d=None, **kwargs)</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>(dict, Tensor)</code> <p>Dict of inputs or input tensor with shape (B, C, H, W)</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>list[torch.Tensor]: List of transformer layer outputs. Shape (B, L, D).</p> Source code in <code>terratorch/models/backbones/terramind/model/terramind_vit.py</code> <pre><code>def forward(self, d: dict[str, torch.Tensor] | torch.Tensor | None = None, **kwargs) -&gt; list[torch.Tensor]:\n    \"\"\"\n    Forward pass of the model.\n\n    Args:\n        d (dict, torch.Tensor): Dict of inputs or input tensor with shape (B, C, H, W)\n\n        Alternatively, keyword arguments with modality=tensor.\n\n    Returns:\n        list[torch.Tensor]: List of transformer layer outputs. Shape (B, L, D).\n    \"\"\"\n    # Handle single image modality\n    if isinstance(d, torch.Tensor):\n        # Assuming first modality\n        d = {self.modalities[0]: d}\n    elif d is None:\n        d = {}\n        assert len(kwargs), \"No input provided.\"\n\n    # Add additional keyword args to input dict\n    for key, value in kwargs.items():\n        d[key] = value\n\n    if self.training and self.modality_drop_rate:\n        # Drop random modalities during training\n        for key in random.sample(list(d.keys()), k=len(d) - 1):\n            if random.random() &lt; self.modality_drop_rate:\n                _ = d.pop(key)\n\n    x = []\n    num_tokens = []\n    image_mod = []\n    for mod, tensor in d.items():\n        assert mod in self.mod_name_mapping.keys(), \\\n            f'No patch embedding layer found for modality {mod}.'\n\n        mod_dict = self.encoder_embeddings[self.mod_name_mapping[mod]](tensor)\n        # Add embeddings to patchified data\n        x.append(mod_dict['x'] + mod_dict['emb'])\n        num_tokens.append(mod_dict['x'].shape[-2])\n        image_mod.append(self.mod_name_mapping[mod] in self.image_modalities)\n\n    # Concatenate along token dim\n    x = torch.cat(x, dim=1)  # Shape: (B, N, D)\n\n    out = []\n    for block in self.encoder:\n        x = block(x)\n        out.append(x.clone())\n\n    out[-1] = self.encoder_norm(x)  # Shape: (B, N, D)\n\n    def _unstack_image_modalities(x):\n        x = torch.split(x, num_tokens, dim=1)  # Split tokens by modality\n        x = [m for m, keep in zip(x, image_mod) if keep]  # Drop sequence modalities\n        x = torch.stack(x, dim=1)  # (B, M, N, D)\n        return x\n\n    # Merge tokens from different modalities\n    if self.merge_method == 'mean':\n        out = [_unstack_image_modalities(x) for x in out]\n        out = [x.mean(dim=1) for x in out]\n\n    elif self.merge_method == 'max':\n        out = [_unstack_image_modalities(x) for x in out]\n        out = [x.max(dim=1)[0] for x in out]\n\n    elif self.merge_method == 'concat':\n        out = [_unstack_image_modalities(x) for x in out]\n        if len(d) &lt; len(self.image_modalities):\n            # Handle missing modalities with missing_mod_token\n            num_missing = len(self.image_modalities) - len(d)\n            missing_tokens = self.missing_mod_token.repeat(out[-1].shape[0], num_missing, out[-1].shape[2], 1)\n            out = [torch.cat([x, missing_tokens], dim=1) for x in out]\n        # Concat along embedding dim\n        out = [torch.cat(x.unbind(dim=1), dim=-1) for x in out]\n\n    elif self.merge_method == 'dict':\n        out = [torch.split(x, num_tokens, dim=1) for x in out]\n        out = [{self.output_mod_name_mapping[mod]: x[i] for i, mod in enumerate(d.keys())} for x in out]\n\n    elif self.merge_method is None:\n        pass  # Do nothing\n    else:\n        raise NotImplementedError(f'Merging method {self.merge_method} is not implemented. '\n                                  f'Select one of mean, max or concat.')\n\n    return out\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.terramind.model.terramind_vit.TerraMindViT.init_weights","title":"<code>init_weights()</code>","text":"<p>Weight initialization following MAE's initialization scheme</p> Source code in <code>terratorch/models/backbones/terramind/model/terramind_vit.py</code> <pre><code>def init_weights(self):\n    \"\"\"Weight initialization following MAE's initialization scheme\"\"\"\n\n    for name, m in self.named_modules():\n        # Skipping tokenizers to avoid reinitializing them\n        if \"tokenizer\" in name:\n            continue\n        # Linear\n        elif isinstance(m, nn.Linear):\n            if 'qkv' in name:\n                # treat the weights of Q, K, V separately\n                val = math.sqrt(6. / float(m.weight.shape[0] // 3 + m.weight.shape[1]))\n                nn.init.uniform_(m.weight, -val, val)\n            elif 'kv' in name:\n                # treat the weights of K, V separately\n                val = math.sqrt(6. / float(m.weight.shape[0] // 2 + m.weight.shape[1]))\n                nn.init.uniform_(m.weight, -val, val)\n            else:\n                nn.init.xavier_uniform_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        # LayerNorm\n        elif isinstance(m, nn.LayerNorm) or isinstance(m, LayerNorm):\n            nn.init.constant_(m.weight, 1.0)\n            nn.init.constant_(m.bias, 0)\n\n        # Embedding\n        elif isinstance(m, nn.Embedding):\n            nn.init.normal_(m.weight, std=0.02)\n        # Conv2d\n        elif isinstance(m, nn.Conv2d):\n            if '.proj' in name:\n                # From MAE, initialize projection like nn.Linear (instead of nn.Conv2d)\n                w = m.weight.data\n                nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.prithvi_mae.PrithviViT","title":"<code>terratorch.models.backbones.prithvi_mae.PrithviViT</code>","text":"<p>               Bases: <code>Module</code></p> <p>Prithvi ViT Encoder</p> Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>class PrithviViT(nn.Module):\n    \"\"\"Prithvi ViT Encoder\"\"\"\n\n    def __init__(\n        self,\n        img_size: int | tuple[int, int] = 224,\n        patch_size: int | tuple[int, int, int] = (1, 16, 16),\n        num_frames: int = 1,\n        in_chans: int = 3,\n        embed_dim: int = 1024,\n        depth: int = 24,\n        num_heads: int = 16,\n        mlp_ratio: float = 4.0,\n        norm_layer: type[nn.Module] = nn.LayerNorm,\n        coords_encoding: list[str] | None = None,\n        coords_scale_learn: bool = False,\n        drop_path: float = 0.0,\n        vpt: bool = False,\n        vpt_n_tokens: int | None = None,\n        vpt_dropout: float = 0,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.in_chans = in_chans\n        self.num_frames = num_frames\n        self.embed_dim = embed_dim\n        self.img_size = to_2tuple(img_size)\n        if isinstance(patch_size, int):\n            patch_size = (1, patch_size, patch_size)\n\n        # 3D patch embedding\n        self.patch_embed = PatchEmbed(\n            input_size=(num_frames,) + self.img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n        )\n        self.out_channels = [embed_dim * self.patch_embed.grid_size[0]] * depth\n\n        # Optional temporal and location embedding\n        coords_encoding = coords_encoding or []\n        self.temporal_encoding = 'time' in coords_encoding\n        self.location_encoding = 'location' in coords_encoding\n        if self.temporal_encoding:\n            assert patch_size[0] == 1, f\"With temporal encoding, patch_size[0] must be 1, received {patch_size[0]}\"\n            self.temporal_embed_enc = TemporalEncoder(embed_dim, coords_scale_learn)\n        if self.location_encoding:\n            self.location_embed_enc = LocationEncoder(embed_dim, coords_scale_learn)\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.register_buffer(\"pos_embed\", torch.zeros(1, self.patch_embed.num_patches + 1, embed_dim))\n\n        # Transformer layers\n        self.blocks = []\n        for i in range(depth):\n            self.blocks.append(Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer,\n                                     drop_path=drop_path,))\n        self.blocks = nn.ModuleList(self.blocks)\n\n        self.norm = norm_layer(embed_dim)\n\n        self.vpt = vpt\n        self.vpt_n_tokens = vpt_n_tokens\n        self.vpt_dropout = vpt_dropout\n        if self.vpt:\n            if self.vpt_n_tokens is None:\n                msg = \"vpt_n_tokens must be provided when using VPT\"\n                raise ValueError(msg)\n            self.vpt_prompt_embeddings = nn.ParameterList(\n                [nn.Parameter(torch.zeros(1, self.vpt_n_tokens, embed_dim)) for _ in range(depth)]\n            )\n            self.vpt_dropout_layers = nn.ModuleList([nn.Dropout(vpt_dropout) for _ in range(depth)])\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        # initialize (and freeze) position embeddings by sin-cos embedding\n        pos_embed = get_3d_sincos_pos_embed(\n            self.pos_embed.shape[-1], self.patch_embed.grid_size, add_cls_token=True\n        )\n        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n\n        # initialize patch_embeddings like nn.Linear (instead of nn.Conv2d)\n        w = self.patch_embed.proj.weight.data\n        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n\n        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n        torch.nn.init.normal_(self.cls_token, std=0.02)\n        self.apply(_init_weights)\n\n        # initialize VPT prompt embeddings\n        if self.vpt:\n            # extracted from https://github.com/KMnP/vpt/blob/4410440ec1b489f24f66b9fad3d9b10ff3443567/src/models/vit_prompt/vit.py#L57\n            val = np.sqrt(6.0 / float(3 * reduce(mul, self.patch_embed.patch_size[1:], 1) + self.embed_dim))\n            for emb in self.vpt_prompt_embeddings:\n                nn.init.uniform_(emb, -val, val)\n\n    def random_masking(self, sequence, mask_ratio, noise=None):\n        \"\"\"\n        Perform per-sample random masking by per-sample shuffling. Per-sample shuffling is done by argsort random\n        noise.\n\n        Args:\n            sequence (`torch.FloatTensor` of shape `(batch_size, sequence_length, dim)`)\n            mask_ratio (float): mask ratio to use.\n            noise (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) which is\n                mainly used for testing purposes to control randomness and maintain the reproducibility\n        \"\"\"\n        batch_size, seq_length, dim = sequence.shape\n        len_keep = int(seq_length * (1 - mask_ratio))\n\n        if noise is None:\n            noise = torch.rand(batch_size, seq_length, device=sequence.device)  # noise in [0, 1]\n\n        # sort noise for each sample\n        ids_shuffle = torch.argsort(noise, dim=1).to(sequence.device)  # ascend: small is keep, large is remove\n        ids_restore = torch.argsort(ids_shuffle, dim=1).to(sequence.device)\n\n        # keep the first subset\n        ids_keep = ids_shuffle[:, :len_keep]\n        sequence_unmasked = torch.gather(sequence, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, dim))\n\n        # generate the binary mask: 0 is keep, 1 is remove\n        mask = torch.ones([batch_size, seq_length], device=sequence.device)\n        mask[:, :len_keep] = 0\n        # unshuffle to get the binary mask\n        mask = torch.gather(mask, dim=1, index=ids_restore)\n\n        return sequence_unmasked, mask, ids_restore\n\n    def interpolate_pos_encoding(self, sample_shape: tuple[int, int, int]):\n\n        pos_embed = _interpolate_pos_encoding(\n            pos_embed=self.pos_embed,\n            grid_size=self.patch_embed.grid_size,\n            patch_size=self.patch_embed.patch_size,\n            shape=sample_shape,\n            embed_dim=self.embed_dim,\n        )\n        return pos_embed\n\n    def forward(\n        self, x: torch.Tensor,\n        temporal_coords: None | torch.Tensor = None,\n        location_coords: None | torch.Tensor = None,\n        mask_ratio=0.75\n    ):\n        if len(x.shape) == 4 and self.patch_embed.input_size[0] == 1:\n            # add time dim\n            x = x.unsqueeze(2)\n        sample_shape = x.shape[-3:]\n\n        # embed patches\n        x = self.patch_embed(x)\n\n        pos_embed = self.interpolate_pos_encoding(sample_shape)\n        # add pos embed w/o cls token\n        x = x + pos_embed[:, 1:, :]\n\n        if self.temporal_encoding and temporal_coords is not None:\n            num_tokens_per_frame = x.shape[1] // self.num_frames\n            temporal_encoding = self.temporal_embed_enc(temporal_coords, num_tokens_per_frame)\n            x = x + temporal_encoding\n        if self.location_encoding and location_coords is not None:\n            location_encoding = self.location_embed_enc(location_coords)\n            x = x + location_encoding\n\n        # masking: length -&gt; length * mask_ratio\n        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n\n        # append cls token\n        cls_token = self.cls_token + pos_embed[:, :1, :]\n        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        # apply Transformer blocks\n        bs = x.shape[0]\n        for idx, block in enumerate(self.blocks):\n            if self.vpt:\n                x = torch.cat(\n                    (\n                        x[:, :1, :],\n                        self.vpt_dropout_layers[idx](self.vpt_prompt_embeddings[idx].expand(bs, -1, -1)),\n                        x[:, 1:, :],\n                    ),\n                    dim=1,\n                )  # (batch_size, cls_token + n_prompt + n_patches, hidden_dim)\n            x = block(x)\n            if self.vpt:\n                x = torch.cat(\n                    (x[:, :1, :], x[:, (1 + self.vpt_n_tokens) :, :]),\n                    dim=1,\n                )\n        x = self.norm(x)\n\n        return x, mask, ids_restore\n\n    def forward_features(\n        self,\n        x: torch.Tensor,\n        temporal_coords: None | torch.Tensor = None,\n        location_coords: None | torch.Tensor = None,\n    ) -&gt; list[torch.Tensor]:\n        if len(x.shape) == 4 and self.patch_embed.input_size[0] == 1:\n            # add time dim\n            x = x.unsqueeze(2)\n        sample_shape = x.shape[-3:]\n\n        # embed patches\n        x = self.patch_embed(x)\n\n        pos_embed = self.interpolate_pos_encoding(sample_shape)\n        # add pos embed w/o cls token\n        x = x + pos_embed[:, 1:, :]\n\n        if self.temporal_encoding and temporal_coords is not None:\n            num_tokens_per_frame = x.shape[1] // self.num_frames\n            temporal_encoding = self.temporal_embed_enc(temporal_coords, num_tokens_per_frame)\n            x = x + temporal_encoding\n        if self.location_encoding and location_coords is not None:\n            location_encoding = self.location_embed_enc(location_coords)\n            x = x + location_encoding\n\n        # append cls token\n        cls_token = self.cls_token + pos_embed[:, :1, :]\n        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        # apply Transformer blocks\n        bs = x.shape[0]\n        out = []\n        for idx, block in enumerate(self.blocks):\n            if self.vpt:\n                x = torch.cat(\n                    (\n                        x[:, :1, :],\n                        self.vpt_dropout_layers[idx](self.vpt_prompt_embeddings[idx].expand(bs, -1, -1)),\n                        x[:, 1:, :],\n                    ),\n                    dim=1,\n                )  # (batch_size, cls_token + n_prompt + n_patches, hidden_dim)\n            x = block(x)\n            if self.vpt:\n                x = torch.cat(\n                    (x[:, :1, :], x[:, (1 + self.vpt_n_tokens) :, :]),\n                    dim=1,\n                )\n            out.append(x.clone())\n\n        x = self.norm(x)\n        out[-1] = x\n        return out\n\n    def prepare_features_for_image_model(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        out = []\n        effective_time_dim = self.patch_embed.input_size[0] // self.patch_embed.patch_size[0]\n        for x in features:\n            x_no_token = x[:, 1:, :]\n            number_of_tokens = x_no_token.shape[1]\n            tokens_per_timestep = number_of_tokens // effective_time_dim\n            h = int(np.sqrt(tokens_per_timestep))\n            encoded = rearrange(\n                x_no_token,\n                \"batch (t h w) e -&gt; batch (t e) h w\",\n                e=self.embed_dim,\n                t=effective_time_dim,\n                h=h,\n            )\n            out.append(encoded)\n        return out\n\n    def freeze(self):\n        for n, param in self.named_parameters():\n            if \"vpt_prompt_embeddings\" not in n:\n                param.requires_grad_(False)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.prithvi_mae.PrithviViT.random_masking","title":"<code>random_masking(sequence, mask_ratio, noise=None)</code>","text":"<p>Perform per-sample random masking by per-sample shuffling. Per-sample shuffling is done by argsort random noise.</p> <p>Parameters:</p> Name Type Description Default <code>mask_ratio</code> <code>float</code> <p>mask ratio to use.</p> required Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>def random_masking(self, sequence, mask_ratio, noise=None):\n    \"\"\"\n    Perform per-sample random masking by per-sample shuffling. Per-sample shuffling is done by argsort random\n    noise.\n\n    Args:\n        sequence (`torch.FloatTensor` of shape `(batch_size, sequence_length, dim)`)\n        mask_ratio (float): mask ratio to use.\n        noise (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) which is\n            mainly used for testing purposes to control randomness and maintain the reproducibility\n    \"\"\"\n    batch_size, seq_length, dim = sequence.shape\n    len_keep = int(seq_length * (1 - mask_ratio))\n\n    if noise is None:\n        noise = torch.rand(batch_size, seq_length, device=sequence.device)  # noise in [0, 1]\n\n    # sort noise for each sample\n    ids_shuffle = torch.argsort(noise, dim=1).to(sequence.device)  # ascend: small is keep, large is remove\n    ids_restore = torch.argsort(ids_shuffle, dim=1).to(sequence.device)\n\n    # keep the first subset\n    ids_keep = ids_shuffle[:, :len_keep]\n    sequence_unmasked = torch.gather(sequence, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, dim))\n\n    # generate the binary mask: 0 is keep, 1 is remove\n    mask = torch.ones([batch_size, seq_length], device=sequence.device)\n    mask[:, :len_keep] = 0\n    # unshuffle to get the binary mask\n    mask = torch.gather(mask, dim=1, index=ids_restore)\n\n    return sequence_unmasked, mask, ids_restore\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.swin_encoder_decoder.MMSegSwinTransformer","title":"<code>terratorch.models.backbones.swin_encoder_decoder.MMSegSwinTransformer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>class MMSegSwinTransformer(nn.Module):\n\n    def __init__(\n        self,\n        pretrain_img_size=224,\n        in_chans=3,\n        embed_dim=96,\n        patch_size=4,\n        window_size=7,\n        mlp_ratio=4,\n        depths=(2, 2, 6, 2),\n        num_heads=(3, 6, 12, 24),\n        strides=(4, 2, 2, 2),\n        num_classes: int = 1000,\n        global_pool: str = \"avg\",\n        out_indices=(0, 1, 2, 3),\n        qkv_bias=True,  # noqa: FBT002\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.1,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        with_cp=False,  # noqa: FBT002\n        frozen_stages=-1,\n    ):\n        \"\"\"MMSeg Swin Transformer backbone.\n\n        This backbone is the implementation of `Swin Transformer:\n        Hierarchical Vision Transformer using Shifted\n        Windows &lt;https://arxiv.org/abs/2103.14030&gt;`_.\n        Inspiration from https://github.com/microsoft/Swin-Transformer.\n\n        Args:\n            pretrain_img_size (int | tuple[int]): The size of input image when\n                pretrain. Defaults: 224.\n            in_chans (int): The num of input channels.\n                Defaults: 3.\n            embed_dim (int): The feature dimension. Default: 96.\n            patch_size (int | tuple[int]): Patch size. Default: 4.\n            window_size (int): Window size. Default: 7.\n            mlp_ratio (int | float): Ratio of mlp hidden dim to embedding dim.\n                Default: 4.\n            depths (tuple[int]): Depths of each Swin Transformer stage.\n                Default: (2, 2, 6, 2).\n            num_heads (tuple[int]): Parallel attention heads of each Swin\n                Transformer stage. Default: (3, 6, 12, 24).\n            strides (tuple[int]): The patch merging or patch embedding stride of\n                each Swin Transformer stage. (In swin, we set kernel size equal to\n                stride.) Default: (4, 2, 2, 2).\n            out_indices (tuple[int]): Output from which stages.\n                Default: (0, 1, 2, 3).\n            qkv_bias (bool, optional): If True, add a learnable bias to query, key,\n                value. Default: True\n            qk_scale (float | None, optional): Override default qk scale of\n                head_dim ** -0.5 if set. Default: None.\n            patch_norm (bool): If add a norm layer for patch embed and patch\n                merging. Default: True.\n            drop_rate (float): Dropout rate. Defaults: 0.\n            attn_drop_rate (float): Attention dropout rate. Default: 0.\n            drop_path_rate (float): Stochastic depth rate. Defaults: 0.1.\n            act_layer (dict): activation layer.\n                Default: nn.GELU.\n            norm_layer (dict): normalization layer at\n                output of backone. Defaults: nn.LayerNorm.\n            with_cp (bool, optional): Use checkpoint or not. Using checkpoint\n                will save some memory while slowing down the training speed.\n                Default: False.\n            frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n                -1 means not freezing any parameters.\n        \"\"\"\n\n        self.frozen_stages = frozen_stages\n        self.output_fmt = \"NHWC\"\n        if isinstance(pretrain_img_size, int):\n            pretrain_img_size = to_2tuple(pretrain_img_size)\n        elif isinstance(pretrain_img_size, tuple):\n            if len(pretrain_img_size) == 1:\n                pretrain_img_size = to_2tuple(pretrain_img_size[0])\n            if not len(pretrain_img_size) == 2:  # noqa: PLR2004\n                msg = f\"The size of image should have length 1 or 2, but got {len(pretrain_img_size)}\"\n                raise Exception(msg)\n\n        super().__init__()\n\n        self.num_layers = len(depths)\n        self.out_indices = out_indices\n        self.feature_info = []\n\n        if not strides[0] == patch_size:\n            msg = \"Use non-overlapping patch embed.\"\n            raise Exception(msg)\n\n        self.patch_embed = PatchEmbed(\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            kernel_size=patch_size,\n            stride=strides[0],\n            padding=\"corner\",\n            norm_layer=norm_layer,\n            padding_mode=\"replicate\",\n            drop_rate=drop_rate,\n        )\n\n        # self.drop_after_pos = nn.Dropout(p=drop_rate)\n\n        # set stochastic depth decay rule\n        total_depth = sum(depths)\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_depth)]\n\n        stages = []\n        in_chans = embed_dim\n        scale = 1\n        for i in range(self.num_layers):\n            if i &lt; self.num_layers - 1:\n                downsample = PatchMerging(\n                    in_chans=in_chans,\n                    out_channels=2 * in_chans,\n                    stride=strides[i + 1],\n                    norm_layer=norm_layer,\n                )\n            else:\n                downsample = None\n\n            stage = SwinBlockSequence(\n                embed_dim=in_chans,\n                num_heads=num_heads[i],\n                feedforward_channels=int(mlp_ratio * in_chans),\n                depth=depths[i],\n                window_size=window_size,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop_rate=drop_rate,\n                attn_drop_rate=attn_drop_rate,\n                drop_path_rate=dpr[sum(depths[:i]) : sum(depths[: i + 1])],\n                downsample=downsample,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                with_cp=with_cp,\n            )\n            stages.append(stage)\n            if i &gt; 0:\n                scale *= 2\n            self.feature_info += [{\"num_chs\": in_chans, \"reduction\": 4 * scale, \"module\": f\"stages.{i}\"}]\n            if downsample:\n                in_chans = downsample.out_channels\n        self.stages = nn.Sequential(*stages)\n        self.num_features = [int(embed_dim * 2**i) for i in range(self.num_layers)]\n        # Add a norm layer for each output\n\n        self.head = ClassifierHead(\n            self.num_features[-1],\n            num_classes,\n            pool_type=global_pool,\n            drop_rate=drop_rate,\n            input_fmt=self.output_fmt,\n        )\n\n    def train(self, mode=True):  # noqa: FBT002\n        \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n        super().train(mode)\n        self._freeze_stages()\n\n    def _freeze_stages(self):\n        if self.frozen_stages &gt;= 0:\n            self.patch_embed.eval()\n            for param in self.patch_embed.parameters():\n                param.requires_grad = False\n            self.drop_after_pos.eval()\n\n        for i in range(1, self.frozen_stages + 1):\n            if (i - 1) in self.out_indices:\n                norm_layer = getattr(self, f\"norm{i-1}\")\n                norm_layer.eval()\n                for param in norm_layer.parameters():\n                    param.requires_grad = False\n\n            m = self.stages[i - 1]\n            m.eval()\n            for param in m.parameters():\n                param.requires_grad = False\n\n    @torch.jit.ignore\n    def init_weights(self, mode=\"\"):\n        modes = (\"jax\", \"jax_nlhb\", \"moco\", \"\")\n        if mode not in modes:\n            msg = f\"mode must be one of {modes}\"\n            raise Exception(msg)\n        head_bias = -math.log(self.num_classes) if \"nlhb\" in mode else 0.0\n        named_apply(get_init_weights_vit(mode, head_bias=head_bias), self)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        nwd = set()\n        for n, _ in self.named_parameters():\n            if \"relative_position_bias_table\" in n:\n                nwd.add(n)\n        return nwd\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):  # noqa: FBT002\n        return {\n            \"stem\": r\"^patch_embed\",  # stem and embed\n            \"blocks\": r\"^layers\\.(\\d+)\"\n            if coarse\n            else [\n                (r\"^layers\\.(\\d+).downsample\", (0,)),\n                (r\"^layers\\.(\\d+)\\.\\w+\\.(\\d+)\", None),\n                (r\"^norm\", (99999,)),\n            ],\n        }\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        self.head.reset(num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self.stages(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):  # noqa: FBT002, FBT001\n        return self.head(x, pre_logits=True) if pre_logits else self.head(x)\n\n    def forward(self, x):\n        features = self.forward_features(x)\n        x = self.forward_head(features[0])\n        return x\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.swin_encoder_decoder.MMSegSwinTransformer.__init__","title":"<code>__init__(pretrain_img_size=224, in_chans=3, embed_dim=96, patch_size=4, window_size=7, mlp_ratio=4, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24), strides=(4, 2, 2, 2), num_classes=1000, global_pool='avg', out_indices=(0, 1, 2, 3), qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.1, act_layer=nn.GELU, norm_layer=nn.LayerNorm, with_cp=False, frozen_stages=-1)</code>","text":"<p>MMSeg Swin Transformer backbone.</p> <p>This backbone is the implementation of <code>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows &lt;https://arxiv.org/abs/2103.14030&gt;</code>_. Inspiration from https://github.com/microsoft/Swin-Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>pretrain_img_size</code> <code>int | tuple[int]</code> <p>The size of input image when pretrain. Defaults: 224.</p> <code>224</code> <code>in_chans</code> <code>int</code> <p>The num of input channels. Defaults: 3.</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>The feature dimension. Default: 96.</p> <code>96</code> <code>patch_size</code> <code>int | tuple[int]</code> <p>Patch size. Default: 4.</p> <code>4</code> <code>window_size</code> <code>int</code> <p>Window size. Default: 7.</p> <code>7</code> <code>mlp_ratio</code> <code>int | float</code> <p>Ratio of mlp hidden dim to embedding dim. Default: 4.</p> <code>4</code> <code>depths</code> <code>tuple[int]</code> <p>Depths of each Swin Transformer stage. Default: (2, 2, 6, 2).</p> <code>(2, 2, 6, 2)</code> <code>num_heads</code> <code>tuple[int]</code> <p>Parallel attention heads of each Swin Transformer stage. Default: (3, 6, 12, 24).</p> <code>(3, 6, 12, 24)</code> <code>strides</code> <code>tuple[int]</code> <p>The patch merging or patch embedding stride of each Swin Transformer stage. (In swin, we set kernel size equal to stride.) Default: (4, 2, 2, 2).</p> <code>(4, 2, 2, 2)</code> <code>out_indices</code> <code>tuple[int]</code> <p>Output from which stages. Default: (0, 1, 2, 3).</p> <code>(0, 1, 2, 3)</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to query, key, value. Default: True</p> <code>True</code> <code>qk_scale</code> <code>float | None</code> <p>Override default qk scale of head_dim ** -0.5 if set. Default: None.</p> <code>None</code> <code>patch_norm</code> <code>bool</code> <p>If add a norm layer for patch embed and patch merging. Default: True.</p> required <code>drop_rate</code> <code>float</code> <p>Dropout rate. Defaults: 0.</p> <code>0.0</code> <code>attn_drop_rate</code> <code>float</code> <p>Attention dropout rate. Default: 0.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float</code> <p>Stochastic depth rate. Defaults: 0.1.</p> <code>0.1</code> <code>act_layer</code> <code>dict</code> <p>activation layer. Default: nn.GELU.</p> <code>GELU</code> <code>norm_layer</code> <code>dict</code> <p>normalization layer at output of backone. Defaults: nn.LayerNorm.</p> <code>LayerNorm</code> <code>with_cp</code> <code>bool</code> <p>Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False.</p> <code>False</code> <code>frozen_stages</code> <code>int</code> <p>Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters.</p> <code>-1</code> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>def __init__(\n    self,\n    pretrain_img_size=224,\n    in_chans=3,\n    embed_dim=96,\n    patch_size=4,\n    window_size=7,\n    mlp_ratio=4,\n    depths=(2, 2, 6, 2),\n    num_heads=(3, 6, 12, 24),\n    strides=(4, 2, 2, 2),\n    num_classes: int = 1000,\n    global_pool: str = \"avg\",\n    out_indices=(0, 1, 2, 3),\n    qkv_bias=True,  # noqa: FBT002\n    qk_scale=None,\n    drop_rate=0.0,\n    attn_drop_rate=0.0,\n    drop_path_rate=0.1,\n    act_layer=nn.GELU,\n    norm_layer=nn.LayerNorm,\n    with_cp=False,  # noqa: FBT002\n    frozen_stages=-1,\n):\n    \"\"\"MMSeg Swin Transformer backbone.\n\n    This backbone is the implementation of `Swin Transformer:\n    Hierarchical Vision Transformer using Shifted\n    Windows &lt;https://arxiv.org/abs/2103.14030&gt;`_.\n    Inspiration from https://github.com/microsoft/Swin-Transformer.\n\n    Args:\n        pretrain_img_size (int | tuple[int]): The size of input image when\n            pretrain. Defaults: 224.\n        in_chans (int): The num of input channels.\n            Defaults: 3.\n        embed_dim (int): The feature dimension. Default: 96.\n        patch_size (int | tuple[int]): Patch size. Default: 4.\n        window_size (int): Window size. Default: 7.\n        mlp_ratio (int | float): Ratio of mlp hidden dim to embedding dim.\n            Default: 4.\n        depths (tuple[int]): Depths of each Swin Transformer stage.\n            Default: (2, 2, 6, 2).\n        num_heads (tuple[int]): Parallel attention heads of each Swin\n            Transformer stage. Default: (3, 6, 12, 24).\n        strides (tuple[int]): The patch merging or patch embedding stride of\n            each Swin Transformer stage. (In swin, we set kernel size equal to\n            stride.) Default: (4, 2, 2, 2).\n        out_indices (tuple[int]): Output from which stages.\n            Default: (0, 1, 2, 3).\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key,\n            value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of\n            head_dim ** -0.5 if set. Default: None.\n        patch_norm (bool): If add a norm layer for patch embed and patch\n            merging. Default: True.\n        drop_rate (float): Dropout rate. Defaults: 0.\n        attn_drop_rate (float): Attention dropout rate. Default: 0.\n        drop_path_rate (float): Stochastic depth rate. Defaults: 0.1.\n        act_layer (dict): activation layer.\n            Default: nn.GELU.\n        norm_layer (dict): normalization layer at\n            output of backone. Defaults: nn.LayerNorm.\n        with_cp (bool, optional): Use checkpoint or not. Using checkpoint\n            will save some memory while slowing down the training speed.\n            Default: False.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n    \"\"\"\n\n    self.frozen_stages = frozen_stages\n    self.output_fmt = \"NHWC\"\n    if isinstance(pretrain_img_size, int):\n        pretrain_img_size = to_2tuple(pretrain_img_size)\n    elif isinstance(pretrain_img_size, tuple):\n        if len(pretrain_img_size) == 1:\n            pretrain_img_size = to_2tuple(pretrain_img_size[0])\n        if not len(pretrain_img_size) == 2:  # noqa: PLR2004\n            msg = f\"The size of image should have length 1 or 2, but got {len(pretrain_img_size)}\"\n            raise Exception(msg)\n\n    super().__init__()\n\n    self.num_layers = len(depths)\n    self.out_indices = out_indices\n    self.feature_info = []\n\n    if not strides[0] == patch_size:\n        msg = \"Use non-overlapping patch embed.\"\n        raise Exception(msg)\n\n    self.patch_embed = PatchEmbed(\n        in_chans=in_chans,\n        embed_dim=embed_dim,\n        kernel_size=patch_size,\n        stride=strides[0],\n        padding=\"corner\",\n        norm_layer=norm_layer,\n        padding_mode=\"replicate\",\n        drop_rate=drop_rate,\n    )\n\n    # self.drop_after_pos = nn.Dropout(p=drop_rate)\n\n    # set stochastic depth decay rule\n    total_depth = sum(depths)\n    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_depth)]\n\n    stages = []\n    in_chans = embed_dim\n    scale = 1\n    for i in range(self.num_layers):\n        if i &lt; self.num_layers - 1:\n            downsample = PatchMerging(\n                in_chans=in_chans,\n                out_channels=2 * in_chans,\n                stride=strides[i + 1],\n                norm_layer=norm_layer,\n            )\n        else:\n            downsample = None\n\n        stage = SwinBlockSequence(\n            embed_dim=in_chans,\n            num_heads=num_heads[i],\n            feedforward_channels=int(mlp_ratio * in_chans),\n            depth=depths[i],\n            window_size=window_size,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            drop_rate=drop_rate,\n            attn_drop_rate=attn_drop_rate,\n            drop_path_rate=dpr[sum(depths[:i]) : sum(depths[: i + 1])],\n            downsample=downsample,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            with_cp=with_cp,\n        )\n        stages.append(stage)\n        if i &gt; 0:\n            scale *= 2\n        self.feature_info += [{\"num_chs\": in_chans, \"reduction\": 4 * scale, \"module\": f\"stages.{i}\"}]\n        if downsample:\n            in_chans = downsample.out_channels\n    self.stages = nn.Sequential(*stages)\n    self.num_features = [int(embed_dim * 2**i) for i in range(self.num_layers)]\n    # Add a norm layer for each output\n\n    self.head = ClassifierHead(\n        self.num_features[-1],\n        num_classes,\n        pool_type=global_pool,\n        drop_rate=drop_rate,\n        input_fmt=self.output_fmt,\n    )\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.swin_encoder_decoder.MMSegSwinTransformer.train","title":"<code>train(mode=True)</code>","text":"<p>Convert the model into training mode while keep layers freezed.</p> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>def train(self, mode=True):  # noqa: FBT002\n    \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n    super().train(mode)\n    self._freeze_stages()\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.unet.UNet","title":"<code>terratorch.models.backbones.unet.UNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>UNet backbone.</p> <p>This backbone is the implementation of <code>U-Net: Convolutional Networks for Biomedical Image Segmentation &lt;https://arxiv.org/abs/1505.04597&gt;</code>_.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input image channels. Default\" 3.</p> <code>3</code> <code>out_channels</code> <code>int</code> <p>Number of base channels of each stage. The output channels of the first stage. Default: 64.</p> <code>64</code> <code>num_stages</code> <code>int</code> <p>Number of stages in encoder, normally 5. Default: 5.</p> <code>5</code> <code>strides</code> <code>Sequence[int 1 | 2]</code> <p>Strides of each stage in encoder. len(strides) is equal to num_stages. Normally the stride of the first stage in encoder is 1. If strides[i]=2, it uses stride convolution to downsample in the correspondence encoder stage. Default: (1, 1, 1, 1, 1).</p> <code>(1, 1, 1, 1, 1)</code> <code>enc_num_convs</code> <code>Sequence[int]</code> <p>Number of convolutional layers in the convolution block of the correspondence encoder stage. Default: (2, 2, 2, 2, 2).</p> <code>(2, 2, 2, 2, 2)</code> <code>dec_num_convs</code> <code>Sequence[int]</code> <p>Number of convolutional layers in the convolution block of the correspondence decoder stage. Default: (2, 2, 2, 2).</p> <code>(2, 2, 2, 2)</code> <code>downsamples</code> <code>Sequence[int]</code> <p>Whether use MaxPool to downsample the feature map after the first stage of encoder (stages: [1, num_stages)). If the correspondence encoder stage use stride convolution (strides[i]=2), it will never use MaxPool to downsample, even downsamples[i-1]=True. Default: (True, True, True, True).</p> <code>(True, True, True, True)</code> <code>enc_dilations</code> <code>Sequence[int]</code> <p>Dilation rate of each stage in encoder. Default: (1, 1, 1, 1, 1).</p> <code>(1, 1, 1, 1, 1)</code> <code>dec_dilations</code> <code>Sequence[int]</code> <p>Dilation rate of each stage in decoder. Default: (1, 1, 1, 1).</p> <code>(1, 1, 1, 1)</code> <code>with_cp</code> <code>bool</code> <p>Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False.</p> <code>False</code> <code>conv_cfg</code> <code>dict | None</code> <p>Config dict for convolution layer. Default: None.</p> <code>None</code> <code>norm_cfg</code> <code>dict | None</code> <p>Config dict for normalization layer. Default: dict(type='BN').</p> <code>dict(type='BN')</code> <code>act_cfg</code> <code>dict | None</code> <p>Config dict for activation layer in ConvModule. Default: dict(type='ReLU').</p> <code>dict(type='ReLU')</code> <code>upsample_cfg</code> <code>dict</code> <p>The upsample config of the upsample module in decoder. Default: dict(type='InterpConv').</p> <code>None</code> <code>norm_eval</code> <code>bool</code> <p>Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. Default: False.</p> <code>False</code> <code>dcn</code> <code>bool</code> <p>Use deformable convolution in convolutional layer or not. Default: None.</p> <code>None</code> <code>plugins</code> <code>dict</code> <p>plugins for convolutional layers. Default: None.</p> <code>None</code> <code>pretrained</code> <code>str</code> <p>model pretrained path. Default: None</p> <code>None</code> <code>init_cfg</code> <code>dict or list[dict]</code> <p>Initialization config dict. Default: None</p> <code>None</code> Notice <p>The input image size should be divisible by the whole downsample rate of the encoder. More detail of the whole downsample rate can be found in <code>UNet._check_input_divisible</code>.</p> Source code in <code>terratorch/models/backbones/unet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\nclass UNet(nn.Module):\n    \"\"\"UNet backbone.\n\n    This backbone is the implementation of `U-Net: Convolutional Networks\n    for Biomedical Image Segmentation &lt;https://arxiv.org/abs/1505.04597&gt;`_.\n\n    Args:\n        in_channels (int): Number of input image channels. Default\" 3.\n        out_channels (int): Number of base channels of each stage.\n            The output channels of the first stage. Default: 64.\n        num_stages (int): Number of stages in encoder, normally 5. Default: 5.\n        strides (Sequence[int 1 | 2]): Strides of each stage in encoder.\n            len(strides) is equal to num_stages. Normally the stride of the\n            first stage in encoder is 1. If strides[i]=2, it uses stride\n            convolution to downsample in the correspondence encoder stage.\n            Default: (1, 1, 1, 1, 1).\n        enc_num_convs (Sequence[int]): Number of convolutional layers in the\n            convolution block of the correspondence encoder stage.\n            Default: (2, 2, 2, 2, 2).\n        dec_num_convs (Sequence[int]): Number of convolutional layers in the\n            convolution block of the correspondence decoder stage.\n            Default: (2, 2, 2, 2).\n        downsamples (Sequence[int]): Whether use MaxPool to downsample the\n            feature map after the first stage of encoder\n            (stages: [1, num_stages)). If the correspondence encoder stage use\n            stride convolution (strides[i]=2), it will never use MaxPool to\n            downsample, even downsamples[i-1]=True.\n            Default: (True, True, True, True).\n        enc_dilations (Sequence[int]): Dilation rate of each stage in encoder.\n            Default: (1, 1, 1, 1, 1).\n        dec_dilations (Sequence[int]): Dilation rate of each stage in decoder.\n            Default: (1, 1, 1, 1).\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed. Default: False.\n        conv_cfg (dict | None): Config dict for convolution layer.\n            Default: None.\n        norm_cfg (dict | None): Config dict for normalization layer.\n            Default: dict(type='BN').\n        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n            Default: dict(type='ReLU').\n        upsample_cfg (dict): The upsample config of the upsample module in\n            decoder. Default: dict(type='InterpConv').\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only. Default: False.\n        dcn (bool): Use deformable convolution in convolutional layer or not.\n            Default: None.\n        plugins (dict): plugins for convolutional layers. Default: None.\n        pretrained (str, optional): model pretrained path. Default: None\n        init_cfg (dict or list[dict], optional): Initialization config dict.\n            Default: None\n\n    Notice:\n        The input image size should be divisible by the whole downsample rate\n        of the encoder. More detail of the whole downsample rate can be found\n        in `UNet._check_input_divisible`.\n    \"\"\"\n\n    def __init__(self,\n                 in_channels=3,\n                 out_channels=64,\n                 num_stages=5,\n                 strides=(1, 1, 1, 1, 1),\n                 enc_num_convs=(2, 2, 2, 2, 2),\n                 dec_num_convs=(2, 2, 2, 2),\n                 downsamples=(True, True, True, True),\n                 enc_dilations=(1, 1, 1, 1, 1),\n                 dec_dilations=(1, 1, 1, 1),\n                 with_cp=False,\n                 conv_cfg=None,\n                 norm_cfg=dict(type='BN'),\n                 act_cfg=dict(type='ReLU'),\n                 upsample_cfg=None,\n                 norm_eval=False,\n                 dcn=None,\n                 plugins=None,\n                 pretrained=None,\n                 init_cfg=None):\n        super(UNet, self).__init__()\n\n        self.pretrained = pretrained\n        assert not (init_cfg and pretrained), \\\n            'init_cfg and pretrained cannot be setting at the same time'\n        if isinstance(pretrained, str):\n            warnings.warn('DeprecationWarning: pretrained is a deprecated, '\n                          'please use \"init_cfg\" instead')\n            self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)\n        elif pretrained is None:\n            if init_cfg is None:\n                self.init_cfg = [\n                    dict(type='Kaiming', layer='Conv2d'),\n                    dict(\n                        type='Constant',\n                        val=1,\n                        layer=['_BatchNorm', 'GroupNorm'])\n                ]\n        else:\n            raise TypeError('pretrained must be a str or None')\n\n        assert dcn is None, 'Not implemented yet.'\n        assert plugins is None, 'Not implemented yet.'\n        assert len(strides) == num_stages, \\\n            'The length of strides should be equal to num_stages, '\\\n            f'while the strides is {strides}, the length of '\\\n            f'strides is {len(strides)}, and the num_stages is '\\\n            f'{num_stages}.'\n        assert len(enc_num_convs) == num_stages, \\\n            'The length of enc_num_convs should be equal to num_stages, '\\\n            f'while the enc_num_convs is {enc_num_convs}, the length of '\\\n            f'enc_num_convs is {len(enc_num_convs)}, and the num_stages is '\\\n            f'{num_stages}.'\n        assert len(dec_num_convs) == (num_stages-1), \\\n            'The length of dec_num_convs should be equal to (num_stages-1), '\\\n            f'while the dec_num_convs is {dec_num_convs}, the length of '\\\n            f'dec_num_convs is {len(dec_num_convs)}, and the num_stages is '\\\n            f'{num_stages}.'\n        assert len(downsamples) == (num_stages-1), \\\n            'The length of downsamples should be equal to (num_stages-1), '\\\n            f'while the downsamples is {downsamples}, the length of '\\\n            f'downsamples is {len(downsamples)}, and the num_stages is '\\\n            f'{num_stages}.'\n        assert len(enc_dilations) == num_stages, \\\n            'The length of enc_dilations should be equal to num_stages, '\\\n            f'while the enc_dilations is {enc_dilations}, the length of '\\\n            f'enc_dilations is {len(enc_dilations)}, and the num_stages is '\\\n            f'{num_stages}.'\n        assert len(dec_dilations) == (num_stages-1), \\\n            'The length of dec_dilations should be equal to (num_stages-1), '\\\n            f'while the dec_dilations is {dec_dilations}, the length of '\\\n            f'dec_dilations is {len(dec_dilations)}, and the num_stages is '\\\n            f'{num_stages}.'\n        self.num_stages = num_stages\n        self.strides = strides\n        self.downsamples = downsamples\n        self.norm_eval = norm_eval\n        self.out_channels = [out_channels * 2**i for i in reversed(range(num_stages))]\n\n        self.encoder = nn.ModuleList()\n        self.decoder = nn.ModuleList()\n\n        for i in range(num_stages):\n            enc_conv_block = []\n            if i != 0:\n                if strides[i] == 1 and downsamples[i - 1]:\n                    enc_conv_block.append(nn.MaxPool2d(kernel_size=2))\n                upsample = (strides[i] != 1 or downsamples[i - 1])\n                self.decoder.append(\n                    UpConvBlock(\n                        conv_block=BasicConvBlock,\n                        in_channels=out_channels * 2**i,\n                        skip_channels=out_channels * 2**(i - 1),\n                        out_channels=out_channels * 2**(i - 1),\n                        num_convs=dec_num_convs[i - 1],\n                        stride=1,\n                        dilation=dec_dilations[i - 1],\n                        with_cp=with_cp,\n                        conv_cfg=conv_cfg,\n                        norm_cfg=norm_cfg,\n                        act_cfg=act_cfg,\n                        upsample_cfg=upsample_cfg if upsample else None,\n                        dcn=None,\n                        plugins=None))\n\n            enc_conv_block.append(\n                BasicConvBlock(\n                    in_channels=in_channels,\n                    out_channels=out_channels * 2**i,\n                    num_convs=enc_num_convs[i],\n                    stride=strides[i],\n                    dilation=enc_dilations[i],\n                    with_cp=with_cp,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    act_cfg=act_cfg,\n                    dcn=None,\n                    plugins=None))\n            self.encoder.append((nn.Sequential(*enc_conv_block)))\n            in_channels = out_channels * 2**i\n\n    def forward(self, x):\n\n        # We can check just the first image, since the batch \n        # already was approved by the stackability test, which means\n        # all images has the same dimensions. \n        self._check_input_divisible(x[0])\n\n        enc_outs = []\n        for enc in self.encoder:\n            x = enc(x)\n            enc_outs.append(x)\n        dec_outs = [x]\n        for i in reversed(range(len(self.decoder))):\n            x = self.decoder[i](enc_outs[i], x)\n            dec_outs.append(x)\n        return dec_outs\n\n    def train(self, mode=True):\n        \"\"\"Convert the model into training mode while keep normalization layer\n        freezed.\"\"\"\n        super(UNet, self).train(mode)\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, _BatchNorm):\n                    m.eval()\n\n    def _check_input_divisible(self, x):\n        h, w = x.shape[-2:]\n        whole_downsample_rate = 1\n        for i in range(1, self.num_stages):\n            if self.strides[i] == 2 or self.downsamples[i - 1]:\n                whole_downsample_rate *= 2\n        assert (h % whole_downsample_rate == 0) \\\n            and (w % whole_downsample_rate == 0),\\\n            f'The input image size {(h, w)} should be divisible by the whole '\\\n            f'downsample rate {whole_downsample_rate}, when num_stages is '\\\n            f'{self.num_stages}, strides is {self.strides}, and downsamples '\\\n            f'is {self.downsamples}.'\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.unet.UNet.train","title":"<code>train(mode=True)</code>","text":"<p>Convert the model into training mode while keep normalization layer freezed.</p> Source code in <code>terratorch/models/backbones/unet.py</code> <pre><code>def train(self, mode=True):\n    \"\"\"Convert the model into training mode while keep normalization layer\n    freezed.\"\"\"\n    super(UNet, self).train(mode)\n    if mode and self.norm_eval:\n        for m in self.modules():\n            # trick: eval have effect on BatchNorm only\n            if isinstance(m, _BatchNorm):\n                m.eval()\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.mmearth_convnextv2.ConvNeXtV2","title":"<code>terratorch.models.backbones.mmearth_convnextv2.ConvNeXtV2</code>","text":"<p>               Bases: <code>Module</code></p> <p>ConvNeXt V2</p> <p>Parameters:</p> Name Type Description Default <code>in_chans</code> <code>int</code> <p>Number of input image channels. Default: 3</p> <code>3</code> <code>num_classes</code> <code>int</code> <p>Number of classes for classification head. Default: 1000</p> <code>1000</code> <code>depths</code> <code>tuple(int</code> <p>Number of blocks at each stage. Default: [3, 3, 9, 3]</p> <code>None</code> <code>dims</code> <code>int</code> <p>Feature dimension at each stage. Default: [96, 192, 384, 768]</p> <code>None</code> <code>drop_path_rate</code> <code>float</code> <p>Stochastic depth rate. Default: 0.</p> <code>0.0</code> <code>head_init_scale</code> <code>float</code> <p>Init scaling value for classifier weights and biases. Default: 1.</p> <code>1.0</code> Source code in <code>terratorch/models/backbones/mmearth_convnextv2.py</code> <pre><code>class ConvNeXtV2(nn.Module):\n    \"\"\"ConvNeXt V2\n\n    Args:\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n        drop_path_rate (float): Stochastic depth rate. Default: 0.\n        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        patch_size: int = 32,\n        img_size: int = 128,\n        in_chans: int = 3,\n        num_classes: int = 1000,\n        depths: list[int] = None,\n        dims: list[int] = None,\n        drop_path_rate: float = 0.0,\n        head_init_scale: float = 1.0,\n        use_orig_stem: bool = False,\n        args: Namespace = None,\n    ):\n        super().__init__()\n        self.depths = depths\n        if self.depths is None:  # set default value\n            self.depths = [3, 3, 9, 3]\n        self.img_size = img_size\n        self.use_orig_stem = use_orig_stem\n        self.num_stage = len(depths)\n        self.downsample_layers = (\n            nn.ModuleList()\n        )  # stem and 3 intermediate downsampling conv layer\n        self.patch_size = patch_size\n        if dims is None:\n            dims = [96, 192, 384, 768]\n\n        if self.use_orig_stem:\n            self.stem_orig = nn.Sequential(\n                nn.Conv2d(\n                    in_chans,\n                    dims[0],\n                    kernel_size=patch_size // (2 ** (self.num_stage - 1)),\n                    stride=patch_size // (2 ** (self.num_stage - 1)),\n                ),\n                LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"),\n            )\n        else:\n            self.initial_conv = nn.Sequential(\n                nn.Conv2d(in_chans, dims[0], kernel_size=3, stride=1),\n                LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"),\n                nn.GELU(),\n            )\n            # depthwise conv for stem\n            self.stem = nn.Sequential(\n                nn.Conv2d(\n                    dims[0],\n                    dims[0],\n                    kernel_size=patch_size // (2 ** (self.num_stage - 1)),\n                    stride=patch_size // (2 ** (self.num_stage - 1)),\n                    padding=(patch_size // (2 ** (self.num_stage - 1))) // 2,\n                    groups=dims[0],\n                ),\n                LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"),\n            )\n\n        for i in range(3):\n            downsample_layer = nn.Sequential(\n                LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n                nn.Conv2d(dims[i], dims[i + 1], kernel_size=2, stride=2),\n            )\n            self.downsample_layers.append(downsample_layer)\n\n        self.stages = (\n            nn.ModuleList()\n        )  # 4 feature resolution stages, each consisting of multiple residual blocks\n        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n        cur = 0\n        for i in range(self.num_stage):\n            stage = nn.Sequential(\n                *[\n                    Block(dim=dims[i], drop_path=dp_rates[cur + j])\n                    for j in range(depths[i])\n                ]\n            )\n            self.stages.append(stage)\n            cur += depths[i]\n\n        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final norm layer\n        self.head = nn.Linear(dims[-1], num_classes)\n\n        self.apply(self._init_weights)\n        self.head.weight.data.mul_(head_init_scale)\n        self.head.bias.data.mul_(head_init_scale)\n\n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.Linear)):\n            trunc_normal_(m.weight, std=0.02)\n            nn.init.constant_(m.bias, 0)\n\n    def forward_features(self, x):\n        if self.use_orig_stem:\n            x = self.stem_orig(x)\n        else:\n            x = self.initial_conv(x)\n            x = self.stem(x)\n\n        x = self.stages[0](x)\n        for i in range(3):\n            x = self.downsample_layers[i](x)\n            x = self.stages[i + 1](x)\n\n        return self.norm(\n            x.mean([-2, -1])\n        )  # global average pooling, (N, C, H, W) -&gt; (N, C)\n\n    def upsample_mask(self, mask, scale):\n        assert len(mask.shape) == 2\n        p = int(mask.shape[1] ** 0.5)\n        return (\n            mask.reshape(-1, p, p)\n            .repeat_interleave(scale, axis=1)\n            .repeat_interleave(scale, axis=2)\n        )\n\n    def forward(self, x: Tensor, mask: Tensor = None) -&gt; Tensor:\n        if mask is not None:  # for the pretraining case\n            num_patches = mask.shape[1]\n            scale = int(self.img_size // (num_patches**0.5))\n            mask = self.upsample_mask(mask, scale)\n\n            mask = mask.unsqueeze(1).type_as(x)\n            x *= 1.0 - mask\n            if self.use_orig_stem:\n                x = self.stem_orig(x)\n            else:\n                x = self.initial_conv(x)\n                x = self.stem(x)\n\n            x = self.stages[0](x)\n            for i in range(3):\n                x = self.downsample_layers[i](x)\n                x = self.stages[i + 1](x)\n            return x\n\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.dofa_vit.DOFAEncoderWrapper","title":"<code>terratorch.models.backbones.dofa_vit.DOFAEncoderWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>A wrapper for DOFA models from torchgeo to return only the forward pass of the encoder  Attributes:     dofa_model (DOFA): The instantiated dofa model Methods:     forward(x: List[torch.Tensor], wavelengths: list[float]) -&gt; torch.Tensor:         Forward pass for embeddings with specified indices.</p> Source code in <code>terratorch/models/backbones/dofa_vit.py</code> <pre><code>class DOFAEncoderWrapper(nn.Module):\n\n    \"\"\"\n    A wrapper for DOFA models from torchgeo to return only the forward pass of the encoder \n    Attributes:\n        dofa_model (DOFA): The instantiated dofa model\n    Methods:\n        forward(x: List[torch.Tensor], wavelengths: list[float]) -&gt; torch.Tensor:\n            Forward pass for embeddings with specified indices.\n    \"\"\"\n\n    def __init__(self, dofa_model, wavelengths, weights=None, out_indices=None) -&gt; None:\n        \"\"\"\n        Args:\n            dofa_model (DOFA): The decoder module to be wrapped.\n            weights ()\n        \"\"\"\n        super().__init__()\n        self.dofa_model = dofa_model\n        self.weights = weights\n        self.wavelengths = wavelengths\n\n        self.out_indices = out_indices if out_indices else [-1]\n        self.out_channels = [self.dofa_model.patch_embed.embed_dim] * len(self.out_indices)\n\n    def forward(self, x: List[torch.Tensor], **kwargs) -&gt; torch.Tensor:\n        wavelist = torch.tensor(self.wavelengths, device=x.device).float()\n\n        x, _ = self.dofa_model.patch_embed(x, wavelist)\n        x = x + self.dofa_model.pos_embed[:, 1:, :]\n        # append cls token\n        cls_token = self.dofa_model.cls_token + self.dofa_model.pos_embed[:, :1, :]\n        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        outs = []\n        # apply Transformer blocks\n        for i, block in enumerate(self.dofa_model.blocks):\n            x = block(x)\n            if i in self.out_indices:\n                outs.append(x)\n            elif (i == (len(self.dofa_model.blocks)-1)) &amp; (-1 in self.out_indices):\n                outs.append(x)\n\n        return tuple(outs)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.dofa_vit.DOFAEncoderWrapper.__init__","title":"<code>__init__(dofa_model, wavelengths, weights=None, out_indices=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dofa_model</code> <code>DOFA</code> <p>The decoder module to be wrapped.</p> required Source code in <code>terratorch/models/backbones/dofa_vit.py</code> <pre><code>def __init__(self, dofa_model, wavelengths, weights=None, out_indices=None) -&gt; None:\n    \"\"\"\n    Args:\n        dofa_model (DOFA): The decoder module to be wrapped.\n        weights ()\n    \"\"\"\n    super().__init__()\n    self.dofa_model = dofa_model\n    self.weights = weights\n    self.wavelengths = wavelengths\n\n    self.out_indices = out_indices if out_indices else [-1]\n    self.out_channels = [self.dofa_model.patch_embed.embed_dim] * len(self.out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.clay_v1.embedder","title":"<code>terratorch.models.backbones.clay_v1.embedder</code>","text":""},{"location":"package/backbones/#terratorch.models.backbones.clay_v1.embedder.Embedder","title":"<code>Embedder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>terratorch/models/backbones/clay_v1/embedder.py</code> <pre><code>class Embedder(nn.Module):\n    default_out_indices = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\n\n    def __init__(\n        self,\n        img_size=256,\n        num_frames=1,\n        ckpt_path=None,\n        bands=[\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"],\n        out_indices: tuple[int] = default_out_indices,\n        vpt: bool = False,\n        vpt_n_tokens: int | None = None,\n        vpt_dropout: float = 0.0,\n        **kwargs,\n    ):\n        super().__init__()\n        self.feature_info = []\n        self.img_size = img_size\n        self.num_frames = num_frames\n        self.bands = bands\n        self.out_indices = out_indices\n\n        self.datacuber = Datacuber(bands=bands)\n\n        # TODO: add support for various clay versions\n        self.clay_encoder = (\n            EmbeddingEncoder(  # Default parameters for the Clay base model\n                img_size=img_size,\n                patch_size=8,\n                dim=768,\n                depth=12,\n                heads=12,\n                dim_head=64,\n                mlp_ratio=4.0,\n                vpt=vpt,\n                vpt_n_tokens=vpt_n_tokens,\n                vpt_dropout=vpt_dropout,\n            )\n        )\n\n        # for use in features list.\n        for i in range(12):\n            self.feature_info.append({\"num_chs\": 768, \"reduction\": 1, \"module\": f\"blocks.{i}\"})\n\n        # assuming this is used to fine tune a network on top of the embeddings\n\n        if ckpt_path:\n            self.load_clay_weights(ckpt_path)\n\n    def load_clay_weights(self, ckpt_path):\n        \"Load the weights from the Clay model encoder.\"\n        ckpt = torch.load(ckpt_path, weights_only=True)\n        state_dict = ckpt.get(\"state_dict\")\n        state_dict = {\n            re.sub(r\"^model\\.encoder\\.\", \"\", name): param\n            for name, param in state_dict.items()\n            if name.startswith(\"model.encoder\")\n        }\n\n        with torch.no_grad():\n            for name, param in self.clay_encoder.named_parameters():\n                if name in state_dict and param.size() == state_dict[name].size():\n                    param.data.copy_(state_dict[name])  # Copy the weights\n                else:\n                    print(\n                        f\"No matching parameter for {name} with size {param.size()}\")\n\n        for param in self.clay_encoder.parameters():\n            param.requires_grad = False\n\n        self.clay_encoder.eval()\n\n    @staticmethod\n    def transform_state_dict(state_dict, model):\n        state_dict = state_dict.get(\"state_dict\")\n        state_dict = {\n            re.sub(r\"^model\\.encoder\\.\", \"clay_encoder.\", name): param\n            for name, param in state_dict.items()\n            if name.startswith(\"model.encoder\")\n        }\n        for k, v in model.state_dict().items():\n            if \"vpt_prompt_embeddings\" in k:\n                state_dict[k] = v\n        return state_dict\n\n    def forward_features(\n        self,\n        x: torch.Tensor,\n        time: torch.Tensor | None = None,\n        latlon: torch.Tensor | None = None,\n        waves: torch.Tensor | None = None,\n        gsd: float | None = None,\n    ):\n        datacube = self.datacuber(x=x, time=time, latlon=latlon, waves=waves, gsd=gsd)\n        embeddings = self.clay_encoder(datacube)\n\n        return [embeddings[i] for i in self.out_indices]\n\n    def fake_datacube(self):\n        \"Generate a fake datacube for model export.\"\n        dummy_datacube = {\n            \"pixels\": torch.randn(2, 3, self.img_size, self.img_size),\n            \"time\": torch.randn(2, 4),\n            \"latlon\": torch.randn(2, 4),\n            \"waves\": torch.randn(3),\n            \"gsd\": torch.randn(1),\n        }\n        dummy_datacube = {k: v\n                          for k, v in dummy_datacube.items()}\n        return dummy_datacube\n\n    def prepare_features_for_image_model(self, features: list[Tensor]) -&gt; list[Tensor]:\n        x_no_token = features[-1][:, 1:, :]\n        encoded = x_no_token.permute(0, 2, 1).reshape(\n            x_no_token.shape[0],\n            -1,\n            int(np.sqrt(x_no_token.shape[1] // self.num_frames)),\n            int(np.sqrt(x_no_token.shape[1] // self.num_frames)),\n        )\n\n        # return as list for features list compatibility\n        return [encoded]\n\n    def freeze(self):\n        for n, param in self.named_parameters():\n            if \"vpt_prompt_embeddings\" not in n:\n                param.requires_grad_(False)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.clay_v1.embedder.Embedder.fake_datacube","title":"<code>fake_datacube()</code>","text":"<p>Generate a fake datacube for model export.</p> Source code in <code>terratorch/models/backbones/clay_v1/embedder.py</code> <pre><code>def fake_datacube(self):\n    \"Generate a fake datacube for model export.\"\n    dummy_datacube = {\n        \"pixels\": torch.randn(2, 3, self.img_size, self.img_size),\n        \"time\": torch.randn(2, 4),\n        \"latlon\": torch.randn(2, 4),\n        \"waves\": torch.randn(3),\n        \"gsd\": torch.randn(1),\n    }\n    dummy_datacube = {k: v\n                      for k, v in dummy_datacube.items()}\n    return dummy_datacube\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.clay_v1.embedder.Embedder.load_clay_weights","title":"<code>load_clay_weights(ckpt_path)</code>","text":"<p>Load the weights from the Clay model encoder.</p> Source code in <code>terratorch/models/backbones/clay_v1/embedder.py</code> <pre><code>def load_clay_weights(self, ckpt_path):\n    \"Load the weights from the Clay model encoder.\"\n    ckpt = torch.load(ckpt_path, weights_only=True)\n    state_dict = ckpt.get(\"state_dict\")\n    state_dict = {\n        re.sub(r\"^model\\.encoder\\.\", \"\", name): param\n        for name, param in state_dict.items()\n        if name.startswith(\"model.encoder\")\n    }\n\n    with torch.no_grad():\n        for name, param in self.clay_encoder.named_parameters():\n            if name in state_dict and param.size() == state_dict[name].size():\n                param.data.copy_(state_dict[name])  # Copy the weights\n            else:\n                print(\n                    f\"No matching parameter for {name} with size {param.size()}\")\n\n    for param in self.clay_encoder.parameters():\n        param.requires_grad = False\n\n    self.clay_encoder.eval()\n</code></pre>"},{"location":"package/backbones/#apis-for-external-models","title":"APIs for External Models","text":"<p>Tip</p> <p>You find a detailed overview of all models in the TorchGeo documentation. </p>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_vit","title":"<code>terratorch.models.backbones.torchgeo_vit</code>","text":""},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_vit.ViTEncoderWrapper","title":"<code>ViTEncoderWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>A wrapper for ViT models from torchgeo to return only the forward pass of the encoder  Attributes:     satlas_model (VisionTransformer): The instantiated dofa model     weights Methods:     forward(x: List[torch.Tensor], wavelengths: list[float]) -&gt; torch.Tensor:         Forward pass for embeddings with specified indices.</p> Source code in <code>terratorch/models/backbones/torchgeo_vit.py</code> <pre><code>class ViTEncoderWrapper(nn.Module):\n\n    \"\"\"\n    A wrapper for ViT models from torchgeo to return only the forward pass of the encoder \n    Attributes:\n        satlas_model (VisionTransformer): The instantiated dofa model\n        weights\n    Methods:\n        forward(x: List[torch.Tensor], wavelengths: list[float]) -&gt; torch.Tensor:\n            Forward pass for embeddings with specified indices.\n    \"\"\"\n\n    def __init__(self, vit_model, vit_meta, weights=None, out_indices=None) -&gt; None:\n        \"\"\"\n        Args:\n            dofa_model (DOFA): The decoder module to be wrapped.\n            weights ()\n        \"\"\"\n        super().__init__()\n        self.vit_model = vit_model\n        self.weights = weights\n        self.out_channels = [x['num_chs'] for x in self.vit_model.feature_info]\n        self.vit_meta = vit_meta\n\n\n    def forward(self, x: List[torch.Tensor]) -&gt; torch.Tensor:\n        return self.vit_model.forward_intermediates(x, intermediates_only=True)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_vit.ViTEncoderWrapper.__init__","title":"<code>__init__(vit_model, vit_meta, weights=None, out_indices=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dofa_model</code> <code>DOFA</code> <p>The decoder module to be wrapped.</p> required Source code in <code>terratorch/models/backbones/torchgeo_vit.py</code> <pre><code>def __init__(self, vit_model, vit_meta, weights=None, out_indices=None) -&gt; None:\n    \"\"\"\n    Args:\n        dofa_model (DOFA): The decoder module to be wrapped.\n        weights ()\n    \"\"\"\n    super().__init__()\n    self.vit_model = vit_model\n    self.weights = weights\n    self.out_channels = [x['num_chs'] for x in self.vit_model.feature_info]\n    self.vit_meta = vit_meta\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_etm_sr_moco","title":"<code>ssl4eol_vit_small_patch16_224_landsat_etm_sr_moco(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_ETM_SR_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_vit.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_vit_small_patch16_224_landsat_etm_sr_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ViTSmall16_Weights.LANDSAT_ETM_SR_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = vit_small_patch16_224(**kwargs)\n    if pretrained:\n        model = load_vit_weights(model, model_bands, ckpt_data, weights)\n    return ViTEncoderWrapper(model, vit_s_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_etm_sr_simclr","title":"<code>ssl4eol_vit_small_patch16_224_landsat_etm_sr_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_ETM_SR_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_vit.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_vit_small_patch16_224_landsat_etm_sr_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None =  ViTSmall16_Weights.LANDSAT_ETM_SR_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = vit_small_patch16_224(**kwargs)\n    if pretrained:\n        model = load_vit_weights(model, model_bands, ckpt_data, weights)\n    return ViTEncoderWrapper(model, vit_s_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_etm_toa_moco","title":"<code>ssl4eol_vit_small_patch16_224_landsat_etm_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_ETM_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_vit.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_vit_small_patch16_224_landsat_etm_toa_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ViTSmall16_Weights.LANDSAT_ETM_TOA_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = vit_small_patch16_224(**kwargs)\n    if pretrained:\n        model = load_vit_weights(model, model_bands, ckpt_data, weights)\n    return ViTEncoderWrapper(model, vit_s_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_etm_toa_simclr","title":"<code>ssl4eol_vit_small_patch16_224_landsat_etm_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_ETM_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_vit.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_vit_small_patch16_224_landsat_etm_toa_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ViTSmall16_Weights.LANDSAT_ETM_TOA_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = vit_small_patch16_224(**kwargs)\n    if pretrained:\n        model = load_vit_weights(model, model_bands, ckpt_data, weights)\n    return ViTEncoderWrapper(model, vit_s_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_oli_sr_moco","title":"<code>ssl4eol_vit_small_patch16_224_landsat_oli_sr_moco(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_OLI_SR_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_vit.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_vit_small_patch16_224_landsat_oli_sr_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None =  ViTSmall16_Weights.LANDSAT_OLI_SR_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = vit_small_patch16_224(**kwargs)\n    if pretrained:\n        model = load_vit_weights(model, model_bands, ckpt_data, weights)\n    return ViTEncoderWrapper(model, vit_s_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_oli_sr_simclr","title":"<code>ssl4eol_vit_small_patch16_224_landsat_oli_sr_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_OLI_SR_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_vit.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_vit_small_patch16_224_landsat_oli_sr_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None =   ViTSmall16_Weights.LANDSAT_OLI_SR_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = vit_small_patch16_224(**kwargs)\n    if pretrained:\n        model = load_vit_weights(model, model_bands, ckpt_data, weights)\n    return ViTEncoderWrapper(model, vit_s_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_oli_tirs_toa_simclr","title":"<code>ssl4eol_vit_small_patch16_224_landsat_oli_tirs_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_OLI_TIRS_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_vit.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_vit_small_patch16_224_landsat_oli_tirs_toa_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None =  ViTSmall16_Weights.LANDSAT_OLI_TIRS_TOA_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = vit_small_patch16_224(**kwargs)\n    if pretrained:\n        model = load_vit_weights(model, model_bands, ckpt_data, weights)\n    return ViTEncoderWrapper(model, vit_s_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_tm_toa_moco","title":"<code>ssl4eol_vit_small_patch16_224_landsat_tm_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_TM_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_vit.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_vit_small_patch16_224_landsat_tm_toa_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ViTSmall16_Weights.LANDSAT_TM_TOA_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = vit_small_patch16_224(**kwargs)\n    if pretrained:\n        model = load_vit_weights(model, model_bands, ckpt_data, weights)\n    return ViTEncoderWrapper(model, vit_s_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_tm_toa_simclr","title":"<code>ssl4eol_vit_small_patch16_224_landsat_tm_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_TM_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_vit.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_vit_small_patch16_224_landsat_tm_toa_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ViTSmall16_Weights.LANDSAT_TM_TOA_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = vit_small_patch16_224(**kwargs)\n    if pretrained:\n        model = load_vit_weights(model, model_bands, ckpt_data, weights)\n    return ViTEncoderWrapper(model, vit_s_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eos12_vit_small_patch16_224_sentinel2_all_dino","title":"<code>ssl4eos12_vit_small_patch16_224_sentinel2_all_dino(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.SENTINEL2_ALL_DINO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_vit.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eos12_vit_small_patch16_224_sentinel2_all_dino(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None =   ViTSmall16_Weights.SENTINEL2_ALL_DINO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = vit_small_patch16_224(**kwargs)\n    if pretrained:\n        model = load_vit_weights(model, model_bands, ckpt_data, weights)\n    return ViTEncoderWrapper(model, vit_s_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eos12_vit_small_patch16_224_sentinel2_all_moco","title":"<code>ssl4eos12_vit_small_patch16_224_sentinel2_all_moco(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.SENTINEL2_ALL_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_vit.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eos12_vit_small_patch16_224_sentinel2_all_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None =   ViTSmall16_Weights.SENTINEL2_ALL_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = vit_small_patch16_224(**kwargs)\n    if pretrained:\n        model = load_vit_weights(model, model_bands, ckpt_data, weights)\n    return ViTEncoderWrapper(model, vit_s_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet","title":"<code>terratorch.models.backbones.torchgeo_resnet</code>","text":""},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ResNetEncoderWrapper","title":"<code>ResNetEncoderWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>A wrapper for ViT models from torchgeo to return only the forward pass of the encoder  Attributes:     satlas_model (VisionTransformer): The instantiated dofa model     weights Methods:     forward(x: List[torch.Tensor], wavelengths: list[float]) -&gt; torch.Tensor:         Forward pass for embeddings with specified indices.</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>class ResNetEncoderWrapper(nn.Module):\n\n    \"\"\"\n    A wrapper for ViT models from torchgeo to return only the forward pass of the encoder \n    Attributes:\n        satlas_model (VisionTransformer): The instantiated dofa model\n        weights\n    Methods:\n        forward(x: List[torch.Tensor], wavelengths: list[float]) -&gt; torch.Tensor:\n            Forward pass for embeddings with specified indices.\n    \"\"\"\n\n    def __init__(self, resnet_model, resnet_meta, weights=None, out_indices=None) -&gt; None:\n        \"\"\"\n        Args:\n            dofa_model (DOFA): The decoder module to be wrapped.\n            weights ()\n        \"\"\"\n        super().__init__()\n        self.resnet_model = resnet_model\n        self.resnet_meta = resnet_meta\n        self.weights = weights\n        self.out_indices = out_indices if out_indices else [-1]\n        self.out_channels = [x['num_chs'] for x in self.resnet_model.feature_info]\n        self.resnet_meta['original_out_channels'] = self.out_channels\n        self.out_channels = [x for i, x in enumerate(self.out_channels) if (i in self.out_indices) | (i == (len(self.out_channels)-1)) &amp; (-1 in self.out_indices)]\n\n\n    def forward(self, x: List[torch.Tensor], **kwargs) -&gt; torch.Tensor:\n\n        features = self.resnet_model.forward_intermediates(x, intermediates_only=True)\n\n        outs = []\n        for i, feature in enumerate(features):\n            if i in self.out_indices:\n                outs.append(feature)\n            elif (i == (len(self.resnet_meta[\"original_out_channels\"])-1)) &amp; (-1 in self.out_indices):\n                outs.append(feature)\n\n        return outs\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ResNetEncoderWrapper.__init__","title":"<code>__init__(resnet_model, resnet_meta, weights=None, out_indices=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dofa_model</code> <code>DOFA</code> <p>The decoder module to be wrapped.</p> required Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>def __init__(self, resnet_model, resnet_meta, weights=None, out_indices=None) -&gt; None:\n    \"\"\"\n    Args:\n        dofa_model (DOFA): The decoder module to be wrapped.\n        weights ()\n    \"\"\"\n    super().__init__()\n    self.resnet_model = resnet_model\n    self.resnet_meta = resnet_meta\n    self.weights = weights\n    self.out_indices = out_indices if out_indices else [-1]\n    self.out_channels = [x['num_chs'] for x in self.resnet_model.feature_info]\n    self.resnet_meta['original_out_channels'] = self.out_channels\n    self.out_channels = [x for i, x in enumerate(self.out_channels) if (i in self.out_indices) | (i == (len(self.out_channels)-1)) &amp; (-1 in self.out_indices)]\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.fmow_resnet50_fmow_rgb_gassl","title":"<code>fmow_resnet50_fmow_rgb_gassl(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.FMOW_RGB_GASSL, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef fmow_resnet50_fmow_rgb_gassl(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.FMOW_RGB_GASSL, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet152_sentinel2_mi_ms","title":"<code>satlas_resnet152_sentinel2_mi_ms(model_bands, pretrained=False, ckpt_data=None, weights=ResNet152_Weights.SENTINEL2_MI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_resnet152_sentinel2_mi_ms(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None =  ResNet152_Weights.SENTINEL2_MI_MS_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet152(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet152_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet152_sentinel2_mi_rgb","title":"<code>satlas_resnet152_sentinel2_mi_rgb(model_bands, pretrained=False, ckpt_data=None, weights=ResNet152_Weights.SENTINEL2_MI_RGB_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_resnet152_sentinel2_mi_rgb(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet152_Weights.SENTINEL2_MI_RGB_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet152(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet152_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet152_sentinel2_si_ms_satlas","title":"<code>satlas_resnet152_sentinel2_si_ms_satlas(model_bands, pretrained=False, ckpt_data=None, weights=ResNet152_Weights.SENTINEL2_SI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_resnet152_sentinel2_si_ms_satlas(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet152_Weights.SENTINEL2_SI_MS_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet152(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet152_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet152_sentinel2_si_rgb_satlas","title":"<code>satlas_resnet152_sentinel2_si_rgb_satlas(model_bands, pretrained=False, ckpt_data=None, weights=ResNet152_Weights.SENTINEL2_SI_RGB_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_resnet152_sentinel2_si_rgb_satlas(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None =  ResNet152_Weights.SENTINEL2_SI_RGB_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet152(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet152_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet50_sentinel2_mi_ms_satlas","title":"<code>satlas_resnet50_sentinel2_mi_ms_satlas(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_MI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_resnet50_sentinel2_mi_ms_satlas(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.SENTINEL2_MI_MS_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet50_sentinel2_mi_rgb_satlas","title":"<code>satlas_resnet50_sentinel2_mi_rgb_satlas(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_MI_RGB_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_resnet50_sentinel2_mi_rgb_satlas(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.SENTINEL2_MI_RGB_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet50_sentinel2_si_ms_satlas","title":"<code>satlas_resnet50_sentinel2_si_ms_satlas(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_SI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_resnet50_sentinel2_si_ms_satlas(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.SENTINEL2_SI_MS_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet50_sentinel2_si_rgb_satlas","title":"<code>satlas_resnet50_sentinel2_si_rgb_satlas(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_SI_RGB_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_resnet50_sentinel2_si_rgb_satlas(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.SENTINEL2_SI_RGB_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.seco_resnet18_sentinel2_rgb_seco","title":"<code>seco_resnet18_sentinel2_rgb_seco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.SENTINEL2_RGB_SECO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef seco_resnet18_sentinel2_rgb_seco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet18_Weights.SENTINEL2_RGB_SECO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet18(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet18_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.seco_resnet50_sentinel2_rgb_seco","title":"<code>seco_resnet50_sentinel2_rgb_seco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_RGB_SECO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef seco_resnet50_sentinel2_rgb_seco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.SENTINEL2_RGB_SECO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_etm_sr_moco","title":"<code>ssl4eol_resnet18_landsat_etm_sr_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_ETM_SR_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet18_landsat_etm_sr_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet18_Weights.LANDSAT_ETM_SR_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet18(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet18_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_etm_sr_simclr","title":"<code>ssl4eol_resnet18_landsat_etm_sr_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_ETM_SR_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet18_landsat_etm_sr_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet18_Weights.LANDSAT_ETM_SR_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet18(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet18_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_etm_toa_moco","title":"<code>ssl4eol_resnet18_landsat_etm_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_ETM_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet18_landsat_etm_toa_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet18_Weights.LANDSAT_ETM_TOA_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet18(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet18_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_etm_toa_simclr","title":"<code>ssl4eol_resnet18_landsat_etm_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_ETM_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet18_landsat_etm_toa_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet18_Weights.LANDSAT_ETM_TOA_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet18(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet18_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_oli_sr_moco","title":"<code>ssl4eol_resnet18_landsat_oli_sr_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_OLI_SR_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet18_landsat_oli_sr_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet18_Weights.LANDSAT_OLI_SR_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet18(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet18_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_oli_sr_simclr","title":"<code>ssl4eol_resnet18_landsat_oli_sr_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_OLI_SR_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet18_landsat_oli_sr_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet18_Weights.LANDSAT_OLI_SR_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet18(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet18_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_oli_tirs_toa_moco","title":"<code>ssl4eol_resnet18_landsat_oli_tirs_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_OLI_TIRS_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet18_landsat_oli_tirs_toa_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet18_Weights.LANDSAT_OLI_TIRS_TOA_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet18(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet18_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_oli_tirs_toa_simclr","title":"<code>ssl4eol_resnet18_landsat_oli_tirs_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_OLI_TIRS_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet18_landsat_oli_tirs_toa_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet18_Weights.LANDSAT_OLI_TIRS_TOA_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet18(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet18_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_tm_toa_moco","title":"<code>ssl4eol_resnet18_landsat_tm_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_TM_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet18_landsat_tm_toa_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet18_Weights.LANDSAT_TM_TOA_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet18(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet18_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_tm_toa_simclr","title":"<code>ssl4eol_resnet18_landsat_tm_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_TM_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet18_landsat_tm_toa_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet18_Weights.LANDSAT_TM_TOA_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet18(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet18_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_etm_sr_moco","title":"<code>ssl4eol_resnet50_landsat_etm_sr_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_ETM_SR_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet50_landsat_etm_sr_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.LANDSAT_ETM_SR_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_etm_sr_simclr","title":"<code>ssl4eol_resnet50_landsat_etm_sr_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_ETM_SR_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet50_landsat_etm_sr_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.LANDSAT_ETM_SR_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_etm_toa_moco","title":"<code>ssl4eol_resnet50_landsat_etm_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_ETM_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet50_landsat_etm_toa_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.LANDSAT_ETM_TOA_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_etm_toa_simclr","title":"<code>ssl4eol_resnet50_landsat_etm_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_ETM_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet50_landsat_etm_toa_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.LANDSAT_ETM_TOA_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_oli_sr_moco","title":"<code>ssl4eol_resnet50_landsat_oli_sr_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_OLI_SR_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet50_landsat_oli_sr_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.LANDSAT_OLI_SR_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_oli_sr_simclr","title":"<code>ssl4eol_resnet50_landsat_oli_sr_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_OLI_SR_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet50_landsat_oli_sr_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.LANDSAT_OLI_SR_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_oli_tirs_toa_moco","title":"<code>ssl4eol_resnet50_landsat_oli_tirs_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_OLI_TIRS_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet50_landsat_oli_tirs_toa_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.LANDSAT_OLI_TIRS_TOA_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_oli_tirs_toa_simclr","title":"<code>ssl4eol_resnet50_landsat_oli_tirs_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_OLI_TIRS_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet50_landsat_oli_tirs_toa_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.LANDSAT_OLI_TIRS_TOA_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_tm_toa_moco","title":"<code>ssl4eol_resnet50_landsat_tm_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_TM_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet50_landsat_tm_toa_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.LANDSAT_TM_TOA_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_tm_toa_simclr","title":"<code>ssl4eol_resnet50_landsat_tm_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_TM_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eol_resnet50_landsat_tm_toa_simclr(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.LANDSAT_TM_TOA_SIMCLR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet18_sentinel2_all_moco","title":"<code>ssl4eos12_resnet18_sentinel2_all_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.SENTINEL2_ALL_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eos12_resnet18_sentinel2_all_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None =  ResNet18_Weights.SENTINEL2_ALL_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet18(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet18_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet18_sentinel2_rgb_moco","title":"<code>ssl4eos12_resnet18_sentinel2_rgb_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.SENTINEL2_RGB_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eos12_resnet18_sentinel2_rgb_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet18_Weights.SENTINEL2_RGB_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet18(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet18_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet50_sentinel1_all_decur","title":"<code>ssl4eos12_resnet50_sentinel1_all_decur(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL1_ALL_DECUR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eos12_resnet50_sentinel1_all_decur(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.SENTINEL1_ALL_DECUR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        if weights is not None:\n            weights.meta['bands'] = ['VV', 'VH']\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet50_sentinel1_all_moco","title":"<code>ssl4eos12_resnet50_sentinel1_all_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL1_ALL_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eos12_resnet50_sentinel1_all_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None =  ResNet50_Weights.SENTINEL1_ALL_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        if weights is not None:\n            weights.meta['bands'] = ['VV', 'VH']\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet50_sentinel2_all_decur","title":"<code>ssl4eos12_resnet50_sentinel2_all_decur(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_ALL_DECUR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eos12_resnet50_sentinel2_all_decur(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.SENTINEL2_ALL_DECUR, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        if weights is not None:\n            weights.meta['bands'] = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12']\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet50_sentinel2_all_dino","title":"<code>ssl4eos12_resnet50_sentinel2_all_dino(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_ALL_DINO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eos12_resnet50_sentinel2_all_dino(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.SENTINEL2_ALL_DINO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        if weights is not None:\n            weights.meta['bands'] = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12']\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet50_sentinel2_all_moco","title":"<code>ssl4eos12_resnet50_sentinel2_all_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_ALL_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eos12_resnet50_sentinel2_all_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.SENTINEL2_ALL_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        if weights is not None:\n            weights.meta['bands'] = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B10', 'B11', 'B12']\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet50_sentinel2_rgb_moco","title":"<code>ssl4eos12_resnet50_sentinel2_rgb_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_RGB_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_resnet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef ssl4eos12_resnet50_sentinel2_rgb_moco(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = ResNet50_Weights.SENTINEL2_RGB_MOCO, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        ViTEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = resnet50(**kwargs)\n    if pretrained:\n        model = load_resnet_weights(model, model_bands, ckpt_data, weights)\n    return ResNetEncoderWrapper(model, resnet50_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas","title":"<code>terratorch.models.backbones.torchgeo_swin_satlas</code>","text":""},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.SwinEncoderWrapper","title":"<code>SwinEncoderWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>A wrapper for Satlas models from torchgeo to return only the forward pass of the encoder  Attributes:     swin_model (SwinTransformer): The instantiated dofa model     weights Methods:     forward(x: List[torch.Tensor], wavelengths: list[float]) -&gt; torch.Tensor:         Forward pass for embeddings with specified indices.</p> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>class SwinEncoderWrapper(nn.Module):\n\n    \"\"\"\n    A wrapper for Satlas models from torchgeo to return only the forward pass of the encoder \n    Attributes:\n        swin_model (SwinTransformer): The instantiated dofa model\n        weights\n    Methods:\n        forward(x: List[torch.Tensor], wavelengths: list[float]) -&gt; torch.Tensor:\n            Forward pass for embeddings with specified indices.\n    \"\"\"\n\n    def __init__(self, swin_model, swin_meta, weights=None, out_indices=None) -&gt; None:\n        \"\"\"\n        Args:\n            swin_model (SwinTransformer): The backbone module to be wrapped.\n            swin_meta (dict): dict containing the metadata for swin.\n            weights (Weights): Weights class for the swin model to be wrapped.\n            out_indices (list): List containing the feature indices to be returned.\n        \"\"\"\n        super().__init__()\n        self.swin_model = swin_model\n        self.weights = weights\n        self.out_indices = out_indices if out_indices else [-1]\n\n        self.out_channels = []\n        for i in range(len(swin_meta[\"depths\"])):\n            self.out_channels.append(swin_meta[\"embed_dim\"] * 2**i)\n        self.out_channels = [elem for elem in self.out_channels for _ in range(2)]\n        self.out_channels = [x for i, x in enumerate(self.out_channels) if (i in self.out_indices) | (i == (len(self.out_channels)-1)) &amp; (-1 in self.out_indices)]\n\n    def forward(self, x: List[torch.Tensor], **kwargs) -&gt; torch.Tensor:\n\n        outs = []\n        for i, layer in enumerate(self.swin_model.features):\n            x = layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n            elif (i == (len(self.swin_model.features)-1)) &amp; (-1 in self.out_indices):\n                outs.append(x)\n\n        return tuple(outs)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.SwinEncoderWrapper.__init__","title":"<code>__init__(swin_model, swin_meta, weights=None, out_indices=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>swin_model</code> <code>SwinTransformer</code> <p>The backbone module to be wrapped.</p> required <code>swin_meta</code> <code>dict</code> <p>dict containing the metadata for swin.</p> required <code>weights</code> <code>Weights</code> <p>Weights class for the swin model to be wrapped.</p> <code>None</code> <code>out_indices</code> <code>list</code> <p>List containing the feature indices to be returned.</p> <code>None</code> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>def __init__(self, swin_model, swin_meta, weights=None, out_indices=None) -&gt; None:\n    \"\"\"\n    Args:\n        swin_model (SwinTransformer): The backbone module to be wrapped.\n        swin_meta (dict): dict containing the metadata for swin.\n        weights (Weights): Weights class for the swin model to be wrapped.\n        out_indices (list): List containing the feature indices to be returned.\n    \"\"\"\n    super().__init__()\n    self.swin_model = swin_model\n    self.weights = weights\n    self.out_indices = out_indices if out_indices else [-1]\n\n    self.out_channels = []\n    for i in range(len(swin_meta[\"depths\"])):\n        self.out_channels.append(swin_meta[\"embed_dim\"] * 2**i)\n    self.out_channels = [elem for elem in self.out_channels for _ in range(2)]\n    self.out_channels = [x for i, x in enumerate(self.out_channels) if (i in self.out_indices) | (i == (len(self.out_channels)-1)) &amp; (-1 in self.out_indices)]\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_landsat_mi_ms","title":"<code>satlas_swin_b_landsat_mi_ms(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.LANDSAT_MI_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_swin_b_landsat_mi_ms(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = Swin_V2_B_Weights.LANDSAT_MI_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        SwinEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = load_model(swin_v2_b, swin_v2_b_meta, **kwargs)\n    if pretrained:\n        model = load_swin_weights(model, model_bands, ckpt_data, weights)\n    return SwinEncoderWrapper(model, swin_v2_b_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_landsat_mi_rgb","title":"<code>satlas_swin_b_landsat_mi_rgb(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.LANDSAT_SI_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_swin_b_landsat_mi_rgb(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = Swin_V2_B_Weights.LANDSAT_SI_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        SwinEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = load_model(swin_v2_b, swin_v2_b_meta, **kwargs)\n    if pretrained:\n        model = load_swin_weights(model, model_bands, ckpt_data, weights)\n    return SwinEncoderWrapper(model, swin_v2_b_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_naip_mi_rgb","title":"<code>satlas_swin_b_naip_mi_rgb(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.NAIP_RGB_MI_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_swin_b_naip_mi_rgb(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = Swin_V2_B_Weights.NAIP_RGB_MI_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        SwinEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = load_model(swin_v2_b, swin_v2_b_meta, **kwargs)\n    if pretrained:\n        model = load_swin_weights(model, model_bands, ckpt_data, weights)\n    return SwinEncoderWrapper(model, swin_v2_b_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_naip_si_rgb","title":"<code>satlas_swin_b_naip_si_rgb(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.NAIP_RGB_SI_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_swin_b_naip_si_rgb(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = Swin_V2_B_Weights.NAIP_RGB_SI_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        SwinEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = load_model(swin_v2_b, swin_v2_b_meta, **kwargs)\n    if pretrained:\n        model = load_swin_weights(model, model_bands, ckpt_data, weights)\n    return SwinEncoderWrapper(model, swin_v2_b_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_sentinel1_mi","title":"<code>satlas_swin_b_sentinel1_mi(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.SENTINEL1_MI_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_swin_b_sentinel1_mi(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = Swin_V2_B_Weights.SENTINEL1_MI_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        SwinEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = load_model(swin_v2_b, swin_v2_b_meta, **kwargs)\n    if pretrained:\n        model = load_swin_weights(model, model_bands, ckpt_data, weights)\n    return SwinEncoderWrapper(model, swin_v2_b_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_sentinel1_si","title":"<code>satlas_swin_b_sentinel1_si(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.SENTINEL1_SI_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_swin_b_sentinel1_si(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = Swin_V2_B_Weights.SENTINEL1_SI_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        SwinEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = load_model(swin_v2_b, swin_v2_b_meta, **kwargs)\n    if pretrained:\n        model = load_swin_weights(model, model_bands, ckpt_data, weights)\n    return SwinEncoderWrapper(model, swin_v2_b_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_sentinel2_mi_ms","title":"<code>satlas_swin_b_sentinel2_mi_ms(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.SENTINEL2_MI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_swin_b_sentinel2_mi_ms(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = Swin_V2_B_Weights.SENTINEL2_MI_MS_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        SwinEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = load_model(swin_v2_b, swin_v2_b_meta, **kwargs)\n    if pretrained:\n        model = load_swin_weights(model, model_bands, ckpt_data, weights)\n    return SwinEncoderWrapper(model, swin_v2_b_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_sentinel2_si_ms","title":"<code>satlas_swin_b_sentinel2_si_ms(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.SENTINEL2_SI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_swin_b_sentinel2_si_ms(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = Swin_V2_B_Weights.SENTINEL2_SI_MS_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        SwinEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = load_model(swin_v2_b, swin_v2_b_meta, **kwargs)\n    if pretrained:\n        model = load_swin_weights(model, model_bands, ckpt_data, weights)\n    return SwinEncoderWrapper(model, swin_v2_b_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_sentinel2_si_rgb","title":"<code>satlas_swin_b_sentinel2_si_rgb(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.SENTINEL2_SI_RGB_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_swin_b_sentinel2_si_rgb(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = Swin_V2_B_Weights.SENTINEL2_SI_RGB_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        SwinEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = load_model(swin_v2_b, swin_v2_b_meta, **kwargs)\n    if pretrained:\n        model = load_swin_weights(model, model_bands, ckpt_data, weights)\n    return SwinEncoderWrapper(model, swin_v2_b_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_t_sentinel2_mi_ms","title":"<code>satlas_swin_t_sentinel2_mi_ms(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_T_Weights.SENTINEL2_MI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_swin_t_sentinel2_mi_ms(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = Swin_V2_T_Weights.SENTINEL2_MI_MS_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        SwinEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = load_model(swin_v2_t, swin_v2_t_meta, **kwargs)\n    if pretrained:\n        model = load_swin_weights(model, model_bands, ckpt_data, weights)\n    return SwinEncoderWrapper(model, swin_v2_t_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_t_sentinel2_mi_rgb","title":"<code>satlas_swin_t_sentinel2_mi_rgb(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_T_Weights.SENTINEL2_MI_RGB_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_swin_t_sentinel2_mi_rgb(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = Swin_V2_T_Weights.SENTINEL2_MI_RGB_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        SwinEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = load_model(swin_v2_t, swin_v2_t_meta, **kwargs)\n    if pretrained:\n        model = load_swin_weights(model, model_bands, ckpt_data, weights)\n    return SwinEncoderWrapper(model, swin_v2_t_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_t_sentinel2_si_ms","title":"<code>satlas_swin_t_sentinel2_si_ms(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_T_Weights.SENTINEL2_SI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_swin_t_sentinel2_si_ms(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = Swin_V2_T_Weights.SENTINEL2_SI_MS_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        SwinEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = load_model(swin_v2_t, swin_v2_t_meta, **kwargs)\n    if pretrained:\n        model = load_swin_weights(model, model_bands, ckpt_data, weights)\n    return SwinEncoderWrapper(model, swin_v2_t_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_t_sentinel2_si_rgb","title":"<code>satlas_swin_t_sentinel2_si_rgb(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_T_Weights.SENTINEL2_SI_RGB_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p> Source code in <code>terratorch/models/backbones/torchgeo_swin_satlas.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\ndef satlas_swin_t_sentinel2_si_rgb(model_bands, pretrained = False, ckpt_data: str | None = None,  weights: Weights | None = Swin_V2_T_Weights.SENTINEL2_SI_RGB_SATLAS, out_indices: list | None = None, **kwargs):\n    \"\"\"\n    Args:\n        model_bands (list[str]): A list containing the names for the bands expected by the model.\n        pretrained (bool): The model is already pretrained (weights are available and can be restored) or not.\n        ckpt_data (str | None): Path for a checkpoint containing the model weights.\n    Returns:\n        SwinEncoderWrapper\n    \"\"\"\n\n    if \"in_chans\" not in kwargs: kwargs[\"in_chans\"] = len(model_bands)\n    model = load_model(swin_v2_t, swin_v2_t_meta, **kwargs)\n    if pretrained:\n        model = load_swin_weights(model, model_bands, ckpt_data, weights)\n    return SwinEncoderWrapper(model, swin_v2_t_meta, weights, out_indices)\n</code></pre>"},{"location":"package/backbones/#timm","title":"Timm","text":"<p>You can use any model from <code>timm</code> as a backbone. </p> <p>Tip</p> <p>List all available models with <code>timm.list_models</code> or filter by name using wildcards:</p> <pre><code>import timm\ntimm.list_models('vit*')\n</code></pre>"},{"location":"package/backbones/#timm.list_models","title":"<code>timm.list_models(filter='', module='', pretrained=False, exclude_filters='', name_matches_cfg=False, include_tags=None)</code>","text":"<p>Return list of available model names, sorted alphabetically</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>models - The sorted list of models</p> Example <p>model_list('gluon_resnet') -- returns all models starting with 'gluon_resnet' model_list('resnext*, 'resnet') -- returns all models with 'resnext' in 'resnet' module</p>"},{"location":"package/datamodules/","title":"Specific Datamodules","text":""},{"location":"package/datamodules/#terratorch.datamodules.torchgeo_data_module","title":"<code>terratorch.datamodules.torchgeo_data_module</code>","text":"<p>Ugly proxy objects so parsing config file works with transforms.</p> <p>These are necessary since, for LightningCLI to instantiate arguments as objects from the config, they must have type annotations</p> <p>In TorchGeo, <code>transforms</code> is passed in **kwargs, so it has no type annotations! To get around that, we create these wrappers that have transforms type annotated. They create the transforms and forward all method and attribute calls to the original TorchGeo datamodule.</p> <p>Additionally, TorchGeo datasets pass the data to the transforms callable as a dict, and as a tensor.</p> <p>Albumentations expects this data not as a dict but as different key-value arguments, and as numpy. We handle that conversion here.</p>"},{"location":"package/datamodules/#terratorch.datamodules.torchgeo_data_module.TorchGeoDataModule","title":"<code>TorchGeoDataModule</code>","text":"<p>               Bases: <code>GeoDataModule</code></p> <p>Proxy object for using Geo data modules defined by TorchGeo.</p> <p>Allows for transforms to be defined and passed using config files. The only reason this class exists is so that we can annotate the transforms argument with a type. This is required for lightningcli and config files. As such, all getattr and setattr will be redirected to the underlying class.</p> Source code in <code>terratorch/datamodules/torchgeo_data_module.py</code> <pre><code>class TorchGeoDataModule(GeoDataModule):\n    \"\"\"Proxy object for using Geo data modules defined by TorchGeo.\n\n    Allows for transforms to be defined and passed using config files.\n    The only reason this class exists is so that we can annotate the transforms argument with a type.\n    This is required for lightningcli and config files.\n    As such, all getattr and setattr will be redirected to the underlying class.\n    \"\"\"\n\n    def __init__(\n        self,\n        cls: type[GeoDataModule],\n        batch_size: int | None = None,\n        num_workers: int = 0,\n        transforms: None | list[BasicTransform] = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Constructor\n\n        Args:\n            cls (type[GeoDataModule]): TorchGeo DataModule class to be instantiated\n            batch_size (int | None, optional): batch_size. Defaults to None.\n            num_workers (int, optional): num_workers. Defaults to 0.\n            transforms (None | list[BasicTransform], optional): List of Albumentations Transforms.\n                Should enc with ToTensorV2. Defaults to None.\n            **kwargs (Any): Arguments passed to instantiate `cls`.\n        \"\"\"\n        if batch_size is not None:\n            kwargs[\"batch_size\"] = batch_size\n        if transforms is not None:\n            transforms_as_callable = albumentations_to_callable_with_dict(transforms)\n            kwargs[\"transforms\"] = build_callable_transform_from_torch_tensor(transforms_as_callable)\n        # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs)\n        self._proxy = cls(num_workers=num_workers, **kwargs)\n        super().__init__(self._proxy.dataset_class)  # dummy arg\n\n    @property\n    def collate_fn(self):\n        return self._proxy.collate_fn\n\n    @collate_fn.setter\n    def collate_fn(self, value):\n        self._proxy.collate_fn = value\n\n    @property\n    def patch_size(self):\n        return self._proxy.patch_size\n\n    @property\n    def length(self):\n        return self._proxy.length\n\n    def setup(self, stage: str):\n        return self._proxy.setup(stage)\n\n    def train_dataloader(self):\n        return self._proxy.train_dataloader()\n\n    def val_dataloader(self):\n        return self._proxy.val_dataloader()\n\n    def test_dataloader(self):\n        return self._proxy.test_dataloader()\n\n    def predict_dataloader(self):\n        return self._proxy.predict_dataloader()\n\n    def transfer_batch_to_device(self, batch, device, dataloader_idx):\n        return self._proxy.predict_dataloader(batch, device, dataloader_idx)\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.torchgeo_data_module.TorchGeoDataModule.__init__","title":"<code>__init__(cls, batch_size=None, num_workers=0, transforms=None, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[GeoDataModule]</code> <p>TorchGeo DataModule class to be instantiated</p> required <code>batch_size</code> <code>int | None</code> <p>batch_size. Defaults to None.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>num_workers. Defaults to 0.</p> <code>0</code> <code>transforms</code> <code>None | list[BasicTransform]</code> <p>List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Arguments passed to instantiate <code>cls</code>.</p> <code>{}</code> Source code in <code>terratorch/datamodules/torchgeo_data_module.py</code> <pre><code>def __init__(\n    self,\n    cls: type[GeoDataModule],\n    batch_size: int | None = None,\n    num_workers: int = 0,\n    transforms: None | list[BasicTransform] = None,\n    **kwargs: Any,\n):\n    \"\"\"Constructor\n\n    Args:\n        cls (type[GeoDataModule]): TorchGeo DataModule class to be instantiated\n        batch_size (int | None, optional): batch_size. Defaults to None.\n        num_workers (int, optional): num_workers. Defaults to 0.\n        transforms (None | list[BasicTransform], optional): List of Albumentations Transforms.\n            Should enc with ToTensorV2. Defaults to None.\n        **kwargs (Any): Arguments passed to instantiate `cls`.\n    \"\"\"\n    if batch_size is not None:\n        kwargs[\"batch_size\"] = batch_size\n    if transforms is not None:\n        transforms_as_callable = albumentations_to_callable_with_dict(transforms)\n        kwargs[\"transforms\"] = build_callable_transform_from_torch_tensor(transforms_as_callable)\n    # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs)\n    self._proxy = cls(num_workers=num_workers, **kwargs)\n    super().__init__(self._proxy.dataset_class)  # dummy arg\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.torchgeo_data_module.TorchNonGeoDataModule","title":"<code>TorchNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>Proxy object for using NonGeo data modules defined by TorchGeo.</p> <p>Allows for transforms to be defined and passed using config files. The only reason this class exists is so that we can annotate the transforms argument with a type. This is required for lightningcli and config files. As such, all getattr and setattr will be redirected to the underlying class.</p> Source code in <code>terratorch/datamodules/torchgeo_data_module.py</code> <pre><code>class TorchNonGeoDataModule(NonGeoDataModule):\n    \"\"\"Proxy object for using NonGeo data modules defined by TorchGeo.\n\n    Allows for transforms to be defined and passed using config files.\n    The only reason this class exists is so that we can annotate the transforms argument with a type.\n    This is required for lightningcli and config files.\n    As such, all getattr and setattr will be redirected to the underlying class.\n    \"\"\"\n\n    def __init__(\n        self,\n        cls: type[NonGeoDataModule],\n        batch_size: int | None = None,\n        num_workers: int = 0,\n        transforms: None | list[BasicTransform] = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Constructor\n\n        Args:\n            cls (type[NonGeoDataModule]): TorchGeo DataModule class to be instantiated\n            batch_size (int | None, optional): batch_size. Defaults to None.\n            num_workers (int, optional): num_workers. Defaults to 0.\n            transforms (None | list[BasicTransform], optional): List of Albumentations Transforms.\n                Should enc with ToTensorV2. Defaults to None.\n            **kwargs (Any): Arguments passed to instantiate `cls`.\n        \"\"\"\n        if batch_size is not None:\n            kwargs[\"batch_size\"] = batch_size\n        if transforms is not None:\n            transforms_as_callable = albumentations_to_callable_with_dict(transforms)\n            kwargs[\"transforms\"] = build_callable_transform_from_torch_tensor(transforms_as_callable)\n        # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs)\n        self._proxy = cls(num_workers=num_workers, **kwargs)\n        super().__init__(self._proxy.dataset_class)  # dummy arg\n\n    @property\n    def collate_fn(self):\n        return self._proxy.collate_fn\n\n    @collate_fn.setter\n    def collate_fn(self, value):\n        self._proxy.collate_fn = value\n\n    def setup(self, stage: str):\n        return self._proxy.setup(stage)\n\n    def train_dataloader(self):\n        return self._proxy.train_dataloader()\n\n    def val_dataloader(self):\n        return self._proxy.val_dataloader()\n\n    def test_dataloader(self):\n        return self._proxy.test_dataloader()\n\n    def predict_dataloader(self):\n        return self._proxy.predict_dataloader()\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.torchgeo_data_module.TorchNonGeoDataModule.__init__","title":"<code>__init__(cls, batch_size=None, num_workers=0, transforms=None, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[NonGeoDataModule]</code> <p>TorchGeo DataModule class to be instantiated</p> required <code>batch_size</code> <code>int | None</code> <p>batch_size. Defaults to None.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>num_workers. Defaults to 0.</p> <code>0</code> <code>transforms</code> <code>None | list[BasicTransform]</code> <p>List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Arguments passed to instantiate <code>cls</code>.</p> <code>{}</code> Source code in <code>terratorch/datamodules/torchgeo_data_module.py</code> <pre><code>def __init__(\n    self,\n    cls: type[NonGeoDataModule],\n    batch_size: int | None = None,\n    num_workers: int = 0,\n    transforms: None | list[BasicTransform] = None,\n    **kwargs: Any,\n):\n    \"\"\"Constructor\n\n    Args:\n        cls (type[NonGeoDataModule]): TorchGeo DataModule class to be instantiated\n        batch_size (int | None, optional): batch_size. Defaults to None.\n        num_workers (int, optional): num_workers. Defaults to 0.\n        transforms (None | list[BasicTransform], optional): List of Albumentations Transforms.\n            Should enc with ToTensorV2. Defaults to None.\n        **kwargs (Any): Arguments passed to instantiate `cls`.\n    \"\"\"\n    if batch_size is not None:\n        kwargs[\"batch_size\"] = batch_size\n    if transforms is not None:\n        transforms_as_callable = albumentations_to_callable_with_dict(transforms)\n        kwargs[\"transforms\"] = build_callable_transform_from_torch_tensor(transforms_as_callable)\n    # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs)\n    self._proxy = cls(num_workers=num_workers, **kwargs)\n    super().__init__(self._proxy.dataset_class)  # dummy arg\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.biomassters","title":"<code>terratorch.datamodules.biomassters</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.biomassters.BioMasstersNonGeoDataModule","title":"<code>BioMasstersNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for BioMassters datamodule.</p> Source code in <code>terratorch/datamodules/biomassters.py</code> <pre><code>class BioMasstersNonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for BioMassters datamodule.\"\"\"\n\n    default_metadata_filename = \"The_BioMassters_-_features_metadata.csv.csv\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        bands: dict[str, Sequence[str]] | Sequence[str] = BioMasstersNonGeo.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        drop_last: bool = True,\n        sensors: Sequence[str] = [\"S1\", \"S2\"],\n        as_time_series: bool = False,\n        metadata_filename: str = default_metadata_filename,\n        max_cloud_percentage: float | None = None,\n        max_red_mean: float | None = None,\n        include_corrupt: bool = True,\n        subset: float = 1,\n        seed: int = 42,\n        use_four_frames: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the DataModule for the non-geospatial BioMassters datamodule.\n\n        Args:\n            data_root (str): Root directory containing the dataset.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            bands (dict[str, Sequence[str]] | Sequence[str], optional): Band configuration; either a dict mapping sensors to bands or a list for the first sensor.\n                Defaults to BioMasstersNonGeo.all_band_names\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            aug (AugmentationSequential, optional): Augmentation or normalization to apply. Defaults to normalization if not provided.\n            drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True.\n            sensors (Sequence[str], optional): List of sensors to use (e.g., [\"S1\", \"S2\"]). Defaults to [\"S1\", \"S2\"].\n            as_time_series (bool, optional): Whether to treat data as a time series. Defaults to False.\n            metadata_filename (str, optional): Metadata filename. Defaults to \"The_BioMassters_-_features_metadata.csv.csv\".\n            max_cloud_percentage (float | None, optional): Maximum allowed cloud percentage. Defaults to None.\n            max_red_mean (float | None, optional): Maximum allowed red band mean. Defaults to None.\n            include_corrupt (bool, optional): Whether to include corrupt data. Defaults to True.\n            subset (float, optional): Fraction of the dataset to use. Defaults to 1.\n            seed (int, optional): Random seed for reproducibility. Defaults to 42.\n            use_four_frames (bool, optional): Whether to use a four frames configuration. Defaults to False.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            None.\n        \"\"\"\n        super().__init__(BioMasstersNonGeo, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n        self.sensors = sensors\n        if isinstance(bands, dict):\n            self.bands = bands\n        else:\n            sens = sensors[0]\n            self.bands = {sens: bands}\n\n        self.means = {}\n        self.stds = {}\n        for sensor in self.sensors:\n            self.means[sensor] = [MEANS[sensor][band] for band in self.bands[sensor]]\n            self.stds[sensor] = [STDS[sensor][band] for band in self.bands[sensor]]\n\n        self.mask_mean = MEANS[\"AGBM\"]\n        self.mask_std = STDS[\"AGBM\"]\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        if len(sensors) == 1:\n            self.aug = Normalize(self.means[sensors[0]], self.stds[sensors[0]]) if aug is None else aug\n        else:\n            MultimodalNormalize(self.means, self.stds) if aug is None else aug\n        self.drop_last = drop_last\n        self.as_time_series = as_time_series\n        self.metadata_filename = metadata_filename\n        self.max_cloud_percentage = max_cloud_percentage\n        self.max_red_mean = max_red_mean\n        self.include_corrupt = include_corrupt\n        self.subset = subset\n        self.seed = seed\n        self.use_four_frames = use_four_frames\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                mask_mean=self.mask_mean,\n                mask_std=self.mask_std,\n                sensors=self.sensors,\n                as_time_series=self.as_time_series,\n                metadata_filename=self.metadata_filename,\n                max_cloud_percentage=self.max_cloud_percentage,\n                max_red_mean=self.max_red_mean,\n                include_corrupt=self.include_corrupt,\n                subset=self.subset,\n                seed=self.seed,\n                use_four_frames=self.use_four_frames,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"test\",\n                root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                mask_mean=self.mask_mean,\n                mask_std=self.mask_std,\n                sensors=self.sensors,\n                as_time_series=self.as_time_series,\n                metadata_filename=self.metadata_filename,\n                max_cloud_percentage=self.max_cloud_percentage,\n                max_red_mean=self.max_red_mean,\n                include_corrupt=self.include_corrupt,\n                subset=self.subset,\n                seed=self.seed,\n                use_four_frames=self.use_four_frames,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"test\",\n                root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                mask_mean=self.mask_mean,\n                mask_std=self.mask_std,\n                sensors=self.sensors,\n                as_time_series=self.as_time_series,\n                metadata_filename=self.metadata_filename,\n                max_cloud_percentage=self.max_cloud_percentage,\n                max_red_mean=self.max_red_mean,\n                include_corrupt=self.include_corrupt,\n                subset=self.subset,\n                seed=self.seed,\n                use_four_frames=self.use_four_frames,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"test\",\n                root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                mask_mean=self.mask_mean,\n                mask_std=self.mask_std,\n                sensors=self.sensors,\n                as_time_series=self.as_time_series,\n                metadata_filename=self.metadata_filename,\n                max_cloud_percentage=self.max_cloud_percentage,\n                max_red_mean=self.max_red_mean,\n                include_corrupt=self.include_corrupt,\n                subset=self.subset,\n                seed=self.seed,\n                use_four_frames=self.use_four_frames,\n            )\n\n    def _dataloader_factory(self, split: str):\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n        return DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=split == \"train\",\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            drop_last=split ==\"train\" and self.drop_last,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.biomassters.BioMasstersNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=BioMasstersNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, aug=None, drop_last=True, sensors=['S1', 'S2'], as_time_series=False, metadata_filename=default_metadata_filename, max_cloud_percentage=None, max_red_mean=None, include_corrupt=True, subset=1, seed=42, use_four_frames=False, **kwargs)</code>","text":"<p>Initializes the DataModule for the non-geospatial BioMassters datamodule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory containing the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>dict[str, Sequence[str]] | Sequence[str]</code> <p>Band configuration; either a dict mapping sensors to bands or a list for the first sensor. Defaults to BioMasstersNonGeo.all_band_names</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation or normalization to apply. Defaults to normalization if not provided.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch. Defaults to True.</p> <code>True</code> <code>sensors</code> <code>Sequence[str]</code> <p>List of sensors to use (e.g., [\"S1\", \"S2\"]). Defaults to [\"S1\", \"S2\"].</p> <code>['S1', 'S2']</code> <code>as_time_series</code> <code>bool</code> <p>Whether to treat data as a time series. Defaults to False.</p> <code>False</code> <code>metadata_filename</code> <code>str</code> <p>Metadata filename. Defaults to \"The_BioMassters_-_features_metadata.csv.csv\".</p> <code>default_metadata_filename</code> <code>max_cloud_percentage</code> <code>float | None</code> <p>Maximum allowed cloud percentage. Defaults to None.</p> <code>None</code> <code>max_red_mean</code> <code>float | None</code> <p>Maximum allowed red band mean. Defaults to None.</p> <code>None</code> <code>include_corrupt</code> <code>bool</code> <p>Whether to include corrupt data. Defaults to True.</p> <code>True</code> <code>subset</code> <code>float</code> <p>Fraction of the dataset to use. Defaults to 1.</p> <code>1</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>use_four_frames</code> <code>bool</code> <p>Whether to use a four frames configuration. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>terratorch/datamodules/biomassters.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    bands: dict[str, Sequence[str]] | Sequence[str] = BioMasstersNonGeo.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    drop_last: bool = True,\n    sensors: Sequence[str] = [\"S1\", \"S2\"],\n    as_time_series: bool = False,\n    metadata_filename: str = default_metadata_filename,\n    max_cloud_percentage: float | None = None,\n    max_red_mean: float | None = None,\n    include_corrupt: bool = True,\n    subset: float = 1,\n    seed: int = 42,\n    use_four_frames: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the DataModule for the non-geospatial BioMassters datamodule.\n\n    Args:\n        data_root (str): Root directory containing the dataset.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        bands (dict[str, Sequence[str]] | Sequence[str], optional): Band configuration; either a dict mapping sensors to bands or a list for the first sensor.\n            Defaults to BioMasstersNonGeo.all_band_names\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        aug (AugmentationSequential, optional): Augmentation or normalization to apply. Defaults to normalization if not provided.\n        drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True.\n        sensors (Sequence[str], optional): List of sensors to use (e.g., [\"S1\", \"S2\"]). Defaults to [\"S1\", \"S2\"].\n        as_time_series (bool, optional): Whether to treat data as a time series. Defaults to False.\n        metadata_filename (str, optional): Metadata filename. Defaults to \"The_BioMassters_-_features_metadata.csv.csv\".\n        max_cloud_percentage (float | None, optional): Maximum allowed cloud percentage. Defaults to None.\n        max_red_mean (float | None, optional): Maximum allowed red band mean. Defaults to None.\n        include_corrupt (bool, optional): Whether to include corrupt data. Defaults to True.\n        subset (float, optional): Fraction of the dataset to use. Defaults to 1.\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n        use_four_frames (bool, optional): Whether to use a four frames configuration. Defaults to False.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        None.\n    \"\"\"\n    super().__init__(BioMasstersNonGeo, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n    self.sensors = sensors\n    if isinstance(bands, dict):\n        self.bands = bands\n    else:\n        sens = sensors[0]\n        self.bands = {sens: bands}\n\n    self.means = {}\n    self.stds = {}\n    for sensor in self.sensors:\n        self.means[sensor] = [MEANS[sensor][band] for band in self.bands[sensor]]\n        self.stds[sensor] = [STDS[sensor][band] for band in self.bands[sensor]]\n\n    self.mask_mean = MEANS[\"AGBM\"]\n    self.mask_std = STDS[\"AGBM\"]\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    if len(sensors) == 1:\n        self.aug = Normalize(self.means[sensors[0]], self.stds[sensors[0]]) if aug is None else aug\n    else:\n        MultimodalNormalize(self.means, self.stds) if aug is None else aug\n    self.drop_last = drop_last\n    self.as_time_series = as_time_series\n    self.metadata_filename = metadata_filename\n    self.max_cloud_percentage = max_cloud_percentage\n    self.max_red_mean = max_red_mean\n    self.include_corrupt = include_corrupt\n    self.subset = subset\n    self.seed = seed\n    self.use_four_frames = use_four_frames\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.biomassters.BioMasstersNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/biomassters.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            mask_mean=self.mask_mean,\n            mask_std=self.mask_std,\n            sensors=self.sensors,\n            as_time_series=self.as_time_series,\n            metadata_filename=self.metadata_filename,\n            max_cloud_percentage=self.max_cloud_percentage,\n            max_red_mean=self.max_red_mean,\n            include_corrupt=self.include_corrupt,\n            subset=self.subset,\n            seed=self.seed,\n            use_four_frames=self.use_four_frames,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"test\",\n            root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            mask_mean=self.mask_mean,\n            mask_std=self.mask_std,\n            sensors=self.sensors,\n            as_time_series=self.as_time_series,\n            metadata_filename=self.metadata_filename,\n            max_cloud_percentage=self.max_cloud_percentage,\n            max_red_mean=self.max_red_mean,\n            include_corrupt=self.include_corrupt,\n            subset=self.subset,\n            seed=self.seed,\n            use_four_frames=self.use_four_frames,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"test\",\n            root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            mask_mean=self.mask_mean,\n            mask_std=self.mask_std,\n            sensors=self.sensors,\n            as_time_series=self.as_time_series,\n            metadata_filename=self.metadata_filename,\n            max_cloud_percentage=self.max_cloud_percentage,\n            max_red_mean=self.max_red_mean,\n            include_corrupt=self.include_corrupt,\n            subset=self.subset,\n            seed=self.seed,\n            use_four_frames=self.use_four_frames,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"test\",\n            root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            mask_mean=self.mask_mean,\n            mask_std=self.mask_std,\n            sensors=self.sensors,\n            as_time_series=self.as_time_series,\n            metadata_filename=self.metadata_filename,\n            max_cloud_percentage=self.max_cloud_percentage,\n            max_red_mean=self.max_red_mean,\n            include_corrupt=self.include_corrupt,\n            subset=self.subset,\n            seed=self.seed,\n            use_four_frames=self.use_four_frames,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.burn_intensity","title":"<code>terratorch.datamodules.burn_intensity</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.burn_intensity.BurnIntensityNonGeoDataModule","title":"<code>BurnIntensityNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for BurnIntensity datamodule.</p> Source code in <code>terratorch/datamodules/burn_intensity.py</code> <pre><code>class BurnIntensityNonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for BurnIntensity datamodule.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        bands: Sequence[str] = BurnIntensityNonGeo.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        use_full_data: bool = True,\n        no_data_replace: float | None = 0.0001,\n        no_label_replace: int | None = -1,\n        use_metadata: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the DataModule for the BurnIntensity non-geospatial datamodule.\n\n        Args:\n            data_root (str): Root directory of the dataset.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            bands (Sequence[str], optional): List of bands to use. Defaults to BurnIntensityNonGeo.all_band_names.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction.\n            use_full_data (bool, optional): Whether to use the full dataset or data with less than 25 percent zeros. Defaults to True.\n            no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001.\n            no_label_replace (int | None, optional): Value to replace missing labels. Defaults to -1.\n            use_metadata (bool): Whether to return metadata info (time and location).\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(BurnIntensityNonGeo, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n\n        means = [MEANS[b] for b in bands]\n        stds = [STDS[b] for b in bands]\n        self.bands = bands\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.aug = NormalizeWithTimesteps(means, stds)\n        self.use_full_data = use_full_data\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.use_metadata = use_metadata\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                use_full_data=self.use_full_data,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                use_full_data=self.use_full_data,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                use_full_data=self.use_full_data,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                use_full_data=self.use_full_data,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.burn_intensity.BurnIntensityNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=BurnIntensityNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, use_full_data=True, no_data_replace=0.0001, no_label_replace=-1, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the DataModule for the BurnIntensity non-geospatial datamodule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of bands to use. Defaults to BurnIntensityNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction.</p> <code>None</code> <code>use_full_data</code> <code>bool</code> <p>Whether to use the full dataset or data with less than 25 percent zeros. Defaults to True.</p> <code>True</code> <code>no_data_replace</code> <code>float | None</code> <p>Value to replace missing data. Defaults to 0.0001.</p> <code>0.0001</code> <code>no_label_replace</code> <code>int | None</code> <p>Value to replace missing labels. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/burn_intensity.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    bands: Sequence[str] = BurnIntensityNonGeo.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    use_full_data: bool = True,\n    no_data_replace: float | None = 0.0001,\n    no_label_replace: int | None = -1,\n    use_metadata: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the DataModule for the BurnIntensity non-geospatial datamodule.\n\n    Args:\n        data_root (str): Root directory of the dataset.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        bands (Sequence[str], optional): List of bands to use. Defaults to BurnIntensityNonGeo.all_band_names.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction.\n        use_full_data (bool, optional): Whether to use the full dataset or data with less than 25 percent zeros. Defaults to True.\n        no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001.\n        no_label_replace (int | None, optional): Value to replace missing labels. Defaults to -1.\n        use_metadata (bool): Whether to return metadata info (time and location).\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(BurnIntensityNonGeo, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n\n    means = [MEANS[b] for b in bands]\n    stds = [STDS[b] for b in bands]\n    self.bands = bands\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.aug = NormalizeWithTimesteps(means, stds)\n    self.use_full_data = use_full_data\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.use_metadata = use_metadata\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.burn_intensity.BurnIntensityNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/burn_intensity.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            use_full_data=self.use_full_data,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            use_full_data=self.use_full_data,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            use_full_data=self.use_full_data,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            use_full_data=self.use_full_data,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.carbonflux","title":"<code>terratorch.datamodules.carbonflux</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.carbonflux.CarbonFluxNonGeoDataModule","title":"<code>CarbonFluxNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Carbon FLux dataset.</p> Source code in <code>terratorch/datamodules/carbonflux.py</code> <pre><code>class CarbonFluxNonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Carbon FLux dataset.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        bands: Sequence[str] = CarbonFluxNonGeo.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        no_data_replace: float | None = 0.0001,\n        use_metadata: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the CarbonFluxNonGeoDataModule.\n\n        Args:\n            data_root (str): Root directory of the dataset.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            bands (Sequence[str], optional): List of bands to use. Defaults to CarbonFluxNonGeo.all_band_names.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            aug (AugmentationSequential, optional): Augmentation sequence; if None, applies multimodal normalization.\n            no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001.\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(CarbonFluxNonGeo, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n\n        means = {\n            m: ([MEANS[m][band] for band in bands] if m == \"image\" else MEANS[m])\n            for m in MEANS.keys()\n        }\n        stds = {\n            m: ([STDS[m][band] for band in bands] if m == \"image\" else STDS[m])\n            for m in STDS.keys()\n        }\n        self.mask_means = MEANS[\"mask\"]\n        self.mask_std = STDS[\"mask\"]\n        self.bands = bands\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.aug = MultimodalNormalize(means, stds) if aug is None else aug\n        self.no_data_replace = no_data_replace\n        self.use_metadata = use_metadata\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                gpp_mean=self.mask_means,\n                gpp_std=self.mask_std,\n                no_data_replace=self.no_data_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                gpp_mean=self.mask_means,\n                gpp_std=self.mask_std,\n                no_data_replace=self.no_data_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                gpp_mean=self.mask_means,\n                gpp_std=self.mask_std,\n                no_data_replace=self.no_data_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                gpp_mean=self.mask_means,\n                gpp_std=self.mask_std,\n                no_data_replace=self.no_data_replace,\n                use_metadata=self.use_metadata,\n            )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.carbonflux.CarbonFluxNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=CarbonFluxNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, aug=None, no_data_replace=0.0001, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the CarbonFluxNonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of bands to use. Defaults to CarbonFluxNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation sequence; if None, applies multimodal normalization.</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Value to replace missing data. Defaults to 0.0001.</p> <code>0.0001</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/carbonflux.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    bands: Sequence[str] = CarbonFluxNonGeo.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    no_data_replace: float | None = 0.0001,\n    use_metadata: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the CarbonFluxNonGeoDataModule.\n\n    Args:\n        data_root (str): Root directory of the dataset.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        bands (Sequence[str], optional): List of bands to use. Defaults to CarbonFluxNonGeo.all_band_names.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        aug (AugmentationSequential, optional): Augmentation sequence; if None, applies multimodal normalization.\n        no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001.\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(CarbonFluxNonGeo, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n\n    means = {\n        m: ([MEANS[m][band] for band in bands] if m == \"image\" else MEANS[m])\n        for m in MEANS.keys()\n    }\n    stds = {\n        m: ([STDS[m][band] for band in bands] if m == \"image\" else STDS[m])\n        for m in STDS.keys()\n    }\n    self.mask_means = MEANS[\"mask\"]\n    self.mask_std = STDS[\"mask\"]\n    self.bands = bands\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.aug = MultimodalNormalize(means, stds) if aug is None else aug\n    self.no_data_replace = no_data_replace\n    self.use_metadata = use_metadata\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.carbonflux.CarbonFluxNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/carbonflux.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            gpp_mean=self.mask_means,\n            gpp_std=self.mask_std,\n            no_data_replace=self.no_data_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            gpp_mean=self.mask_means,\n            gpp_std=self.mask_std,\n            no_data_replace=self.no_data_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            gpp_mean=self.mask_means,\n            gpp_std=self.mask_std,\n            no_data_replace=self.no_data_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            gpp_mean=self.mask_means,\n            gpp_std=self.mask_std,\n            no_data_replace=self.no_data_replace,\n            use_metadata=self.use_metadata,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.forestnet","title":"<code>terratorch.datamodules.forestnet</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.forestnet.ForestNetNonGeoDataModule","title":"<code>ForestNetNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Landslide4Sense dataset.</p> Source code in <code>terratorch/datamodules/forestnet.py</code> <pre><code>class ForestNetNonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Landslide4Sense dataset.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        label_map: dict[str, int] = ForestNetNonGeo.default_label_map,\n        bands: Sequence[str] = ForestNetNonGeo.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        fraction: float = 1.0,\n        aug: AugmentationSequential = None,\n        use_metadata: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the ForestNetNonGeoDataModule.\n\n        Args:\n            data_root (str): Directory containing the dataset.\n            batch_size (int, optional): Batch size for data loaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            label_map (dict[str, int], optional): Mapping of labels to integers. Defaults to ForestNetNonGeo.default_label_map.\n            bands (Sequence[str], optional): List of band names to use. Defaults to ForestNetNonGeo.all_band_names.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction.\n            fraction (float, optional): Fraction of data to use. Defaults to 1.0.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline; if None, uses Normalize.\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(ForestNetNonGeo, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n\n        self.means = [MEANS[b] for b in bands]\n        self.stds = [STDS[b] for b in bands]\n        self.label_map = label_map\n        self.bands = bands\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.aug = Normalize(self.means, self.stds) if aug is None else aug\n        self.fraction = fraction\n        self.use_metadata = use_metadata\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                data_root=self.data_root,\n                label_map=self.label_map,\n                transform=self.train_transform,\n                bands=self.bands,\n                fraction=self.fraction,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                label_map=self.label_map,\n                transform=self.val_transform,\n                bands=self.bands,\n                fraction=self.fraction,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                label_map=self.label_map,\n                transform=self.test_transform,\n                bands=self.bands,\n                fraction=self.fraction,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                label_map=self.label_map,\n                transform=self.predict_transform,\n                bands=self.bands,\n                fraction=self.fraction,\n                use_metadata=self.use_metadata,\n            )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.forestnet.ForestNetNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, label_map=ForestNetNonGeo.default_label_map, bands=ForestNetNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, fraction=1.0, aug=None, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the ForestNetNonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Directory containing the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for data loaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>label_map</code> <code>dict[str, int]</code> <p>Mapping of labels to integers. Defaults to ForestNetNonGeo.default_label_map.</p> <code>default_label_map</code> <code>bands</code> <code>Sequence[str]</code> <p>List of band names to use. Defaults to ForestNetNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction.</p> <code>None</code> <code>fraction</code> <code>float</code> <p>Fraction of data to use. Defaults to 1.0.</p> <code>1.0</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline; if None, uses Normalize.</p> <code>None</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/forestnet.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    label_map: dict[str, int] = ForestNetNonGeo.default_label_map,\n    bands: Sequence[str] = ForestNetNonGeo.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    fraction: float = 1.0,\n    aug: AugmentationSequential = None,\n    use_metadata: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the ForestNetNonGeoDataModule.\n\n    Args:\n        data_root (str): Directory containing the dataset.\n        batch_size (int, optional): Batch size for data loaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        label_map (dict[str, int], optional): Mapping of labels to integers. Defaults to ForestNetNonGeo.default_label_map.\n        bands (Sequence[str], optional): List of band names to use. Defaults to ForestNetNonGeo.all_band_names.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction.\n        fraction (float, optional): Fraction of data to use. Defaults to 1.0.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline; if None, uses Normalize.\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(ForestNetNonGeo, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n\n    self.means = [MEANS[b] for b in bands]\n    self.stds = [STDS[b] for b in bands]\n    self.label_map = label_map\n    self.bands = bands\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.aug = Normalize(self.means, self.stds) if aug is None else aug\n    self.fraction = fraction\n    self.use_metadata = use_metadata\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.forestnet.ForestNetNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/forestnet.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            data_root=self.data_root,\n            label_map=self.label_map,\n            transform=self.train_transform,\n            bands=self.bands,\n            fraction=self.fraction,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            label_map=self.label_map,\n            transform=self.val_transform,\n            bands=self.bands,\n            fraction=self.fraction,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            label_map=self.label_map,\n            transform=self.test_transform,\n            bands=self.bands,\n            fraction=self.fraction,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            label_map=self.label_map,\n            transform=self.predict_transform,\n            bands=self.bands,\n            fraction=self.fraction,\n            use_metadata=self.use_metadata,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.fire_scars","title":"<code>terratorch.datamodules.fire_scars</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.fire_scars.FireScarsDataModule","title":"<code>FireScarsDataModule</code>","text":"<p>               Bases: <code>GeoDataModule</code></p> <p>Geo Fire Scars data module implementation that merges input data with ground truth segmentation masks.</p> Source code in <code>terratorch/datamodules/fire_scars.py</code> <pre><code>class FireScarsDataModule(GeoDataModule):\n    \"\"\"Geo Fire Scars data module implementation that merges input data with ground truth segmentation masks.\"\"\"\n\n    def __init__(self, data_root: str, **kwargs: Any) -&gt; None:\n        super().__init__(FireScarsSegmentationMask, 4, 224, 100, 0, **kwargs)\n        means = list(MEANS.values())\n        stds = list(STDS.values())\n        self.train_aug = AugmentationSequential(K.RandomCrop(224, 224), K.Normalize(means, stds), data_keys=None)\n        self.aug = AugmentationSequential(K.Normalize(means, stds), data_keys=None)\n        self.data_root = data_root\n\n    def setup(self, stage: str) -&gt; None:\n        self.images = FireScarsHLS(\n            os.path.join(self.data_root, \"training/\")\n        )\n        self.labels = FireScarsSegmentationMask(\n            os.path.join(self.data_root, \"training/\")\n        )\n        self.dataset = self.images &amp; self.labels\n        self.train_aug = AugmentationSequential(K.RandomCrop(224, 224), K.normalize(), data_keys=None)\n\n        self.images_test = FireScarsHLS(\n            os.path.join(self.data_root, \"validation/\")\n        )\n        self.labels_test = FireScarsSegmentationMask(\n            os.path.join(self.data_root, \"validation/\")\n        )\n        self.val_dataset = self.images_test &amp; self.labels_test\n\n        if stage in [\"fit\"]:\n            self.train_batch_sampler = RandomBatchGeoSampler(self.dataset, self.patch_size, self.batch_size, None)\n        if stage in [\"fit\", \"validate\"]:\n            self.val_sampler = GridGeoSampler(self.val_dataset, self.patch_size, self.patch_size)\n        if stage in [\"test\"]:\n            self.test_sampler = GridGeoSampler(self.val_dataset, self.patch_size, self.patch_size)\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.fire_scars.FireScarsNonGeoDataModule","title":"<code>FireScarsNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Fire Scars dataset.</p> Source code in <code>terratorch/datamodules/fire_scars.py</code> <pre><code>class FireScarsNonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Fire Scars dataset.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        bands: Sequence[str] = FireScarsNonGeo.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        drop_last: bool = True,\n        no_data_replace: float | None = 0,\n        no_label_replace: int | None = -1,\n        use_metadata: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the FireScarsNonGeoDataModule.\n\n        Args:\n            data_root (str): Root directory of the dataset.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            bands (Sequence[str], optional): List of band names. Defaults to FireScarsNonGeo.all_band_names.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction.\n            drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True.\n            no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0.\n            no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1.\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(FireScarsNonGeo, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n\n        means = [MEANS[b] for b in bands]\n        stds = [STDS[b] for b in bands]\n        self.bands = bands\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.aug = AugmentationSequential(K.Normalize(means, stds), data_keys=None)\n        self.drop_last = drop_last\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.use_metadata = use_metadata\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n\n    def _dataloader_factory(self, split: str) -&gt; DataLoader[dict[str, Tensor]]:\n        \"\"\"Implement one or more PyTorch DataLoaders.\n\n        Args:\n            split: Either 'train', 'val', 'test', or 'predict'.\n\n        Returns:\n            A collection of data loaders specifying samples.\n\n        Raises:\n            MisconfigurationException: If :meth:`setup` does not define a\n                dataset or sampler, or if the dataset or sampler has length 0.\n        \"\"\"\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n        return DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=split == \"train\",\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            drop_last=split == \"train\" and self.drop_last,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.fire_scars.FireScarsNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=FireScarsNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, drop_last=True, no_data_replace=0, no_label_replace=-1, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the FireScarsNonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of band names. Defaults to FireScarsNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch. Defaults to True.</p> <code>True</code> <code>no_data_replace</code> <code>float | None</code> <p>Replacement value for missing data. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replacement value for missing labels. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/fire_scars.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    bands: Sequence[str] = FireScarsNonGeo.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    drop_last: bool = True,\n    no_data_replace: float | None = 0,\n    no_label_replace: int | None = -1,\n    use_metadata: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the FireScarsNonGeoDataModule.\n\n    Args:\n        data_root (str): Root directory of the dataset.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        bands (Sequence[str], optional): List of band names. Defaults to FireScarsNonGeo.all_band_names.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction.\n        drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True.\n        no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0.\n        no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1.\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(FireScarsNonGeo, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n\n    means = [MEANS[b] for b in bands]\n    stds = [STDS[b] for b in bands]\n    self.bands = bands\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.aug = AugmentationSequential(K.Normalize(means, stds), data_keys=None)\n    self.drop_last = drop_last\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.use_metadata = use_metadata\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.fire_scars.FireScarsNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/fire_scars.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.landslide4sense","title":"<code>terratorch.datamodules.landslide4sense</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.landslide4sense.Landslide4SenseNonGeoDataModule","title":"<code>Landslide4SenseNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Landslide4Sense dataset.</p> Source code in <code>terratorch/datamodules/landslide4sense.py</code> <pre><code>class Landslide4SenseNonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Landslide4Sense dataset.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        bands: Sequence[str] = Landslide4SenseNonGeo.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the Landslide4SenseNonGeoDataModule.\n\n        Args:\n            data_root (str): Root directory of the dataset.\n            batch_size (int, optional): Batch size for data loaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            bands (Sequence[str], optional): List of band names to use. Defaults to Landslide4SenseNonGeo.all_band_names.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            aug (AugmentationSequential, optional): Augmentation pipeline; if None, applies normalization using computed means and stds.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(Landslide4SenseNonGeo, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n\n        self.means = [MEANS[b] for b in bands]\n        self.stds = [STDS[b] for b in bands]\n        self.bands = bands\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.aug = (\n            AugmentationSequential(K.Normalize(self.means, self.stds), data_keys=None) if aug is None else aug\n        )\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands\n            )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.landslide4sense.Landslide4SenseNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=Landslide4SenseNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, aug=None, **kwargs)</code>","text":"<p>Initializes the Landslide4SenseNonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for data loaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of band names to use. Defaults to Landslide4SenseNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation pipeline; if None, applies normalization using computed means and stds.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/landslide4sense.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    bands: Sequence[str] = Landslide4SenseNonGeo.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Landslide4SenseNonGeoDataModule.\n\n    Args:\n        data_root (str): Root directory of the dataset.\n        batch_size (int, optional): Batch size for data loaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        bands (Sequence[str], optional): List of band names to use. Defaults to Landslide4SenseNonGeo.all_band_names.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        aug (AugmentationSequential, optional): Augmentation pipeline; if None, applies normalization using computed means and stds.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(Landslide4SenseNonGeo, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n\n    self.means = [MEANS[b] for b in bands]\n    self.stds = [STDS[b] for b in bands]\n    self.bands = bands\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.aug = (\n        AugmentationSequential(K.Normalize(self.means, self.stds), data_keys=None) if aug is None else aug\n    )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.landslide4sense.Landslide4SenseNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/landslide4sense.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_eurosat","title":"<code>terratorch.datamodules.m_eurosat</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.m_eurosat.MEuroSATNonGeoDataModule","title":"<code>MEuroSATNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-EuroSAT dataset.</p> Source code in <code>terratorch/datamodules/m_eurosat.py</code> <pre><code>class MEuroSATNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-EuroSAT dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MEuroSATNonGeoDataModule for the MEuroSATNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MEuroSATNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            **kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_eurosat.MEuroSATNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MEuroSATNonGeoDataModule for the MEuroSATNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_eurosat.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MEuroSATNonGeoDataModule for the MEuroSATNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MEuroSATNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        **kwargs,\n    )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_bigearthnet","title":"<code>terratorch.datamodules.m_bigearthnet</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.m_bigearthnet.MBigEarthNonGeoDataModule","title":"<code>MBigEarthNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-BigEarthNet dataset.</p> Source code in <code>terratorch/datamodules/m_bigearthnet.py</code> <pre><code>class MBigEarthNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-BigEarthNet dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MBigEarthNonGeoDataModule for the M-BigEarthNet dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MBigEarthNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            **kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_bigearthnet.MBigEarthNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MBigEarthNonGeoDataModule for the M-BigEarthNet dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_bigearthnet.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MBigEarthNonGeoDataModule for the M-BigEarthNet dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MBigEarthNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        **kwargs,\n    )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_brick_kiln","title":"<code>terratorch.datamodules.m_brick_kiln</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.m_brick_kiln.MBrickKilnNonGeoDataModule","title":"<code>MBrickKilnNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-BrickKiln dataset.</p> Source code in <code>terratorch/datamodules/m_brick_kiln.py</code> <pre><code>class MBrickKilnNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-BrickKiln dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MBrickKilnNonGeoDataModule for the M-BrickKilnNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MBrickKilnNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            **kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_brick_kiln.MBrickKilnNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MBrickKilnNonGeoDataModule for the M-BrickKilnNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_brick_kiln.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MBrickKilnNonGeoDataModule for the M-BrickKilnNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MBrickKilnNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        **kwargs,\n    )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_forestnet","title":"<code>terratorch.datamodules.m_forestnet</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.m_forestnet.MForestNetNonGeoDataModule","title":"<code>MForestNetNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-ForestNet dataset.</p> Source code in <code>terratorch/datamodules/m_forestnet.py</code> <pre><code>class MForestNetNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-ForestNet dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,  # noqa: FBT002, FBT001\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MForestNetNonGeoDataModule for the MForestNetNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MForestNetNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            use_metadata=use_metadata,\n            **kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_forestnet.MForestNetNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MForestNetNonGeoDataModule for the MForestNetNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_forestnet.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,  # noqa: FBT002, FBT001\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MForestNetNonGeoDataModule for the MForestNetNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MForestNetNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        use_metadata=use_metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_so2sat","title":"<code>terratorch.datamodules.m_so2sat</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.m_so2sat.MSo2SatNonGeoDataModule","title":"<code>MSo2SatNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-So2Sat dataset.</p> Source code in <code>terratorch/datamodules/m_so2sat.py</code> <pre><code>class MSo2SatNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-So2Sat dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MSo2SatNonGeoDataModule for the MSo2SatNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MSo2SatNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            **kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_so2sat.MSo2SatNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MSo2SatNonGeoDataModule for the MSo2SatNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_so2sat.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MSo2SatNonGeoDataModule for the MSo2SatNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MSo2SatNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        **kwargs,\n    )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_pv4ger","title":"<code>terratorch.datamodules.m_pv4ger</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.m_pv4ger.MPv4gerNonGeoDataModule","title":"<code>MPv4gerNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-Pv4ger dataset.</p> Source code in <code>terratorch/datamodules/m_pv4ger.py</code> <pre><code>class MPv4gerNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-Pv4ger dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,  # noqa: FBT002, FBT001\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MPv4gerNonGeoDataModule for the MPv4gerNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MPv4gerNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            use_metadata=use_metadata,\n            **kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_pv4ger.MPv4gerNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MPv4gerNonGeoDataModule for the MPv4gerNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_pv4ger.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,  # noqa: FBT002, FBT001\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MPv4gerNonGeoDataModule for the MPv4gerNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MPv4gerNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        use_metadata=use_metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_cashew_plantation","title":"<code>terratorch.datamodules.m_cashew_plantation</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.m_cashew_plantation.MBeninSmallHolderCashewsNonGeoDataModule","title":"<code>MBeninSmallHolderCashewsNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-Cashew Plantation dataset.</p> Source code in <code>terratorch/datamodules/m_cashew_plantation.py</code> <pre><code>class MBeninSmallHolderCashewsNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-Cashew Plantation dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,  # noqa: FBT002, FBT001\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MBeninSmallHolderCashewsNonGeoDataModule for the M-BeninSmallHolderCashewsNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MBeninSmallHolderCashewsNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            use_metadata=use_metadata,\n            **kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_cashew_plantation.MBeninSmallHolderCashewsNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MBeninSmallHolderCashewsNonGeoDataModule for the M-BeninSmallHolderCashewsNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_cashew_plantation.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,  # noqa: FBT002, FBT001\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MBeninSmallHolderCashewsNonGeoDataModule for the M-BeninSmallHolderCashewsNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MBeninSmallHolderCashewsNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        use_metadata=use_metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_nz_cattle","title":"<code>terratorch.datamodules.m_nz_cattle</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.m_nz_cattle.MNzCattleNonGeoDataModule","title":"<code>MNzCattleNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-NZCattle dataset.</p> Source code in <code>terratorch/datamodules/m_nz_cattle.py</code> <pre><code>class MNzCattleNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-NZCattle dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,  # noqa: FBT002, FBT001\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MNzCattleNonGeoDataModule for the MNzCattleNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MNzCattleNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            use_metadata=use_metadata,\n            **kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_nz_cattle.MNzCattleNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MNzCattleNonGeoDataModule for the MNzCattleNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_nz_cattle.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,  # noqa: FBT002, FBT001\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MNzCattleNonGeoDataModule for the MNzCattleNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MNzCattleNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        use_metadata=use_metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_chesapeake_landcover","title":"<code>terratorch.datamodules.m_chesapeake_landcover</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.m_chesapeake_landcover.MChesapeakeLandcoverNonGeoDataModule","title":"<code>MChesapeakeLandcoverNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-ChesapeakeLandcover dataset.</p> Source code in <code>terratorch/datamodules/m_chesapeake_landcover.py</code> <pre><code>class MChesapeakeLandcoverNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-ChesapeakeLandcover dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MChesapeakeLandcoverNonGeoDataModule for the M-BigEarthNet dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MChesapeakeLandcoverNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            **kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_chesapeake_landcover.MChesapeakeLandcoverNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MChesapeakeLandcoverNonGeoDataModule for the M-BigEarthNet dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_chesapeake_landcover.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MChesapeakeLandcoverNonGeoDataModule for the M-BigEarthNet dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MChesapeakeLandcoverNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        **kwargs,\n    )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_pv4ger_seg","title":"<code>terratorch.datamodules.m_pv4ger_seg</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.m_pv4ger_seg.MPv4gerSegNonGeoDataModule","title":"<code>MPv4gerSegNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-Pv4gerSeg dataset.</p> Source code in <code>terratorch/datamodules/m_pv4ger_seg.py</code> <pre><code>class MPv4gerSegNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-Pv4gerSeg dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,  # noqa: FBT002, FBT001\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MPv4gerNonGeoDataModule for the MPv4gerSegNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MPv4gerSegNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            use_metadata=use_metadata,\n            **kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_pv4ger_seg.MPv4gerSegNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MPv4gerNonGeoDataModule for the MPv4gerSegNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_pv4ger_seg.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,  # noqa: FBT002, FBT001\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MPv4gerNonGeoDataModule for the MPv4gerSegNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MPv4gerSegNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        use_metadata=use_metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_SA_crop_type","title":"<code>terratorch.datamodules.m_SA_crop_type</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.m_SA_crop_type.MSACropTypeNonGeoDataModule","title":"<code>MSACropTypeNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-SA-CropType dataset.</p> Source code in <code>terratorch/datamodules/m_SA_crop_type.py</code> <pre><code>class MSACropTypeNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-SA-CropType dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MSACropTypeNonGeoDataModule for the MSACropTypeNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MSACropTypeNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            **kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_SA_crop_type.MSACropTypeNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MSACropTypeNonGeoDataModule for the MSACropTypeNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_SA_crop_type.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MSACropTypeNonGeoDataModule for the MSACropTypeNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MSACropTypeNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        **kwargs,\n    )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_neontree","title":"<code>terratorch.datamodules.m_neontree</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.m_neontree.MNeonTreeNonGeoDataModule","title":"<code>MNeonTreeNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-NeonTree dataset.</p> Source code in <code>terratorch/datamodules/m_neontree.py</code> <pre><code>class MNeonTreeNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-NeonTree dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MNeonTreeNonGeoDataModule for the MNeonTreeNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MNeonTreeNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            **kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.m_neontree.MNeonTreeNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MNeonTreeNonGeoDataModule for the MNeonTreeNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_neontree.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MNeonTreeNonGeoDataModule for the MNeonTreeNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MNeonTreeNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        **kwargs,\n    )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.multi_temporal_crop_classification","title":"<code>terratorch.datamodules.multi_temporal_crop_classification</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule","title":"<code>MultiTemporalCropClassificationDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for multi-temporal crop classification.</p> Source code in <code>terratorch/datamodules/multi_temporal_crop_classification.py</code> <pre><code>class MultiTemporalCropClassificationDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for multi-temporal crop classification.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        bands: Sequence[str] = MultiTemporalCropClassification.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        drop_last: bool = True,\n        no_data_replace: float | None = 0,\n        no_label_replace: int | None = -1,\n        expand_temporal_dimension: bool = True,\n        reduce_zero_label: bool = True,\n        use_metadata: bool = False,\n        metadata_file_name: str = \"chips_df.csv\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MultiTemporalCropClassificationDataModule for multi-temporal crop classification.\n\n        Args:\n            data_root (str): Directory containing the dataset.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            bands (Sequence[str], optional): List of bands to use. Defaults to MultiTemporalCropClassification.all_band_names.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            drop_last (bool, optional): Whether to drop the last incomplete batch during training. Defaults to True.\n            no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0.\n            no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1.\n            expand_temporal_dimension (bool, optional): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to True.\n            reduce_zero_label (bool, optional): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to True.\n            use_metadata (bool): Whether to return metadata info (time and location).\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(MultiTemporalCropClassification, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n\n        self.means = [MEANS[b] for b in bands]\n        self.stds = [STDS[b] for b in bands]\n        self.bands = bands\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.aug = Normalize(self.means, self.stds)\n        self.drop_last = drop_last\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.expand_temporal_dimension = expand_temporal_dimension\n        self.reduce_zero_label = reduce_zero_label\n        self.use_metadata = use_metadata\n        self.metadata_file_name = metadata_file_name\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension = self.expand_temporal_dimension,\n                reduce_zero_label = self.reduce_zero_label,\n                use_metadata=self.use_metadata,\n                metadata_file_name=self.metadata_file_name,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension = self.expand_temporal_dimension,\n                reduce_zero_label = self.reduce_zero_label,\n                use_metadata=self.use_metadata,\n                metadata_file_name=self.metadata_file_name,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension = self.expand_temporal_dimension,\n                reduce_zero_label = self.reduce_zero_label,\n                use_metadata=self.use_metadata,\n                metadata_file_name=self.metadata_file_name,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension = self.expand_temporal_dimension,\n                reduce_zero_label = self.reduce_zero_label,\n                use_metadata=self.use_metadata,\n                metadata_file_name=self.metadata_file_name,\n            )\n\n    def _dataloader_factory(self, split: str) -&gt; DataLoader[dict[str, Tensor]]:\n        \"\"\"Implement one or more PyTorch DataLoaders.\n\n        Args:\n            split: Either 'train', 'val', 'test', or 'predict'.\n\n        Returns:\n            A collection of data loaders specifying samples.\n\n        Raises:\n            MisconfigurationException: If :meth:`setup` does not define a\n                dataset or sampler, or if the dataset or sampler has length 0.\n        \"\"\"\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n        return DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=split == \"train\",\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            drop_last=split == \"train\" and self.drop_last,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=MultiTemporalCropClassification.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, drop_last=True, no_data_replace=0, no_label_replace=-1, expand_temporal_dimension=True, reduce_zero_label=True, use_metadata=False, metadata_file_name='chips_df.csv', **kwargs)</code>","text":"<p>Initializes the MultiTemporalCropClassificationDataModule for multi-temporal crop classification.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Directory containing the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of bands to use. Defaults to MultiTemporalCropClassification.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch during training. Defaults to True.</p> <code>True</code> <code>no_data_replace</code> <code>float | None</code> <p>Replacement value for missing data. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replacement value for missing labels. Defaults to -1.</p> <code>-1</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True.</p> <code>True</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True.</p> <code>True</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/multi_temporal_crop_classification.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    bands: Sequence[str] = MultiTemporalCropClassification.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    drop_last: bool = True,\n    no_data_replace: float | None = 0,\n    no_label_replace: int | None = -1,\n    expand_temporal_dimension: bool = True,\n    reduce_zero_label: bool = True,\n    use_metadata: bool = False,\n    metadata_file_name: str = \"chips_df.csv\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MultiTemporalCropClassificationDataModule for multi-temporal crop classification.\n\n    Args:\n        data_root (str): Directory containing the dataset.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        bands (Sequence[str], optional): List of bands to use. Defaults to MultiTemporalCropClassification.all_band_names.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        drop_last (bool, optional): Whether to drop the last incomplete batch during training. Defaults to True.\n        no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0.\n        no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1.\n        expand_temporal_dimension (bool, optional): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to True.\n        reduce_zero_label (bool, optional): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to True.\n        use_metadata (bool): Whether to return metadata info (time and location).\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(MultiTemporalCropClassification, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n\n    self.means = [MEANS[b] for b in bands]\n    self.stds = [STDS[b] for b in bands]\n    self.bands = bands\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.aug = Normalize(self.means, self.stds)\n    self.drop_last = drop_last\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.expand_temporal_dimension = expand_temporal_dimension\n    self.reduce_zero_label = reduce_zero_label\n    self.use_metadata = use_metadata\n    self.metadata_file_name = metadata_file_name\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/multi_temporal_crop_classification.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            expand_temporal_dimension = self.expand_temporal_dimension,\n            reduce_zero_label = self.reduce_zero_label,\n            use_metadata=self.use_metadata,\n            metadata_file_name=self.metadata_file_name,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            expand_temporal_dimension = self.expand_temporal_dimension,\n            reduce_zero_label = self.reduce_zero_label,\n            use_metadata=self.use_metadata,\n            metadata_file_name=self.metadata_file_name,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            expand_temporal_dimension = self.expand_temporal_dimension,\n            reduce_zero_label = self.reduce_zero_label,\n            use_metadata=self.use_metadata,\n            metadata_file_name=self.metadata_file_name,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            expand_temporal_dimension = self.expand_temporal_dimension,\n            reduce_zero_label = self.reduce_zero_label,\n            use_metadata=self.use_metadata,\n            metadata_file_name=self.metadata_file_name,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.open_sentinel_map","title":"<code>terratorch.datamodules.open_sentinel_map</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.open_sentinel_map.OpenSentinelMapDataModule","title":"<code>OpenSentinelMapDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Open Sentinel Map.</p> Source code in <code>terratorch/datamodules/open_sentinel_map.py</code> <pre><code>class OpenSentinelMapDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Open Sentinel Map.\"\"\"\n\n    def __init__(\n        self,\n        bands: list[str] | None = None,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        spatial_interpolate_and_stack_temporally: bool = True,  # noqa: FBT001, FBT002\n        pad_image: int | None = None,\n        truncate_image: int | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the OpenSentinelMapDataModule for the Open Sentinel Map dataset.\n\n        Args:\n            bands (list[str] | None, optional): List of bands to use. Defaults to None.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            spatial_interpolate_and_stack_temporally (bool, optional): If True, the bands are interpolated and concatenated over time.\n                Default is True.\n            pad_image (int | None, optional): Number of timesteps to pad the time dimension of the image.\n                If None, no padding is applied.\n            truncate_image (int | None, optional):  Number of timesteps to truncate the time dimension of the image.\n                If None, no truncation is performed.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            OpenSentinelMap,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            **kwargs,\n        )\n        self.bands = bands\n        self.spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally\n        self.pad_image = pad_image\n        self.truncate_image = truncate_image\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.data_root = data_root\n        self.kwargs = kwargs\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = OpenSentinelMap(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n                pad_image = self.pad_image,\n                truncate_image = self.truncate_image,\n                **self.kwargs,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = OpenSentinelMap(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n                pad_image = self.pad_image,\n                truncate_image = self.truncate_image,\n                **self.kwargs,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = OpenSentinelMap(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n                pad_image = self.pad_image,\n                truncate_image = self.truncate_image,\n                **self.kwargs,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = OpenSentinelMap(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n                pad_image = self.pad_image,\n                truncate_image = self.truncate_image,\n                **self.kwargs,\n            )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.open_sentinel_map.OpenSentinelMapDataModule.__init__","title":"<code>__init__(bands=None, batch_size=8, num_workers=0, data_root='./', train_transform=None, val_transform=None, test_transform=None, predict_transform=None, spatial_interpolate_and_stack_temporally=True, pad_image=None, truncate_image=None, **kwargs)</code>","text":"<p>Initializes the OpenSentinelMapDataModule for the Open Sentinel Map dataset.</p> <p>Parameters:</p> Name Type Description Default <code>bands</code> <code>list[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>spatial_interpolate_and_stack_temporally</code> <code>bool</code> <p>If True, the bands are interpolated and concatenated over time. Default is True.</p> <code>True</code> <code>pad_image</code> <code>int | None</code> <p>Number of timesteps to pad the time dimension of the image. If None, no padding is applied.</p> <code>None</code> <code>truncate_image</code> <code>int | None</code> <p>Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/open_sentinel_map.py</code> <pre><code>def __init__(\n    self,\n    bands: list[str] | None = None,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    spatial_interpolate_and_stack_temporally: bool = True,  # noqa: FBT001, FBT002\n    pad_image: int | None = None,\n    truncate_image: int | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the OpenSentinelMapDataModule for the Open Sentinel Map dataset.\n\n    Args:\n        bands (list[str] | None, optional): List of bands to use. Defaults to None.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        spatial_interpolate_and_stack_temporally (bool, optional): If True, the bands are interpolated and concatenated over time.\n            Default is True.\n        pad_image (int | None, optional): Number of timesteps to pad the time dimension of the image.\n            If None, no padding is applied.\n        truncate_image (int | None, optional):  Number of timesteps to truncate the time dimension of the image.\n            If None, no truncation is performed.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        OpenSentinelMap,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        **kwargs,\n    )\n    self.bands = bands\n    self.spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally\n    self.pad_image = pad_image\n    self.truncate_image = truncate_image\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.data_root = data_root\n    self.kwargs = kwargs\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.open_sentinel_map.OpenSentinelMapDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/open_sentinel_map.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = OpenSentinelMap(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n            pad_image = self.pad_image,\n            truncate_image = self.truncate_image,\n            **self.kwargs,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = OpenSentinelMap(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n            pad_image = self.pad_image,\n            truncate_image = self.truncate_image,\n            **self.kwargs,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = OpenSentinelMap(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n            pad_image = self.pad_image,\n            truncate_image = self.truncate_image,\n            **self.kwargs,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = OpenSentinelMap(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n            pad_image = self.pad_image,\n            truncate_image = self.truncate_image,\n            **self.kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.openearthmap","title":"<code>terratorch.datamodules.openearthmap</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.openearthmap.OpenEarthMapNonGeoDataModule","title":"<code>OpenEarthMapNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Open Earth Map.</p> Source code in <code>terratorch/datamodules/openearthmap.py</code> <pre><code>class OpenEarthMapNonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Open Earth Map.\"\"\"\n\n    def __init__(\n        self, \n        batch_size: int = 8, \n        num_workers: int = 0, \n        data_root: str = \"./\",\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        **kwargs: Any\n    ) -&gt; None:\n        \"\"\"\n        Initializes the OpenEarthMapNonGeoDataModule for the Open Earth Map dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            aug (AugmentationSequential, optional): Augmentation pipeline; if None, defaults to normalization using computed means and stds.\n            **kwargs: Additional keyword arguments. Can include 'bands' (list[str]) to specify the bands; defaults to OpenEarthMapNonGeo.all_band_names if not provided.\n        \"\"\"\n        super().__init__(OpenEarthMapNonGeo, batch_size, num_workers, **kwargs)\n\n        bands = kwargs.get(\"bands\", OpenEarthMapNonGeo.all_band_names)\n        self.means = torch.tensor([MEANS[b] for b in bands])\n        self.stds = torch.tensor([STDS[b] for b in bands])\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.data_root = data_root\n        self.aug = AugmentationSequential(K.Normalize(self.means, self.stds), data_keys=None) if aug is None else aug\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(  \n                split=\"train\", data_root=self.data_root, transform=self.train_transform, **self.kwargs\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"val\", data_root=self.data_root, transform=self.val_transform, **self.kwargs\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"test\",data_root=self.data_root, transform=self.test_transform, **self.kwargs\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"test\",data_root=self.data_root, transform=self.predict_transform, **self.kwargs\n            )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.openearthmap.OpenEarthMapNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', train_transform=None, val_transform=None, test_transform=None, predict_transform=None, aug=None, **kwargs)</code>","text":"<p>Initializes the OpenEarthMapNonGeoDataModule for the Open Earth Map dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for test data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation pipeline; if None, defaults to normalization using computed means and stds.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Can include 'bands' (list[str]) to specify the bands; defaults to OpenEarthMapNonGeo.all_band_names if not provided.</p> <code>{}</code> Source code in <code>terratorch/datamodules/openearthmap.py</code> <pre><code>def __init__(\n    self, \n    batch_size: int = 8, \n    num_workers: int = 0, \n    data_root: str = \"./\",\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Initializes the OpenEarthMapNonGeoDataModule for the Open Earth Map dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        aug (AugmentationSequential, optional): Augmentation pipeline; if None, defaults to normalization using computed means and stds.\n        **kwargs: Additional keyword arguments. Can include 'bands' (list[str]) to specify the bands; defaults to OpenEarthMapNonGeo.all_band_names if not provided.\n    \"\"\"\n    super().__init__(OpenEarthMapNonGeo, batch_size, num_workers, **kwargs)\n\n    bands = kwargs.get(\"bands\", OpenEarthMapNonGeo.all_band_names)\n    self.means = torch.tensor([MEANS[b] for b in bands])\n    self.stds = torch.tensor([STDS[b] for b in bands])\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.data_root = data_root\n    self.aug = AugmentationSequential(K.Normalize(self.means, self.stds), data_keys=None) if aug is None else aug\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.openearthmap.OpenEarthMapNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/openearthmap.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(  \n            split=\"train\", data_root=self.data_root, transform=self.train_transform, **self.kwargs\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"val\", data_root=self.data_root, transform=self.val_transform, **self.kwargs\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"test\",data_root=self.data_root, transform=self.test_transform, **self.kwargs\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"test\",data_root=self.data_root, transform=self.predict_transform, **self.kwargs\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.pastis","title":"<code>terratorch.datamodules.pastis</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.pastis.PASTISDataModule","title":"<code>PASTISDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for PASTIS.</p> Source code in <code>terratorch/datamodules/pastis.py</code> <pre><code>class PASTISDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for PASTIS.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        truncate_image: int | None = None,\n        pad_image: int | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the PASTISDataModule for the PASTIS dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Directory containing the dataset. Defaults to \"./\".\n            truncate_image (int, optional): Truncate the time dimension of the image to \n                a specified number of timesteps. If None, no truncation is performed.\n            pad_image (int, optional): Pad the time dimension of the image to a specified \n                number of timesteps. If None, no padding is applied.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            PASTIS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            **kwargs,\n        )\n        self.truncate_image = truncate_image\n        self.pad_image = pad_image\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.data_root = data_root\n        self.kwargs = kwargs\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = PASTIS(\n                folds=[1, 2, 3],\n                data_root=self.data_root,\n                transform=self.train_transform,\n                truncate_image=self.truncate_image,\n                pad_image=self.pad_image,\n                **self.kwargs,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = PASTIS(\n                folds=[4],\n                data_root=self.data_root,\n                transform=self.val_transform,\n                truncate_image=self.truncate_image,\n                pad_image=self.pad_image,\n                **self.kwargs,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = PASTIS(\n                folds=[5],\n                data_root=self.data_root,\n                transform=self.test_transform,\n                truncate_image=self.truncate_image,\n                pad_image=self.pad_image,\n                **self.kwargs,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = PASTIS(\n                folds=[5],\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                truncate_image=self.truncate_image,\n                pad_image=self.pad_image,\n                **self.kwargs,\n            )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.pastis.PASTISDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', truncate_image=None, pad_image=None, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, **kwargs)</code>","text":"<p>Initializes the PASTISDataModule for the PASTIS dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Directory containing the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>truncate_image</code> <code>int</code> <p>Truncate the time dimension of the image to  a specified number of timesteps. If None, no truncation is performed.</p> <code>None</code> <code>pad_image</code> <code>int</code> <p>Pad the time dimension of the image to a specified  number of timesteps. If None, no padding is applied.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/pastis.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    truncate_image: int | None = None,\n    pad_image: int | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the PASTISDataModule for the PASTIS dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Directory containing the dataset. Defaults to \"./\".\n        truncate_image (int, optional): Truncate the time dimension of the image to \n            a specified number of timesteps. If None, no truncation is performed.\n        pad_image (int, optional): Pad the time dimension of the image to a specified \n            number of timesteps. If None, no padding is applied.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        PASTIS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        **kwargs,\n    )\n    self.truncate_image = truncate_image\n    self.pad_image = pad_image\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.data_root = data_root\n    self.kwargs = kwargs\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.pastis.PASTISDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/pastis.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = PASTIS(\n            folds=[1, 2, 3],\n            data_root=self.data_root,\n            transform=self.train_transform,\n            truncate_image=self.truncate_image,\n            pad_image=self.pad_image,\n            **self.kwargs,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = PASTIS(\n            folds=[4],\n            data_root=self.data_root,\n            transform=self.val_transform,\n            truncate_image=self.truncate_image,\n            pad_image=self.pad_image,\n            **self.kwargs,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = PASTIS(\n            folds=[5],\n            data_root=self.data_root,\n            transform=self.test_transform,\n            truncate_image=self.truncate_image,\n            pad_image=self.pad_image,\n            **self.kwargs,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = PASTIS(\n            folds=[5],\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            truncate_image=self.truncate_image,\n            pad_image=self.pad_image,\n            **self.kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.sen1floods11","title":"<code>terratorch.datamodules.sen1floods11</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule","title":"<code>Sen1Floods11NonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Fire Scars.</p> Source code in <code>terratorch/datamodules/sen1floods11.py</code> <pre><code>class Sen1Floods11NonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Fire Scars.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        bands: Sequence[str] = Sen1Floods11NonGeo.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        drop_last: bool = True,\n        constant_scale: float = 0.0001,\n        no_data_replace: float | None = 0,\n        no_label_replace: int | None = -1,\n        use_metadata: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the Sen1Floods11NonGeoDataModule.\n\n        Args:\n            data_root (str): Root directory of the dataset.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            bands (Sequence[str], optional): List of bands to use. Defaults to Sen1Floods11NonGeo.all_band_names.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True.\n            constant_scale (float, optional): Scale constant applied to the dataset. Defaults to 0.0001.\n            no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0.\n            no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1.\n            use_metadata (bool): Whether to return metadata info (time and location).\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(Sen1Floods11NonGeo, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n\n        means = [MEANS[b] for b in bands]\n        stds = [STDS[b] for b in bands]\n        self.bands = bands\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.aug = AugmentationSequential(K.Normalize(means, stds), data_keys=None)\n        self.drop_last = drop_last\n        self.constant_scale = constant_scale\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.use_metadata = use_metadata\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                constant_scale=self.constant_scale,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                constant_scale=self.constant_scale,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                constant_scale=self.constant_scale,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                constant_scale=self.constant_scale,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n\n    def _dataloader_factory(self, split: str) -&gt; DataLoader[dict[str, Tensor]]:\n        \"\"\"Implement one or more PyTorch DataLoaders.\n\n        Args:\n            split: Either 'train', 'val', 'test', or 'predict'.\n\n        Returns:\n            A collection of data loaders specifying samples.\n\n        Raises:\n            MisconfigurationException: If :meth:`setup` does not define a\n                dataset or sampler, or if the dataset or sampler has length 0.\n        \"\"\"\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n        return DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=split == \"train\",\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            drop_last=split == \"train\" and self.drop_last,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=Sen1Floods11NonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, drop_last=True, constant_scale=0.0001, no_data_replace=0, no_label_replace=-1, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the Sen1Floods11NonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of bands to use. Defaults to Sen1Floods11NonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for test data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch. Defaults to True.</p> <code>True</code> <code>constant_scale</code> <code>float</code> <p>Scale constant applied to the dataset. Defaults to 0.0001.</p> <code>0.0001</code> <code>no_data_replace</code> <code>float | None</code> <p>Replacement value for missing data. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replacement value for missing labels. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/sen1floods11.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    bands: Sequence[str] = Sen1Floods11NonGeo.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    drop_last: bool = True,\n    constant_scale: float = 0.0001,\n    no_data_replace: float | None = 0,\n    no_label_replace: int | None = -1,\n    use_metadata: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Sen1Floods11NonGeoDataModule.\n\n    Args:\n        data_root (str): Root directory of the dataset.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        bands (Sequence[str], optional): List of bands to use. Defaults to Sen1Floods11NonGeo.all_band_names.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True.\n        constant_scale (float, optional): Scale constant applied to the dataset. Defaults to 0.0001.\n        no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0.\n        no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1.\n        use_metadata (bool): Whether to return metadata info (time and location).\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(Sen1Floods11NonGeo, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n\n    means = [MEANS[b] for b in bands]\n    stds = [STDS[b] for b in bands]\n    self.bands = bands\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.aug = AugmentationSequential(K.Normalize(means, stds), data_keys=None)\n    self.drop_last = drop_last\n    self.constant_scale = constant_scale\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.use_metadata = use_metadata\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/sen1floods11.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            constant_scale=self.constant_scale,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            constant_scale=self.constant_scale,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            constant_scale=self.constant_scale,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            constant_scale=self.constant_scale,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.sen4agrinet","title":"<code>terratorch.datamodules.sen4agrinet</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.sen4agrinet.Sen4AgriNetDataModule","title":"<code>Sen4AgriNetDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Sen4AgriNet.</p> Source code in <code>terratorch/datamodules/sen4agrinet.py</code> <pre><code>class Sen4AgriNetDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Sen4AgriNet.\"\"\"\n\n    def __init__(\n        self,\n        bands: list[str] | None = None,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        seed: int = 42,\n        scenario: str = \"random\",\n        requires_norm: bool = True,\n        binary_labels: bool = False,\n        linear_encoder: dict = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the Sen4AgriNetDataModule for the Sen4AgriNet dataset.\n\n        Args:\n            bands (list[str] | None, optional): List of bands to use. Defaults to None.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            seed (int, optional): Random seed for reproducibility. Defaults to 42.\n            scenario (str): Defines the splitting scenario to use. Options are:\n                - 'random': Random split of the data.\n                - 'spatial': Split by geographical regions (Catalonia and France).\n                - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020).\n            requires_norm (bool, optional): Whether normalization is required. Defaults to True.\n            binary_labels (bool, optional): Whether to use binary labels. Defaults to False.\n            linear_encoder (dict, optional): Mapping for label encoding. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            Sen4AgriNet,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            **kwargs,\n        )\n        self.bands = bands\n        self.seed = seed\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.data_root = data_root\n        self.scenario = scenario\n        self.requires_norm = requires_norm\n        self.binary_labels = binary_labels\n        self.linear_encoder = linear_encoder\n        self.kwargs = kwargs\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = Sen4AgriNet(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                seed=self.seed,\n                scenario=self.scenario,\n                requires_norm=self.requires_norm,\n                binary_labels=self.binary_labels,\n                linear_encoder=self.linear_encoder,\n                **self.kwargs,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = Sen4AgriNet(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                seed=self.seed,\n                scenario=self.scenario,\n                requires_norm=self.requires_norm,\n                binary_labels=self.binary_labels,\n                linear_encoder=self.linear_encoder,\n                **self.kwargs,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = Sen4AgriNet(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                seed=self.seed,\n                scenario=self.scenario,\n                requires_norm=self.requires_norm,\n                binary_labels=self.binary_labels,\n                linear_encoder=self.linear_encoder,\n                **self.kwargs,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = Sen4AgriNet(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                seed=self.seed,\n                scenario=self.scenario,\n                requires_norm=self.requires_norm,\n                binary_labels=self.binary_labels,\n                linear_encoder=self.linear_encoder,\n                **self.kwargs,\n            )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.sen4agrinet.Sen4AgriNetDataModule.__init__","title":"<code>__init__(bands=None, batch_size=8, num_workers=0, data_root='./', train_transform=None, val_transform=None, test_transform=None, predict_transform=None, seed=42, scenario='random', requires_norm=True, binary_labels=False, linear_encoder=None, **kwargs)</code>","text":"<p>Initializes the Sen4AgriNetDataModule for the Sen4AgriNet dataset.</p> <p>Parameters:</p> Name Type Description Default <code>bands</code> <code>list[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for test data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>scenario</code> <code>str</code> <p>Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020).</p> <code>'random'</code> <code>requires_norm</code> <code>bool</code> <p>Whether normalization is required. Defaults to True.</p> <code>True</code> <code>binary_labels</code> <code>bool</code> <p>Whether to use binary labels. Defaults to False.</p> <code>False</code> <code>linear_encoder</code> <code>dict</code> <p>Mapping for label encoding. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/sen4agrinet.py</code> <pre><code>def __init__(\n    self,\n    bands: list[str] | None = None,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    seed: int = 42,\n    scenario: str = \"random\",\n    requires_norm: bool = True,\n    binary_labels: bool = False,\n    linear_encoder: dict = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Sen4AgriNetDataModule for the Sen4AgriNet dataset.\n\n    Args:\n        bands (list[str] | None, optional): List of bands to use. Defaults to None.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n        scenario (str): Defines the splitting scenario to use. Options are:\n            - 'random': Random split of the data.\n            - 'spatial': Split by geographical regions (Catalonia and France).\n            - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020).\n        requires_norm (bool, optional): Whether normalization is required. Defaults to True.\n        binary_labels (bool, optional): Whether to use binary labels. Defaults to False.\n        linear_encoder (dict, optional): Mapping for label encoding. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        Sen4AgriNet,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        **kwargs,\n    )\n    self.bands = bands\n    self.seed = seed\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.data_root = data_root\n    self.scenario = scenario\n    self.requires_norm = requires_norm\n    self.binary_labels = binary_labels\n    self.linear_encoder = linear_encoder\n    self.kwargs = kwargs\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.sen4agrinet.Sen4AgriNetDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/sen4agrinet.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = Sen4AgriNet(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            seed=self.seed,\n            scenario=self.scenario,\n            requires_norm=self.requires_norm,\n            binary_labels=self.binary_labels,\n            linear_encoder=self.linear_encoder,\n            **self.kwargs,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = Sen4AgriNet(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            seed=self.seed,\n            scenario=self.scenario,\n            requires_norm=self.requires_norm,\n            binary_labels=self.binary_labels,\n            linear_encoder=self.linear_encoder,\n            **self.kwargs,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = Sen4AgriNet(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            seed=self.seed,\n            scenario=self.scenario,\n            requires_norm=self.requires_norm,\n            binary_labels=self.binary_labels,\n            linear_encoder=self.linear_encoder,\n            **self.kwargs,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = Sen4AgriNet(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            seed=self.seed,\n            scenario=self.scenario,\n            requires_norm=self.requires_norm,\n            binary_labels=self.binary_labels,\n            linear_encoder=self.linear_encoder,\n            **self.kwargs,\n        )\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.sen4map","title":"<code>terratorch.datamodules.sen4map</code>","text":""},{"location":"package/datamodules/#terratorch.datamodules.sen4map.Sen4MapLucasDataModule","title":"<code>Sen4MapLucasDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>NonGeo LightningDataModule implementation for Sen4map.</p> Source code in <code>terratorch/datamodules/sen4map.py</code> <pre><code>class Sen4MapLucasDataModule(pl.LightningDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Sen4map.\"\"\"\n\n    def __init__(\n            self, \n            batch_size,\n            num_workers,\n            prefetch_factor = 0,\n            # dataset_bands:list[HLSBands|int] = None,\n            # input_bands:list[HLSBands|int] = None,\n            train_hdf5_path = None,\n            train_hdf5_keys_path = None,\n            test_hdf5_path = None,\n            test_hdf5_keys_path = None,\n            val_hdf5_path = None,\n            val_hdf5_keys_path = None,\n            **kwargs\n            ):\n        \"\"\"\n        Initializes the Sen4MapLucasDataModule for handling Sen4Map monthly composites.\n\n        Args:\n            batch_size (int): Batch size for DataLoaders.\n            num_workers (int): Number of worker processes for data loading.\n            prefetch_factor (int, optional): Number of samples to prefetch per worker. Defaults to 0.\n            train_hdf5_path (str, optional): Path to the training HDF5 file.\n            train_hdf5_keys_path (str, optional): Path to the training HDF5 keys file.\n            test_hdf5_path (str, optional): Path to the testing HDF5 file.\n            test_hdf5_keys_path (str, optional): Path to the testing HDF5 keys file.\n            val_hdf5_path (str, optional): Path to the validation HDF5 file.\n            val_hdf5_keys_path (str, optional): Path to the validation HDF5 keys file.\n            train_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated train keys.\n            test_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated test keys.\n            val_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated validation keys.\n            shuffle (bool, optional): Global shuffle flag.\n            train_shuffle (bool, optional): Shuffle flag for training data; defaults to global shuffle if unset.\n            val_shuffle (bool, optional): Shuffle flag for validation data.\n            test_shuffle (bool, optional): Shuffle flag for test data.\n            train_data_fraction (float, optional): Fraction of training data to use. Defaults to 1.0.\n            val_data_fraction (float, optional): Fraction of validation data to use. Defaults to 1.0.\n            test_data_fraction (float, optional): Fraction of test data to use. Defaults to 1.0.\n            all_hdf5_data_path (str, optional): General HDF5 data path for all splits. If provided, overrides specific paths.\n            resize (bool, optional): Whether to resize images. Defaults to False.\n            resize_to (int or tuple, optional): Target size for resizing images.\n            resize_interpolation (str, optional): Interpolation mode for resizing ('bilinear', 'bicubic', etc.).\n            resize_antialiasing (bool, optional): Whether to apply antialiasing during resizing. Defaults to True.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        self.prepare_data_per_node = False\n        self._log_hyperparams = None\n        self.allow_zero_length_dataloader_with_multiple_devices = False\n\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.prefetch_factor = prefetch_factor\n\n        self.train_hdf5_path = train_hdf5_path\n        self.test_hdf5_path = test_hdf5_path\n        self.val_hdf5_path = val_hdf5_path\n\n        self.train_hdf5_keys_path = train_hdf5_keys_path\n        self.test_hdf5_keys_path = test_hdf5_keys_path\n        self.val_hdf5_keys_path = val_hdf5_keys_path\n\n        if train_hdf5_path and not train_hdf5_keys_path: print(f\"Train dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\")\n        if test_hdf5_path and not test_hdf5_keys_path: print(f\"Test dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\")\n        if val_hdf5_path and not val_hdf5_keys_path: print(f\"Val dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\")\n\n        self.train_hdf5_keys_save_path = kwargs.pop(\"train_hdf5_keys_save_path\", None)\n        self.test_hdf5_keys_save_path = kwargs.pop(\"test_hdf5_keys_save_path\", None)\n        self.val_hdf5_keys_save_path = kwargs.pop(\"val_hdf5_keys_save_path\", None)\n\n        self.shuffle = kwargs.pop(\"shuffle\", None)\n        self.train_shuffle = kwargs.pop(\"train_shuffle\", None) or self.shuffle\n        self.val_shuffle = kwargs.pop(\"val_shuffle\", None)\n        self.test_shuffle = kwargs.pop(\"test_shuffle\", None)\n\n        self.train_data_fraction = kwargs.pop(\"train_data_fraction\", 1.0)\n        self.val_data_fraction = kwargs.pop(\"val_data_fraction\", 1.0)\n        self.test_data_fraction = kwargs.pop(\"test_data_fraction\", 1.0)\n\n        if self.train_data_fraction != 1.0  and  not train_hdf5_keys_path: raise ValueError(f\"train_data_fraction provided as non-unity but train_hdf5_keys_path is unset.\")\n        if self.val_data_fraction != 1.0  and  not val_hdf5_keys_path: raise ValueError(f\"val_data_fraction provided as non-unity but val_hdf5_keys_path is unset.\")\n        if self.test_data_fraction != 1.0  and  not test_hdf5_keys_path: raise ValueError(f\"test_data_fraction provided as non-unity but test_hdf5_keys_path is unset.\")\n\n        all_hdf5_data_path = kwargs.pop(\"all_hdf5_data_path\", None)\n        if all_hdf5_data_path is not None:\n            print(f\"all_hdf5_data_path provided, will be interpreted as the general data path for all splits.\\nKeys in provided train_hdf5_keys_path assumed to encompass all keys for entire data. Validation and Test keys will be subtracted from Train keys.\")\n            if self.train_hdf5_path: raise ValueError(f\"Both general all_hdf5_data_path provided and a specific train_hdf5_path, remove the train_hdf5_path\")\n            if self.val_hdf5_path: raise ValueError(f\"Both general all_hdf5_data_path provided and a specific val_hdf5_path, remove the val_hdf5_path\")\n            if self.test_hdf5_path: raise ValueError(f\"Both general all_hdf5_data_path provided and a specific test_hdf5_path, remove the test_hdf5_path\")\n            self.train_hdf5_path = all_hdf5_data_path\n            self.val_hdf5_path = all_hdf5_data_path\n            self.test_hdf5_path = all_hdf5_data_path\n            self.reduce_train_keys = True\n        else:\n            self.reduce_train_keys = False\n\n        self.resize = kwargs.pop(\"resize\", False)\n        self.resize_to = kwargs.pop(\"resize_to\", None)\n        if self.resize and self.resize_to is None:\n            raise ValueError(f\"Config provided resize as True, but resize_to parameter not given\")\n        self.resize_interpolation = kwargs.pop(\"resize_interpolation\", None)\n        if self.resize and self.resize_interpolation is None:\n            print(f\"Config provided resize as True, but resize_interpolation mode not given. Will assume default bilinear\")\n            self.resize_interpolation = \"bilinear\"\n        interpolation_dict = {\n            \"bilinear\": InterpolationMode.BILINEAR,\n            \"bicubic\": InterpolationMode.BICUBIC,\n            \"nearest\": InterpolationMode.NEAREST,\n            \"nearest_exact\": InterpolationMode.NEAREST_EXACT\n        }\n        if self.resize:\n            if self.resize_interpolation not in interpolation_dict.keys():\n                raise ValueError(f\"resize_interpolation provided as {self.resize_interpolation}, but valid options are: {interpolation_dict.keys()}\")\n            self.resize_interpolation = interpolation_dict[self.resize_interpolation]\n        self.resize_antialiasing = kwargs.pop(\"resize_antialiasing\", True)\n\n        self.kwargs = kwargs\n\n    def _load_hdf5_keys_from_path(self, path, fraction=1.0):\n        if path is None: return None\n        with open(path, \"rb\") as f:\n            keys = pickle.load(f)\n            return keys[:int(fraction*len(keys))]\n\n    def setup(self, stage: str):\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, test.\n        \"\"\"\n        if stage == \"fit\":\n            train_keys = self._load_hdf5_keys_from_path(self.train_hdf5_keys_path, fraction=self.train_data_fraction)\n            val_keys = self._load_hdf5_keys_from_path(self.val_hdf5_keys_path, fraction=self.val_data_fraction)\n            if self.reduce_train_keys:\n                test_keys = self._load_hdf5_keys_from_path(self.test_hdf5_keys_path, fraction=self.test_data_fraction)\n                train_keys = list(set(train_keys) - set(val_keys) - set(test_keys))\n            train_file = h5py.File(self.train_hdf5_path, 'r')\n            self.lucasS2_train = Sen4MapDatasetMonthlyComposites(\n                train_file, \n                h5data_keys = train_keys, \n                resize = self.resize,\n                resize_to = self.resize_to,\n                resize_interpolation = self.resize_interpolation,\n                resize_antialiasing = self.resize_antialiasing,\n                save_keys_path = self.train_hdf5_keys_save_path,\n                **self.kwargs\n            )\n            val_file = h5py.File(self.val_hdf5_path, 'r')\n            self.lucasS2_val = Sen4MapDatasetMonthlyComposites(\n                val_file, \n                h5data_keys=val_keys, \n                resize = self.resize,\n                resize_to = self.resize_to,\n                resize_interpolation = self.resize_interpolation,\n                resize_antialiasing = self.resize_antialiasing,\n                save_keys_path = self.val_hdf5_keys_save_path,\n                **self.kwargs\n            )\n        if stage == \"test\":\n            test_file = h5py.File(self.test_hdf5_path, 'r')\n            test_keys = self._load_hdf5_keys_from_path(self.test_hdf5_keys_path, fraction=self.test_data_fraction)\n            self.lucasS2_test = Sen4MapDatasetMonthlyComposites(\n                test_file, \n                h5data_keys=test_keys, \n                resize = self.resize,\n                resize_to = self.resize_to,\n                resize_interpolation = self.resize_interpolation,\n                resize_antialiasing = self.resize_antialiasing,\n                save_keys_path = self.test_hdf5_keys_save_path,\n                **self.kwargs\n            )\n\n    def train_dataloader(self):\n        return DataLoader(self.lucasS2_train, batch_size=self.batch_size, num_workers=self.num_workers, prefetch_factor=self.prefetch_factor, shuffle=self.train_shuffle)\n\n    def val_dataloader(self):\n        return DataLoader(self.lucasS2_val, batch_size=self.batch_size, num_workers=self.num_workers, prefetch_factor=self.prefetch_factor, shuffle=self.val_shuffle)\n\n    def test_dataloader(self):\n        return DataLoader(self.lucasS2_test, batch_size=self.batch_size, num_workers=self.num_workers, prefetch_factor=self.prefetch_factor, shuffle=self.test_shuffle)\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.sen4map.Sen4MapLucasDataModule.__init__","title":"<code>__init__(batch_size, num_workers, prefetch_factor=0, train_hdf5_path=None, train_hdf5_keys_path=None, test_hdf5_path=None, test_hdf5_keys_path=None, val_hdf5_path=None, val_hdf5_keys_path=None, **kwargs)</code>","text":"<p>Initializes the Sen4MapLucasDataModule for handling Sen4Map monthly composites.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders.</p> required <code>num_workers</code> <code>int</code> <p>Number of worker processes for data loading.</p> required <code>prefetch_factor</code> <code>int</code> <p>Number of samples to prefetch per worker. Defaults to 0.</p> <code>0</code> <code>train_hdf5_path</code> <code>str</code> <p>Path to the training HDF5 file.</p> <code>None</code> <code>train_hdf5_keys_path</code> <code>str</code> <p>Path to the training HDF5 keys file.</p> <code>None</code> <code>test_hdf5_path</code> <code>str</code> <p>Path to the testing HDF5 file.</p> <code>None</code> <code>test_hdf5_keys_path</code> <code>str</code> <p>Path to the testing HDF5 keys file.</p> <code>None</code> <code>val_hdf5_path</code> <code>str</code> <p>Path to the validation HDF5 file.</p> <code>None</code> <code>val_hdf5_keys_path</code> <code>str</code> <p>Path to the validation HDF5 keys file.</p> <code>None</code> <code>train_hdf5_keys_save_path</code> <code>str</code> <p>(from kwargs) Path to save generated train keys.</p> required <code>test_hdf5_keys_save_path</code> <code>str</code> <p>(from kwargs) Path to save generated test keys.</p> required <code>val_hdf5_keys_save_path</code> <code>str</code> <p>(from kwargs) Path to save generated validation keys.</p> required <code>shuffle</code> <code>bool</code> <p>Global shuffle flag.</p> required <code>train_shuffle</code> <code>bool</code> <p>Shuffle flag for training data; defaults to global shuffle if unset.</p> required <code>val_shuffle</code> <code>bool</code> <p>Shuffle flag for validation data.</p> required <code>test_shuffle</code> <code>bool</code> <p>Shuffle flag for test data.</p> required <code>train_data_fraction</code> <code>float</code> <p>Fraction of training data to use. Defaults to 1.0.</p> required <code>val_data_fraction</code> <code>float</code> <p>Fraction of validation data to use. Defaults to 1.0.</p> required <code>test_data_fraction</code> <code>float</code> <p>Fraction of test data to use. Defaults to 1.0.</p> required <code>all_hdf5_data_path</code> <code>str</code> <p>General HDF5 data path for all splits. If provided, overrides specific paths.</p> required <code>resize</code> <code>bool</code> <p>Whether to resize images. Defaults to False.</p> required <code>resize_to</code> <code>int or tuple</code> <p>Target size for resizing images.</p> required <code>resize_interpolation</code> <code>str</code> <p>Interpolation mode for resizing ('bilinear', 'bicubic', etc.).</p> required <code>resize_antialiasing</code> <code>bool</code> <p>Whether to apply antialiasing during resizing. Defaults to True.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/sen4map.py</code> <pre><code>def __init__(\n        self, \n        batch_size,\n        num_workers,\n        prefetch_factor = 0,\n        # dataset_bands:list[HLSBands|int] = None,\n        # input_bands:list[HLSBands|int] = None,\n        train_hdf5_path = None,\n        train_hdf5_keys_path = None,\n        test_hdf5_path = None,\n        test_hdf5_keys_path = None,\n        val_hdf5_path = None,\n        val_hdf5_keys_path = None,\n        **kwargs\n        ):\n    \"\"\"\n    Initializes the Sen4MapLucasDataModule for handling Sen4Map monthly composites.\n\n    Args:\n        batch_size (int): Batch size for DataLoaders.\n        num_workers (int): Number of worker processes for data loading.\n        prefetch_factor (int, optional): Number of samples to prefetch per worker. Defaults to 0.\n        train_hdf5_path (str, optional): Path to the training HDF5 file.\n        train_hdf5_keys_path (str, optional): Path to the training HDF5 keys file.\n        test_hdf5_path (str, optional): Path to the testing HDF5 file.\n        test_hdf5_keys_path (str, optional): Path to the testing HDF5 keys file.\n        val_hdf5_path (str, optional): Path to the validation HDF5 file.\n        val_hdf5_keys_path (str, optional): Path to the validation HDF5 keys file.\n        train_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated train keys.\n        test_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated test keys.\n        val_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated validation keys.\n        shuffle (bool, optional): Global shuffle flag.\n        train_shuffle (bool, optional): Shuffle flag for training data; defaults to global shuffle if unset.\n        val_shuffle (bool, optional): Shuffle flag for validation data.\n        test_shuffle (bool, optional): Shuffle flag for test data.\n        train_data_fraction (float, optional): Fraction of training data to use. Defaults to 1.0.\n        val_data_fraction (float, optional): Fraction of validation data to use. Defaults to 1.0.\n        test_data_fraction (float, optional): Fraction of test data to use. Defaults to 1.0.\n        all_hdf5_data_path (str, optional): General HDF5 data path for all splits. If provided, overrides specific paths.\n        resize (bool, optional): Whether to resize images. Defaults to False.\n        resize_to (int or tuple, optional): Target size for resizing images.\n        resize_interpolation (str, optional): Interpolation mode for resizing ('bilinear', 'bicubic', etc.).\n        resize_antialiasing (bool, optional): Whether to apply antialiasing during resizing. Defaults to True.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    self.prepare_data_per_node = False\n    self._log_hyperparams = None\n    self.allow_zero_length_dataloader_with_multiple_devices = False\n\n    self.batch_size = batch_size\n    self.num_workers = num_workers\n    self.prefetch_factor = prefetch_factor\n\n    self.train_hdf5_path = train_hdf5_path\n    self.test_hdf5_path = test_hdf5_path\n    self.val_hdf5_path = val_hdf5_path\n\n    self.train_hdf5_keys_path = train_hdf5_keys_path\n    self.test_hdf5_keys_path = test_hdf5_keys_path\n    self.val_hdf5_keys_path = val_hdf5_keys_path\n\n    if train_hdf5_path and not train_hdf5_keys_path: print(f\"Train dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\")\n    if test_hdf5_path and not test_hdf5_keys_path: print(f\"Test dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\")\n    if val_hdf5_path and not val_hdf5_keys_path: print(f\"Val dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\")\n\n    self.train_hdf5_keys_save_path = kwargs.pop(\"train_hdf5_keys_save_path\", None)\n    self.test_hdf5_keys_save_path = kwargs.pop(\"test_hdf5_keys_save_path\", None)\n    self.val_hdf5_keys_save_path = kwargs.pop(\"val_hdf5_keys_save_path\", None)\n\n    self.shuffle = kwargs.pop(\"shuffle\", None)\n    self.train_shuffle = kwargs.pop(\"train_shuffle\", None) or self.shuffle\n    self.val_shuffle = kwargs.pop(\"val_shuffle\", None)\n    self.test_shuffle = kwargs.pop(\"test_shuffle\", None)\n\n    self.train_data_fraction = kwargs.pop(\"train_data_fraction\", 1.0)\n    self.val_data_fraction = kwargs.pop(\"val_data_fraction\", 1.0)\n    self.test_data_fraction = kwargs.pop(\"test_data_fraction\", 1.0)\n\n    if self.train_data_fraction != 1.0  and  not train_hdf5_keys_path: raise ValueError(f\"train_data_fraction provided as non-unity but train_hdf5_keys_path is unset.\")\n    if self.val_data_fraction != 1.0  and  not val_hdf5_keys_path: raise ValueError(f\"val_data_fraction provided as non-unity but val_hdf5_keys_path is unset.\")\n    if self.test_data_fraction != 1.0  and  not test_hdf5_keys_path: raise ValueError(f\"test_data_fraction provided as non-unity but test_hdf5_keys_path is unset.\")\n\n    all_hdf5_data_path = kwargs.pop(\"all_hdf5_data_path\", None)\n    if all_hdf5_data_path is not None:\n        print(f\"all_hdf5_data_path provided, will be interpreted as the general data path for all splits.\\nKeys in provided train_hdf5_keys_path assumed to encompass all keys for entire data. Validation and Test keys will be subtracted from Train keys.\")\n        if self.train_hdf5_path: raise ValueError(f\"Both general all_hdf5_data_path provided and a specific train_hdf5_path, remove the train_hdf5_path\")\n        if self.val_hdf5_path: raise ValueError(f\"Both general all_hdf5_data_path provided and a specific val_hdf5_path, remove the val_hdf5_path\")\n        if self.test_hdf5_path: raise ValueError(f\"Both general all_hdf5_data_path provided and a specific test_hdf5_path, remove the test_hdf5_path\")\n        self.train_hdf5_path = all_hdf5_data_path\n        self.val_hdf5_path = all_hdf5_data_path\n        self.test_hdf5_path = all_hdf5_data_path\n        self.reduce_train_keys = True\n    else:\n        self.reduce_train_keys = False\n\n    self.resize = kwargs.pop(\"resize\", False)\n    self.resize_to = kwargs.pop(\"resize_to\", None)\n    if self.resize and self.resize_to is None:\n        raise ValueError(f\"Config provided resize as True, but resize_to parameter not given\")\n    self.resize_interpolation = kwargs.pop(\"resize_interpolation\", None)\n    if self.resize and self.resize_interpolation is None:\n        print(f\"Config provided resize as True, but resize_interpolation mode not given. Will assume default bilinear\")\n        self.resize_interpolation = \"bilinear\"\n    interpolation_dict = {\n        \"bilinear\": InterpolationMode.BILINEAR,\n        \"bicubic\": InterpolationMode.BICUBIC,\n        \"nearest\": InterpolationMode.NEAREST,\n        \"nearest_exact\": InterpolationMode.NEAREST_EXACT\n    }\n    if self.resize:\n        if self.resize_interpolation not in interpolation_dict.keys():\n            raise ValueError(f\"resize_interpolation provided as {self.resize_interpolation}, but valid options are: {interpolation_dict.keys()}\")\n        self.resize_interpolation = interpolation_dict[self.resize_interpolation]\n    self.resize_antialiasing = kwargs.pop(\"resize_antialiasing\", True)\n\n    self.kwargs = kwargs\n</code></pre>"},{"location":"package/datamodules/#terratorch.datamodules.sen4map.Sen4MapLucasDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, test.</p> required Source code in <code>terratorch/datamodules/sen4map.py</code> <pre><code>def setup(self, stage: str):\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, test.\n    \"\"\"\n    if stage == \"fit\":\n        train_keys = self._load_hdf5_keys_from_path(self.train_hdf5_keys_path, fraction=self.train_data_fraction)\n        val_keys = self._load_hdf5_keys_from_path(self.val_hdf5_keys_path, fraction=self.val_data_fraction)\n        if self.reduce_train_keys:\n            test_keys = self._load_hdf5_keys_from_path(self.test_hdf5_keys_path, fraction=self.test_data_fraction)\n            train_keys = list(set(train_keys) - set(val_keys) - set(test_keys))\n        train_file = h5py.File(self.train_hdf5_path, 'r')\n        self.lucasS2_train = Sen4MapDatasetMonthlyComposites(\n            train_file, \n            h5data_keys = train_keys, \n            resize = self.resize,\n            resize_to = self.resize_to,\n            resize_interpolation = self.resize_interpolation,\n            resize_antialiasing = self.resize_antialiasing,\n            save_keys_path = self.train_hdf5_keys_save_path,\n            **self.kwargs\n        )\n        val_file = h5py.File(self.val_hdf5_path, 'r')\n        self.lucasS2_val = Sen4MapDatasetMonthlyComposites(\n            val_file, \n            h5data_keys=val_keys, \n            resize = self.resize,\n            resize_to = self.resize_to,\n            resize_interpolation = self.resize_interpolation,\n            resize_antialiasing = self.resize_antialiasing,\n            save_keys_path = self.val_hdf5_keys_save_path,\n            **self.kwargs\n        )\n    if stage == \"test\":\n        test_file = h5py.File(self.test_hdf5_path, 'r')\n        test_keys = self._load_hdf5_keys_from_path(self.test_hdf5_keys_path, fraction=self.test_data_fraction)\n        self.lucasS2_test = Sen4MapDatasetMonthlyComposites(\n            test_file, \n            h5data_keys=test_keys, \n            resize = self.resize,\n            resize_to = self.resize_to,\n            resize_interpolation = self.resize_interpolation,\n            resize_antialiasing = self.resize_antialiasing,\n            save_keys_path = self.test_hdf5_keys_save_path,\n            **self.kwargs\n        )\n</code></pre>"},{"location":"package/datasets/","title":"Specific Datasets","text":""},{"location":"package/datasets/#terratorch.datasets.biomassters","title":"<code>terratorch.datasets.biomassters</code>","text":""},{"location":"package/datasets/#terratorch.datasets.biomassters.BioMasstersNonGeo","title":"<code>BioMasstersNonGeo</code>","text":"<p>               Bases: <code>BioMassters</code></p> <p>BioMassters Dataset for Aboveground Biomass prediction.</p> <p>Dataset intended for Aboveground Biomass (AGB) prediction over Finnish forests based on Sentinel 1 and 2 data with corresponding target AGB mask values generated by Light Detection and Ranging (LiDAR).</p> <p>Dataset Format:</p> <ul> <li>.tif files for Sentinel 1 and 2 data</li> <li>.tif file for pixel wise AGB target mask</li> <li>.csv files for metadata regarding features and targets</li> </ul> <p>Dataset Features:</p> <ul> <li>13,000 target AGB masks of size (256x256px)</li> <li>12 months of data per target mask</li> <li>Sentinel 1 and Sentinel 2 data for each location</li> <li>Sentinel 1 available for every month</li> <li>Sentinel 2 available for almost every month   (not available for every month due to ESA acquisition halt over the region   during particular periods)</li> </ul> <p>If you use this dataset in your research, please cite the following paper:</p> <ul> <li>https://nascetti-a.github.io/BioMasster/</li> </ul> <p>.. versionadded:: 0.5</p> Source code in <code>terratorch/datasets/biomassters.py</code> <pre><code>class BioMasstersNonGeo(BioMassters):\n    \"\"\"[BioMassters Dataset](https://huggingface.co/datasets/ibm-nasa-geospatial/BioMassters) for Aboveground Biomass prediction.\n\n    Dataset intended for Aboveground Biomass (AGB) prediction\n    over Finnish forests based on Sentinel 1 and 2 data with\n    corresponding target AGB mask values generated by Light Detection\n    and Ranging (LiDAR).\n\n    Dataset Format:\n\n    * .tif files for Sentinel 1 and 2 data\n    * .tif file for pixel wise AGB target mask\n    * .csv files for metadata regarding features and targets\n\n    Dataset Features:\n\n    * 13,000 target AGB masks of size (256x256px)\n    * 12 months of data per target mask\n    * Sentinel 1 and Sentinel 2 data for each location\n    * Sentinel 1 available for every month\n    * Sentinel 2 available for almost every month\n      (not available for every month due to ESA acquisition halt over the region\n      during particular periods)\n\n    If you use this dataset in your research, please cite the following paper:\n\n    * https://nascetti-a.github.io/BioMasster/\n\n    .. versionadded:: 0.5\n    \"\"\"\n\n    S1_BAND_NAMES = [\"VV_Asc\", \"VH_Asc\", \"VV_Desc\", \"VH_Desc\", \"RVI_Asc\", \"RVI_Desc\"]\n    S2_BAND_NAMES = [\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"RED_EDGE_1\",\n        \"RED_EDGE_2\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"NIR_NARROW\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n        \"CLOUD_PROBABILITY\",\n    ]\n\n    all_band_names = {\n        \"S1\": S1_BAND_NAMES,\n        \"S2\": S2_BAND_NAMES,\n    }\n\n    rgb_bands = {\n        \"S1\": [],\n        \"S2\": [\"RED\", \"GREEN\", \"BLUE\"],\n    }\n\n    valid_splits = (\"train\", \"test\")\n    valid_sensors = (\"S1\", \"S2\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    default_metadata_filename = \"The_BioMassters_-_features_metadata.csv.csv\"\n\n    def __init__(\n        self,\n        root = \"data\",\n        split: str = \"train\",\n        bands: dict[str, Sequence[str]] | Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        mask_mean: float | None = 63.4584,\n        mask_std: float | None = 72.21242,\n        sensors: Sequence[str] = [\"S1\", \"S2\"],\n        as_time_series: bool = False,\n        metadata_filename: str = default_metadata_filename,\n        max_cloud_percentage: float | None = None,\n        max_red_mean: float | None = None,\n        include_corrupt: bool = True,\n        subset: float = 1,\n        seed: int = 42,\n        use_four_frames: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize a new instance of BioMassters dataset.\n\n        If ``as_time_series=False`` (the default), each time step becomes its own\n        sample with the target being shared across multiple samples.\n\n        Args:\n            root: root directory where dataset can be found\n            split: train or test split\n            sensors: which sensors to consider for the sample, Sentinel 1 and/or\n                Sentinel 2 ('S1', 'S2')\n            as_time_series: whether or not to return all available\n                time-steps or just a single one for a given target location\n            metadata_filename: metadata file to be used\n            max_cloud_percentage: maximum allowed cloud percentage for images\n            max_red_mean: maximum allowed red_mean value for images\n            include_corrupt: whether to include images marked as corrupted\n\n        Raises:\n            AssertionError: if ``split`` or ``sensors`` is invalid\n            DatasetNotFoundError: If dataset is not found.\n        \"\"\"\n        self.root = root\n        self.sensors = sensors\n        self.bands = bands\n        assert (\n            split in self.valid_splits\n        ), f\"Please choose one of the valid splits: {self.valid_splits}.\"\n        self.split = split\n\n        assert set(sensors).issubset(\n            set(self.valid_sensors)\n        ), f\"Please choose a subset of valid sensors: {self.valid_sensors}.\"\n\n        if len(self.sensors) == 1:\n            sens = self.sensors[0]\n            self.band_indices = [\n                self.all_band_names[sens].index(band) for band in self.bands[sens]\n            ]\n        else:\n            self.band_indices = {\n                sens: [self.all_band_names[sens].index(band) for band in self.bands[sens]]\n                for sens in self.sensors\n            }\n\n        self.mask_mean = mask_mean\n        self.mask_std = mask_std\n        self.as_time_series = as_time_series\n        self.metadata_filename = metadata_filename\n        self.max_cloud_percentage = max_cloud_percentage\n        self.max_red_mean = max_red_mean\n        self.include_corrupt = include_corrupt\n        self.subset = subset\n        self.seed = seed\n        self.use_four_frames = use_four_frames\n\n        self._verify()\n\n        # open metadata csv files\n        self.df = pd.read_csv(os.path.join(self.root, self.metadata_filename))\n\n        # Filter sensors\n        self.df = self.df[self.df[\"satellite\"].isin(self.sensors)]\n\n        # Filter split\n        self.df = self.df[self.df[\"split\"] == self.split]\n\n        # Optional filtering\n        self._filter_and_select_data()\n\n        # Optional subsampling\n        self._random_subsample()\n\n        # generate numerical month from filename since first month is September\n        # and has numerical index of 0\n        self.df[\"num_month\"] = (\n            self.df[\"filename\"]\n            .str.split(\"_\", expand=True)[2]\n            .str.split(\".\", expand=True)[0]\n            .astype(int)\n        )\n\n        # Set dataframe index depending on the task for easier indexing\n        if self.as_time_series:\n            self.df[\"num_index\"] = self.df.groupby([\"chip_id\"]).ngroup()\n        else:\n            filter_df = (\n                self.df.groupby([\"chip_id\", \"month\"])[\"satellite\"].count().reset_index()\n            )\n            filter_df = filter_df[\n                filter_df[\"satellite\"] == len(self.sensors)\n            ].drop(\"satellite\", axis=1)\n            # Guarantee that each sample has corresponding number of images available\n            self.df = self.df.merge(filter_df, on=[\"chip_id\", \"month\"], how=\"inner\")\n\n            self.df[\"num_index\"] = self.df.groupby([\"chip_id\", \"month\"]).ngroup()\n\n        # Adjust transforms based on the number of sensors\n        if len(self.sensors) == 1:\n            self.transform = transform if transform else default_transform\n        elif transform is None:\n            self.transform = MultimodalToTensor(self.sensors)\n        else:\n            transform = {\n                s: transform[s] if s in transform else default_transform\n                for s in self.sensors\n            }\n            self.transform = MultimodalTransforms(transform, shared=False)\n\n        if self.use_four_frames:\n            self._select_4_frames()\n\n    def __len__(self) -&gt; int:\n        return len(self.df[\"num_index\"].unique())\n\n    def _load_input(self, filenames: list[Path]) -&gt; Tensor:\n        \"\"\"Load the input imagery at the index.\n\n        Args:\n            filenames: list of filenames corresponding to input\n\n        Returns:\n            input image\n        \"\"\"\n        filepaths = [\n            os.path.join(self.root, f\"{self.split}_features\", f) for f in filenames\n        ]\n        arr_list = [rasterio.open(fp).read() for fp in filepaths]\n\n        if self.as_time_series:\n            arr = np.stack(arr_list, axis=0) # (T, C, H, W)\n        else:\n            arr = np.concatenate(arr_list, axis=0)\n        return arr.astype(np.int32)\n\n    def _load_target(self, filename: Path) -&gt; Tensor:\n        \"\"\"Load the target mask at the index.\n\n        Args:\n            filename: filename of target to index\n\n        Returns:\n            target mask\n        \"\"\"\n        with rasterio.open(os.path.join(self.root, f\"{self.split}_agbm\", filename), \"r\") as src:\n            arr: np.typing.NDArray[np.float64] = src.read()\n\n        return arr\n\n    def _compute_rvi(self, img: np.ndarray, linear: np.ndarray, sens: str) -&gt; np.ndarray:\n        \"\"\"Compute the RVI indices for S1 data.\"\"\"\n        rvi_channels = []\n        if self.as_time_series:\n            if \"RVI_Asc\" in self.bands[sens]:\n                try:\n                    vv_asc_index = self.all_band_names[\"S1\"].index(\"VV_Asc\")\n                    vh_asc_index = self.all_band_names[\"S1\"].index(\"VH_Asc\")\n                except ValueError as e:\n                    msg = f\"RVI_Asc needs band: {e}\"\n                    raise ValueError(msg) from e\n\n                VV = linear[:, vv_asc_index, :, :]\n                VH = linear[:, vh_asc_index, :, :]\n                rvi_asc = 4 * VH / (VV + VH + 1e-6)\n                rvi_asc = np.expand_dims(rvi_asc, axis=1)\n                rvi_channels.append(rvi_asc)\n            if \"RVI_Desc\" in self.bands[sens]:\n                try:\n                    vv_desc_index = self.all_band_names[\"S1\"].index(\"VV_Desc\")\n                    vh_desc_index = self.all_band_names[\"S1\"].index(\"VH_Desc\")\n                except ValueError as e:\n                    msg = f\"RVI_Desc needs band: {e}\"\n                    raise ValueError(msg) from e\n\n                VV_desc = linear[:, vv_desc_index, :, :]\n                VH_desc = linear[:, vh_desc_index, :, :]\n                rvi_desc = 4 * VH_desc / (VV_desc + VH_desc + 1e-6)\n                rvi_desc = np.expand_dims(rvi_desc, axis=1)\n                rvi_channels.append(rvi_desc)\n            if rvi_channels:\n                rvi_concat = np.concatenate(rvi_channels, axis=1)\n                img = np.concatenate([img, rvi_concat], axis=1)\n        else:\n            if \"RVI_Asc\" in self.bands[sens]:\n                if linear.shape[0] &lt; 2:\n                    msg = f\"Not enough bands to calculate RVI_Asc. Available bands: {linear.shape[0]}\"\n                    raise ValueError(msg)\n                VV = linear[0]\n                VH = linear[1]\n                rvi_asc = 4 * VH / (VV + VH + 1e-6)\n                rvi_asc = np.expand_dims(rvi_asc, axis=0)\n                rvi_channels.append(rvi_asc)\n            if \"RVI_Desc\" in self.bands[sens]:\n                if linear.shape[0] &lt; 4:\n                    msg = f\"Not enough bands to calculate RVI_Desc. Available bands: {linear.shape[0]}\"\n                    raise ValueError(msg)\n                VV_desc = linear[2]\n                VH_desc = linear[3]\n                rvi_desc = 4 * VH_desc / (VV_desc + VH_desc + 1e-6)\n                rvi_desc = np.expand_dims(rvi_desc, axis=0) \n                rvi_channels.append(rvi_desc)\n            if rvi_channels:\n                rvi_concat = np.concatenate(rvi_channels, axis=0)\n                img = np.concatenate([linear, rvi_concat], axis=0)\n        return img\n\n    def _select_4_frames(self):\n        \"\"\"Filter the dataset to select only 4 frames per sample.\"\"\"\n\n        if \"cloud_percentage\" in self.df.columns:\n            self.df = self.df.sort_values(by=[\"chip_id\", \"cloud_percentage\"])\n        else:\n            self.df = self.df.sort_values(by=[\"chip_id\", \"num_month\"])\n\n        self.df = (\n            self.df.groupby(\"chip_id\")\n            .head(4)  # Select the first 4 frames per chip\n            .reset_index(drop=True)\n        )\n\n    def _process_sensor_images(self, sens: str, sens_filepaths: list[str]) -&gt; np.ndarray:\n        \"\"\"Process images for a given sensor.\"\"\"\n        img = self._load_input(sens_filepaths)\n        if sens == \"S1\":\n            img = img.astype(np.float32)\n            linear = 10 ** (img / 10)\n            img = self._compute_rvi(img, linear, sens)\n        if self.as_time_series:\n            img = img.transpose(0, 2, 3, 1)  # (T, H, W, C)\n        else:\n            img = img.transpose(1, 2, 0)  # (H, W, C)\n        if len(self.sensors) == 1:\n            img = img[..., self.band_indices]\n        else:\n            img = img[..., self.band_indices[sens]]\n        return img\n\n    def __getitem__(self, index: int) -&gt; dict:\n        sample_df = self.df[self.df[\"num_index\"] == index].copy()\n        # Sort by satellite and month\n        sample_df.sort_values(\n            by=[\"satellite\", \"num_month\"], inplace=True, ascending=True\n        )\n\n        filepaths = sample_df[\"filename\"].tolist()\n        output = {}\n\n        if len(self.sensors) == 1:\n            sens = self.sensors[0]\n            sens_filepaths = [fp for fp in filepaths if sens in fp]\n            img = self._process_sensor_images(sens, sens_filepaths)\n            output[\"image\"] = img.astype(np.float32)\n        else:\n            for sens in self.sensors:\n                sens_filepaths = [fp for fp in filepaths if sens in fp]\n                img = self._process_sensor_images(sens, sens_filepaths)\n                output[sens] = img.astype(np.float32)\n\n        # Load target\n        target_filename = sample_df[\"corresponding_agbm\"].unique()[0]\n        target = np.array(self._load_target(Path(target_filename)))\n        target = target.transpose(1, 2, 0)\n        output[\"mask\"] = target\n        if self.transform:\n            if len(self.sensors) == 1:\n                output = self.transform(**output)\n            else:\n                output = self.transform(output)\n        output[\"mask\"] = output[\"mask\"].squeeze().float()\n        return output\n\n    def _filter_and_select_data(self):\n        if (\n            self.max_cloud_percentage is not None\n            and \"cloud_percentage\" in self.df.columns\n        ):\n            self.df = self.df[self.df[\"cloud_percentage\"] &lt;= self.max_cloud_percentage]\n\n        if self.max_red_mean is not None and \"red_mean\" in self.df.columns:\n            self.df = self.df[self.df[\"red_mean\"] &lt;= self.max_red_mean]\n\n        if not self.include_corrupt and \"corrupt_values\" in self.df.columns:\n            self.df = self.df[self.df[\"corrupt_values\"] is False]\n\n    def _random_subsample(self):\n        if self.split == \"train\" and self.subset &lt; 1.0:\n            num_samples = int(len(self.df[\"num_index\"].unique()) * self.subset)\n            if self.seed is not None:\n                random.seed(self.seed)\n            selected_indices = random.sample(\n                list(self.df[\"num_index\"].unique()), num_samples\n            )\n            self.df = self.df[self.df[\"num_index\"].isin(selected_indices)]\n            self.df.reset_index(drop=True, inplace=True)\n\n    def plot(\n        self,\n        sample: dict[str, Tensor],\n        show_titles: bool = True,\n        suptitle: str | None = None,\n    ) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            show_titles: flag indicating whether to show titles above each panel\n            suptitle: optional suptitle to use for figure\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n        \"\"\"\n        # Determine if the sample contains multiple sensors or a single sensor\n        if isinstance(sample[\"image\"], dict):\n            ncols = len(self.sensors) + 1\n        else:\n            ncols = 2  # One for the image and one for the mask\n\n        showing_predictions = \"prediction\" in sample\n        if showing_predictions:\n            ncols += 1\n\n        fig, axs = plt.subplots(1, ncols=ncols, figsize=(5 * ncols, 10))\n\n        if isinstance(sample[\"image\"], dict):\n            # Multiple sensors case\n            for idx, sens in enumerate(self.sensors):\n                img = sample[\"image\"][sens].numpy()\n                if self.as_time_series:\n                    # Plot last time step\n                    img = img[:, -1, ...]\n                if sens == \"S2\":\n                    img = img[[2, 1, 0], ...].transpose(1, 2, 0)\n                    img = percentile_normalization(img)\n                else:\n                    co_polarization = img[0]  # transmit == receive\n                    cross_polarization = img[1]  # transmit != receive\n                    ratio = co_polarization / (cross_polarization + 1e-6)\n\n                    co_polarization = np.clip(co_polarization / 0.3, 0, 1)\n                    cross_polarization = np.clip(cross_polarization / 0.05, 0, 1)\n                    ratio = np.clip(ratio / 25, 0, 1)\n\n                    img = np.stack(\n                        (co_polarization, cross_polarization, ratio), axis=0\n                    )\n                    img = img.transpose(1, 2, 0)  # Convert to (H, W, 3)\n\n                axs[idx].imshow(img)\n                axs[idx].axis(\"off\")\n                if show_titles:\n                    axs[idx].set_title(sens)\n            mask_idx = len(self.sensors)\n        else:\n            # Single sensor case\n            sens = self.sensors[0]\n            img = sample[\"image\"].numpy()\n            if self.as_time_series:\n                # Plot last time step\n                img = img[:, -1, ...]\n            if sens == \"S2\":\n                img = img[[2, 1, 0], ...].transpose(1, 2, 0)\n                img = percentile_normalization(img)\n            else:\n                co_polarization = img[0]  # transmit == receive\n                cross_polarization = img[1]  # transmit != receive\n                ratio = co_polarization / (cross_polarization + 1e-6)\n\n                co_polarization = np.clip(co_polarization / 0.3, 0, 1)\n                cross_polarization = np.clip(cross_polarization / 0.05, 0, 1)\n                ratio = np.clip(ratio / 25, 0, 1)\n\n                img = np.stack(\n                    (co_polarization, cross_polarization, ratio), axis=0\n                )\n                img = img.transpose(1, 2, 0)  # Convert to (H, W, 3)\n\n            axs[0].imshow(img)\n            axs[0].axis(\"off\")\n            if show_titles:\n                axs[0].set_title(sens)\n            mask_idx = 1\n\n        # Plot target mask\n        if \"mask\" in sample:\n            target = sample[\"mask\"].squeeze()\n            target_im = axs[mask_idx].imshow(target, cmap=\"YlGn\")\n            plt.colorbar(target_im, ax=axs[mask_idx], fraction=0.046, pad=0.04)\n            axs[mask_idx].axis(\"off\")\n            if show_titles:\n                axs[mask_idx].set_title(\"Target\")\n\n        # Plot prediction if available\n        if showing_predictions:\n            pred_idx = mask_idx + 1\n            prediction = sample[\"prediction\"].squeeze()\n            pred_im = axs[pred_idx].imshow(prediction, cmap=\"YlGn\")\n            plt.colorbar(pred_im, ax=axs[pred_idx], fraction=0.046, pad=0.04)\n            axs[pred_idx].axis(\"off\")\n            if show_titles:\n                axs[pred_idx].set_title(\"Prediction\")\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.biomassters.BioMasstersNonGeo.__init__","title":"<code>__init__(root='data', split='train', bands=BAND_SETS['all'], transform=None, mask_mean=63.4584, mask_std=72.21242, sensors=['S1', 'S2'], as_time_series=False, metadata_filename=default_metadata_filename, max_cloud_percentage=None, max_red_mean=None, include_corrupt=True, subset=1, seed=42, use_four_frames=False)</code>","text":"<p>Initialize a new instance of BioMassters dataset.</p> <p>If <code>as_time_series=False</code> (the default), each time step becomes its own sample with the target being shared across multiple samples.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <p>root directory where dataset can be found</p> <code>'data'</code> <code>split</code> <code>str</code> <p>train or test split</p> <code>'train'</code> <code>sensors</code> <code>Sequence[str]</code> <p>which sensors to consider for the sample, Sentinel 1 and/or Sentinel 2 ('S1', 'S2')</p> <code>['S1', 'S2']</code> <code>as_time_series</code> <code>bool</code> <p>whether or not to return all available time-steps or just a single one for a given target location</p> <code>False</code> <code>metadata_filename</code> <code>str</code> <p>metadata file to be used</p> <code>default_metadata_filename</code> <code>max_cloud_percentage</code> <code>float | None</code> <p>maximum allowed cloud percentage for images</p> <code>None</code> <code>max_red_mean</code> <code>float | None</code> <p>maximum allowed red_mean value for images</p> <code>None</code> <code>include_corrupt</code> <code>bool</code> <p>whether to include images marked as corrupted</p> <code>True</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if <code>split</code> or <code>sensors</code> is invalid</p> <code>DatasetNotFoundError</code> <p>If dataset is not found.</p> Source code in <code>terratorch/datasets/biomassters.py</code> <pre><code>def __init__(\n    self,\n    root = \"data\",\n    split: str = \"train\",\n    bands: dict[str, Sequence[str]] | Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    mask_mean: float | None = 63.4584,\n    mask_std: float | None = 72.21242,\n    sensors: Sequence[str] = [\"S1\", \"S2\"],\n    as_time_series: bool = False,\n    metadata_filename: str = default_metadata_filename,\n    max_cloud_percentage: float | None = None,\n    max_red_mean: float | None = None,\n    include_corrupt: bool = True,\n    subset: float = 1,\n    seed: int = 42,\n    use_four_frames: bool = False\n) -&gt; None:\n    \"\"\"Initialize a new instance of BioMassters dataset.\n\n    If ``as_time_series=False`` (the default), each time step becomes its own\n    sample with the target being shared across multiple samples.\n\n    Args:\n        root: root directory where dataset can be found\n        split: train or test split\n        sensors: which sensors to consider for the sample, Sentinel 1 and/or\n            Sentinel 2 ('S1', 'S2')\n        as_time_series: whether or not to return all available\n            time-steps or just a single one for a given target location\n        metadata_filename: metadata file to be used\n        max_cloud_percentage: maximum allowed cloud percentage for images\n        max_red_mean: maximum allowed red_mean value for images\n        include_corrupt: whether to include images marked as corrupted\n\n    Raises:\n        AssertionError: if ``split`` or ``sensors`` is invalid\n        DatasetNotFoundError: If dataset is not found.\n    \"\"\"\n    self.root = root\n    self.sensors = sensors\n    self.bands = bands\n    assert (\n        split in self.valid_splits\n    ), f\"Please choose one of the valid splits: {self.valid_splits}.\"\n    self.split = split\n\n    assert set(sensors).issubset(\n        set(self.valid_sensors)\n    ), f\"Please choose a subset of valid sensors: {self.valid_sensors}.\"\n\n    if len(self.sensors) == 1:\n        sens = self.sensors[0]\n        self.band_indices = [\n            self.all_band_names[sens].index(band) for band in self.bands[sens]\n        ]\n    else:\n        self.band_indices = {\n            sens: [self.all_band_names[sens].index(band) for band in self.bands[sens]]\n            for sens in self.sensors\n        }\n\n    self.mask_mean = mask_mean\n    self.mask_std = mask_std\n    self.as_time_series = as_time_series\n    self.metadata_filename = metadata_filename\n    self.max_cloud_percentage = max_cloud_percentage\n    self.max_red_mean = max_red_mean\n    self.include_corrupt = include_corrupt\n    self.subset = subset\n    self.seed = seed\n    self.use_four_frames = use_four_frames\n\n    self._verify()\n\n    # open metadata csv files\n    self.df = pd.read_csv(os.path.join(self.root, self.metadata_filename))\n\n    # Filter sensors\n    self.df = self.df[self.df[\"satellite\"].isin(self.sensors)]\n\n    # Filter split\n    self.df = self.df[self.df[\"split\"] == self.split]\n\n    # Optional filtering\n    self._filter_and_select_data()\n\n    # Optional subsampling\n    self._random_subsample()\n\n    # generate numerical month from filename since first month is September\n    # and has numerical index of 0\n    self.df[\"num_month\"] = (\n        self.df[\"filename\"]\n        .str.split(\"_\", expand=True)[2]\n        .str.split(\".\", expand=True)[0]\n        .astype(int)\n    )\n\n    # Set dataframe index depending on the task for easier indexing\n    if self.as_time_series:\n        self.df[\"num_index\"] = self.df.groupby([\"chip_id\"]).ngroup()\n    else:\n        filter_df = (\n            self.df.groupby([\"chip_id\", \"month\"])[\"satellite\"].count().reset_index()\n        )\n        filter_df = filter_df[\n            filter_df[\"satellite\"] == len(self.sensors)\n        ].drop(\"satellite\", axis=1)\n        # Guarantee that each sample has corresponding number of images available\n        self.df = self.df.merge(filter_df, on=[\"chip_id\", \"month\"], how=\"inner\")\n\n        self.df[\"num_index\"] = self.df.groupby([\"chip_id\", \"month\"]).ngroup()\n\n    # Adjust transforms based on the number of sensors\n    if len(self.sensors) == 1:\n        self.transform = transform if transform else default_transform\n    elif transform is None:\n        self.transform = MultimodalToTensor(self.sensors)\n    else:\n        transform = {\n            s: transform[s] if s in transform else default_transform\n            for s in self.sensors\n        }\n        self.transform = MultimodalTransforms(transform, shared=False)\n\n    if self.use_four_frames:\n        self._select_4_frames()\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.biomassters.BioMasstersNonGeo.plot","title":"<code>plot(sample, show_titles=True, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>show_titles</code> <code>bool</code> <p>flag indicating whether to show titles above each panel</p> <code>True</code> <code>suptitle</code> <code>str | None</code> <p>optional suptitle to use for figure</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> Source code in <code>terratorch/datasets/biomassters.py</code> <pre><code>def plot(\n    self,\n    sample: dict[str, Tensor],\n    show_titles: bool = True,\n    suptitle: str | None = None,\n) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        show_titles: flag indicating whether to show titles above each panel\n        suptitle: optional suptitle to use for figure\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n    \"\"\"\n    # Determine if the sample contains multiple sensors or a single sensor\n    if isinstance(sample[\"image\"], dict):\n        ncols = len(self.sensors) + 1\n    else:\n        ncols = 2  # One for the image and one for the mask\n\n    showing_predictions = \"prediction\" in sample\n    if showing_predictions:\n        ncols += 1\n\n    fig, axs = plt.subplots(1, ncols=ncols, figsize=(5 * ncols, 10))\n\n    if isinstance(sample[\"image\"], dict):\n        # Multiple sensors case\n        for idx, sens in enumerate(self.sensors):\n            img = sample[\"image\"][sens].numpy()\n            if self.as_time_series:\n                # Plot last time step\n                img = img[:, -1, ...]\n            if sens == \"S2\":\n                img = img[[2, 1, 0], ...].transpose(1, 2, 0)\n                img = percentile_normalization(img)\n            else:\n                co_polarization = img[0]  # transmit == receive\n                cross_polarization = img[1]  # transmit != receive\n                ratio = co_polarization / (cross_polarization + 1e-6)\n\n                co_polarization = np.clip(co_polarization / 0.3, 0, 1)\n                cross_polarization = np.clip(cross_polarization / 0.05, 0, 1)\n                ratio = np.clip(ratio / 25, 0, 1)\n\n                img = np.stack(\n                    (co_polarization, cross_polarization, ratio), axis=0\n                )\n                img = img.transpose(1, 2, 0)  # Convert to (H, W, 3)\n\n            axs[idx].imshow(img)\n            axs[idx].axis(\"off\")\n            if show_titles:\n                axs[idx].set_title(sens)\n        mask_idx = len(self.sensors)\n    else:\n        # Single sensor case\n        sens = self.sensors[0]\n        img = sample[\"image\"].numpy()\n        if self.as_time_series:\n            # Plot last time step\n            img = img[:, -1, ...]\n        if sens == \"S2\":\n            img = img[[2, 1, 0], ...].transpose(1, 2, 0)\n            img = percentile_normalization(img)\n        else:\n            co_polarization = img[0]  # transmit == receive\n            cross_polarization = img[1]  # transmit != receive\n            ratio = co_polarization / (cross_polarization + 1e-6)\n\n            co_polarization = np.clip(co_polarization / 0.3, 0, 1)\n            cross_polarization = np.clip(cross_polarization / 0.05, 0, 1)\n            ratio = np.clip(ratio / 25, 0, 1)\n\n            img = np.stack(\n                (co_polarization, cross_polarization, ratio), axis=0\n            )\n            img = img.transpose(1, 2, 0)  # Convert to (H, W, 3)\n\n        axs[0].imshow(img)\n        axs[0].axis(\"off\")\n        if show_titles:\n            axs[0].set_title(sens)\n        mask_idx = 1\n\n    # Plot target mask\n    if \"mask\" in sample:\n        target = sample[\"mask\"].squeeze()\n        target_im = axs[mask_idx].imshow(target, cmap=\"YlGn\")\n        plt.colorbar(target_im, ax=axs[mask_idx], fraction=0.046, pad=0.04)\n        axs[mask_idx].axis(\"off\")\n        if show_titles:\n            axs[mask_idx].set_title(\"Target\")\n\n    # Plot prediction if available\n    if showing_predictions:\n        pred_idx = mask_idx + 1\n        prediction = sample[\"prediction\"].squeeze()\n        pred_im = axs[pred_idx].imshow(prediction, cmap=\"YlGn\")\n        plt.colorbar(pred_im, ax=axs[pred_idx], fraction=0.046, pad=0.04)\n        axs[pred_idx].axis(\"off\")\n        if show_titles:\n            axs[pred_idx].set_title(\"Prediction\")\n\n    if suptitle is not None:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.burn_intensity","title":"<code>terratorch.datasets.burn_intensity</code>","text":""},{"location":"package/datasets/#terratorch.datasets.burn_intensity.BurnIntensityNonGeo","title":"<code>BurnIntensityNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>Dataset implementation for Burn Intensity classification.</p> Source code in <code>terratorch/datasets/burn_intensity.py</code> <pre><code>class BurnIntensityNonGeo(NonGeoDataset):\n    \"\"\"Dataset implementation for [Burn Intensity classification](https://huggingface.co/datasets/ibm-nasa-geospatial/burn_intensity).\"\"\"\n\n    all_band_names = (\n        \"BLUE\", \"GREEN\", \"RED\", \"NIR\", \"SWIR_1\", \"SWIR_2\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    class_names = (\n        \"No burn\",\n        \"Unburned to Very Low\",\n        \"Low Severity\",\n        \"Moderate Severity\",\n        \"High Severity\"\n    )\n\n    CSV_FILES = {\n        \"limited\": \"BS_files_with_less_than_25_percent_zeros.csv\",\n        \"full\": \"BS_files_raw.csv\",\n    }\n\n    num_classes = 5\n    splits = {\"train\": \"train\", \"val\": \"val\"}\n    time_steps = [\"pre\", \"during\", \"post\"]\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        use_full_data: bool = True,\n        no_data_replace: float | None = 0.0001,\n        no_label_replace: int | None = -1,\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the BurnIntensity dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train' or 'val'.\n            bands (Sequence[str]): Bands to output. Defaults to all bands.\n            transform (Optional[A.Compose]): Albumentations transform to be applied.\n            use_metadata (bool): Whether to return metadata info (location).\n            use_full_data (bool): Wheter to use full data or data with less than 25 percent zeros.\n            no_data_replace (Optional[float]): Value to replace NaNs in images.\n            no_label_replace (Optional[int]): Value to replace NaNs in labels.\n        \"\"\"\n        super().__init__()\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n\n        self.data_root = Path(data_root)\n\n        # Read the CSV file to get the list of cases to include\n        csv_file_key = \"full\" if use_full_data else \"limited\"\n        csv_path = self.data_root / self.CSV_FILES[csv_file_key]\n        df = pd.read_csv(csv_path)\n        casenames = df[\"Case_Name\"].tolist()\n\n        split_file = self.data_root / f\"{split}.txt\"\n        with open(split_file) as f:\n            split_images = [line.strip() for line in f.readlines()]\n\n        split_images = [img for img in split_images if self._extract_casename(img) in casenames]\n\n        # Build the samples list\n        self.samples = []\n        for image_filename in split_images:\n            image_files = []\n            for time_step in self.time_steps:\n                image_file = self.data_root / time_step / image_filename\n                image_files.append(str(image_file))\n            mask_filename = image_filename.replace(\"HLS_\", \"BS_\")\n            mask_file = self.data_root / \"pre\" / mask_filename\n            self.samples.append({\n                \"image_files\": image_files,\n                \"mask_file\": str(mask_file),\n                \"casename\": self._extract_casename(image_filename),\n            })\n\n        self.use_metadata = use_metadata\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n\n        self.transform = transform if transform else default_transform\n\n    def _extract_basename(self, filepath: str) -&gt; str:\n        \"\"\"Extract the base filename without extension.\"\"\"\n        return os.path.splitext(os.path.basename(filepath))[0]\n\n    def _extract_casename(self, filename: str) -&gt; str:\n        \"\"\"Extract the casename from the filename.\"\"\"\n        basename = self._extract_basename(filename)\n        # Remove 'HLS_' or 'BS_' prefix\n        casename = basename.replace(\"HLS_\", \"\").replace(\"BS_\", \"\")\n        return casename\n\n    def __len__(self) -&gt; int:\n        return len(self.samples)\n\n    def _get_coords(self, image: DataArray) -&gt; torch.Tensor:\n        pixel_scale = image.rio.resolution()\n        width, height = image.rio.width, image.rio.height\n\n        left, bottom, right, top = image.rio.bounds()\n        tie_point_x, tie_point_y = left, top\n\n        center_col = width / 2\n        center_row = height / 2\n\n        center_lon = tie_point_x + (center_col * pixel_scale[0])\n        center_lat = tie_point_y - (center_row * pixel_scale[1])\n\n        lat_lon = np.asarray([center_lat, center_lon])\n        return torch.tensor(lat_lon, dtype=torch.float32)\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        sample = self.samples[index]\n        image_files = sample[\"image_files\"]\n        mask_file = sample[\"mask_file\"]\n\n        images = []\n        for idx, image_file in enumerate(image_files):\n            image = self._load_file(Path(image_file), nan_replace=self.no_data_replace)\n            if idx == 0 and self.use_metadata:\n                location_coords = self._get_coords(image)\n            image = image.to_numpy()\n            image = np.moveaxis(image, 0, -1)\n            image = image[..., self.band_indices]\n            images.append(image)\n\n        images = np.stack(images, axis=0)  # (T, H, W, C)\n\n        output = {\n            \"image\": images.astype(np.float32),\n            \"mask\": self._load_file(Path(mask_file), nan_replace=self.no_label_replace).to_numpy()[0]\n        }\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"mask\"] = output[\"mask\"].long()\n        if self.use_metadata:\n            output[\"location_coords\"] = location_coords\n\n        return output\n\n    def _load_file(self, path: Path, nan_replace: float | int | None = None) -&gt; DataArray:\n        data = rioxarray.open_rasterio(path, masked=True)\n        if nan_replace is not None:\n            data = data.fillna(nan_replace)\n        return data\n\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Any:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: A sample returned by `__getitem__`.\n            suptitle: Optional string to use as a suptitle.\n\n        Returns:\n            A matplotlib Figure with the rendered sample.\n        \"\"\"\n        num_images = len(self.time_steps) + 2\n        if \"prediction\" in sample:\n            num_images += 1\n\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        images = sample[\"image\"]  # (C, T, H, W)\n        mask = sample[\"mask\"].numpy()\n        num_classes = len(np.unique(mask))\n\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 5, 5))\n\n        for i in range(len(self.time_steps)):\n            image = images[:, i, :, :]  # (C, H, W)\n            image = np.transpose(image, (1, 2, 0))  # (H, W, C)\n            rgb_image = image[..., rgb_indices]\n            rgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min() + 1e-8)\n            rgb_image = np.clip(rgb_image, 0, 1)\n            ax[i].imshow(rgb_image)\n            ax[i].axis(\"off\")\n            ax[i].set_title(f\"{self.time_steps[i].capitalize()} Image\")\n\n        cmap = plt.get_cmap(\"jet\", num_classes)\n        norm = Normalize(vmin=0, vmax=num_classes - 1)\n\n        mask_ax_index = len(self.time_steps)\n        ax[mask_ax_index].imshow(mask, cmap=cmap, norm=norm)\n        ax[mask_ax_index].axis(\"off\")\n        ax[mask_ax_index].set_title(\"Ground Truth Mask\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"].numpy()\n            pred_ax_index = mask_ax_index + 1\n            ax[pred_ax_index].imshow(prediction, cmap=cmap, norm=norm)\n            ax[pred_ax_index].axis(\"off\")\n            ax[pred_ax_index].set_title(\"Predicted Mask\")\n\n        legend_ax_index = -1\n        class_names = sample.get(\"class_names\", self.class_names)\n        positions = np.linspace(0, 1, num_classes) if num_classes &gt; 1 else [0.5]\n\n        legend_handles = [\n            mpatches.Patch(color=cmap(pos), label=class_names[i])\n            for i, pos in enumerate(positions)\n        ]\n        ax[legend_ax_index].legend(handles=legend_handles, loc=\"center\")\n        ax[legend_ax_index].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        plt.tight_layout()\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.burn_intensity.BurnIntensityNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, use_full_data=True, no_data_replace=0.0001, no_label_replace=-1, use_metadata=False)</code>","text":"<p>Initialize the BurnIntensity dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train' or 'val'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to output. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Optional[Compose]</code> <p>Albumentations transform to be applied.</p> <code>None</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (location).</p> <code>False</code> <code>use_full_data</code> <code>bool</code> <p>Wheter to use full data or data with less than 25 percent zeros.</p> <code>True</code> <code>no_data_replace</code> <code>Optional[float]</code> <p>Value to replace NaNs in images.</p> <code>0.0001</code> <code>no_label_replace</code> <code>Optional[int]</code> <p>Value to replace NaNs in labels.</p> <code>-1</code> Source code in <code>terratorch/datasets/burn_intensity.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    use_full_data: bool = True,\n    no_data_replace: float | None = 0.0001,\n    no_label_replace: int | None = -1,\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the BurnIntensity dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train' or 'val'.\n        bands (Sequence[str]): Bands to output. Defaults to all bands.\n        transform (Optional[A.Compose]): Albumentations transform to be applied.\n        use_metadata (bool): Whether to return metadata info (location).\n        use_full_data (bool): Wheter to use full data or data with less than 25 percent zeros.\n        no_data_replace (Optional[float]): Value to replace NaNs in images.\n        no_label_replace (Optional[int]): Value to replace NaNs in labels.\n    \"\"\"\n    super().__init__()\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n\n    self.data_root = Path(data_root)\n\n    # Read the CSV file to get the list of cases to include\n    csv_file_key = \"full\" if use_full_data else \"limited\"\n    csv_path = self.data_root / self.CSV_FILES[csv_file_key]\n    df = pd.read_csv(csv_path)\n    casenames = df[\"Case_Name\"].tolist()\n\n    split_file = self.data_root / f\"{split}.txt\"\n    with open(split_file) as f:\n        split_images = [line.strip() for line in f.readlines()]\n\n    split_images = [img for img in split_images if self._extract_casename(img) in casenames]\n\n    # Build the samples list\n    self.samples = []\n    for image_filename in split_images:\n        image_files = []\n        for time_step in self.time_steps:\n            image_file = self.data_root / time_step / image_filename\n            image_files.append(str(image_file))\n        mask_filename = image_filename.replace(\"HLS_\", \"BS_\")\n        mask_file = self.data_root / \"pre\" / mask_filename\n        self.samples.append({\n            \"image_files\": image_files,\n            \"mask_file\": str(mask_file),\n            \"casename\": self._extract_casename(image_filename),\n        })\n\n    self.use_metadata = use_metadata\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.burn_intensity.BurnIntensityNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by <code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/burn_intensity.py</code> <pre><code>def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Any:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: A sample returned by `__getitem__`.\n        suptitle: Optional string to use as a suptitle.\n\n    Returns:\n        A matplotlib Figure with the rendered sample.\n    \"\"\"\n    num_images = len(self.time_steps) + 2\n    if \"prediction\" in sample:\n        num_images += 1\n\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    images = sample[\"image\"]  # (C, T, H, W)\n    mask = sample[\"mask\"].numpy()\n    num_classes = len(np.unique(mask))\n\n    fig, ax = plt.subplots(1, num_images, figsize=(num_images * 5, 5))\n\n    for i in range(len(self.time_steps)):\n        image = images[:, i, :, :]  # (C, H, W)\n        image = np.transpose(image, (1, 2, 0))  # (H, W, C)\n        rgb_image = image[..., rgb_indices]\n        rgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min() + 1e-8)\n        rgb_image = np.clip(rgb_image, 0, 1)\n        ax[i].imshow(rgb_image)\n        ax[i].axis(\"off\")\n        ax[i].set_title(f\"{self.time_steps[i].capitalize()} Image\")\n\n    cmap = plt.get_cmap(\"jet\", num_classes)\n    norm = Normalize(vmin=0, vmax=num_classes - 1)\n\n    mask_ax_index = len(self.time_steps)\n    ax[mask_ax_index].imshow(mask, cmap=cmap, norm=norm)\n    ax[mask_ax_index].axis(\"off\")\n    ax[mask_ax_index].set_title(\"Ground Truth Mask\")\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"].numpy()\n        pred_ax_index = mask_ax_index + 1\n        ax[pred_ax_index].imshow(prediction, cmap=cmap, norm=norm)\n        ax[pred_ax_index].axis(\"off\")\n        ax[pred_ax_index].set_title(\"Predicted Mask\")\n\n    legend_ax_index = -1\n    class_names = sample.get(\"class_names\", self.class_names)\n    positions = np.linspace(0, 1, num_classes) if num_classes &gt; 1 else [0.5]\n\n    legend_handles = [\n        mpatches.Patch(color=cmap(pos), label=class_names[i])\n        for i, pos in enumerate(positions)\n    ]\n    ax[legend_ax_index].legend(handles=legend_handles, loc=\"center\")\n    ax[legend_ax_index].axis(\"off\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.carbonflux","title":"<code>terratorch.datasets.carbonflux</code>","text":""},{"location":"package/datasets/#terratorch.datasets.carbonflux.CarbonFluxNonGeo","title":"<code>CarbonFluxNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>Dataset for Carbon Flux regression from HLS images and MERRA data.</p> Source code in <code>terratorch/datasets/carbonflux.py</code> <pre><code>class CarbonFluxNonGeo(NonGeoDataset):\n    \"\"\"Dataset for [Carbon Flux](https://huggingface.co/datasets/ibm-nasa-geospatial/hls_merra2_gppFlux) regression from HLS images and MERRA data.\"\"\"\n\n    all_band_names = (\n        \"BLUE\", \"GREEN\", \"RED\", \"NIR\", \"SWIR_1\", \"SWIR_2\",\n    )\n\n    rgb_bands = (\n        \"RED\", \"GREEN\", \"BLUE\",\n    )\n\n    merra_var_names = (\n        \"T2MIN\", \"T2MAX\", \"T2MEAN\", \"TSMDEWMEAN\", \"GWETROOT\",\n        \"LHLAND\", \"SHLAND\", \"SWLAND\", \"PARDFLAND\", \"PRECTOTLAND\"\n    )\n\n    splits = {\"train\": \"train\", \"test\": \"test\"}\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    metadata_file = \"data_train_hls_37sites_v0_1.csv\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        gpp_mean: float | None = None,\n        gpp_std: float | None = None,\n        no_data_replace: float | None = 0.0001,\n        use_metadata: bool = False,\n        modalities: Sequence[str] = (\"image\", \"merra_vars\")\n    ) -&gt; None:\n        \"\"\"Initialize the CarbonFluxNonGeo dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): 'train' or 'test'.\n            bands (Sequence[str]): Bands to use. Defaults to all bands.\n            transform (Optional[A.Compose]): Albumentations transform to be applied.\n            use_metadata (bool): Whether to return metadata (coordinates and date).\n            merra_means (Sequence[float]): Means for MERRA data normalization.\n            merra_stds (Sequence[float]): Standard deviations for MERRA data normalization.\n            gpp_mean (float): Mean for GPP normalization.\n            gpp_std (float): Standard deviation for GPP normalization.\n            no_data_replace (Optional[float]): Value to replace NO_DATA values in images.\n        \"\"\"\n        super().__init__()\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = [self.all_band_names.index(band) for band in bands]\n\n        self.data_root = Path(data_root)\n\n        # Load the CSV file with metadata\n        csv_file = self.data_root / self.metadata_file\n        df = pd.read_csv(csv_file)\n\n        # Get list of image filenames in the split directory\n        image_dir = self.data_root / self.split\n        image_files = [f.name for f in image_dir.glob(\"*.tiff\")]\n\n        df[\"Chip\"] = df[\"Chip\"].str.replace(\".tif$\", \".tiff\", regex=True)\n        # Filter the DataFrame to include only rows with 'Chip' in image_files\n        df = df[df[\"Chip\"].isin(image_files)]\n\n        # Build the samples list\n        self.samples = []\n        for _, row in df.iterrows():\n            image_filename = row[\"Chip\"]\n            image_path = image_dir / image_filename\n            # MERRA vectors\n            merra_vars = row[list(self.merra_var_names)].values.astype(np.float32)\n            # GPP target\n            gpp = row[\"GPP\"]\n\n            image_path = image_dir / row[\"Chip\"]\n            merra_vars = row[list(self.merra_var_names)].values.astype(np.float32)\n            gpp = row[\"GPP\"]\n            self.samples.append({\n                \"image_path\": str(image_path),\n                \"merra_vars\": merra_vars,\n                \"gpp\": gpp,\n            })\n\n        if gpp_mean is None or gpp_std is None:\n            msg = \"Mean and standard deviation for GPP must be provided.\"\n            raise ValueError(msg)\n        self.gpp_mean = gpp_mean\n        self.gpp_std = gpp_std\n\n        self.use_metadata = use_metadata\n        self.modalities = modalities\n        self.no_data_replace = no_data_replace\n\n        if transform is None:\n            self.transform = MultimodalToTensor(self.modalities)\n        else:\n            transform = {m: transform[m] if m in transform else default_transform\n                for m in self.modalities}\n            self.transform = MultimodalTransforms(transform, shared=False)\n\n    def __len__(self) -&gt; int:\n        return len(self.samples)\n\n    def _load_file(self, path: str, nan_replace: float | int | None = None):\n        data = rioxarray.open_rasterio(path, masked=True)\n        if nan_replace is not None:\n            data = data.fillna(nan_replace)\n        return data\n\n    def _get_coords(self, image) -&gt; torch.Tensor:\n        \"\"\"Extract the center coordinates from the image geospatial metadata.\"\"\"\n        pixel_scale = image.rio.resolution()\n        width, height = image.rio.width, image.rio.height\n\n        left, bottom, right, top = image.rio.bounds()\n        tie_point_x, tie_point_y = left, top\n\n        center_col = width / 2\n        center_row = height / 2\n\n        center_lon = tie_point_x + (center_col * pixel_scale[0])\n        center_lat = tie_point_y - (center_row * pixel_scale[1])\n\n        src_crs = image.rio.crs\n        dst_crs = \"EPSG:4326\"\n\n        transformer = pyproj.Transformer.from_crs(src_crs, dst_crs, always_xy=True)\n        lon, lat = transformer.transform(center_lon, center_lat)\n\n        coords = np.array([lat, lon], dtype=np.float32)\n        return torch.from_numpy(coords)\n\n    def _get_date(self, filename: str) -&gt; torch.Tensor:\n        \"\"\"Extract the date from the filename.\"\"\"\n        base_filename = os.path.basename(filename)\n        pattern = r\"HLS\\..{3}\\.[A-Z0-9]{6}\\.(?P&lt;date&gt;\\d{7}T\\d{6})\\..*\\.tiff$\"\n        match = re.match(pattern, base_filename)\n        if not match:\n            msg = f\"Filename {filename} does not match expected pattern.\"\n            raise ValueError(msg)\n\n        date_str = match.group(\"date\")\n        year = int(date_str[:4])\n        julian_day = int(date_str[4:7])\n\n        date_tensor = torch.tensor([year, julian_day], dtype=torch.int32)\n        return date_tensor\n\n    def __getitem__(self, idx: int) -&gt; dict[str, Any]:\n        sample = self.samples[idx]\n        image_path = sample[\"image_path\"]\n\n        image = self._load_file(image_path, nan_replace=self.no_data_replace)\n\n        if self.use_metadata:\n            location_coords = self._get_coords(image)\n            temporal_coords = self._get_date(os.path.basename(image_path))\n\n        image = image.to_numpy()  # (C, H, W)\n        image = image[self.band_indices, ...]\n        image = np.moveaxis(image, 0, -1) # (H, W, C)\n\n        merra_vars = np.array(sample[\"merra_vars\"])\n        target = np.array(sample[\"gpp\"])\n        target_norm = (target - self.gpp_mean) / self.gpp_std\n        target_norm = torch.tensor(target_norm, dtype=torch.float32)\n        output = {\n            \"image\": image.astype(np.float32),\n            \"merra_vars\": merra_vars,\n        }\n\n        if self.transform:\n            output = self.transform(output)\n\n        output = {\n            \"image\": {m: output[m] for m in self.modalities if m in output},\n            \"mask\": target_norm\n        }\n        if self.use_metadata:\n            output[\"location_coords\"] = location_coords\n            output[\"temporal_coords\"] = temporal_coords\n\n        return output\n\n    def plot(self, sample: dict[str, Any], suptitle: str | None = None) -&gt; Any:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: A sample returned by `__getitem__`.\n            suptitle: Optional title for the figure.\n\n        Returns:\n            A matplotlib figure with the rendered sample.\n        \"\"\"\n        image = sample[\"image\"].numpy()\n\n        image = np.transpose(image, (1, 2, 0))  # (H, W, C)\n\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        rgb_image = image[..., rgb_indices]\n\n        rgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min() + 1e-8)\n        rgb_image = np.clip(rgb_image, 0, 1)\n\n        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n        ax.imshow(rgb_image)\n        ax.axis(\"off\")\n        ax.set_title(\"Image\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        plt.tight_layout()\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.carbonflux.CarbonFluxNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, gpp_mean=None, gpp_std=None, no_data_replace=0.0001, use_metadata=False, modalities=('image', 'merra_vars'))</code>","text":"<p>Initialize the CarbonFluxNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>'train' or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to use. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Optional[Compose]</code> <p>Albumentations transform to be applied.</p> <code>None</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata (coordinates and date).</p> <code>False</code> <code>merra_means</code> <code>Sequence[float]</code> <p>Means for MERRA data normalization.</p> required <code>merra_stds</code> <code>Sequence[float]</code> <p>Standard deviations for MERRA data normalization.</p> required <code>gpp_mean</code> <code>float</code> <p>Mean for GPP normalization.</p> <code>None</code> <code>gpp_std</code> <code>float</code> <p>Standard deviation for GPP normalization.</p> <code>None</code> <code>no_data_replace</code> <code>Optional[float]</code> <p>Value to replace NO_DATA values in images.</p> <code>0.0001</code> Source code in <code>terratorch/datasets/carbonflux.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    gpp_mean: float | None = None,\n    gpp_std: float | None = None,\n    no_data_replace: float | None = 0.0001,\n    use_metadata: bool = False,\n    modalities: Sequence[str] = (\"image\", \"merra_vars\")\n) -&gt; None:\n    \"\"\"Initialize the CarbonFluxNonGeo dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): 'train' or 'test'.\n        bands (Sequence[str]): Bands to use. Defaults to all bands.\n        transform (Optional[A.Compose]): Albumentations transform to be applied.\n        use_metadata (bool): Whether to return metadata (coordinates and date).\n        merra_means (Sequence[float]): Means for MERRA data normalization.\n        merra_stds (Sequence[float]): Standard deviations for MERRA data normalization.\n        gpp_mean (float): Mean for GPP normalization.\n        gpp_std (float): Standard deviation for GPP normalization.\n        no_data_replace (Optional[float]): Value to replace NO_DATA values in images.\n    \"\"\"\n    super().__init__()\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = [self.all_band_names.index(band) for band in bands]\n\n    self.data_root = Path(data_root)\n\n    # Load the CSV file with metadata\n    csv_file = self.data_root / self.metadata_file\n    df = pd.read_csv(csv_file)\n\n    # Get list of image filenames in the split directory\n    image_dir = self.data_root / self.split\n    image_files = [f.name for f in image_dir.glob(\"*.tiff\")]\n\n    df[\"Chip\"] = df[\"Chip\"].str.replace(\".tif$\", \".tiff\", regex=True)\n    # Filter the DataFrame to include only rows with 'Chip' in image_files\n    df = df[df[\"Chip\"].isin(image_files)]\n\n    # Build the samples list\n    self.samples = []\n    for _, row in df.iterrows():\n        image_filename = row[\"Chip\"]\n        image_path = image_dir / image_filename\n        # MERRA vectors\n        merra_vars = row[list(self.merra_var_names)].values.astype(np.float32)\n        # GPP target\n        gpp = row[\"GPP\"]\n\n        image_path = image_dir / row[\"Chip\"]\n        merra_vars = row[list(self.merra_var_names)].values.astype(np.float32)\n        gpp = row[\"GPP\"]\n        self.samples.append({\n            \"image_path\": str(image_path),\n            \"merra_vars\": merra_vars,\n            \"gpp\": gpp,\n        })\n\n    if gpp_mean is None or gpp_std is None:\n        msg = \"Mean and standard deviation for GPP must be provided.\"\n        raise ValueError(msg)\n    self.gpp_mean = gpp_mean\n    self.gpp_std = gpp_std\n\n    self.use_metadata = use_metadata\n    self.modalities = modalities\n    self.no_data_replace = no_data_replace\n\n    if transform is None:\n        self.transform = MultimodalToTensor(self.modalities)\n    else:\n        transform = {m: transform[m] if m in transform else default_transform\n            for m in self.modalities}\n        self.transform = MultimodalTransforms(transform, shared=False)\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.carbonflux.CarbonFluxNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Any]</code> <p>A sample returned by <code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional title for the figure.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A matplotlib figure with the rendered sample.</p> Source code in <code>terratorch/datasets/carbonflux.py</code> <pre><code>def plot(self, sample: dict[str, Any], suptitle: str | None = None) -&gt; Any:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: A sample returned by `__getitem__`.\n        suptitle: Optional title for the figure.\n\n    Returns:\n        A matplotlib figure with the rendered sample.\n    \"\"\"\n    image = sample[\"image\"].numpy()\n\n    image = np.transpose(image, (1, 2, 0))  # (H, W, C)\n\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    rgb_image = image[..., rgb_indices]\n\n    rgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min() + 1e-8)\n    rgb_image = np.clip(rgb_image, 0, 1)\n\n    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n    ax.imshow(rgb_image)\n    ax.axis(\"off\")\n    ax.set_title(\"Image\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.forestnet","title":"<code>terratorch.datasets.forestnet</code>","text":""},{"location":"package/datasets/#terratorch.datasets.forestnet.ForestNetNonGeo","title":"<code>ForestNetNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for ForestNet.</p> Source code in <code>terratorch/datasets/forestnet.py</code> <pre><code>class ForestNetNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [ForestNet](https://huggingface.co/datasets/ibm-nasa-geospatial/ForestNet).\"\"\"\n\n    all_band_names = (\n        \"RED\", \"GREEN\", \"BLUE\", \"NIR\", \"SWIR_1\", \"SWIR_2\"\n    )\n\n    rgb_bands = (\n        \"RED\", \"GREEN\", \"BLUE\",\n    )\n\n    splits = (\"train\", \"test\", \"val\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    default_label_map = {  # noqa: RUF012\n        \"Plantation\": 0,\n        \"Smallholder agriculture\": 1,\n        \"Grassland shrubland\": 2,\n        \"Other\": 3,\n    }\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        label_map: dict[str, int] = default_label_map,\n        transform: A.Compose | None = None,\n        fraction: float = 1.0,\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ForestNetNonGeo dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            label_map (Dict[str, int]): Mapping from label names to integer labels.\n            transform: Transformations to be applied to the images.\n            fraction (float): Fraction of the dataset to use. Defaults to 1.0 (use all data).\n        \"\"\"\n        super().__init__()\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits)}.\"\n            raise ValueError(msg)\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n        self.use_metadata = use_metadata\n\n        self.data_root = Path(data_root)\n        self.label_map = label_map\n\n        # Load the CSV file corresponding to the split\n        csv_file = self.data_root / f\"{split}_filtered.csv\"\n        original_df = pd.read_csv(csv_file)\n\n        # Apply stratified sampling if fraction &lt; 1.0\n        if fraction &lt; 1.0:\n            sss = StratifiedShuffleSplit(n_splits=1, test_size=1 - fraction, random_state=47)\n            stratified_indices, _ = next(sss.split(original_df, original_df[\"merged_label\"]))\n            self.dataset = original_df.iloc[stratified_indices].reset_index(drop=True)\n        else:\n            self.dataset = original_df\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.dataset)\n\n    def _get_coords(self, event_path: Path) -&gt; torch.Tensor:\n        auxiliary_path = event_path / \"auxiliary\"\n        osm_json_path = auxiliary_path / \"osm.json\"\n\n        with open(osm_json_path) as f:\n            osm_data = json.load(f)\n            lat = float(osm_data[\"closest_city\"][\"lat\"])\n            lon = float(osm_data[\"closest_city\"][\"lon\"])\n            lat_lon = np.asarray([lat, lon])\n\n        return torch.tensor(lat_lon, dtype=torch.float32)\n\n    def _get_dates(self, image_files: list) -&gt; list:\n        dates = []\n        pattern = re.compile(r\"(\\d{4})_(\\d{2})_(\\d{2})_cloud_\\d+\\.(png|npy)\")\n        for img_path in image_files:\n            match = pattern.search(img_path)\n            year, month, day = int(match.group(1)), int(match.group(2)), int(match.group(3))\n            date_obj = datetime.datetime(year, month, day)  # noqa: DTZ001\n            julian_day = date_obj.timetuple().tm_yday\n            date_tensor = torch.tensor([year, julian_day], dtype=torch.int32)\n            dates.append(date_tensor)\n        return torch.stack(dates, dim=0)\n\n    def __getitem__(self, index: int):\n        path = self.data_root / self.dataset[\"example_path\"][index]\n        label = self.map_label(index)\n\n        visible_images, infrared_images, temporal_coords = self._load_images(path)\n\n        visible_images = np.stack(visible_images, axis=0)\n        infrared_images = np.stack(infrared_images, axis=0)\n        merged_images = np.concatenate([visible_images, infrared_images], axis=-1)\n        merged_images = merged_images[..., self.band_indices] # (T, H, W, 2C)\n        output = {\n            \"image\": merged_images.astype(np.float32)\n        }\n\n        if self.transform:\n            output = self.transform(**output)\n\n        if self.use_metadata:\n            location_coords = self._get_coords(path)\n            output[\"location_coords\"] = location_coords\n            output[\"temporal_coords\"] = temporal_coords\n\n        output[\"label\"] = label\n\n        return output\n\n    def _load_images(self, path: str):\n        \"\"\"Load visible and infrared images from the given event path\"\"\"\n        visible_image_files = glob.glob(os.path.join(path, \"images/visible/*_cloud_*.png\"))\n        infra_image_files = glob.glob(os.path.join(path, \"images/infrared/*_cloud_*.npy\"))\n\n        selected_visible_images = self.select_images(visible_image_files)\n        selected_infra_images = self.select_images(infra_image_files)\n\n        dates = None\n        if self.use_metadata:\n            dates = self._get_dates(selected_visible_images)\n\n        vis_images = [np.array(Image.open(img)) for img in selected_visible_images] # (T, H, W, C)\n        inf_images = [np.load(img, allow_pickle=True) for img in selected_infra_images] # (T, H, W, C)\n        return vis_images, inf_images, dates\n\n    def least_cloudy_image(self, image_files):\n        pattern = re.compile(r\"(\\d{4})_\\d{2}_\\d{2}_cloud_(\\d+)\\.(png|npy)\")\n        lowest_cloud_images = defaultdict(lambda: {\"path\": None, \"cloud_value\": float(\"inf\")})\n\n        for path in image_files:\n            match = pattern.search(path)\n            if match:\n                year, cloud_value = match.group(1), int(match.group(2))\n                if cloud_value &lt; lowest_cloud_images[year][\"cloud_value\"]:\n                    lowest_cloud_images[year] = {\"path\": path, \"cloud_value\": cloud_value}\n\n        return [info[\"path\"] for info in lowest_cloud_images.values()]\n\n    def match_timesteps(self, image_files, selected_images):\n        if len(selected_images) &lt; 3:\n            extra_imgs = [img for img in image_files if img not in selected_images]\n            selected_images += extra_imgs[:3 - len(selected_images)]\n\n        while len(selected_images) &lt; 3:\n            selected_images.append(selected_images[-1])\n        return selected_images[:3]\n\n    def select_images(self, image_files):\n        selected = self.least_cloudy_image(image_files)\n        return self.match_timesteps(image_files, selected)\n\n    def map_label(self, index: int) -&gt; torch.Tensor:\n        \"\"\"Map the label name to an integer label.\"\"\"\n        label_name = self.dataset[\"merged_label\"][index]\n        label = self.label_map[label_name]\n        return label\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None):\n\n        num_images = sample[\"image\"].shape[1] + 1\n\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        fig, ax = plt.subplots(1, num_images, figsize=(15, 5))\n\n        for i in range(sample[\"image\"].shape[1]):\n            image = sample[\"image\"][:, i, :, :]\n            if torch.is_tensor(image):\n                image = image.permute(1, 2, 0).numpy()\n            rgb_image = image[..., rgb_indices]\n            rgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min() + 1e-8)\n            rgb_image = np.clip(rgb_image, 0, 1)\n            ax[i].imshow(rgb_image)\n            ax[i].axis(\"off\")\n            ax[i].set_title(f\"Timestep {i + 1}\")\n\n        legend_handles = [Rectangle((0, 0), 1, 1, color=\"blue\")]\n        legend_label = [self.label_map.get(sample[\"label\"], \"Unknown Label\")]\n        ax[-1].legend(legend_handles, legend_label, loc=\"center\")\n        ax[-1].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        plt.tight_layout()\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.forestnet.ForestNetNonGeo.__init__","title":"<code>__init__(data_root, split='train', label_map=default_label_map, transform=None, fraction=1.0, bands=BAND_SETS['all'], use_metadata=False)</code>","text":"<p>Initialize the ForestNetNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>label_map</code> <code>Dict[str, int]</code> <p>Mapping from label names to integer labels.</p> <code>default_label_map</code> <code>transform</code> <code>Compose | None</code> <p>Transformations to be applied to the images.</p> <code>None</code> <code>fraction</code> <code>float</code> <p>Fraction of the dataset to use. Defaults to 1.0 (use all data).</p> <code>1.0</code> Source code in <code>terratorch/datasets/forestnet.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    label_map: dict[str, int] = default_label_map,\n    transform: A.Compose | None = None,\n    fraction: float = 1.0,\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the ForestNetNonGeo dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        label_map (Dict[str, int]): Mapping from label names to integer labels.\n        transform: Transformations to be applied to the images.\n        fraction (float): Fraction of the dataset to use. Defaults to 1.0 (use all data).\n    \"\"\"\n    super().__init__()\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits)}.\"\n        raise ValueError(msg)\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n    self.use_metadata = use_metadata\n\n    self.data_root = Path(data_root)\n    self.label_map = label_map\n\n    # Load the CSV file corresponding to the split\n    csv_file = self.data_root / f\"{split}_filtered.csv\"\n    original_df = pd.read_csv(csv_file)\n\n    # Apply stratified sampling if fraction &lt; 1.0\n    if fraction &lt; 1.0:\n        sss = StratifiedShuffleSplit(n_splits=1, test_size=1 - fraction, random_state=47)\n        stratified_indices, _ = next(sss.split(original_df, original_df[\"merged_label\"]))\n        self.dataset = original_df.iloc[stratified_indices].reset_index(drop=True)\n    else:\n        self.dataset = original_df\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.forestnet.ForestNetNonGeo.map_label","title":"<code>map_label(index)</code>","text":"<p>Map the label name to an integer label.</p> Source code in <code>terratorch/datasets/forestnet.py</code> <pre><code>def map_label(self, index: int) -&gt; torch.Tensor:\n    \"\"\"Map the label name to an integer label.\"\"\"\n    label_name = self.dataset[\"merged_label\"][index]\n    label = self.label_map[label_name]\n    return label\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.fire_scars","title":"<code>terratorch.datasets.fire_scars</code>","text":""},{"location":"package/datasets/#terratorch.datasets.fire_scars.FireScarsHLS","title":"<code>FireScarsHLS</code>","text":"<p>               Bases: <code>RasterDataset</code></p> <p>RasterDataset implementation for fire scars input images.</p> Source code in <code>terratorch/datasets/fire_scars.py</code> <pre><code>class FireScarsHLS(RasterDataset):\n    \"\"\"RasterDataset implementation for fire scars input images.\"\"\"\n\n    filename_glob = \"subsetted*_merged.tif\"\n    filename_regex = r\"subsetted_512x512_HLS\\..30\\..{6}\\.(?P&lt;date&gt;[0-9]*)\\.v1.4_merged.tif\"\n    date_format = \"%Y%j\"\n    is_image = True\n    separate_files = False\n    all_bands = dataclasses.field(default_factory=[\"B02\", \"B03\", \"B04\", \"B8A\", \"B11\", \"B12\"])\n    rgb_bands = dataclasses.field(default_factory=[\"B04\", \"B03\", \"B02\"])\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.fire_scars.FireScarsNonGeo","title":"<code>FireScarsNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for fire scars.</p> Source code in <code>terratorch/datasets/fire_scars.py</code> <pre><code>class FireScarsNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [fire scars](https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars).\"\"\"\n    all_band_names = (\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"NIR_NARROW\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    num_classes = 2\n    splits = {\"train\": \"training\", \"val\": \"validation\"}   # Only train and val splits available\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        no_data_replace: float | None = 0,\n        no_label_replace: int | None = -1,\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (str): Path to the data root directory.\n            bands (list[str]): Bands that should be output by the dataset. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). If used through the corresponding data module,\n                should not include normalization. Defaults to None, which applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input images with this value.\n                If None, does no replacement. Defaults to 0.\n            no_label_replace (int | None): Replace nan values in label with this value.\n                If none, does no replacement. Defaults to -1.\n            use_metadata (bool): whether to return metadata info (time and location).\n        \"\"\"\n        super().__init__()\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {self.splits}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n        self.data_root = Path(data_root)\n\n        input_dir = self.data_root / split_name\n        self.image_files = sorted(glob.glob(os.path.join(input_dir, \"*_merged.tif\")))\n        self.segmentation_mask_files = sorted(glob.glob(os.path.join(input_dir, \"*.mask.tif\")))\n\n        self.use_metadata = use_metadata\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n\n        # If no transform is given, apply only to transform to torch tensor\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def _get_date(self, index: int) -&gt; torch.Tensor:\n        file_name = self.image_files[index]\n        base_filename = os.path.basename(file_name)\n\n        filename_regex = r\"subsetted_512x512_HLS\\.S30\\.T[0-9A-Z]{5}\\.(?P&lt;date&gt;[0-9]+)\\.v1\\.4_merged\\.tif\"\n        match = re.match(filename_regex, base_filename)\n        date_str = match.group(\"date\")\n        year = int(date_str[:4])\n        julian_day = int(date_str[4:])\n\n        return torch.tensor([[year, julian_day]], dtype=torch.float32)\n\n    def _get_coords(self, image: DataArray) -&gt; torch.Tensor:\n        px = image.x.shape[0] // 2\n        py = image.y.shape[0] // 2\n\n        # get center point to reproject to lat/lon\n        point = image.isel(band=0, x=slice(px, px + 1), y=slice(py, py + 1))\n        point = point.rio.reproject(\"epsg:4326\")\n\n        lat_lon = np.asarray([point.y[0], point.x[0]])\n\n        return torch.tensor(lat_lon, dtype=torch.float32)\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        image = self._load_file(self.image_files[index], nan_replace=self.no_data_replace)\n\n        location_coords, temporal_coords = None, None\n        if self.use_metadata:\n            location_coords = self._get_coords(image)\n            temporal_coords = self._get_date(index)\n\n        # to channels last\n        image = image.to_numpy()\n        image = np.moveaxis(image, 0, -1)\n\n        # filter bands\n        image = image[..., self.band_indices]\n\n        output = {\n            \"image\": image.astype(np.float32),\n            \"mask\": self._load_file(\n                self.segmentation_mask_files[index], nan_replace=self.no_label_replace).to_numpy()[0],\n        }\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        if self.use_metadata:\n            output[\"location_coords\"] = location_coords\n            output[\"temporal_coords\"] = temporal_coords\n\n        return output\n\n    def _load_file(self, path: Path, nan_replace: int | float | None = None) -&gt; DataArray:\n        data = rioxarray.open_rasterio(path, masked=True)\n        if nan_replace is not None:\n            data = data.fillna(nan_replace)\n        return data\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            suptitle: optional string to use as a suptitle\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n        \"\"\"\n        num_images = 4\n\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        # RGB -&gt; channels-last\n        image = sample[\"image\"][rgb_indices, ...].permute(1, 2, 0).numpy()\n        mask = sample[\"mask\"].numpy()\n\n        image = clip_image_percentile(image)\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"]\n            num_images += 1\n        else:\n            prediction = None\n\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 5), layout=\"compressed\")\n\n        ax[0].axis(\"off\")\n\n        norm = mpl.colors.Normalize(vmin=0, vmax=self.num_classes - 1)\n        ax[1].axis(\"off\")\n        ax[1].title.set_text(\"Image\")\n        ax[1].imshow(image)\n\n        ax[2].axis(\"off\")\n        ax[2].title.set_text(\"Ground Truth Mask\")\n        ax[2].imshow(mask, cmap=\"jet\", norm=norm)\n\n        ax[3].axis(\"off\")\n        ax[3].title.set_text(\"GT Mask on Image\")\n        ax[3].imshow(image)\n        ax[3].imshow(mask, cmap=\"jet\", alpha=0.3, norm=norm)\n\n        if \"prediction\" in sample:\n            ax[4].title.set_text(\"Predicted Mask\")\n            ax[4].imshow(prediction, cmap=\"jet\", norm=norm)\n\n        cmap = plt.get_cmap(\"jet\")\n        legend_data = [[i, cmap(norm(i)), str(i)] for i in range(self.num_classes)]\n        handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n        labels = [n for k, c, n in legend_data]\n        ax[0].legend(handles, labels, loc=\"center\")\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.fire_scars.FireScarsNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, no_data_replace=0, no_label_replace=-1, use_metadata=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>bands</code> <code>list[str]</code> <p>Bands that should be output by the dataset. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If None, does no replacement. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>whether to return metadata info (time and location).</p> <code>False</code> Source code in <code>terratorch/datasets/fire_scars.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    no_data_replace: float | None = 0,\n    no_label_replace: int | None = -1,\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (str): Path to the data root directory.\n        bands (list[str]): Bands that should be output by the dataset. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). If used through the corresponding data module,\n            should not include normalization. Defaults to None, which applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input images with this value.\n            If None, does no replacement. Defaults to 0.\n        no_label_replace (int | None): Replace nan values in label with this value.\n            If none, does no replacement. Defaults to -1.\n        use_metadata (bool): whether to return metadata info (time and location).\n    \"\"\"\n    super().__init__()\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {self.splits}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n    self.data_root = Path(data_root)\n\n    input_dir = self.data_root / split_name\n    self.image_files = sorted(glob.glob(os.path.join(input_dir, \"*_merged.tif\")))\n    self.segmentation_mask_files = sorted(glob.glob(os.path.join(input_dir, \"*.mask.tif\")))\n\n    self.use_metadata = use_metadata\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n\n    # If no transform is given, apply only to transform to torch tensor\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.fire_scars.FireScarsNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> Source code in <code>terratorch/datasets/fire_scars.py</code> <pre><code>def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        suptitle: optional string to use as a suptitle\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n    \"\"\"\n    num_images = 4\n\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    # RGB -&gt; channels-last\n    image = sample[\"image\"][rgb_indices, ...].permute(1, 2, 0).numpy()\n    mask = sample[\"mask\"].numpy()\n\n    image = clip_image_percentile(image)\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"]\n        num_images += 1\n    else:\n        prediction = None\n\n    fig, ax = plt.subplots(1, num_images, figsize=(12, 5), layout=\"compressed\")\n\n    ax[0].axis(\"off\")\n\n    norm = mpl.colors.Normalize(vmin=0, vmax=self.num_classes - 1)\n    ax[1].axis(\"off\")\n    ax[1].title.set_text(\"Image\")\n    ax[1].imshow(image)\n\n    ax[2].axis(\"off\")\n    ax[2].title.set_text(\"Ground Truth Mask\")\n    ax[2].imshow(mask, cmap=\"jet\", norm=norm)\n\n    ax[3].axis(\"off\")\n    ax[3].title.set_text(\"GT Mask on Image\")\n    ax[3].imshow(image)\n    ax[3].imshow(mask, cmap=\"jet\", alpha=0.3, norm=norm)\n\n    if \"prediction\" in sample:\n        ax[4].title.set_text(\"Predicted Mask\")\n        ax[4].imshow(prediction, cmap=\"jet\", norm=norm)\n\n    cmap = plt.get_cmap(\"jet\")\n    legend_data = [[i, cmap(norm(i)), str(i)] for i in range(self.num_classes)]\n    handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n    labels = [n for k, c, n in legend_data]\n    ax[0].legend(handles, labels, loc=\"center\")\n    if suptitle is not None:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.fire_scars.FireScarsSegmentationMask","title":"<code>FireScarsSegmentationMask</code>","text":"<p>               Bases: <code>RasterDataset</code></p> <p>RasterDataset implementation for fire scars segmentation mask. Can be easily merged with input images using the &amp; operator.</p> Source code in <code>terratorch/datasets/fire_scars.py</code> <pre><code>class FireScarsSegmentationMask(RasterDataset):\n    \"\"\"RasterDataset implementation for fire scars segmentation mask.\n    Can be easily merged with input images using the &amp; operator.\n    \"\"\"\n\n    filename_glob = \"subsetted*.mask.tif\"\n    filename_regex = r\"subsetted_512x512_HLS\\..30\\..{6}\\.(?P&lt;date&gt;[0-9]*)\\.v1.4.mask.tif\"\n    date_format = \"%Y%j\"\n    is_image = False\n    separate_files = False\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.landslide4sense","title":"<code>terratorch.datasets.landslide4sense</code>","text":""},{"location":"package/datasets/#terratorch.datasets.landslide4sense.Landslide4SenseNonGeo","title":"<code>Landslide4SenseNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for Landslide4Sense.</p> Source code in <code>terratorch/datasets/landslide4sense.py</code> <pre><code>class Landslide4SenseNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [Landslide4Sense](https://huggingface.co/datasets/ibm-nasa-geospatial/Landslide4sense).\"\"\"\n    all_band_names = (\n        \"COASTAL AEROSOL\",\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"RED_EDGE_1\",\n        \"RED_EDGE_2\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"WATER_VAPOR\",\n        \"CIRRUS\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n        \"SLOPE\",\n        \"DEM\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"validation\", \"test\": \"test\"}\n\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Landslide4Sense dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'validation', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n        self.data_directory = Path(data_root)\n\n        images_dir = self.data_directory / \"images\" / split_name\n        annotations_dir = self.data_directory / \"annotations\" / split_name\n\n        self.image_files = sorted(images_dir.glob(\"image_*.h5\"))\n        self.mask_files = sorted(annotations_dir.glob(\"mask_*.h5\"))\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        image_file = self.image_files[index]\n        mask_file = self.mask_files[index]\n\n        with h5py.File(image_file, \"r\") as h5file:\n            image = np.array(h5file[\"img\"])[..., self.band_indices]\n\n        with h5py.File(mask_file, \"r\") as h5file:\n            mask = np.array(h5file[\"mask\"])\n\n        output = {\"image\": image.astype(np.float32), \"mask\": mask}\n\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        mask = sample[\"mask\"].numpy()\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()\n\n        rgb_image = image[:, :, rgb_indices]\n\n        rgb_image = (rgb_image - rgb_image.min(axis=(0, 1))) * (1 / rgb_image.max(axis=(0, 1)))\n        rgb_image = np.clip(rgb_image, 0, 1)\n\n        num_classes = len(np.unique(mask))\n        cmap = colormaps[\"jet\"]\n        norm = Normalize(vmin=0, vmax=num_classes - 1)\n\n        num_images = 4 if \"prediction\" in sample else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n        ax[0].imshow(rgb_image)\n        ax[0].set_title(\"Image\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(mask, cmap=cmap, norm=norm)\n        ax[1].set_title(\"Ground Truth Mask\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(rgb_image)\n        ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n        ax[2].set_title(\"GT Mask on Image\")\n        ax[2].axis(\"off\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"]\n            ax[3].imshow(prediction, cmap=cmap, norm=norm)\n            ax[3].set_title(\"Predicted Mask\")\n            ax[3].axis(\"off\")\n\n        if sample.get(\"class_names\"):\n            class_names = sample[\"class_names\"]\n            legend_handles = [\n                mpatches.Patch(color=cmap(i), label=class_names[i]) for i in range(num_classes)\n            ]\n            ax[0].legend(handles=legend_handles, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.landslide4sense.Landslide4SenseNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None)</code>","text":"<p>Initialize the Landslide4Sense dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'validation', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> Source code in <code>terratorch/datasets/landslide4sense.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Landslide4Sense dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'validation', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n    self.data_directory = Path(data_root)\n\n    images_dir = self.data_directory / \"images\" / split_name\n    annotations_dir = self.data_directory / \"annotations\" / split_name\n\n    self.image_files = sorted(images_dir.glob(\"image_*.h5\"))\n    self.mask_files = sorted(annotations_dir.glob(\"mask_*.h5\"))\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_eurosat","title":"<code>terratorch.datasets.m_eurosat</code>","text":""},{"location":"package/datasets/#terratorch.datasets.m_eurosat.MEuroSATNonGeo","title":"<code>MEuroSATNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-EuroSAT.</p> Source code in <code>terratorch/datasets/m_eurosat.py</code> <pre><code>class MEuroSATNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-EuroSAT](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\n        \"COASTAL_AEROSOL\",\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"RED_EDGE_1\",\n        \"RED_EDGE_2\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"NIR_NARROW\",\n        \"WATER_VAPOR\",\n        \"CIRRUS\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-eurosat\"\n    partition_file_template = \"{partition}_partition.json\"\n    label_map_file = \"label_map.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = [self.all_band_names.index(b) for b in bands]\\\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        label_map_path = self.data_directory / self.label_map_file\n        with open(label_map_path) as file:\n            self.label_map = json.load(file)\n\n        self.id_to_class = {img_id: cls for cls, ids in self.label_map.items() for img_id in ids}\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        image_id = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n\n        label_class = self.id_to_class[image_id]\n        label_index = list(self.label_map.keys()).index(label_class)\n\n        output = {\"image\": image.astype(np.float32)}\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"label\"] = label_index\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        label_index = sample[\"label\"]\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        class_names = list(self.label_map.keys())\n        class_name = class_names[label_index]\n\n        fig, ax = plt.subplots(figsize=(6, 6))\n        ax.imshow(rgb_image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Class: {class_name}\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_eurosat.MEuroSATNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> Source code in <code>terratorch/datasets/m_eurosat.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = [self.all_band_names.index(b) for b in bands]\\\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    label_map_path = self.data_directory / self.label_map_file\n    with open(label_map_path) as file:\n        self.label_map = json.load(file)\n\n    self.id_to_class = {img_id: cls for cls, ids in self.label_map.items() for img_id in ids}\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_eurosat.MEuroSATNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_eurosat.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    label_index = sample[\"label\"]\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    class_names = list(self.label_map.keys())\n    class_name = class_names[label_index]\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.imshow(rgb_image)\n    ax.axis(\"off\")\n    ax.set_title(f\"Class: {class_name}\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_bigearthnet","title":"<code>terratorch.datasets.m_bigearthnet</code>","text":""},{"location":"package/datasets/#terratorch.datasets.m_bigearthnet.MBigEarthNonGeo","title":"<code>MBigEarthNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-BigEarthNet.</p> Source code in <code>terratorch/datasets/m_bigearthnet.py</code> <pre><code>class MBigEarthNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-BigEarthNet](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\n        \"COASTAL_AEROSOL\",\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"RED_EDGE_1\",\n        \"RED_EDGE_2\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"NIR_NARROW\",\n        \"WATER_VAPOR\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-bigearthnet\"\n    label_map_file = \"label_stats.json\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        label_map_path = self.data_directory / self.label_map_file\n        with open(label_map_path) as file:\n            self.label_map = json.load(file)\n\n        self.num_classes = len(next(iter(self.label_map.values())))\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found in partition file.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        image_id = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n\n        labels_vector = self.label_map[image_id]\n        labels_tensor = torch.tensor(labels_vector, dtype=torch.float)\n\n        output = {\"image\": image}\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"label\"] = labels_tensor\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        label = sample[\"label\"].numpy()\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()  # Convert to (H, W, C)\n\n        rgb_image = image[:, :, rgb_indices]\n\n        rgb_image = clip_image(rgb_image)\n\n        active_labels = [i for i, lbl in enumerate(label) if lbl == 1]\n\n        fig, ax = plt.subplots(figsize=(6, 6))\n\n        ax.imshow(rgb_image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Active Labels: {active_labels}\")\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_bigearthnet.MBigEarthNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> Source code in <code>terratorch/datasets/m_bigearthnet.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    label_map_path = self.data_directory / self.label_map_file\n    with open(label_map_path) as file:\n        self.label_map = json.load(file)\n\n    self.num_classes = len(next(iter(self.label_map.values())))\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found in partition file.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_bigearthnet.MBigEarthNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_bigearthnet.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    label = sample[\"label\"].numpy()\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()  # Convert to (H, W, C)\n\n    rgb_image = image[:, :, rgb_indices]\n\n    rgb_image = clip_image(rgb_image)\n\n    active_labels = [i for i, lbl in enumerate(label) if lbl == 1]\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n\n    ax.imshow(rgb_image)\n    ax.axis(\"off\")\n    ax.set_title(f\"Active Labels: {active_labels}\")\n\n    if suptitle is not None:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_brick_kiln","title":"<code>terratorch.datasets.m_brick_kiln</code>","text":""},{"location":"package/datasets/#terratorch.datasets.m_brick_kiln.MBrickKilnNonGeo","title":"<code>MBrickKilnNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-BrickKiln.</p> Source code in <code>terratorch/datasets/m_brick_kiln.py</code> <pre><code>class MBrickKilnNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-BrickKiln](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\n        \"COASTAL_AEROSOL\",\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"RED_EDGE_1\",\n        \"RED_EDGE_2\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"NIR_NARROW\",\n        \"WATER_VAPOR\",\n        \"CIRRUS\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-brick-kiln\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found in partition file.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        image_id = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            attr_dict = pickle.loads(ast.literal_eval(h5file.attrs[\"pickle\"]))\n            class_index = attr_dict[\"label\"]\n\n        output = {\"image\": image.astype(np.float32)}\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"label\"] = class_index\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        label = sample[\"label\"]\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()  # Convert to (H, W, C)\n\n        rgb_image = image[:, :, rgb_indices]\n\n        rgb_image = clip_image(rgb_image)\n\n        fig, ax = plt.subplots(figsize=(6, 6))\n\n        ax.imshow(rgb_image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Class: {label}\")\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_brick_kiln.MBrickKilnNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> Source code in <code>terratorch/datasets/m_brick_kiln.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found in partition file.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_brick_kiln.MBrickKilnNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_brick_kiln.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    label = sample[\"label\"]\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()  # Convert to (H, W, C)\n\n    rgb_image = image[:, :, rgb_indices]\n\n    rgb_image = clip_image(rgb_image)\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n\n    ax.imshow(rgb_image)\n    ax.axis(\"off\")\n    ax.set_title(f\"Class: {label}\")\n\n    if suptitle is not None:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_forestnet","title":"<code>terratorch.datasets.m_forestnet</code>","text":""},{"location":"package/datasets/#terratorch.datasets.m_forestnet.MForestNetNonGeo","title":"<code>MForestNetNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-ForestNet.</p> Source code in <code>terratorch/datasets/m_forestnet.py</code> <pre><code>class MForestNetNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-ForestNet](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"NIR\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-forestnet\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n            use_metadata (bool): Whether to return metadata info (time and location).\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.use_metadata = use_metadata\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        image_id = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            attr_dict = pickle.loads(ast.literal_eval(h5file.attrs[\"pickle\"]))  # noqa: S301\n            class_index = attr_dict[\"label\"]\n\n        output = {\"image\": image.astype(np.float32)}\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"label\"] = class_index\n\n        if self.use_metadata:\n            temporal_coords = self._get_date(image_id)\n            location_coords = self._get_coords(image_id)\n\n            output[\"temporal_coords\"] = temporal_coords\n            output[\"location_coords\"] = location_coords\n\n        return output\n\n    def _get_coords(self, image_id: str) -&gt; torch.Tensor:\n        \"\"\"Extract spatial coordinates from the image ID.\n\n        Args:\n            image_id (str): The ID of the image.\n\n        Returns:\n            torch.Tensor: Tensor containing latitude and longitude.\n        \"\"\"\n        lat_str, lon_str, _ = image_id.split(\"_\", 2)\n        latitude = float(lat_str)\n        longitude = float(lon_str)\n        return torch.tensor([latitude, longitude], dtype=torch.float32)\n\n    def _get_date(self, image_id: str) -&gt; torch.Tensor:\n        _, _, date_str = image_id.split(\"_\", 2)\n        date = pd.to_datetime(date_str, format=\"%Y_%m_%d\")\n\n        return torch.tensor([[date.year, date.dayofyear - 1]], dtype=torch.float32)\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        label = sample[\"label\"]\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        fig, ax = plt.subplots(figsize=(6, 6))\n        ax.imshow(rgb_image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Class: {label}\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_forestnet.MForestNetNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code> Source code in <code>terratorch/datasets/m_forestnet.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        use_metadata (bool): Whether to return metadata info (time and location).\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.use_metadata = use_metadata\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_forestnet.MForestNetNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_forestnet.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    label = sample[\"label\"]\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.imshow(rgb_image)\n    ax.axis(\"off\")\n    ax.set_title(f\"Class: {label}\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_so2sat","title":"<code>terratorch.datasets.m_so2sat</code>","text":""},{"location":"package/datasets/#terratorch.datasets.m_so2sat.MSo2SatNonGeo","title":"<code>MSo2SatNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-So2Sat.</p> Source code in <code>terratorch/datasets/m_so2sat.py</code> <pre><code>class MSo2SatNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-So2Sat](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\n        \"VH_REAL\",\n        \"BLUE\",\n        \"VH_IMAGINARY\",\n        \"GREEN\",\n        \"VV_REAL\",\n        \"RED\",\n        \"VV_IMAGINARY\",\n        \"VH_LEE_FILTERED\",\n        \"RED_EDGE_1\",\n        \"VV_LEE_FILTERED\",\n        \"RED_EDGE_2\",\n        \"VH_LEE_FILTERED_REAL\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"VV_LEE_FILTERED_IMAGINARY\",\n        \"NIR_NARROW\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-so2sat\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        image_id = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            attr_dict = pickle.loads(ast.literal_eval(h5file.attrs[\"pickle\"]))\n            class_index = attr_dict[\"label\"]\n\n        output = {\"image\": image.astype(np.float32)}\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"label\"] = class_index\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        label_index = sample[\"label\"]\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        class_name = str(label_index)\n\n        fig, ax = plt.subplots(figsize=(6, 6))\n        ax.imshow(rgb_image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Class: {class_name}\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_so2sat.MSo2SatNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> Source code in <code>terratorch/datasets/m_so2sat.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_so2sat.MSo2SatNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_so2sat.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    label_index = sample[\"label\"]\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    class_name = str(label_index)\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.imshow(rgb_image)\n    ax.axis(\"off\")\n    ax.set_title(f\"Class: {class_name}\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_pv4ger","title":"<code>terratorch.datasets.m_pv4ger</code>","text":""},{"location":"package/datasets/#terratorch.datasets.m_pv4ger.MPv4gerNonGeo","title":"<code>MPv4gerNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-PV4GER.</p> Source code in <code>terratorch/datasets/m_pv4ger.py</code> <pre><code>class MPv4gerNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-PV4GER](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\"BLUE\", \"GREEN\", \"RED\")\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-pv4ger\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n            use_metadata (bool): Whether to return metadata info (location coordinates).\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.use_metadata = use_metadata\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        image_id = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            attr_dict = pickle.loads(ast.literal_eval(h5file.attrs[\"pickle\"]))  # noqa: S301\n            class_index = attr_dict[\"label\"]\n\n        output = {\"image\": image.astype(np.float32)}\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"label\"] = class_index\n\n        if self.use_metadata:\n            output[\"location_coords\"] = self._get_coords(image_id)\n\n        return output\n\n    def _get_coords(self, image_id: str) -&gt; torch.Tensor:\n        \"\"\"Extract spatial coordinates from the image ID.\"\"\"\n        lat_str, lon_str = image_id.split(\",\")\n        latitude = float(lat_str)\n        longitude = float(lon_str)\n        return torch.tensor([latitude, longitude], dtype=torch.float32)\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        label = sample[\"label\"]\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        fig, ax = plt.subplots(figsize=(6, 6))\n        ax.imshow(rgb_image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Class: {label}\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_pv4ger.MPv4gerNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (location coordinates).</p> <code>False</code> Source code in <code>terratorch/datasets/m_pv4ger.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        use_metadata (bool): Whether to return metadata info (location coordinates).\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.use_metadata = use_metadata\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_pv4ger.MPv4gerNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_pv4ger.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    label = sample[\"label\"]\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.imshow(rgb_image)\n    ax.axis(\"off\")\n    ax.set_title(f\"Class: {label}\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_cashew_plantation","title":"<code>terratorch.datasets.m_cashew_plantation</code>","text":""},{"location":"package/datasets/#terratorch.datasets.m_cashew_plantation.MBeninSmallHolderCashewsNonGeo","title":"<code>MBeninSmallHolderCashewsNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-BeninSmallHolderCashews.</p> Source code in <code>terratorch/datasets/m_cashew_plantation.py</code> <pre><code>class MBeninSmallHolderCashewsNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-BeninSmallHolderCashews](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\n        \"COASTAL_AEROSOL\",\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"RED_EDGE_1\",\n        \"RED_EDGE_2\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"NIR_NARROW\",\n        \"WATER_VAPOR\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n        \"CLOUD_PROBABILITY\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-cashew-plant\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n            use_metadata (bool): Whether to return metadata info (time).\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.use_metadata = use_metadata\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found in partition file.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def _get_date(self, keys) -&gt; torch.Tensor:\n        date_pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}\")\n\n        date_str = None\n        for key in keys:\n            match = date_pattern.search(key)\n            if match:\n                date_str = match.group()\n                break\n\n        date = torch.zeros((1, 2), dtype=torch.float32)\n        if date_str:\n            date = pd.to_datetime(date_str, format=\"%Y-%m-%d\")\n            date = torch.tensor([[date.year, date.dayofyear - 1]], dtype=torch.float32)\n\n        return date\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            temporal_coords = self._get_date(h5file)\n            mask = np.array(h5file[\"label\"])\n\n        output = {\"image\": image.astype(np.float32), \"mask\": mask}\n\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n        if self.use_metadata:\n            output[\"temporal_coords\"] = temporal_coords\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        mask = sample[\"mask\"].numpy()\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        num_classes = len(np.unique(mask))\n        cmap = plt.get_cmap(\"jet\")\n        norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n        num_images = 4 if \"prediction\" in sample else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n        ax[0].imshow(rgb_image)\n        ax[0].set_title(\"Image\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(mask, cmap=cmap, norm=norm)\n        ax[1].set_title(\"Ground Truth Mask\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(rgb_image)\n        ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n        ax[2].set_title(\"GT Mask on Image\")\n        ax[2].axis(\"off\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"].numpy()\n            ax[3].imshow(prediction, cmap=cmap, norm=norm)\n            ax[3].set_title(\"Predicted Mask\")\n            ax[3].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_cashew_plantation.MBeninSmallHolderCashewsNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time).</p> <code>False</code> Source code in <code>terratorch/datasets/m_cashew_plantation.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        use_metadata (bool): Whether to return metadata info (time).\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.use_metadata = use_metadata\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found in partition file.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_cashew_plantation.MBeninSmallHolderCashewsNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_cashew_plantation.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    mask = sample[\"mask\"].numpy()\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    num_classes = len(np.unique(mask))\n    cmap = plt.get_cmap(\"jet\")\n    norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n    num_images = 4 if \"prediction\" in sample else 3\n    fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n    ax[0].imshow(rgb_image)\n    ax[0].set_title(\"Image\")\n    ax[0].axis(\"off\")\n\n    ax[1].imshow(mask, cmap=cmap, norm=norm)\n    ax[1].set_title(\"Ground Truth Mask\")\n    ax[1].axis(\"off\")\n\n    ax[2].imshow(rgb_image)\n    ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n    ax[2].set_title(\"GT Mask on Image\")\n    ax[2].axis(\"off\")\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"].numpy()\n        ax[3].imshow(prediction, cmap=cmap, norm=norm)\n        ax[3].set_title(\"Predicted Mask\")\n        ax[3].axis(\"off\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_nz_cattle","title":"<code>terratorch.datasets.m_nz_cattle</code>","text":""},{"location":"package/datasets/#terratorch.datasets.m_nz_cattle.MNzCattleNonGeo","title":"<code>MNzCattleNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-NZ-Cattle.</p> Source code in <code>terratorch/datasets/m_nz_cattle.py</code> <pre><code>class MNzCattleNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-NZ-Cattle](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\"BLUE\", \"GREEN\", \"RED\")\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-nz-cattle\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n            use_metadata (bool): Whether to return metadata info (time and location).\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n        self.use_metadata = use_metadata\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        file_name = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n\n            data_keys = [key for key in keys if \"label\" not in key]\n            label_keys = [key for key in keys if \"label\" in key]\n\n            temporal_coords = self._get_date(data_keys[0])\n\n            bands = [np.array(h5file[key]) for key in data_keys]\n            image = np.stack(bands, axis=-1)\n\n            mask = np.array(h5file[label_keys[0]])\n\n        output = {\"image\": image.astype(np.float32), \"mask\": mask}\n\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        if self.use_metadata:\n            location_coords = self._get_coords(file_name)\n            output[\"location_coords\"] = location_coords\n            output[\"temporal_coords\"] = temporal_coords\n\n        return output\n\n    def _get_coords(self, file_name: str) -&gt; torch.Tensor:\n        \"\"\"Extract spatial coordinates from the file name.\"\"\"\n        match = re.search(r\"_(\\-?\\d+\\.\\d+),(\\-?\\d+\\.\\d+)\", file_name)\n        if match:\n            longitude, latitude = map(float, match.groups())\n\n        return torch.tensor([latitude, longitude], dtype=torch.float32)\n\n    def _get_date(self, band_name: str) -&gt; torch.Tensor:\n        date_str = band_name.split(\"_\")[-1]\n        date = pd.to_datetime(date_str, format=\"%Y-%m-%d\")\n\n        return torch.tensor([[date.year, date.dayofyear - 1]], dtype=torch.float32)\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        mask = sample[\"mask\"].numpy()\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        num_classes = len(np.unique(mask))\n        cmap = plt.get_cmap(\"jet\")\n        norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n        num_images = 4 if \"prediction\" in sample else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n        ax[0].imshow(rgb_image)\n        ax[0].set_title(\"Image\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(mask, cmap=cmap, norm=norm)\n        ax[1].set_title(\"Ground Truth Mask\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(rgb_image)\n        ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n        ax[2].set_title(\"GT Mask on Image\")\n        ax[2].axis(\"off\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"].numpy()\n            ax[3].imshow(prediction, cmap=cmap, norm=norm)\n            ax[3].set_title(\"Predicted Mask\")\n            ax[3].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_nz_cattle.MNzCattleNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code> Source code in <code>terratorch/datasets/m_nz_cattle.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        use_metadata (bool): Whether to return metadata info (time and location).\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n    self.use_metadata = use_metadata\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_nz_cattle.MNzCattleNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_nz_cattle.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    mask = sample[\"mask\"].numpy()\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    num_classes = len(np.unique(mask))\n    cmap = plt.get_cmap(\"jet\")\n    norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n    num_images = 4 if \"prediction\" in sample else 3\n    fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n    ax[0].imshow(rgb_image)\n    ax[0].set_title(\"Image\")\n    ax[0].axis(\"off\")\n\n    ax[1].imshow(mask, cmap=cmap, norm=norm)\n    ax[1].set_title(\"Ground Truth Mask\")\n    ax[1].axis(\"off\")\n\n    ax[2].imshow(rgb_image)\n    ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n    ax[2].set_title(\"GT Mask on Image\")\n    ax[2].axis(\"off\")\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"].numpy()\n        ax[3].imshow(prediction, cmap=cmap, norm=norm)\n        ax[3].set_title(\"Predicted Mask\")\n        ax[3].axis(\"off\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_chesapeake_landcover","title":"<code>terratorch.datasets.m_chesapeake_landcover</code>","text":""},{"location":"package/datasets/#terratorch.datasets.m_chesapeake_landcover.MChesapeakeLandcoverNonGeo","title":"<code>MChesapeakeLandcoverNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-ChesapeakeLandcover.</p> Source code in <code>terratorch/datasets/m_chesapeake_landcover.py</code> <pre><code>class MChesapeakeLandcoverNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-ChesapeakeLandcover](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\"BLUE\", \"GREEN\", \"NIR\", \"RED\")\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-chesapeake\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found in partition file.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            mask = np.array(h5file[\"label\"])\n\n        output = {\"image\": image.astype(np.float32), \"mask\": mask}\n\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        mask = sample[\"mask\"].numpy()\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        num_classes = len(np.unique(mask))\n        cmap = plt.get_cmap(\"jet\")\n        norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n        num_images = 4 if \"prediction\" in sample else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n        ax[0].imshow(rgb_image)\n        ax[0].set_title(\"Image\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(mask, cmap=cmap, norm=norm)\n        ax[1].set_title(\"Ground Truth Mask\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(rgb_image)\n        ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n        ax[2].set_title(\"GT Mask on Image\")\n        ax[2].axis(\"off\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"].numpy()\n            ax[3].imshow(prediction, cmap=cmap, norm=norm)\n            ax[3].set_title(\"Predicted Mask\")\n            ax[3].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_chesapeake_landcover.MChesapeakeLandcoverNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> Source code in <code>terratorch/datasets/m_chesapeake_landcover.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found in partition file.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_chesapeake_landcover.MChesapeakeLandcoverNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_chesapeake_landcover.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    mask = sample[\"mask\"].numpy()\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    num_classes = len(np.unique(mask))\n    cmap = plt.get_cmap(\"jet\")\n    norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n    num_images = 4 if \"prediction\" in sample else 3\n    fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n    ax[0].imshow(rgb_image)\n    ax[0].set_title(\"Image\")\n    ax[0].axis(\"off\")\n\n    ax[1].imshow(mask, cmap=cmap, norm=norm)\n    ax[1].set_title(\"Ground Truth Mask\")\n    ax[1].axis(\"off\")\n\n    ax[2].imshow(rgb_image)\n    ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n    ax[2].set_title(\"GT Mask on Image\")\n    ax[2].axis(\"off\")\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"].numpy()\n        ax[3].imshow(prediction, cmap=cmap, norm=norm)\n        ax[3].set_title(\"Predicted Mask\")\n        ax[3].axis(\"off\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_pv4ger_seg","title":"<code>terratorch.datasets.m_pv4ger_seg</code>","text":""},{"location":"package/datasets/#terratorch.datasets.m_pv4ger_seg.MPv4gerSegNonGeo","title":"<code>MPv4gerSegNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-PV4GER-SEG.</p> Source code in <code>terratorch/datasets/m_pv4ger_seg.py</code> <pre><code>class MPv4gerSegNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-PV4GER-SEG](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\"BLUE\", \"GREEN\", \"RED\")\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-pv4ger-seg\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n            use_metadata (bool): Whether to return metadata info (location coordinates).\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.use_metadata = use_metadata\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        image_id = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            mask = np.array(h5file[\"label\"])\n\n        output = {\"image\": image.astype(np.float32), \"mask\": mask}\n\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        if self.use_metadata:\n            output[\"location_coords\"] = self._get_coords(image_id)\n\n        return output\n\n    def _get_coords(self, image_id: str) -&gt; torch.Tensor:\n        \"\"\"Extract spatial coordinates from the image ID.\"\"\"\n        lat_str, lon_str = image_id.split(\",\")\n        latitude = float(lat_str)\n        longitude = float(lon_str)\n        return torch.tensor([latitude, longitude], dtype=torch.float32)\n\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        mask = sample[\"mask\"].numpy()\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        num_classes = len(np.unique(mask))\n        cmap = plt.get_cmap(\"jet\")\n        norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n        num_images = 4 if \"prediction\" in sample else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n        ax[0].imshow(rgb_image)\n        ax[0].set_title(\"Image\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(mask, cmap=cmap, norm=norm)\n        ax[1].set_title(\"Ground Truth Mask\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(rgb_image)\n        ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n        ax[2].set_title(\"GT Mask on Image\")\n        ax[2].axis(\"off\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"].numpy()\n            ax[3].imshow(prediction, cmap=cmap, norm=norm)\n            ax[3].set_title(\"Predicted Mask\")\n            ax[3].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_pv4ger_seg.MPv4gerSegNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (location coordinates).</p> <code>False</code> Source code in <code>terratorch/datasets/m_pv4ger_seg.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        use_metadata (bool): Whether to return metadata info (location coordinates).\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.use_metadata = use_metadata\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_pv4ger_seg.MPv4gerSegNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_pv4ger_seg.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    mask = sample[\"mask\"].numpy()\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    num_classes = len(np.unique(mask))\n    cmap = plt.get_cmap(\"jet\")\n    norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n    num_images = 4 if \"prediction\" in sample else 3\n    fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n    ax[0].imshow(rgb_image)\n    ax[0].set_title(\"Image\")\n    ax[0].axis(\"off\")\n\n    ax[1].imshow(mask, cmap=cmap, norm=norm)\n    ax[1].set_title(\"Ground Truth Mask\")\n    ax[1].axis(\"off\")\n\n    ax[2].imshow(rgb_image)\n    ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n    ax[2].set_title(\"GT Mask on Image\")\n    ax[2].axis(\"off\")\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"].numpy()\n        ax[3].imshow(prediction, cmap=cmap, norm=norm)\n        ax[3].set_title(\"Predicted Mask\")\n        ax[3].axis(\"off\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_SA_crop_type","title":"<code>terratorch.datasets.m_SA_crop_type</code>","text":""},{"location":"package/datasets/#terratorch.datasets.m_SA_crop_type.MSACropTypeNonGeo","title":"<code>MSACropTypeNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-SA-Crop-Type.</p> Source code in <code>terratorch/datasets/m_SA_crop_type.py</code> <pre><code>class MSACropTypeNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-SA-Crop-Type](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\n        \"COASTAL_AEROSOL\",\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"RED_EDGE_1\",\n        \"RED_EDGE_2\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"NIR_NARROW\",\n        \"WATER_VAPOR\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n        \"CLOUD_PROBABILITY\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-SA-crop-type\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            mask = np.array(h5file[\"label\"])\n\n        output = {\"image\": image.astype(np.float32), \"mask\": mask}\n\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        mask = sample[\"mask\"].numpy()\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        num_classes = len(np.unique(mask))\n        cmap = plt.get_cmap(\"jet\")\n        norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n        num_images = 4 if \"prediction\" in sample else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n        ax[0].imshow(rgb_image)\n        ax[0].set_title(\"Image\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(mask, cmap=cmap, norm=norm)\n        ax[1].set_title(\"Ground Truth Mask\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(rgb_image)\n        ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n        ax[2].set_title(\"GT Mask on Image\")\n        ax[2].axis(\"off\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"].numpy()\n            ax[3].imshow(prediction, cmap=cmap, norm=norm)\n            ax[3].set_title(\"Predicted Mask\")\n            ax[3].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_SA_crop_type.MSACropTypeNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> Source code in <code>terratorch/datasets/m_SA_crop_type.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_SA_crop_type.MSACropTypeNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_SA_crop_type.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    mask = sample[\"mask\"].numpy()\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    num_classes = len(np.unique(mask))\n    cmap = plt.get_cmap(\"jet\")\n    norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n    num_images = 4 if \"prediction\" in sample else 3\n    fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n    ax[0].imshow(rgb_image)\n    ax[0].set_title(\"Image\")\n    ax[0].axis(\"off\")\n\n    ax[1].imshow(mask, cmap=cmap, norm=norm)\n    ax[1].set_title(\"Ground Truth Mask\")\n    ax[1].axis(\"off\")\n\n    ax[2].imshow(rgb_image)\n    ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n    ax[2].set_title(\"GT Mask on Image\")\n    ax[2].axis(\"off\")\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"].numpy()\n        ax[3].imshow(prediction, cmap=cmap, norm=norm)\n        ax[3].set_title(\"Predicted Mask\")\n        ax[3].axis(\"off\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_neontree","title":"<code>terratorch.datasets.m_neontree</code>","text":""},{"location":"package/datasets/#terratorch.datasets.m_neontree.MNeonTreeNonGeo","title":"<code>MNeonTreeNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-NeonTree.</p> Source code in <code>terratorch/datasets/m_neontree.py</code> <pre><code>class MNeonTreeNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-NeonTree](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\"BLUE\", \"CANOPY_HEIGHT_MODEL\", \"GREEN\", \"NEON\", \"RED\")\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-NeonTree\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = rgb_bands,\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to RGB bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            mask = np.array(h5file[\"label\"])\n\n        output = {\"image\": image.astype(np.float32), \"mask\": mask}\n\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n        image = sample[\"image\"]\n        mask = sample[\"mask\"].numpy()\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        num_classes = len(np.unique(mask))\n        cmap = plt.get_cmap(\"jet\")\n        norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n        num_images = 4 if \"prediction\" in sample else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n        ax[0].imshow(rgb_image)\n        ax[0].set_title(\"Image\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(mask, cmap=cmap, norm=norm)\n        ax[1].set_title(\"Ground Truth Mask\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(rgb_image)\n        ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n        ax[2].set_title(\"GT Mask on Image\")\n        ax[2].axis(\"off\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"].numpy()\n            ax[3].imshow(prediction, cmap=cmap, norm=norm)\n            ax[3].set_title(\"Predicted Mask\")\n            ax[3].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_neontree.MNeonTreeNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=rgb_bands, transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to RGB bands.</p> <code>rgb_bands</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> Source code in <code>terratorch/datasets/m_neontree.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = rgb_bands,\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to RGB bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.m_neontree.MNeonTreeNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_neontree.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n    image = sample[\"image\"]\n    mask = sample[\"mask\"].numpy()\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    num_classes = len(np.unique(mask))\n    cmap = plt.get_cmap(\"jet\")\n    norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n    num_images = 4 if \"prediction\" in sample else 3\n    fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n    ax[0].imshow(rgb_image)\n    ax[0].set_title(\"Image\")\n    ax[0].axis(\"off\")\n\n    ax[1].imshow(mask, cmap=cmap, norm=norm)\n    ax[1].set_title(\"Ground Truth Mask\")\n    ax[1].axis(\"off\")\n\n    ax[2].imshow(rgb_image)\n    ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n    ax[2].set_title(\"GT Mask on Image\")\n    ax[2].axis(\"off\")\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"].numpy()\n        ax[3].imshow(prediction, cmap=cmap, norm=norm)\n        ax[3].set_title(\"Predicted Mask\")\n        ax[3].axis(\"off\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.multi_temporal_crop_classification","title":"<code>terratorch.datasets.multi_temporal_crop_classification</code>","text":""},{"location":"package/datasets/#terratorch.datasets.multi_temporal_crop_classification.MultiTemporalCropClassification","title":"<code>MultiTemporalCropClassification</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for multi-temporal crop classification.</p> Source code in <code>terratorch/datasets/multi_temporal_crop_classification.py</code> <pre><code>class MultiTemporalCropClassification(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [multi-temporal crop classification](https://huggingface.co/datasets/ibm-nasa-geospatial/multi-temporal-crop-classification).\"\"\"\n\n    all_band_names = (\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"NIR_NARROW\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    )\n\n    class_names = (\n        \"Natural Vegetation\",\n        \"Forest\",\n        \"Corn\",\n        \"Soybeans\",\n        \"Wetlands\",\n        \"Developed / Barren\",\n        \"Open Water\",\n        \"Winter Wheat\",\n        \"Alfalfa\",\n        \"Fallow / Idle Cropland\",\n        \"Cotton\",\n        \"Sorghum\",\n        \"Other\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    num_classes = 13\n    time_steps = 3\n    splits = {\"train\": \"training\", \"val\": \"validation\"}  # Only train and val splits available\n    col_name = \"chip_id\"\n    date_columns = [\"first_img_date\", \"middle_img_date\", \"last_img_date\"]\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = None,\n        expand_temporal_dimension: bool = True,\n        reduce_zero_label: bool = True,\n        use_metadata: bool = False,\n        metadata_file_name: str = \"chips_df.csv\",\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): one of 'train' or 'val'.\n            bands (list[str]): Bands that should be output by the dataset. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). If used through the corresponding data module,\n                should not include normalization. Defaults to None, which applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input images with this value.\n                If None, does no replacement. Defaults to None.\n            no_label_replace (int | None): Replace nan values in label with this value.\n                If none, does no replacement. Defaults to None.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to True.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to True.\n            use_metadata (bool): whether to return metadata info (time and location).\n        \"\"\"\n        super().__init__()\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {self.splits}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n        self.data_root = Path(data_root)\n\n        data_dir = self.data_root / f\"{split_name}_chips\"\n        self.image_files = sorted(glob.glob(os.path.join(data_dir, \"*_merged.tif\")))\n        self.segmentation_mask_files = sorted(glob.glob(os.path.join(data_dir, \"*.mask.tif\")))\n        split_file = self.data_root / f\"{split_name}_data.txt\"\n\n        with open(split_file) as f:\n            split = f.readlines()\n        valid_files = {rf\"{substring.strip()}\" for substring in split}\n        self.image_files = filter_valid_files(\n            self.image_files,\n            valid_files=valid_files,\n            ignore_extensions=True,\n            allow_substring=True,\n        )\n        self.segmentation_mask_files = filter_valid_files(\n            self.segmentation_mask_files,\n            valid_files=valid_files,\n            ignore_extensions=True,\n            allow_substring=True,\n        )\n\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.reduce_zero_label = reduce_zero_label\n        self.expand_temporal_dimension = expand_temporal_dimension\n        self.use_metadata = use_metadata\n        self.metadata = None\n        self.metadata_file_name = metadata_file_name\n        if self.use_metadata:\n            metadata_file = self.data_root / self.metadata_file_name\n            self.metadata = pd.read_csv(metadata_file)\n            self._build_image_metadata_mapping()\n\n        # If no transform is given, apply only to transform to torch tensor\n        self.transform = transform if transform else default_transform\n\n    def _build_image_metadata_mapping(self):\n        \"\"\"Build a mapping from image filenames to metadata indices.\"\"\"\n        self.image_to_metadata_index = dict()\n\n        for idx, image_file in enumerate(self.image_files):\n            image_filename = Path(image_file).name\n            image_id = image_filename.replace(\"_merged.tif\", \"\").replace(\".tif\", \"\")\n            metadata_indices = self.metadata.index[self.metadata[self.col_name] == image_id].tolist()\n            self.image_to_metadata_index[idx] = metadata_indices[0]\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def _get_date(self, row: pd.Series) -&gt; torch.Tensor:\n        \"\"\"Extract and format temporal coordinates (T, date) from metadata.\"\"\"\n        temporal_coords = []\n        for col in self.date_columns:\n            date_str = row[col]\n            date = pd.to_datetime(date_str)\n            temporal_coords.append([date.year, date.dayofyear - 1])\n\n        return torch.tensor(temporal_coords, dtype=torch.float32)\n\n    def _get_coords(self, image: DataArray) -&gt; torch.Tensor:\n        px = image.x.shape[0] // 2\n        py = image.y.shape[0] // 2\n\n        # get center point to reproject to lat/lon\n        point = image.isel(band=0, x=slice(px, px + 1), y=slice(py, py + 1))\n        point = point.rio.reproject(\"epsg:4326\")\n\n        lat_lon = np.asarray([point.y[0], point.x[0]])\n\n        return torch.tensor(lat_lon, dtype=torch.float32)\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        image = self._load_file(self.image_files[index], nan_replace=self.no_data_replace)\n\n        location_coords, temporal_coords = None, None\n        if self.use_metadata:\n            location_coords = self._get_coords(image)\n            metadata_idx = self.image_to_metadata_index.get(index, None)\n            if metadata_idx is not None:\n                row = self.metadata.iloc[metadata_idx]\n                temporal_coords = self._get_date(row)\n\n        # to channels last\n        image = image.to_numpy()\n        if self.expand_temporal_dimension:\n            image = rearrange(image, \"(channels time) h w -&gt; channels time h w\", channels=len(self.bands))\n        image = np.moveaxis(image, 0, -1)\n\n        # filter bands\n        image = image[..., self.band_indices]\n\n        output = {\n            \"image\": image.astype(np.float32),\n            \"mask\": self._load_file(\n                self.segmentation_mask_files[index], nan_replace=self.no_label_replace).to_numpy()[0],\n        }\n\n        if self.reduce_zero_label:\n            output[\"mask\"] -= 1\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        if self.use_metadata:\n            output[\"location_coords\"] = location_coords\n            output[\"temporal_coords\"] = temporal_coords\n\n        return output\n\n    def _load_file(self, path: Path, nan_replace: int | float | None = None) -&gt; DataArray:\n        data = rioxarray.open_rasterio(path, masked=True)\n        if nan_replace is not None:\n            data = data.fillna(nan_replace)\n        return data\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            suptitle: optional string to use as a suptitle\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n        \"\"\"\n        num_images = self.time_steps + 2\n\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        images = sample[\"image\"]\n        images = images[rgb_indices, ...]  # Shape: (T, 3, H, W)\n\n        processed_images = []\n        for t in range(self.time_steps):\n            img = images[t]\n            img = img.permute(1, 2, 0)\n            img = img.numpy()\n            img = clip_image(img)\n            processed_images.append(img)\n\n        mask = sample[\"mask\"].numpy()\n        if \"prediction\" in sample:\n            num_images += 1\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 5), layout=\"compressed\")\n        ax[0].axis(\"off\")\n\n        norm = mpl.colors.Normalize(vmin=0, vmax=self.num_classes - 1)\n        for i, img in enumerate(processed_images):\n            ax[i + 1].axis(\"off\")\n            ax[i + 1].title.set_text(f\"T{i}\")\n            ax[i + 1].imshow(img)\n\n        ax[self.time_steps + 1].axis(\"off\")\n        ax[self.time_steps + 1].title.set_text(\"Ground Truth Mask\")\n        ax[self.time_steps + 1].imshow(mask, cmap=\"jet\", norm=norm)\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"]\n            ax[self.time_steps + 2].axis(\"off\")\n            ax[self.time_steps + 2].title.set_text(\"Predicted Mask\")\n            ax[self.time_steps + 2].imshow(prediction, cmap=\"jet\", norm=norm)\n\n        cmap = plt.get_cmap(\"jet\")\n        legend_data = [[i, cmap(norm(i)), self.class_names[i]] for i in range(self.num_classes)]\n        handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n        labels = [n for k, c, n in legend_data]\n        ax[0].legend(handles, labels, loc=\"center\")\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.multi_temporal_crop_classification.MultiTemporalCropClassification.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=True, reduce_zero_label=True, use_metadata=False, metadata_file_name='chips_df.csv')</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>one of 'train' or 'val'.</p> <code>'train'</code> <code>bands</code> <code>list[str]</code> <p>Bands that should be output by the dataset. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If None, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True.</p> <code>True</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True.</p> <code>True</code> <code>use_metadata</code> <code>bool</code> <p>whether to return metadata info (time and location).</p> <code>False</code> Source code in <code>terratorch/datasets/multi_temporal_crop_classification.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = None,\n    expand_temporal_dimension: bool = True,\n    reduce_zero_label: bool = True,\n    use_metadata: bool = False,\n    metadata_file_name: str = \"chips_df.csv\",\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): one of 'train' or 'val'.\n        bands (list[str]): Bands that should be output by the dataset. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). If used through the corresponding data module,\n            should not include normalization. Defaults to None, which applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input images with this value.\n            If None, does no replacement. Defaults to None.\n        no_label_replace (int | None): Replace nan values in label with this value.\n            If none, does no replacement. Defaults to None.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to True.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to True.\n        use_metadata (bool): whether to return metadata info (time and location).\n    \"\"\"\n    super().__init__()\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {self.splits}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n    self.data_root = Path(data_root)\n\n    data_dir = self.data_root / f\"{split_name}_chips\"\n    self.image_files = sorted(glob.glob(os.path.join(data_dir, \"*_merged.tif\")))\n    self.segmentation_mask_files = sorted(glob.glob(os.path.join(data_dir, \"*.mask.tif\")))\n    split_file = self.data_root / f\"{split_name}_data.txt\"\n\n    with open(split_file) as f:\n        split = f.readlines()\n    valid_files = {rf\"{substring.strip()}\" for substring in split}\n    self.image_files = filter_valid_files(\n        self.image_files,\n        valid_files=valid_files,\n        ignore_extensions=True,\n        allow_substring=True,\n    )\n    self.segmentation_mask_files = filter_valid_files(\n        self.segmentation_mask_files,\n        valid_files=valid_files,\n        ignore_extensions=True,\n        allow_substring=True,\n    )\n\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.reduce_zero_label = reduce_zero_label\n    self.expand_temporal_dimension = expand_temporal_dimension\n    self.use_metadata = use_metadata\n    self.metadata = None\n    self.metadata_file_name = metadata_file_name\n    if self.use_metadata:\n        metadata_file = self.data_root / self.metadata_file_name\n        self.metadata = pd.read_csv(metadata_file)\n        self._build_image_metadata_mapping()\n\n    # If no transform is given, apply only to transform to torch tensor\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.multi_temporal_crop_classification.MultiTemporalCropClassification.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> Source code in <code>terratorch/datasets/multi_temporal_crop_classification.py</code> <pre><code>def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        suptitle: optional string to use as a suptitle\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n    \"\"\"\n    num_images = self.time_steps + 2\n\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    images = sample[\"image\"]\n    images = images[rgb_indices, ...]  # Shape: (T, 3, H, W)\n\n    processed_images = []\n    for t in range(self.time_steps):\n        img = images[t]\n        img = img.permute(1, 2, 0)\n        img = img.numpy()\n        img = clip_image(img)\n        processed_images.append(img)\n\n    mask = sample[\"mask\"].numpy()\n    if \"prediction\" in sample:\n        num_images += 1\n    fig, ax = plt.subplots(1, num_images, figsize=(12, 5), layout=\"compressed\")\n    ax[0].axis(\"off\")\n\n    norm = mpl.colors.Normalize(vmin=0, vmax=self.num_classes - 1)\n    for i, img in enumerate(processed_images):\n        ax[i + 1].axis(\"off\")\n        ax[i + 1].title.set_text(f\"T{i}\")\n        ax[i + 1].imshow(img)\n\n    ax[self.time_steps + 1].axis(\"off\")\n    ax[self.time_steps + 1].title.set_text(\"Ground Truth Mask\")\n    ax[self.time_steps + 1].imshow(mask, cmap=\"jet\", norm=norm)\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"]\n        ax[self.time_steps + 2].axis(\"off\")\n        ax[self.time_steps + 2].title.set_text(\"Predicted Mask\")\n        ax[self.time_steps + 2].imshow(prediction, cmap=\"jet\", norm=norm)\n\n    cmap = plt.get_cmap(\"jet\")\n    legend_data = [[i, cmap(norm(i)), self.class_names[i]] for i in range(self.num_classes)]\n    handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n    labels = [n for k, c, n in legend_data]\n    ax[0].legend(handles, labels, loc=\"center\")\n\n    if suptitle is not None:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.open_sentinel_map","title":"<code>terratorch.datasets.open_sentinel_map</code>","text":""},{"location":"package/datasets/#terratorch.datasets.open_sentinel_map.OpenSentinelMap","title":"<code>OpenSentinelMap</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>Pytorch Dataset class to load samples from the OpenSentinelMap dataset, supporting multiple bands and temporal sampling strategies.</p> Source code in <code>terratorch/datasets/open_sentinel_map.py</code> <pre><code>class OpenSentinelMap(NonGeoDataset):\n    \"\"\"\n    Pytorch Dataset class to load samples from the [OpenSentinelMap](https://visionsystemsinc.github.io/open-sentinel-map/) dataset, supporting\n    multiple bands and temporal sampling strategies.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: list[str] | None = None,\n        transform: A.Compose | None = None,\n        spatial_interpolate_and_stack_temporally: bool = True,  # noqa: FBT001, FBT002\n        pad_image: int | None = None,\n        truncate_image: int | None = None,\n        target: int = 0,\n        pick_random_pair: bool = True,  # noqa: FBT002, FBT001\n    ) -&gt; None:\n        \"\"\"\n\n        Args:\n            data_root (str): Path to the root directory of the dataset.\n            split (str): Dataset split to load. Options are 'train', 'val', or 'test'. Defaults to 'train'.\n            bands (list of str, optional): List of band names to load. Defaults to ['gsd_10', 'gsd_20', 'gsd_60'].\n            transform (albumentations.Compose, optional): Albumentations transformations to apply to the data.\n            spatial_interpolate_and_stack_temporally (bool): If True, the bands are interpolated and concatenated over time.\n                Default is True.\n            pad_image (int, optional): Number of timesteps to pad the time dimension of the image.\n                If None, no padding is applied.\n            truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image.\n                If None, no truncation is performed.\n            target (int): Specifies which target class to use from the mask. Default is 0.\n            pick_random_pair (bool): If True, selects two random images from the temporal sequence. Default is True.\n        \"\"\"\n        split = \"test\"\n        if bands is None:\n            bands = [\"gsd_10\", \"gsd_20\", \"gsd_60\"]\n\n        allowed_bands = {\"gsd_10\", \"gsd_20\", \"gsd_60\"}\n        for band in bands:\n            if band not in allowed_bands:\n                msg = f\"Band '{band}' is not recognized. Available values are: {', '.join(allowed_bands)}\"\n                raise ValueError(msg)\n\n        if split not in [\"train\", \"val\", \"test\"]:\n            msg = f\"Split '{split}' not recognized. Use 'train', 'val', or 'test'.\"\n            raise ValueError(msg)\n\n        self.data_root = Path(data_root)\n        split_mapping = {\"train\": \"training\", \"val\": \"validation\", \"test\": \"testing\"}\n        split = split_mapping[split]\n        self.imagery_root = self.data_root / \"osm_sentinel_imagery\"\n        self.label_root = self.data_root / \"osm_label_images_v10\"\n        self.auxiliary_data = pd.read_csv(self.data_root / \"spatial_cell_info.csv\")\n        self.auxiliary_data = self.auxiliary_data[self.auxiliary_data[\"split\"] == split]\n        self.bands = bands\n        self.transform = transform if transform else lambda **batch: to_tensor(batch)\n        self.label_mappings = self._load_label_mappings()\n        self.split_data = self.auxiliary_data[self.auxiliary_data[\"split\"] == split]\n        self.spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally\n        self.pad_image = pad_image\n        self.truncate_image = truncate_image\n        self.target = target\n        self.pick_random_pair = pick_random_pair\n\n        self.image_files = []\n        self.label_files = []\n\n        for _, row in self.split_data.iterrows():\n            mgrs_tile = row[\"MGRS_tile\"]\n            spatial_cell = str(row[\"cell_id\"])\n\n            label_file = self.label_root / mgrs_tile / f\"{spatial_cell}.png\"\n\n            if label_file.exists():\n                self.image_files.append((mgrs_tile, spatial_cell))\n                self.label_files.append(label_file)\n\n    def _load_label_mappings(self):\n        with open(self.data_root / \"osm_categories.json\") as f:\n            return json.load(f)\n\n    def _extract_date_from_filename(self, filename: str) -&gt; str:\n        match = re.search(r\"(\\d{8})\", filename)\n        if match:\n            return match.group(1)\n        else:\n            msg = f\"Date not found in filename {filename}\"\n            raise ValueError(msg)\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None, show_axes: bool | None = False) -&gt; Figure:\n        if \"gsd_10\" not in self.bands:\n            return None\n\n        num_images = len([key for key in sample if key.startswith(\"image\")])\n        images = []\n\n        for i in range(1, num_images + 1):\n            image_dict = sample[f\"image{i}\"]\n            image = image_dict[\"gsd_10\"]\n            if isinstance(image, Tensor):\n                image = image.numpy()\n\n            image = image.take(range(3), axis=2)\n            image = image.squeeze()\n            image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n            image = np.clip(image, 0, 1)\n            images.append(image)\n\n        label_mask = sample[\"mask\"]\n        if isinstance(label_mask, Tensor):\n            label_mask = label_mask.numpy()\n\n        return self._plot_sample(images, label_mask, suptitle=suptitle, show_axes=show_axes)\n\n    def _plot_sample(\n        self,\n        images: list[np.ndarray],\n        label: np.ndarray,\n        suptitle: str | None = None,\n        show_axes: bool = False,\n    ) -&gt; Figure:\n        num_images = len(images)\n        fig, ax = plt.subplots(1, num_images + 1, figsize=(15, 5))\n        axes_visibility = \"on\" if show_axes else \"off\"\n\n        for i, image in enumerate(images):\n            ax[i].imshow(image)\n            ax[i].set_title(f\"Image {i + 1}\")\n            ax[i].axis(axes_visibility)\n\n        ax[-1].imshow(label, cmap=\"gray\")\n        ax[-1].set_title(\"Ground Truth Mask\")\n        ax[-1].axis(axes_visibility)\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        mgrs_tile, spatial_cell = self.image_files[index]\n        spatial_cell_path = self.imagery_root / mgrs_tile / spatial_cell\n\n        npz_files = list(spatial_cell_path.glob(\"*.npz\"))\n        npz_files.sort(key=lambda x: self._extract_date_from_filename(x.stem))\n\n        if self.pick_random_pair:\n            npz_files = random.sample(npz_files, 2)\n            npz_files.sort(key=lambda x: self._extract_date_from_filename(x.stem))\n\n        output = {}\n\n        if self.spatial_interpolate_and_stack_temporally:\n            images_over_time = []\n            for _, npz_file in enumerate(npz_files):\n                data = np.load(npz_file)\n                interpolated_bands = []\n                for band in self.bands:\n                    band_frame = data[band]\n                    band_frame = torch.from_numpy(band_frame).float()\n                    band_frame = band_frame.permute(2, 0, 1)\n                    interpolated = F.interpolate(\n                        band_frame.unsqueeze(0), size=MAX_TEMPORAL_IMAGE_SIZE, mode=\"bilinear\", align_corners=False\n                    ).squeeze(0)\n                    interpolated_bands.append(interpolated)\n                concatenated_bands = torch.cat(interpolated_bands, dim=0)\n                images_over_time.append(concatenated_bands)\n\n            images = torch.stack(images_over_time, dim=0).numpy()\n            if self.truncate_image:\n                images = images[-self.truncate_image :]\n            if self.pad_image:\n                images = pad_numpy(images, self.pad_image)\n\n            output[\"image\"] = images.transpose(0, 2, 3, 1)\n        else:\n            image_dict = {band: [] for band in self.bands}\n            for _, npz_file in enumerate(npz_files):\n                data = np.load(npz_file)\n                for band in self.bands:\n                    band_frames = data[band]\n                    band_frames = band_frames.astype(np.float32)\n                    band_frames = np.transpose(band_frames, (2, 0, 1))\n                    image_dict[band].append(band_frames)\n\n            final_image_dict = {}\n            for band in self.bands:\n                band_images = image_dict[band]\n                if self.truncate_image:\n                    band_images = band_images[-self.truncate_image :]\n                if self.pad_image:\n                    band_images = [pad_numpy(img, self.pad_image) for img in band_images]\n                band_images = np.stack(band_images, axis=0)\n                final_image_dict[band] = band_images\n\n            output[\"image\"] = final_image_dict\n\n        label_file = self.label_files[index]\n        mask = np.array(Image.open(label_file)).astype(int)\n\n        # Map 'unlabel' (254) and 'none' (255) to unused classes 15 and 16 for processing\n        mask[mask == 254] = 15  # noqa: PLR2004\n        mask[mask == 255] = 16  # noqa: PLR2004\n        output[\"mask\"] = mask[:, :, self.target]\n\n        if self.transform:\n            output = self.transform(**output)\n\n        return output\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.open_sentinel_map.OpenSentinelMap.__init__","title":"<code>__init__(data_root, split='train', bands=None, transform=None, spatial_interpolate_and_stack_temporally=True, pad_image=None, truncate_image=None, target=0, pick_random_pair=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>str</code> <p>Dataset split to load. Options are 'train', 'val', or 'test'. Defaults to 'train'.</p> <code>'train'</code> <code>bands</code> <code>list of str</code> <p>List of band names to load. Defaults to ['gsd_10', 'gsd_20', 'gsd_60'].</p> <code>None</code> <code>transform</code> <code>Compose</code> <p>Albumentations transformations to apply to the data.</p> <code>None</code> <code>spatial_interpolate_and_stack_temporally</code> <code>bool</code> <p>If True, the bands are interpolated and concatenated over time. Default is True.</p> <code>True</code> <code>pad_image</code> <code>int</code> <p>Number of timesteps to pad the time dimension of the image. If None, no padding is applied.</p> <code>None</code> <code>truncate_image</code> <code>int</code> <p>Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed.</p> <code>None</code> <code>target</code> <code>int</code> <p>Specifies which target class to use from the mask. Default is 0.</p> <code>0</code> <code>pick_random_pair</code> <code>bool</code> <p>If True, selects two random images from the temporal sequence. Default is True.</p> <code>True</code> Source code in <code>terratorch/datasets/open_sentinel_map.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: list[str] | None = None,\n    transform: A.Compose | None = None,\n    spatial_interpolate_and_stack_temporally: bool = True,  # noqa: FBT001, FBT002\n    pad_image: int | None = None,\n    truncate_image: int | None = None,\n    target: int = 0,\n    pick_random_pair: bool = True,  # noqa: FBT002, FBT001\n) -&gt; None:\n    \"\"\"\n\n    Args:\n        data_root (str): Path to the root directory of the dataset.\n        split (str): Dataset split to load. Options are 'train', 'val', or 'test'. Defaults to 'train'.\n        bands (list of str, optional): List of band names to load. Defaults to ['gsd_10', 'gsd_20', 'gsd_60'].\n        transform (albumentations.Compose, optional): Albumentations transformations to apply to the data.\n        spatial_interpolate_and_stack_temporally (bool): If True, the bands are interpolated and concatenated over time.\n            Default is True.\n        pad_image (int, optional): Number of timesteps to pad the time dimension of the image.\n            If None, no padding is applied.\n        truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image.\n            If None, no truncation is performed.\n        target (int): Specifies which target class to use from the mask. Default is 0.\n        pick_random_pair (bool): If True, selects two random images from the temporal sequence. Default is True.\n    \"\"\"\n    split = \"test\"\n    if bands is None:\n        bands = [\"gsd_10\", \"gsd_20\", \"gsd_60\"]\n\n    allowed_bands = {\"gsd_10\", \"gsd_20\", \"gsd_60\"}\n    for band in bands:\n        if band not in allowed_bands:\n            msg = f\"Band '{band}' is not recognized. Available values are: {', '.join(allowed_bands)}\"\n            raise ValueError(msg)\n\n    if split not in [\"train\", \"val\", \"test\"]:\n        msg = f\"Split '{split}' not recognized. Use 'train', 'val', or 'test'.\"\n        raise ValueError(msg)\n\n    self.data_root = Path(data_root)\n    split_mapping = {\"train\": \"training\", \"val\": \"validation\", \"test\": \"testing\"}\n    split = split_mapping[split]\n    self.imagery_root = self.data_root / \"osm_sentinel_imagery\"\n    self.label_root = self.data_root / \"osm_label_images_v10\"\n    self.auxiliary_data = pd.read_csv(self.data_root / \"spatial_cell_info.csv\")\n    self.auxiliary_data = self.auxiliary_data[self.auxiliary_data[\"split\"] == split]\n    self.bands = bands\n    self.transform = transform if transform else lambda **batch: to_tensor(batch)\n    self.label_mappings = self._load_label_mappings()\n    self.split_data = self.auxiliary_data[self.auxiliary_data[\"split\"] == split]\n    self.spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally\n    self.pad_image = pad_image\n    self.truncate_image = truncate_image\n    self.target = target\n    self.pick_random_pair = pick_random_pair\n\n    self.image_files = []\n    self.label_files = []\n\n    for _, row in self.split_data.iterrows():\n        mgrs_tile = row[\"MGRS_tile\"]\n        spatial_cell = str(row[\"cell_id\"])\n\n        label_file = self.label_root / mgrs_tile / f\"{spatial_cell}.png\"\n\n        if label_file.exists():\n            self.image_files.append((mgrs_tile, spatial_cell))\n            self.label_files.append(label_file)\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.openearthmap","title":"<code>terratorch.datasets.openearthmap</code>","text":""},{"location":"package/datasets/#terratorch.datasets.openearthmap.OpenEarthMapNonGeo","title":"<code>OpenEarthMapNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>OpenEarthMapNonGeo Dataset for non-georeferenced imagery.</p> <p>This dataset class handles non-georeferenced image data from the OpenEarthMap dataset. It supports configurable band sets and transformations, and performs cropping operations to ensure that the images conform to the required input dimensions. The dataset is split into \"train\", \"test\", and \"val\" subsets based on the provided split parameter.</p> Source code in <code>terratorch/datasets/openearthmap.py</code> <pre><code>class OpenEarthMapNonGeo(NonGeoDataset):\n    \"\"\"\n    [OpenEarthMapNonGeo](https://open-earth-map.org/) Dataset for non-georeferenced imagery.\n\n    This dataset class handles non-georeferenced image data from the OpenEarthMap dataset.\n    It supports configurable band sets and transformations, and performs cropping operations\n    to ensure that the images conform to the required input dimensions. The dataset is split\n    into \"train\", \"test\", and \"val\" subsets based on the provided split parameter.\n    \"\"\"\n\n\n    all_band_names = (\"BLUE\",\"GREEN\",\"RED\")\n\n    rgb_bands = (\"RED\",\"GREEN\",\"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    def __init__(self, data_root: str,\n                 bands: Sequence[str] = BAND_SETS[\"all\"],\n                 transform: A.Compose | None = None,\n                 split=\"train\",\n                 crop_size: int = 256,\n                 random_crop: bool = True) -&gt; None:\n        \"\"\"\n        Initialize a new instance of the OpenEarthMapNonGeo dataset.\n\n        Args:\n            data_root (str): The root directory containing the dataset files.\n            bands (Sequence[str], optional): A list of band names to be used. Default is BAND_SETS[\"all\"].\n            transform (A.Compose or None, optional): A transformation pipeline to be applied to the data.\n                If None, a default transform converting the data to a tensor is applied.\n            split (str, optional): The dataset split to use (\"train\", \"test\", or \"val\"). Default is \"train\".\n            crop_size (int, optional): The size (in pixels) of the crop to apply to images. Must be greater than 0.\n                Default is 256.\n            random_crop (bool, optional): If True, performs a random crop; otherwise, performs a center crop.\n                Default is True.\n\n        Raises:\n            Exception: If the provided split is not one of \"train\", \"test\", or \"val\".\n            AssertionError: If crop_size is not greater than 0.\n        \"\"\"\n        super().__init__()\n        if split not in [\"train\", \"test\", \"val\"]:\n            msg = \"Split must be one of train, test, val.\"\n            raise Exception(msg)\n\n        self.transform = transform if transform else lambda **batch: to_tensor(batch, transpose=False)\n        self.split = split\n        self.data_root = data_root\n\n        # images in openearthmap are not all 1024x1024 and must be cropped\n        self.crop_size = crop_size\n        self.random_crop = random_crop\n\n        assert self.crop_size &gt; 0, \"Crop size must be greater than 0\"\n\n        self.image_files = self._get_file_paths(Path(self.data_root, f\"{split}.txt\"))\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        image_path, label_path = self.image_files[index]\n\n        with rasterio.open(image_path) as src:\n            image = src.read()\n        with rasterio.open(label_path) as src:\n            mask = src.read()\n\n        # some images in the dataset are not perfect squares\n        # cropping to fit to the prepare_features_for_image_model call\n        if self.random_crop:\n            image, mask = self._random_crop(image, mask)\n        else:\n            image, mask = self._center_crop(image, mask)\n\n        output =  {\n            \"image\": image.astype(np.float32),\n            \"mask\": mask\n        }\n\n        output = self.transform(**output)\n        output['mask'] = output['mask'].long()\n\n        return output\n\n    def _parse_file_name(self, file_name: str):\n        underscore_pos = file_name.rfind('_')\n        folder_name = file_name[:underscore_pos]\n        region_path = Path(self.data_root, folder_name)\n        image_path = Path(region_path, \"images\", file_name)\n        label_path = Path(region_path, \"labels\", file_name)\n        return image_path, label_path\n\n    def _get_file_paths(self, text_file_path: str):\n        with open(text_file_path, 'r') as file:\n            lines = file.readlines()\n            file_paths = [self._parse_file_name(line.strip()) for line in lines]\n        return file_paths\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def _random_crop(self, image, mask):\n        h, w = image.shape[1:]\n        top = np.random.randint(0, h - self.crop_size)\n        left = np.random.randint(0, w - self.crop_size)\n\n        image = image[:, top: top + self.crop_size, left: left + self.crop_size]\n        mask = mask[:, top: top + self.crop_size, left: left + self.crop_size]\n\n        return image, mask\n\n    def _center_crop(self, image, mask):\n        h, w = image.shape[1:]\n        top = (h - self.crop_size) // 2\n        left = (w - self.crop_size) // 2\n\n        image = image[:, top: top + self.crop_size, left: left + self.crop_size]\n        mask = mask[:, top: top + self.crop_size, left: left + self.crop_size]\n\n        return image, mask\n\n    def plot(self, arg, suptitle: str | None = None) -&gt; None:\n        pass\n\n    def plot_sample(self, sample, prediction=None, suptitle: str | None = None, class_names=None):\n        pass\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.openearthmap.OpenEarthMapNonGeo.__init__","title":"<code>__init__(data_root, bands=BAND_SETS['all'], transform=None, split='train', crop_size=256, random_crop=True)</code>","text":"<p>Initialize a new instance of the OpenEarthMapNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>The root directory containing the dataset files.</p> required <code>bands</code> <code>Sequence[str]</code> <p>A list of band names to be used. Default is BAND_SETS[\"all\"].</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose or None</code> <p>A transformation pipeline to be applied to the data. If None, a default transform converting the data to a tensor is applied.</p> <code>None</code> <code>split</code> <code>str</code> <p>The dataset split to use (\"train\", \"test\", or \"val\"). Default is \"train\".</p> <code>'train'</code> <code>crop_size</code> <code>int</code> <p>The size (in pixels) of the crop to apply to images. Must be greater than 0. Default is 256.</p> <code>256</code> <code>random_crop</code> <code>bool</code> <p>If True, performs a random crop; otherwise, performs a center crop. Default is True.</p> <code>True</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the provided split is not one of \"train\", \"test\", or \"val\".</p> <code>AssertionError</code> <p>If crop_size is not greater than 0.</p> Source code in <code>terratorch/datasets/openearthmap.py</code> <pre><code>def __init__(self, data_root: str,\n             bands: Sequence[str] = BAND_SETS[\"all\"],\n             transform: A.Compose | None = None,\n             split=\"train\",\n             crop_size: int = 256,\n             random_crop: bool = True) -&gt; None:\n    \"\"\"\n    Initialize a new instance of the OpenEarthMapNonGeo dataset.\n\n    Args:\n        data_root (str): The root directory containing the dataset files.\n        bands (Sequence[str], optional): A list of band names to be used. Default is BAND_SETS[\"all\"].\n        transform (A.Compose or None, optional): A transformation pipeline to be applied to the data.\n            If None, a default transform converting the data to a tensor is applied.\n        split (str, optional): The dataset split to use (\"train\", \"test\", or \"val\"). Default is \"train\".\n        crop_size (int, optional): The size (in pixels) of the crop to apply to images. Must be greater than 0.\n            Default is 256.\n        random_crop (bool, optional): If True, performs a random crop; otherwise, performs a center crop.\n            Default is True.\n\n    Raises:\n        Exception: If the provided split is not one of \"train\", \"test\", or \"val\".\n        AssertionError: If crop_size is not greater than 0.\n    \"\"\"\n    super().__init__()\n    if split not in [\"train\", \"test\", \"val\"]:\n        msg = \"Split must be one of train, test, val.\"\n        raise Exception(msg)\n\n    self.transform = transform if transform else lambda **batch: to_tensor(batch, transpose=False)\n    self.split = split\n    self.data_root = data_root\n\n    # images in openearthmap are not all 1024x1024 and must be cropped\n    self.crop_size = crop_size\n    self.random_crop = random_crop\n\n    assert self.crop_size &gt; 0, \"Crop size must be greater than 0\"\n\n    self.image_files = self._get_file_paths(Path(self.data_root, f\"{split}.txt\"))\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.pastis","title":"<code>terratorch.datasets.pastis</code>","text":""},{"location":"package/datasets/#terratorch.datasets.pastis.PASTIS","title":"<code>PASTIS</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>\" Pytorch Dataset class to load samples from the PASTIS dataset, for semantic and panoptic segmentation.</p> Source code in <code>terratorch/datasets/pastis.py</code> <pre><code>class PASTIS(NonGeoDataset):\n    \"\"\" \"\n    Pytorch Dataset class to load samples from the [PASTIS](https://github.com/VSainteuf/pastis-benchmark) dataset,\n    for semantic and panoptic segmentation.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_root,\n        norm=True,  # noqa: FBT002\n        target=\"semantic\",\n        folds=None,\n        reference_date=\"2018-09-01\",\n        date_interval=(-200, 600),\n        class_mapping=None,\n        transform=None,\n        truncate_image=None,\n        pad_image=None,\n        satellites=[\"S2\"],  # noqa: B006\n    ):\n        \"\"\"\n\n        Args:\n            data_root (str): Path to the dataset.\n            norm (bool): If true, images are standardised using pre-computed\n                channel-wise means and standard deviations.\n            reference_date (str, Format : 'YYYY-MM-DD'): Defines the reference date\n                based on which all observation dates are expressed. Along with the image\n                time series and the target tensor, this dataloader yields the sequence\n                of observation dates (in terms of number of days since the reference\n                date). This sequence of dates is used for instance for the positional\n                encoding in attention based approaches.\n            target (str): 'semantic' or 'instance'. Defines which type of target is\n                returned by the dataloader.\n                * If 'semantic' the target tensor is a tensor containing the class of\n                each pixel.\n                * If 'instance' the target tensor is the concatenation of several\n                signals, necessary to train the Parcel-as-Points module:\n                    - the centerness heatmap,\n                    - the instance ids,\n                    - the voronoi partitioning of the patch with regards to the parcels'\n                    centers,\n                    - the (height, width) size of each parcel,\n                    - the semantic label of each parcel,\n                    - the semantic label of each pixel.\n            folds (list, optional): List of ints specifying which of the 5 official\n                folds to load. By default (when None is specified), all folds are loaded.\n            class_mapping (dict, optional): A dictionary to define a mapping between the\n                default 18 class nomenclature and another class grouping. If not provided,\n                the default class mapping is used.\n            transform (callable, optional): A transform to apply to the loaded data\n                (images, dates, and masks). By default, no transformation is applied.\n            truncate_image (int, optional): Truncate the time dimension of the image to\n                a specified number of timesteps. If None, no truncation is performed.\n            pad_image (int, optional): Pad the time dimension of the image to a specified\n                number of timesteps. If None, no padding is applied.\n            satellites (list): Defines the satellites to use. If you are using PASTIS-R, you\n                have access to Sentinel-2 imagery and Sentinel-1 observations in Ascending\n                and Descending orbits, respectively S2, S1A, and S1D. For example, use\n                satellites=['S2', 'S1A'] for Sentinel-2 + Sentinel-1 ascending time series,\n                or satellites=['S2', 'S1A', 'S1D'] to retrieve all time series. If you are using\n                PASTIS, only S2 observations are available.\n        \"\"\"\n        if target not in [\"semantic\", \"instance\"]:\n            msg = f\"Target '{target}' not recognized. Use 'semantic', or 'instance'.\"\n            raise ValueError(msg)\n        valid_satellites = {\"S2\", \"S1A\", \"S1D\"}\n        for sat in satellites:\n            if sat not in valid_satellites:\n                msg = f\"Satellite '{sat}' not recognized. Valid options are {valid_satellites}.\"\n                raise ValueError(msg)\n\n        super().__init__()\n        self.data_root = data_root\n        self.norm = norm\n        self.reference_date = datetime(*map(int, reference_date.split(\"-\")), tzinfo=timezone.utc)\n        self.class_mapping = np.vectorize(lambda x: class_mapping[x]) if class_mapping is not None else class_mapping\n        self.target = target\n        self.satellites = satellites\n        self.transform = transform\n        self.truncate_image = truncate_image\n        self.pad_image = pad_image\n        # loads patches metadata\n        self.meta_patch = gpd.read_file(os.path.join(data_root, \"metadata.geojson\"))\n        self.meta_patch.index = self.meta_patch[\"ID_PATCH\"].astype(int)\n        self.meta_patch.sort_index(inplace=True)\n        # stores table for each satalite date\n        self.date_tables = {s: None for s in satellites}\n        # date interval used in the PASTIS benchmark paper.\n        date_interval_begin, date_interval_end = date_interval\n        self.date_range = np.array(range(date_interval_begin, date_interval_end))\n        for s in satellites:\n            # maps patches to its observation dates\n            dates = self.meta_patch[f\"dates-{s}\"]\n            date_table = pd.DataFrame(index=self.meta_patch.index, columns=self.date_range, dtype=int)\n            for pid, date_seq in dates.items():\n                if type(date_seq) is str:\n                    date_seq = json.loads(date_seq)  # noqa: PLW2901\n                # convert date to days since obersavation format\n                d = pd.DataFrame().from_dict(date_seq, orient=\"index\")\n                d = d[0].apply(\n                    lambda x: (\n                        datetime(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:]), tzinfo=timezone.utc)\n                        - self.reference_date\n                    ).days\n                )\n                date_table.loc[pid, d.values] = 1\n            date_table = date_table.fillna(0)\n            self.date_tables[s] = {\n                index: np.array(list(d.values())) for index, d in date_table.to_dict(orient=\"index\").items()\n            }\n\n        # selects patches correspondig to selected folds\n        if folds is not None:\n            self.meta_patch = pd.concat([self.meta_patch[self.meta_patch[\"Fold\"] == f] for f in folds])\n\n        self.len = self.meta_patch.shape[0]\n        self.id_patches = self.meta_patch.index\n\n        # loads normalization values\n        if norm:\n            self.norm = {}\n            for s in self.satellites:\n                with open(os.path.join(data_root, f\"NORM_{s}_patch.json\")) as file:\n                    normvals = json.loads(file.read())\n                selected_folds = folds if folds is not None else range(1, 6)\n                means = [normvals[f\"Fold_{f}\"][\"mean\"] for f in selected_folds]\n                stds = [normvals[f\"Fold_{f}\"][\"std\"] for f in selected_folds]\n                self.norm[s] = np.stack(means).mean(axis=0), np.stack(stds).mean(axis=0)\n                self.norm[s] = (\n                    self.norm[s][0],\n                    self.norm[s][1],\n                )\n        else:\n            self.norm = None\n\n    def __len__(self):\n        return self.len\n\n    def get_dates(self, id_patch, sat):\n        return self.date_range[np.where(self.date_tables[sat][id_patch] == 1)[0]]\n\n    def __getitem__(self, item):\n        id_patch = self.id_patches[item]\n        output = {}\n        satellites = {}\n        for satellite in self.satellites:\n            data = np.load(\n                os.path.join(\n                    self.data_root,\n                    f\"DATA_{satellite}\",\n                    f\"{satellite}_{id_patch}.npy\",\n                )\n            ).astype(np.float32)\n\n            if self.norm is not None:\n                data = data - self.norm[satellite][0][None, :, None, None]\n                data = data / self.norm[satellite][1][None, :, None, None]\n\n            if self.truncate_image and data.shape[0] &gt; self.truncate_image:\n                data = data[-self.truncate_image :]\n\n            if self.pad_image and data.shape[0] &lt; self.pad_image:\n                data = pad_numpy(data, self.pad_image)\n\n            satellites[satellite] = data.astype(np.float32)\n\n        if self.target == \"semantic\":\n            target = np.load(os.path.join(self.data_root, \"ANNOTATIONS\", f\"TARGET_{id_patch}.npy\"))\n            target = target[0].astype(int)\n            if self.class_mapping is not None:\n                target = self.class_mapping(target)\n        elif self.target == \"instance\":\n            heatmap = np.load(os.path.join(self.data_root, \"INSTANCE_ANNOTATIONS\", f\"HEATMAP_{id_patch}.npy\"))\n            instance_ids = np.load(os.path.join(self.data_root, \"INSTANCE_ANNOTATIONS\", f\"INSTANCES_{id_patch}.npy\"))\n            zones_path = os.path.join(self.data_root, \"INSTANCE_ANNOTATIONS\", f\"ZONES_{id_patch}.npy\")\n            pixel_to_object_mapping = np.load(zones_path)\n            pixel_semantic_annotation = np.load(os.path.join(self.data_root, \"ANNOTATIONS\", f\"TARGET_{id_patch}.npy\"))\n\n            if self.class_mapping is not None:\n                pixel_semantic_annotation = self.class_mapping(pixel_semantic_annotation[0])\n            else:\n                pixel_semantic_annotation = pixel_semantic_annotation[0]\n\n            size = np.zeros((*instance_ids.shape, 2))\n            object_semantic_annotation = np.zeros(instance_ids.shape)\n            for instance_id in np.unique(instance_ids):\n                if instance_id != 0:\n                    h = (instance_ids == instance_id).any(axis=-1).sum()\n                    w = (instance_ids == instance_id).any(axis=-2).sum()\n                    size[pixel_to_object_mapping == instance_id] = (h, w)\n                    semantic_value = pixel_semantic_annotation[instance_ids == instance_id][0]\n                    object_semantic_annotation[pixel_to_object_mapping == instance_id] = semantic_value\n\n            target = np.concatenate(\n                [\n                    heatmap[:, :, None],\n                    instance_ids[:, :, None],\n                    pixel_to_object_mapping[:, :, None],\n                    size,\n                    object_semantic_annotation[:, :, None],\n                    pixel_semantic_annotation[:, :, None],\n                ],\n                axis=-1,\n            ).astype(np.float32)\n\n        dates = {}\n        for satellite in self.satellites:\n            date = np.array(self.get_dates(id_patch, satellite))\n\n            if self.truncate_image and len(date) &gt; self.truncate_image:\n                date = date[-self.truncate_image :]\n\n            if self.pad_image and len(date) &lt; self.pad_image:\n                date = pad_dates_numpy(date, self.pad_image)\n\n            dates[satellite] = torch.from_numpy(date)\n\n        output[\"image\"] = satellites[\"S2\"].transpose(0, 2, 3, 1)\n        output[\"mask\"] = target\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output.update(satellites)\n        output[\"dates\"] = dates\n\n        return output\n\n    def plot(self, sample, suptitle=None, show_axes=False):\n        dates = sample[\"dates\"]\n        target = sample[\"target\"]\n\n        if \"S2\" not in sample:\n            warnings.warn(\"No RGB image.\", stacklevel=2)\n            return None\n\n        image_data = sample[\"S2\"]\n        date_data = dates[\"S2\"]\n\n        rgb_images = []\n        for i in range(image_data.shape[0]):\n            rgb_image = image_data[i, :3, :, :].numpy().transpose(1, 2, 0)\n\n            rgb_min = rgb_image.min(axis=(0, 1), keepdims=True)\n            rgb_max = rgb_image.max(axis=(0, 1), keepdims=True)\n            denom = rgb_max - rgb_min\n            denom[denom == 0] = 1\n            rgb_image = (rgb_image - rgb_min) / denom\n\n            rgb_images.append(np.clip(rgb_image, 0, 1))\n\n        return self._plot_sample(rgb_images, date_data, target, suptitle=suptitle, show_axes=show_axes)\n\n    def _plot_sample(\n        self,\n        images: list[np.ndarray],\n        dates: torch.Tensor,\n        target: torch.Tensor | None,\n        suptitle: str | None = None,\n        show_axes: bool | None = False,\n    ):\n        num_images = len(images)\n        cols = 5\n        rows = (num_images + cols) // cols\n\n        fig, ax = plt.subplots(rows, cols, figsize=(20, 4 * rows))\n        axes_visibility = \"on\" if show_axes else \"off\"\n\n        for i, image in enumerate(images):\n            ax[i // cols, i % cols].imshow(image)\n            ax[i // cols, i % cols].set_title(f\"Image {i + 1} - Day {dates[i].item()}\")\n            ax[i // cols, i % cols].axis(axes_visibility)\n\n        if target is not None:\n            if rows * cols &gt; num_images:\n                target_ax = ax[(num_images) // cols, (num_images) % cols]\n            else:\n                fig.add_subplot(rows + 1, 1, 1)\n                target_ax = fig.gca()\n\n            target_ax.imshow(target.numpy(), cmap=\"tab20\")\n            target_ax.set_title(\"Target\")\n            target_ax.axis(axes_visibility)\n\n        for k in range(num_images + 1, rows * cols):\n            ax[k // cols, k % cols].axis(axes_visibility)\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        plt.tight_layout()\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.pastis.PASTIS.__init__","title":"<code>__init__(data_root, norm=True, target='semantic', folds=None, reference_date='2018-09-01', date_interval=(-200, 600), class_mapping=None, transform=None, truncate_image=None, pad_image=None, satellites=['S2'])</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the dataset.</p> required <code>norm</code> <code>bool</code> <p>If true, images are standardised using pre-computed channel-wise means and standard deviations.</p> <code>True</code> <code>reference_date</code> <code>(str, Format)</code> <p>'YYYY-MM-DD'): Defines the reference date based on which all observation dates are expressed. Along with the image time series and the target tensor, this dataloader yields the sequence of observation dates (in terms of number of days since the reference date). This sequence of dates is used for instance for the positional encoding in attention based approaches.</p> <code>'2018-09-01'</code> <code>target</code> <code>str</code> <p>'semantic' or 'instance'. Defines which type of target is returned by the dataloader. * If 'semantic' the target tensor is a tensor containing the class of each pixel. * If 'instance' the target tensor is the concatenation of several signals, necessary to train the Parcel-as-Points module:     - the centerness heatmap,     - the instance ids,     - the voronoi partitioning of the patch with regards to the parcels'     centers,     - the (height, width) size of each parcel,     - the semantic label of each parcel,     - the semantic label of each pixel.</p> <code>'semantic'</code> <code>folds</code> <code>list</code> <p>List of ints specifying which of the 5 official folds to load. By default (when None is specified), all folds are loaded.</p> <code>None</code> <code>class_mapping</code> <code>dict</code> <p>A dictionary to define a mapping between the default 18 class nomenclature and another class grouping. If not provided, the default class mapping is used.</p> <code>None</code> <code>transform</code> <code>callable</code> <p>A transform to apply to the loaded data (images, dates, and masks). By default, no transformation is applied.</p> <code>None</code> <code>truncate_image</code> <code>int</code> <p>Truncate the time dimension of the image to a specified number of timesteps. If None, no truncation is performed.</p> <code>None</code> <code>pad_image</code> <code>int</code> <p>Pad the time dimension of the image to a specified number of timesteps. If None, no padding is applied.</p> <code>None</code> <code>satellites</code> <code>list</code> <p>Defines the satellites to use. If you are using PASTIS-R, you have access to Sentinel-2 imagery and Sentinel-1 observations in Ascending and Descending orbits, respectively S2, S1A, and S1D. For example, use satellites=['S2', 'S1A'] for Sentinel-2 + Sentinel-1 ascending time series, or satellites=['S2', 'S1A', 'S1D'] to retrieve all time series. If you are using PASTIS, only S2 observations are available.</p> <code>['S2']</code> Source code in <code>terratorch/datasets/pastis.py</code> <pre><code>def __init__(\n    self,\n    data_root,\n    norm=True,  # noqa: FBT002\n    target=\"semantic\",\n    folds=None,\n    reference_date=\"2018-09-01\",\n    date_interval=(-200, 600),\n    class_mapping=None,\n    transform=None,\n    truncate_image=None,\n    pad_image=None,\n    satellites=[\"S2\"],  # noqa: B006\n):\n    \"\"\"\n\n    Args:\n        data_root (str): Path to the dataset.\n        norm (bool): If true, images are standardised using pre-computed\n            channel-wise means and standard deviations.\n        reference_date (str, Format : 'YYYY-MM-DD'): Defines the reference date\n            based on which all observation dates are expressed. Along with the image\n            time series and the target tensor, this dataloader yields the sequence\n            of observation dates (in terms of number of days since the reference\n            date). This sequence of dates is used for instance for the positional\n            encoding in attention based approaches.\n        target (str): 'semantic' or 'instance'. Defines which type of target is\n            returned by the dataloader.\n            * If 'semantic' the target tensor is a tensor containing the class of\n            each pixel.\n            * If 'instance' the target tensor is the concatenation of several\n            signals, necessary to train the Parcel-as-Points module:\n                - the centerness heatmap,\n                - the instance ids,\n                - the voronoi partitioning of the patch with regards to the parcels'\n                centers,\n                - the (height, width) size of each parcel,\n                - the semantic label of each parcel,\n                - the semantic label of each pixel.\n        folds (list, optional): List of ints specifying which of the 5 official\n            folds to load. By default (when None is specified), all folds are loaded.\n        class_mapping (dict, optional): A dictionary to define a mapping between the\n            default 18 class nomenclature and another class grouping. If not provided,\n            the default class mapping is used.\n        transform (callable, optional): A transform to apply to the loaded data\n            (images, dates, and masks). By default, no transformation is applied.\n        truncate_image (int, optional): Truncate the time dimension of the image to\n            a specified number of timesteps. If None, no truncation is performed.\n        pad_image (int, optional): Pad the time dimension of the image to a specified\n            number of timesteps. If None, no padding is applied.\n        satellites (list): Defines the satellites to use. If you are using PASTIS-R, you\n            have access to Sentinel-2 imagery and Sentinel-1 observations in Ascending\n            and Descending orbits, respectively S2, S1A, and S1D. For example, use\n            satellites=['S2', 'S1A'] for Sentinel-2 + Sentinel-1 ascending time series,\n            or satellites=['S2', 'S1A', 'S1D'] to retrieve all time series. If you are using\n            PASTIS, only S2 observations are available.\n    \"\"\"\n    if target not in [\"semantic\", \"instance\"]:\n        msg = f\"Target '{target}' not recognized. Use 'semantic', or 'instance'.\"\n        raise ValueError(msg)\n    valid_satellites = {\"S2\", \"S1A\", \"S1D\"}\n    for sat in satellites:\n        if sat not in valid_satellites:\n            msg = f\"Satellite '{sat}' not recognized. Valid options are {valid_satellites}.\"\n            raise ValueError(msg)\n\n    super().__init__()\n    self.data_root = data_root\n    self.norm = norm\n    self.reference_date = datetime(*map(int, reference_date.split(\"-\")), tzinfo=timezone.utc)\n    self.class_mapping = np.vectorize(lambda x: class_mapping[x]) if class_mapping is not None else class_mapping\n    self.target = target\n    self.satellites = satellites\n    self.transform = transform\n    self.truncate_image = truncate_image\n    self.pad_image = pad_image\n    # loads patches metadata\n    self.meta_patch = gpd.read_file(os.path.join(data_root, \"metadata.geojson\"))\n    self.meta_patch.index = self.meta_patch[\"ID_PATCH\"].astype(int)\n    self.meta_patch.sort_index(inplace=True)\n    # stores table for each satalite date\n    self.date_tables = {s: None for s in satellites}\n    # date interval used in the PASTIS benchmark paper.\n    date_interval_begin, date_interval_end = date_interval\n    self.date_range = np.array(range(date_interval_begin, date_interval_end))\n    for s in satellites:\n        # maps patches to its observation dates\n        dates = self.meta_patch[f\"dates-{s}\"]\n        date_table = pd.DataFrame(index=self.meta_patch.index, columns=self.date_range, dtype=int)\n        for pid, date_seq in dates.items():\n            if type(date_seq) is str:\n                date_seq = json.loads(date_seq)  # noqa: PLW2901\n            # convert date to days since obersavation format\n            d = pd.DataFrame().from_dict(date_seq, orient=\"index\")\n            d = d[0].apply(\n                lambda x: (\n                    datetime(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:]), tzinfo=timezone.utc)\n                    - self.reference_date\n                ).days\n            )\n            date_table.loc[pid, d.values] = 1\n        date_table = date_table.fillna(0)\n        self.date_tables[s] = {\n            index: np.array(list(d.values())) for index, d in date_table.to_dict(orient=\"index\").items()\n        }\n\n    # selects patches correspondig to selected folds\n    if folds is not None:\n        self.meta_patch = pd.concat([self.meta_patch[self.meta_patch[\"Fold\"] == f] for f in folds])\n\n    self.len = self.meta_patch.shape[0]\n    self.id_patches = self.meta_patch.index\n\n    # loads normalization values\n    if norm:\n        self.norm = {}\n        for s in self.satellites:\n            with open(os.path.join(data_root, f\"NORM_{s}_patch.json\")) as file:\n                normvals = json.loads(file.read())\n            selected_folds = folds if folds is not None else range(1, 6)\n            means = [normvals[f\"Fold_{f}\"][\"mean\"] for f in selected_folds]\n            stds = [normvals[f\"Fold_{f}\"][\"std\"] for f in selected_folds]\n            self.norm[s] = np.stack(means).mean(axis=0), np.stack(stds).mean(axis=0)\n            self.norm[s] = (\n                self.norm[s][0],\n                self.norm[s][1],\n            )\n    else:\n        self.norm = None\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.sen1floods11","title":"<code>terratorch.datasets.sen1floods11</code>","text":""},{"location":"package/datasets/#terratorch.datasets.sen1floods11.Sen1Floods11NonGeo","title":"<code>Sen1Floods11NonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for sen1floods11.</p> Source code in <code>terratorch/datasets/sen1floods11.py</code> <pre><code>class Sen1Floods11NonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [sen1floods11](https://github.com/cloudtostreet/Sen1Floods11).\"\"\"\n\n    all_band_names = (\n            \"COASTAL_AEROSOL\",\n            \"BLUE\",\n            \"GREEN\",\n            \"RED\",\n            \"RED_EDGE_1\",\n            \"RED_EDGE_2\",\n            \"RED_EDGE_3\",\n            \"NIR_BROAD\",\n            \"NIR_NARROW\",\n            \"WATER_VAPOR\",\n            \"CIRRUS\",\n            \"SWIR_1\",\n            \"SWIR_2\",\n    )\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n    num_classes = 2\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n    data_dir = \"v1.1/data/flood_events/HandLabeled/S2Hand\"\n    label_dir = \"v1.1/data/flood_events/HandLabeled/LabelHand\"\n    split_dir = \"v1.1/splits/flood_handlabeled\"\n    metadata_file = \"v1.1/Sen1Floods11_Metadata.geojson\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        constant_scale: float = 0.0001,\n        no_data_replace: float | None = 0,\n        no_label_replace: int | None = -1,\n        use_metadata: bool = False,  # noqa: FBT001, FBT002\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): one of 'train', 'val' or 'test'.\n            bands (list[str]): Bands that should be output by the dataset. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). Defaults to None, which applies ToTensorV2().\n            constant_scale (float): Factor to multiply image values by. Defaults to 0.0001.\n            no_data_replace (float | None): Replace nan values in input images with this value.\n                If None, does no replacement. Defaults to 0.\n            no_label_replace (int | None): Replace nan values in label with this value.\n                If none, does no replacement. Defaults to -1.\n            use_metadata (bool): whether to return metadata info (time and location).\n        \"\"\"\n        super().__init__()\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {self.splits}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n        self.constant_scale = constant_scale\n        self.data_root = Path(data_root)\n\n        data_dir = self.data_root / self.data_dir\n        label_dir = self.data_root / self.label_dir\n\n        self.image_files = sorted(glob.glob(os.path.join(data_dir, \"*_S2Hand.tif\")))\n        self.segmentation_mask_files = sorted(glob.glob(os.path.join(label_dir, \"*_LabelHand.tif\")))\n\n        split_file = self.data_root / self.split_dir / f\"flood_{split_name}_data.txt\"\n        with open(split_file) as f:\n            split = f.readlines()\n        valid_files = {rf\"{substring.strip()}\" for substring in split}\n        self.image_files = filter_valid_files(\n            self.image_files,\n            valid_files=valid_files,\n            ignore_extensions=True,\n            allow_substring=True,\n        )\n        self.segmentation_mask_files = filter_valid_files(\n            self.segmentation_mask_files,\n            valid_files=valid_files,\n            ignore_extensions=True,\n            allow_substring=True,\n        )\n\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.use_metadata = use_metadata\n        self.metadata = None\n        if self.use_metadata:\n            self.metadata = geopandas.read_file(self.data_root / self.metadata_file)\n\n        # If no transform is given, apply only to transform to torch tensor\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def _get_date(self, index: int) -&gt; torch.Tensor:\n        file_name = self.image_files[index]\n        location = os.path.basename(file_name).split(\"_\")[0]\n        if self.metadata[self.metadata[\"location\"] == location].shape[0] != 1:\n            date = pd.to_datetime(\"13-10-1998\", dayfirst=True)\n        else:\n            date = pd.to_datetime(self.metadata[self.metadata[\"location\"] == location][\"s2_date\"].item())\n\n        return torch.tensor([[date.year, date.dayofyear - 1]], dtype=torch.float32)  # (n_timesteps, coords)\n\n    def _get_coords(self, image: DataArray) -&gt; torch.Tensor:\n\n        center_lat = image.y[image.y.shape[0] // 2]\n        center_lon = image.x[image.x.shape[0] // 2]\n        lat_lon = np.asarray([center_lat, center_lon])\n\n        return torch.tensor(lat_lon, dtype=torch.float32)\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        image = self._load_file(self.image_files[index], nan_replace=self.no_data_replace)\n\n        location_coords, temporal_coords = None, None\n        if self.use_metadata:\n            location_coords = self._get_coords(image)\n            temporal_coords = self._get_date(index)\n\n        # to channels last\n        image = image.to_numpy()\n        image = np.moveaxis(image, 0, -1)\n\n        # filter bands\n        image = image[..., self.band_indices]\n\n        output = {\n            \"image\": image.astype(np.float32) * self.constant_scale,\n            \"mask\": self._load_file(\n                self.segmentation_mask_files[index], nan_replace=self.no_label_replace).to_numpy()[0],\n        }\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        if self.use_metadata:\n            output[\"location_coords\"] = location_coords\n            output[\"temporal_coords\"] = temporal_coords\n\n        return output\n\n    def _load_file(self, path: Path, nan_replace: int | float | None = None) -&gt; DataArray:\n        data = rioxarray.open_rasterio(path, masked=True)\n        if nan_replace is not None:\n            data = data.fillna(nan_replace)\n        return data\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            suptitle: optional string to use as a suptitle\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n        \"\"\"\n        num_images = 4\n\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        # RGB -&gt; channels-last\n        image = sample[\"image\"][rgb_indices, ...].permute(1, 2, 0).numpy()\n        mask = sample[\"mask\"].numpy()\n\n        image = clip_image(image)\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"]\n            num_images += 1\n        else:\n            prediction = None\n\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 5), layout=\"compressed\")\n\n        ax[0].axis(\"off\")\n\n        norm = mpl.colors.Normalize(vmin=0, vmax=self.num_classes - 1)\n        ax[1].axis(\"off\")\n        ax[1].title.set_text(\"Image\")\n        ax[1].imshow(image)\n\n        ax[2].axis(\"off\")\n        ax[2].title.set_text(\"Ground Truth Mask\")\n        ax[2].imshow(mask, cmap=\"jet\", norm=norm)\n\n        ax[3].axis(\"off\")\n        ax[3].title.set_text(\"GT Mask on Image\")\n        ax[3].imshow(image)\n        ax[3].imshow(mask, cmap=\"jet\", alpha=0.3, norm=norm)\n\n        if \"prediction\" in sample:\n            ax[4].title.set_text(\"Predicted Mask\")\n            ax[4].imshow(prediction, cmap=\"jet\", norm=norm)\n\n        cmap = plt.get_cmap(\"jet\")\n        legend_data = [[i, cmap(norm(i)), str(i)] for i in range(self.num_classes)]\n        handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n        labels = [n for k, c, n in legend_data]\n        ax[0].legend(handles, labels, loc=\"center\")\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.sen1floods11.Sen1Floods11NonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, constant_scale=0.0001, no_data_replace=0, no_label_replace=-1, use_metadata=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>one of 'train', 'val' or 'test'.</p> <code>'train'</code> <code>bands</code> <code>list[str]</code> <p>Bands that should be output by the dataset. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). Defaults to None, which applies ToTensorV2().</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 0.0001.</p> <code>0.0001</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If None, does no replacement. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>whether to return metadata info (time and location).</p> <code>False</code> Source code in <code>terratorch/datasets/sen1floods11.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    constant_scale: float = 0.0001,\n    no_data_replace: float | None = 0,\n    no_label_replace: int | None = -1,\n    use_metadata: bool = False,  # noqa: FBT001, FBT002\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): one of 'train', 'val' or 'test'.\n        bands (list[str]): Bands that should be output by the dataset. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). Defaults to None, which applies ToTensorV2().\n        constant_scale (float): Factor to multiply image values by. Defaults to 0.0001.\n        no_data_replace (float | None): Replace nan values in input images with this value.\n            If None, does no replacement. Defaults to 0.\n        no_label_replace (int | None): Replace nan values in label with this value.\n            If none, does no replacement. Defaults to -1.\n        use_metadata (bool): whether to return metadata info (time and location).\n    \"\"\"\n    super().__init__()\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {self.splits}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n    self.constant_scale = constant_scale\n    self.data_root = Path(data_root)\n\n    data_dir = self.data_root / self.data_dir\n    label_dir = self.data_root / self.label_dir\n\n    self.image_files = sorted(glob.glob(os.path.join(data_dir, \"*_S2Hand.tif\")))\n    self.segmentation_mask_files = sorted(glob.glob(os.path.join(label_dir, \"*_LabelHand.tif\")))\n\n    split_file = self.data_root / self.split_dir / f\"flood_{split_name}_data.txt\"\n    with open(split_file) as f:\n        split = f.readlines()\n    valid_files = {rf\"{substring.strip()}\" for substring in split}\n    self.image_files = filter_valid_files(\n        self.image_files,\n        valid_files=valid_files,\n        ignore_extensions=True,\n        allow_substring=True,\n    )\n    self.segmentation_mask_files = filter_valid_files(\n        self.segmentation_mask_files,\n        valid_files=valid_files,\n        ignore_extensions=True,\n        allow_substring=True,\n    )\n\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.use_metadata = use_metadata\n    self.metadata = None\n    if self.use_metadata:\n        self.metadata = geopandas.read_file(self.data_root / self.metadata_file)\n\n    # If no transform is given, apply only to transform to torch tensor\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.sen1floods11.Sen1Floods11NonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> Source code in <code>terratorch/datasets/sen1floods11.py</code> <pre><code>def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        suptitle: optional string to use as a suptitle\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n    \"\"\"\n    num_images = 4\n\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    # RGB -&gt; channels-last\n    image = sample[\"image\"][rgb_indices, ...].permute(1, 2, 0).numpy()\n    mask = sample[\"mask\"].numpy()\n\n    image = clip_image(image)\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"]\n        num_images += 1\n    else:\n        prediction = None\n\n    fig, ax = plt.subplots(1, num_images, figsize=(12, 5), layout=\"compressed\")\n\n    ax[0].axis(\"off\")\n\n    norm = mpl.colors.Normalize(vmin=0, vmax=self.num_classes - 1)\n    ax[1].axis(\"off\")\n    ax[1].title.set_text(\"Image\")\n    ax[1].imshow(image)\n\n    ax[2].axis(\"off\")\n    ax[2].title.set_text(\"Ground Truth Mask\")\n    ax[2].imshow(mask, cmap=\"jet\", norm=norm)\n\n    ax[3].axis(\"off\")\n    ax[3].title.set_text(\"GT Mask on Image\")\n    ax[3].imshow(image)\n    ax[3].imshow(mask, cmap=\"jet\", alpha=0.3, norm=norm)\n\n    if \"prediction\" in sample:\n        ax[4].title.set_text(\"Predicted Mask\")\n        ax[4].imshow(prediction, cmap=\"jet\", norm=norm)\n\n    cmap = plt.get_cmap(\"jet\")\n    legend_data = [[i, cmap(norm(i)), str(i)] for i in range(self.num_classes)]\n    handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n    labels = [n for k, c, n in legend_data]\n    ax[0].legend(handles, labels, loc=\"center\")\n    if suptitle is not None:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.sen4agrinet","title":"<code>terratorch.datasets.sen4agrinet</code>","text":""},{"location":"package/datasets/#terratorch.datasets.sen4agrinet.Sen4AgriNet","title":"<code>Sen4AgriNet</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> Source code in <code>terratorch/datasets/sen4agrinet.py</code> <pre><code>class Sen4AgriNet(NonGeoDataset):\n    def __init__(\n        self,\n        data_root: str,\n        bands: list[str] | None = None,\n        scenario: str = \"random\",\n        split: str = \"train\",\n        transform: A.Compose = None,\n        truncate_image: int | None = 4,\n        pad_image: int | None = 4,\n        spatial_interpolate_and_stack_temporally: bool = True,  # noqa: FBT001, FBT002\n        seed: int = 42,\n    ):\n        \"\"\"\n        Pytorch Dataset class to load samples from the [Sen4AgriNet](https://github.com/Orion-AI-Lab/S4A) dataset, supporting\n        multiple scenarios for splitting the data.\n\n        Args:\n            data_root (str): Root directory of the dataset.\n            bands (list of str, optional): List of band names to load. Defaults to all available bands.\n            scenario (str): Defines the splitting scenario to use. Options are:\n                - 'random': Random split of the data.\n                - 'spatial': Split by geographical regions (Catalonia and France).\n                - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020).\n            split (str): Specifies the dataset split. Options are 'train', 'val', or 'test'.\n            transform (albumentations.Compose, optional): Albumentations transformations to apply to the data.\n            truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image.\n                If None, no truncation is applied. Default is 4.\n            pad_image (int, optional): Number of timesteps to pad the time dimension of the image.\n                If None, no padding is applied. Default is 4.\n            spatial_interpolate_and_stack_temporally (bool): Whether to interpolate bands and concatenate them over time\n            seed (int): Random seed used for data splitting.\n        \"\"\"\n        self.data_root = Path(data_root) / \"data\"\n        self.transform = transform if transform else lambda **batch: to_tensor(batch)\n        self.scenario = scenario\n        self.seed = seed\n        self.truncate_image = truncate_image\n        self.pad_image = pad_image\n        self.spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally\n\n        if bands is None:\n            bands = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B09\", \"B10\", \"B11\", \"B12\", \"B8A\"]\n        self.bands = bands\n\n        self.image_files = list(self.data_root.glob(\"**/*.nc\"))\n\n        self.train_files, self.val_files, self.test_files = self.split_data()\n\n        if split == \"train\":\n            self.image_files = self.train_files\n        elif split == \"val\":\n            self.image_files = self.val_files\n        elif split == \"test\":\n            self.image_files = self.test_files\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def split_data(self):\n        random.seed(self.seed)\n\n        if self.scenario == \"random\":\n            random.shuffle(self.image_files)\n            total_files = len(self.image_files)\n            train_split = int(0.6 * total_files)\n            val_split = int(0.8 * total_files)\n\n            train_files = self.image_files[:train_split]\n            val_files = self.image_files[train_split:val_split]\n            test_files = self.image_files[val_split:]\n\n        elif self.scenario == \"spatial\":\n            catalonia_files = [f for f in self.image_files if any(tile in f.stem for tile in CAT_TILES)]\n            france_files = [f for f in self.image_files if any(tile in f.stem for tile in FR_TILES)]\n\n            val_split_cat = int(0.2 * len(catalonia_files))\n            train_files = catalonia_files[val_split_cat:]\n            val_files = catalonia_files[:val_split_cat]\n            test_files = france_files\n\n        elif self.scenario == \"spatio-temporal\":\n            france_files = [f for f in self.image_files if any(tile in f.stem for tile in FR_TILES)]\n            catalonia_files = [f for f in self.image_files if any(tile in f.stem for tile in CAT_TILES)]\n\n            france_2019_files = [f for f in france_files if \"2019\" in f.stem]\n            catalonia_2020_files = [f for f in catalonia_files if \"2020\" in f.stem]\n\n            val_split_france_2019 = int(0.2 * len(france_2019_files))\n            train_files = france_2019_files[val_split_france_2019:]\n            val_files = france_2019_files[:val_split_france_2019]\n            test_files = catalonia_2020_files\n\n        return train_files, val_files, test_files\n\n    def __getitem__(self, index: int):\n        patch_file = self.image_files[index]\n\n        with h5py.File(patch_file, \"r\") as patch_data:\n            output = {}\n            images_over_time = []\n            for band in self.bands:\n                band_group = patch_data[band]\n                band_data = band_group[f\"{band}\"][:]\n                time_vector = band_group[\"time\"][:]\n\n                sorted_indices = np.argsort(time_vector)\n                band_data = band_data[sorted_indices].astype(np.float32)\n\n                if self.truncate_image:\n                    band_data = band_data[-self.truncate_image :]\n                if self.pad_image:\n                    band_data = pad_numpy(band_data, self.pad_image)\n\n                if self.spatial_interpolate_and_stack_temporally:\n                    band_data = torch.from_numpy(band_data)\n                    band_data = band_data.clone().detach()\n\n                    interpolated = F.interpolate(\n                        band_data.unsqueeze(0), size=MAX_TEMPORAL_IMAGE_SIZE, mode=\"bilinear\", align_corners=False\n                    ).squeeze(0)\n                    images_over_time.append(interpolated)\n                else:\n                    output[band] = band_data\n\n            if self.spatial_interpolate_and_stack_temporally:\n                images = torch.stack(images_over_time, dim=0).numpy()\n                output[\"image\"] = images\n\n            labels = patch_data[\"labels\"][\"labels\"][:].astype(int)\n            parcels = patch_data[\"parcels\"][\"parcels\"][:].astype(int)\n\n        output[\"mask\"] = labels\n\n        image_shape = output[\"image\"].shape[-2:]\n        mask_shape = output[\"mask\"].shape\n\n        if image_shape != mask_shape:\n            diff_h = mask_shape[0] - image_shape[0]\n            diff_w = mask_shape[1] - image_shape[1]\n\n            output[\"image\"] = np.pad(\n                output[\"image\"],\n                [(0, 0), (0, 0), (diff_h // 2, diff_h - diff_h // 2), (diff_w // 2, diff_w - diff_w // 2)],\n                mode=\"constant\",\n                constant_values=0,\n            )\n\n        linear_encoder = {val: i + 1 for i, val in enumerate(sorted(SELECTED_CLASSES))}\n        linear_encoder[0] = 0\n\n        output[\"image\"] = output[\"image\"].transpose(0, 2, 3, 1)\n        output[\"mask\"] = self.map_mask_to_discrete_classes(output[\"mask\"], linear_encoder)\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"parcels\"] = parcels\n\n        return output\n\n    def plot(self, sample, suptitle=None, show_axes=False):\n        rgb_bands = [\"B04\", \"B03\", \"B02\"]\n\n        if not all(band in sample for band in rgb_bands):\n            warnings.warn(\"No RGB image.\")  # noqa: B028\n            return None\n\n        rgb_images = []\n        for t in range(sample[\"B04\"].shape[0]):\n            rgb_image = torch.stack([sample[band][t] for band in rgb_bands])\n\n            # Normalization\n            rgb_min = rgb_image.min(dim=1, keepdim=True).values.min(dim=2, keepdim=True).values\n            rgb_max = rgb_image.max(dim=1, keepdim=True).values.max(dim=2, keepdim=True).values\n            denom = rgb_max - rgb_min\n            denom[denom == 0] = 1\n            rgb_image = (rgb_image - rgb_min) / denom\n\n            rgb_image = rgb_image.permute(1, 2, 0).numpy()\n            rgb_images.append(np.clip(rgb_image, 0, 1))\n\n        dates = torch.arange(sample[\"B04\"].shape[0])\n\n        return self._plot_sample(rgb_images, dates, sample.get(\"labels\"), suptitle=suptitle, show_axes=show_axes)\n\n    def _plot_sample(self, images, dates, labels=None, suptitle=None, show_axes=False):\n        num_images = len(images)\n        cols = 5\n        rows = (num_images + cols - 1) // cols\n\n        fig, ax = plt.subplots(rows, cols, figsize=(20, 4 * rows))\n        axes_visibility = \"on\" if show_axes else \"off\"\n\n        for i, image in enumerate(images):\n            ax[i // cols, i % cols].imshow(image)\n            ax[i // cols, i % cols].set_title(f\"T{i+1} - Day {dates[i].item()}\")\n            ax[i // cols, i % cols].axis(axes_visibility)\n\n        if labels is not None:\n            if rows * cols &gt; num_images:\n                target_ax = ax[(num_images) // cols, (num_images) % cols]\n            else:\n                fig.add_subplot(rows + 1, 1, 1)\n                target_ax = fig.gca()\n\n            target_ax.imshow(labels.numpy(), cmap=\"tab20\")\n            target_ax.set_title(\"Labels\")\n            target_ax.axis(axes_visibility)\n\n        for k in range(num_images, rows * cols):\n            ax[k // cols, k % cols].axis(axes_visibility)\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        plt.tight_layout()\n        plt.show()\n\n    def map_mask_to_discrete_classes(self, mask, encoder):\n        map_func = np.vectorize(lambda x: encoder.get(x, 0))\n        return map_func(mask)\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.sen4agrinet.Sen4AgriNet.__init__","title":"<code>__init__(data_root, bands=None, scenario='random', split='train', transform=None, truncate_image=4, pad_image=4, spatial_interpolate_and_stack_temporally=True, seed=42)</code>","text":"<p>Pytorch Dataset class to load samples from the Sen4AgriNet dataset, supporting multiple scenarios for splitting the data.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>bands</code> <code>list of str</code> <p>List of band names to load. Defaults to all available bands.</p> <code>None</code> <code>scenario</code> <code>str</code> <p>Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020).</p> <code>'random'</code> <code>split</code> <code>str</code> <p>Specifies the dataset split. Options are 'train', 'val', or 'test'.</p> <code>'train'</code> <code>transform</code> <code>Compose</code> <p>Albumentations transformations to apply to the data.</p> <code>None</code> <code>truncate_image</code> <code>int</code> <p>Number of timesteps to truncate the time dimension of the image. If None, no truncation is applied. Default is 4.</p> <code>4</code> <code>pad_image</code> <code>int</code> <p>Number of timesteps to pad the time dimension of the image. If None, no padding is applied. Default is 4.</p> <code>4</code> <code>spatial_interpolate_and_stack_temporally</code> <code>bool</code> <p>Whether to interpolate bands and concatenate them over time</p> <code>True</code> <code>seed</code> <code>int</code> <p>Random seed used for data splitting.</p> <code>42</code> Source code in <code>terratorch/datasets/sen4agrinet.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    bands: list[str] | None = None,\n    scenario: str = \"random\",\n    split: str = \"train\",\n    transform: A.Compose = None,\n    truncate_image: int | None = 4,\n    pad_image: int | None = 4,\n    spatial_interpolate_and_stack_temporally: bool = True,  # noqa: FBT001, FBT002\n    seed: int = 42,\n):\n    \"\"\"\n    Pytorch Dataset class to load samples from the [Sen4AgriNet](https://github.com/Orion-AI-Lab/S4A) dataset, supporting\n    multiple scenarios for splitting the data.\n\n    Args:\n        data_root (str): Root directory of the dataset.\n        bands (list of str, optional): List of band names to load. Defaults to all available bands.\n        scenario (str): Defines the splitting scenario to use. Options are:\n            - 'random': Random split of the data.\n            - 'spatial': Split by geographical regions (Catalonia and France).\n            - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020).\n        split (str): Specifies the dataset split. Options are 'train', 'val', or 'test'.\n        transform (albumentations.Compose, optional): Albumentations transformations to apply to the data.\n        truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image.\n            If None, no truncation is applied. Default is 4.\n        pad_image (int, optional): Number of timesteps to pad the time dimension of the image.\n            If None, no padding is applied. Default is 4.\n        spatial_interpolate_and_stack_temporally (bool): Whether to interpolate bands and concatenate them over time\n        seed (int): Random seed used for data splitting.\n    \"\"\"\n    self.data_root = Path(data_root) / \"data\"\n    self.transform = transform if transform else lambda **batch: to_tensor(batch)\n    self.scenario = scenario\n    self.seed = seed\n    self.truncate_image = truncate_image\n    self.pad_image = pad_image\n    self.spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally\n\n    if bands is None:\n        bands = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B09\", \"B10\", \"B11\", \"B12\", \"B8A\"]\n    self.bands = bands\n\n    self.image_files = list(self.data_root.glob(\"**/*.nc\"))\n\n    self.train_files, self.val_files, self.test_files = self.split_data()\n\n    if split == \"train\":\n        self.image_files = self.train_files\n    elif split == \"val\":\n        self.image_files = self.val_files\n    elif split == \"test\":\n        self.image_files = self.test_files\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.sen4map","title":"<code>terratorch.datasets.sen4map</code>","text":""},{"location":"package/datasets/#terratorch.datasets.sen4map.Sen4MapDatasetMonthlyComposites","title":"<code>Sen4MapDatasetMonthlyComposites</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Sen4Map Dataset for Monthly Composites.</p> <p>Dataset intended for land-cover and crop classification tasks based on monthly composites derived from multi-temporal satellite data stored in HDF5 files.</p> <p>Dataset Format:</p> <ul> <li>HDF5 files containing multi-temporal acquisitions with spectral bands (e.g., B2, B3, \u2026, B12)</li> <li>Composite images computed as the median across available acquisitions for each month.</li> <li>Classification labels provided via HDF5 attributes (e.g., 'lc1') with mappings defined for:<ul> <li>Land-cover: using <code>land_cover_classification_map</code></li> <li>Crops: using <code>crop_classification_map</code></li> </ul> </li> </ul> <p>Dataset Features:</p> <ul> <li>Supports two classification tasks: \"land-cover\" (default) and \"crops\".</li> <li>Pre-processing options include center cropping, reverse tiling, and resizing.</li> <li>Option to save the keys HDF5 for later filtering.</li> <li>Input channel selection via a mapping between available bands and input bands.</li> </ul> Source code in <code>terratorch/datasets/sen4map.py</code> <pre><code>class Sen4MapDatasetMonthlyComposites(Dataset):\n    \"\"\"[Sen4Map](https://gitlab.jsc.fz-juelich.de/sdlrs/sen4map-benchmark-dataset) Dataset for Monthly Composites.\n\n    Dataset intended for land-cover and crop classification tasks based on monthly composites\n    derived from multi-temporal satellite data stored in HDF5 files.\n\n    Dataset Format:\n\n    * HDF5 files containing multi-temporal acquisitions with spectral bands (e.g., B2, B3, \u2026, B12)\n    * Composite images computed as the median across available acquisitions for each month.\n    * Classification labels provided via HDF5 attributes (e.g., 'lc1') with mappings defined for:\n        - Land-cover: using `land_cover_classification_map`\n        - Crops: using `crop_classification_map`\n\n    Dataset Features:\n\n    * Supports two classification tasks: \"land-cover\" (default) and \"crops\".\n    * Pre-processing options include center cropping, reverse tiling, and resizing.\n    * Option to save the keys HDF5 for later filtering.\n    * Input channel selection via a mapping between available bands and input bands.\n\n\n    \"\"\"\n    land_cover_classification_map={'A10':0, 'A11':0, 'A12':0, 'A13':0, \n    'A20':0, 'A21':0, 'A30':0, \n    'A22':1, 'F10':1, 'F20':1, \n    'F30':1, 'F40':1,\n    'E10':2, 'E20':2, 'E30':2, 'B50':2, 'B51':2, 'B52':2,\n    'B53':2, 'B54':2, 'B55':2,\n    'B10':3, 'B11':3, 'B12':3, 'B13':3, 'B14':3, 'B15':3,\n    'B16':3, 'B17':3, 'B18':3, 'B19':3, 'B10':3, 'B20':3, \n    'B21':3, 'B22':3, 'B23':3, 'B30':3, 'B31':3, 'B32':3,\n    'B33':3, 'B34':3, 'B35':3, 'B30':3, 'B36':3, 'B37':3,\n    'B40':3, 'B41':3, 'B42':3, 'B43':3, 'B44':3, 'B45':3,\n    'B70':3, 'B71':3, 'B72':3, 'B73':3, 'B74':3, 'B75':3,\n    'B76':3, 'B77':3, 'B80':3, 'B81':3, 'B82':3, 'B83':3,\n    'B84':3, \n    'BX1':3, 'BX2':3,\n    'C10':4, 'C20':5, 'C21':5, 'C22':5,\n    'C23':5, 'C30':5, 'C31':5, 'C32':5,\n    'C33':5, \n    'CXX1':5, 'CXX2':5, 'CXX3':5, 'CXX4':5, 'CXX5':5,\n    'CXX5':5, 'CXX6':5, 'CXX7':5, 'CXX8':5, 'CXX9':5,\n    'CXXA':5, 'CXXB':5, 'CXXC':5, 'CXXD':5, 'CXXE':5,\n    'D10':6, 'D20':6, 'D10':6,\n    'G10':7, 'G11':7, 'G12':7, 'G20':7, 'G21':7, 'G22':7, 'G30':7, \n    'G40':7,\n    'G50':7,\n    'H10':8, 'H11':8, 'H12':8, 'H11':8,'H20':8, 'H21':8,\n    'H22':8, 'H23':8, '': 9}\n    #  This dictionary maps the LUCAS classes to crop classes.\n    crop_classification_map = {\n        \"B11\":0, \"B12\":0, \"B13\":0, \"B14\":0, \"B15\":0, \"B16\":0, \"B17\":0, \"B18\":0, \"B19\":0,  # Cereals\n        \"B21\":1, \"B22\":1, \"B23\":1,  # Root Crops\n        \"B31\":2, \"B32\":2, \"B33\":2, \"B34\":2, \"B35\":2, \"B36\":2, \"B37\":2,  # Nonpermanent Industrial Crops\n        \"B41\":3, \"B42\":3, \"B43\":3, \"B44\":3, \"B45\":3,  # Dry Pulses, Vegetables and Flowers\n        \"B51\":4, \"B52\":4, \"B53\":4, \"B54\":4,  # Fodder Crops\n        \"F10\":5, \"F20\":5, \"F30\":5, \"F40\":5,  # Bareland\n        \"B71\":6, \"B72\":6, \"B73\":6, \"B74\":6, \"B75\":6, \"B76\":6, \"B77\":6, \n        \"B81\":6, \"B82\":6, \"B83\":6, \"B84\":6, \"C10\":6, \"C21\":6, \"C22\":6, \"C23\":6, \"C31\":6, \"C32\":6, \"C33\":6, \"D10\":6, \"D20\":6,  # Woodland and Shrubland\n        \"B55\":7, \"E10\":7, \"E20\":7, \"E30\":7,  # Grassland\n    }\n\n    def __init__(\n            self,\n            h5py_file_object:h5py.File,\n            h5data_keys = None,\n            crop_size:None|int = None,\n            dataset_bands:list[HLSBands|int]|None = None,\n            input_bands:list[HLSBands|int]|None = None,\n            resize = False,\n            resize_to = [224, 224],\n            resize_interpolation = InterpolationMode.BILINEAR,\n            resize_antialiasing = True,\n            reverse_tile = False,\n            reverse_tile_size = 3,\n            save_keys_path = None,\n            classification_map = \"land-cover\"\n            ):\n        \"\"\"Initialize a new instance of Sen4MapDatasetMonthlyComposites.\n\n        This dataset loads data from an HDF5 file object containing multi-temporal satellite data and computes\n        monthly composite images by aggregating acquisitions (via median).\n\n        Args:\n            h5py_file_object: An open h5py.File object containing the dataset.\n            h5data_keys: Optional list of keys to select a subset of data samples from the HDF5 file.\n                If None, all keys are used.\n            crop_size: Optional integer specifying the square crop size for the output image.\n            dataset_bands: Optional list of bands available in the dataset.\n            input_bands: Optional list of bands to be used as input channels.\n                Must be provided along with `dataset_bands`.\n            resize: Boolean flag indicating whether the image should be resized. Default is False.\n            resize_to: Target dimensions [height, width] for resizing. Default is [224, 224].\n            resize_interpolation: Interpolation mode used for resizing. Default is InterpolationMode.BILINEAR.\n            resize_antialiasing: Boolean flag to apply antialiasing during resizing. Default is True.\n            reverse_tile: Boolean flag indicating whether to apply reverse tiling to the image. Default is False.\n            reverse_tile_size: Kernel size for the reverse tiling operation. Must be an odd number &gt;= 3. Default is 3.\n            save_keys_path: Optional file path to save the list of dataset keys.\n            classification_map: String specifying the classification mapping to use (\"land-cover\" or \"crops\").\n                Default is \"land-cover\".\n\n        Raises:\n            ValueError: If `input_bands` is provided without specifying `dataset_bands`.\n            ValueError: If an invalid `classification_map` is provided.\n        \"\"\"\n        self.h5data = h5py_file_object\n        if h5data_keys is None:\n            if classification_map == \"crops\": print(f\"Crop classification task chosen but no keys supplied. Will fail unless dataset hdf5 files have been filtered. Either filter dataset files or create a filtered set of keys.\")\n            self.h5data_keys = list(self.h5data.keys())\n            if save_keys_path is not None:\n                with open(save_keys_path, \"wb\") as file:\n                    pickle.dump(self.h5data_keys, file)\n        else:\n            self.h5data_keys = h5data_keys\n        self.crop_size = crop_size\n        if input_bands and not dataset_bands:\n            raise ValueError(f\"input_bands was provided without specifying the dataset_bands\")\n        # self.dataset_bands = dataset_bands\n        # self.input_bands = input_bands\n        if input_bands and dataset_bands:\n            self.input_channels = [dataset_bands.index(band_ind) for band_ind in input_bands if band_ind in dataset_bands]\n        else: self.input_channels = None\n\n        classification_maps = {\"land-cover\": Sen4MapDatasetMonthlyComposites.land_cover_classification_map,\n                               \"crops\": Sen4MapDatasetMonthlyComposites.crop_classification_map}\n        if classification_map not in classification_maps.keys():\n            raise ValueError(f\"Provided classification_map of: {classification_map}, is not from the list of valid ones: {classification_maps}\")\n        self.classification_map = classification_maps[classification_map]\n\n        self.resize = resize\n        self.resize_to = resize_to\n        self.resize_interpolation = resize_interpolation\n        self.resize_antialiasing = resize_antialiasing\n\n        self.reverse_tile = reverse_tile\n        self.reverse_tile_size = reverse_tile_size\n\n    def __getitem__(self, index):\n        # we can call dataset with an index, eg. dataset[0]\n        im = self.h5data[self.h5data_keys[index]]\n        Image, Label = self.get_data(im)\n        Image = self.min_max_normalize(Image, [67.0, 122.0, 93.27, 158.5, 160.77, 174.27, 162.27, 149.0, 84.5, 66.27 ],\n                                    [2089.0, 2598.45, 3214.5, 3620.45, 4033.61, 4613.0, 4825.45, 4945.72, 5140.84, 4414.45])\n\n        Image = Image.clip(0,1)\n        Label = torch.LongTensor(Label)\n        if self.input_channels:\n            Image = Image[self.input_channels, ...]\n\n        return {\"image\":Image, \"label\":Label}\n\n    def __len__(self):\n        return len(self.h5data_keys)\n\n    def get_data(self, im):\n        mask = im['SCL'] &lt; 9\n\n        B2= np.where(mask==1, im['B2'], 0)\n        B3= np.where(mask==1, im['B3'], 0)\n        B4= np.where(mask==1, im['B4'], 0)\n        B5= np.where(mask==1, im['B5'], 0)\n        B6= np.where(mask==1, im['B6'], 0)\n        B7= np.where(mask==1, im['B7'], 0)\n        B8= np.where(mask==1, im['B8'], 0)\n        B8A= np.where(mask==1, im['B8A'], 0)\n        B11= np.where(mask==1, im['B11'], 0)\n        B12= np.where(mask==1, im['B12'], 0)\n        Image = np.stack((B2,B3,B4,B5,B6,B7,B8,B8A,B11,B12), axis=0, dtype=\"float32\")\n        Image = np.moveaxis(Image, [0],[1])\n        Image = torch.from_numpy(Image)\n\n        # Composites:\n        n1= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201801' in s]\n        n2= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201802' in s]\n        n3= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201803' in s]\n        n4= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201804' in s]\n        n5= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201805' in s]\n        n6= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201806' in s]\n        n7= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201807' in s]\n        n8= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201808' in s]\n        n9= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201809' in s]\n        n10= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201810' in s]\n        n11= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201811' in s]\n        n12= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201812' in s]\n\n\n        Jan= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n1 else n1\n        Feb= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n2 else n2\n        Mar= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n3 else n3\n        Apr= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n4 else n4\n        May= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n5 else n5\n        Jun= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n6 else n6\n        Jul= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n7 else n7\n        Aug= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n8 else n8\n        Sep= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n9 else n9\n        Oct= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n10 else n10\n        Nov= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n11 else n11\n        Dec= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n12 else n12\n\n        month_indices = [Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec]\n\n        month_medians = [torch.stack([Image[month_indices[i][j]] for j in range(len(month_indices[i]))]).median(dim=0).values for i in range(12)]\n\n\n        Image = torch.stack(month_medians, dim=0)\n        Image = torch.moveaxis(Image, 0, 1)\n\n        if self.crop_size: Image = self.crop_center(Image, self.crop_size, self.crop_size)\n        if self.reverse_tile:\n            Image = self.reverse_tiling_pytorch(Image, kernel_size=self.reverse_tile_size)\n        if self.resize:\n            Image = resize(Image, size=self.resize_to, interpolation=self.resize_interpolation, antialias=self.resize_antialiasing)\n\n        Label = im.attrs['lc1']\n        Label = self.classification_map[Label]\n        Label = np.array(Label)\n        Label = Label.astype('float32')\n\n        return Image, Label\n\n    def crop_center(self, img_b:torch.Tensor, cropx, cropy) -&gt; torch.Tensor:\n        c, t, y, x = img_b.shape\n        startx = x//2-(cropx//2)\n        starty = y//2-(cropy//2)    \n        return img_b[0:c, 0:t, starty:starty+cropy, startx:startx+cropx]\n\n\n    def reverse_tiling_pytorch(self, img_tensor: torch.Tensor, kernel_size: int=3):\n        \"\"\"\n        Upscales an image where every pixel is expanded into `kernel_size`*`kernel_size` pixels.\n        Used to test whether the benefit of resizing images to the pre-trained size comes from the bilnearly interpolated pixels,\n        or if the same would be realized with no interpolated pixels.\n        \"\"\"\n        assert kernel_size % 2 == 1\n        assert kernel_size &gt;= 3\n        padding = (kernel_size - 1) // 2\n        # img_tensor shape: (batch_size, channels, H, W)\n        batch_size, channels, H, W = img_tensor.shape\n        # Unfold: Extract 3x3 patches with padding of 1 to cover borders\n        img_tensor = F.pad(img_tensor, pad=(padding,padding,padding,padding), mode=\"replicate\")\n        patches = F.unfold(img_tensor, kernel_size=kernel_size, padding=0)  # Shape: (batch_size, channels*9, H*W)\n        # Reshape to organize the 9 values from each 3x3 neighborhood\n        patches = patches.view(batch_size, channels, kernel_size*kernel_size, H, W)  # Shape: (batch_size, channels, 9, H, W)\n        # Rearrange the patches into (batch_size, channels, 3, 3, H, W)\n        patches = patches.view(batch_size, channels, kernel_size, kernel_size, H, W)\n        # Permute to have the spatial dimensions first and unfold them\n        patches = patches.permute(0, 1, 4, 2, 5, 3)  # Shape: (batch_size, channels, H, 3, W, 3)\n        # Reshape to get the final expanded image of shape (batch_size, channels, H*3, W*3)\n        expanded_img = patches.reshape(batch_size, channels, H * kernel_size, W * kernel_size)\n        return expanded_img\n\n    def min_max_normalize(self, tensor:torch.Tensor, q_low:list[float], q_hi:list[float]) -&gt; torch.Tensor:\n        dtype = tensor.dtype\n        q_low = torch.as_tensor(q_low, dtype=dtype, device=tensor.device)\n        q_hi = torch.as_tensor(q_hi, dtype=dtype, device=tensor.device)\n        x = torch.tensor(-12.0)\n        y = torch.exp(x)\n        tensor.sub_(q_low[:, None, None, None]).div_((q_hi[:, None, None, None].sub_(q_low[:, None, None, None])).add(y))\n        return tensor\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.sen4map.Sen4MapDatasetMonthlyComposites.__init__","title":"<code>__init__(h5py_file_object, h5data_keys=None, crop_size=None, dataset_bands=None, input_bands=None, resize=False, resize_to=[224, 224], resize_interpolation=InterpolationMode.BILINEAR, resize_antialiasing=True, reverse_tile=False, reverse_tile_size=3, save_keys_path=None, classification_map='land-cover')</code>","text":"<p>Initialize a new instance of Sen4MapDatasetMonthlyComposites.</p> <p>This dataset loads data from an HDF5 file object containing multi-temporal satellite data and computes monthly composite images by aggregating acquisitions (via median).</p> <p>Parameters:</p> Name Type Description Default <code>h5py_file_object</code> <code>File</code> <p>An open h5py.File object containing the dataset.</p> required <code>h5data_keys</code> <p>Optional list of keys to select a subset of data samples from the HDF5 file. If None, all keys are used.</p> <code>None</code> <code>crop_size</code> <code>None | int</code> <p>Optional integer specifying the square crop size for the output image.</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Optional list of bands available in the dataset.</p> <code>None</code> <code>input_bands</code> <code>list[HLSBands | int] | None</code> <p>Optional list of bands to be used as input channels. Must be provided along with <code>dataset_bands</code>.</p> <code>None</code> <code>resize</code> <p>Boolean flag indicating whether the image should be resized. Default is False.</p> <code>False</code> <code>resize_to</code> <p>Target dimensions [height, width] for resizing. Default is [224, 224].</p> <code>[224, 224]</code> <code>resize_interpolation</code> <p>Interpolation mode used for resizing. Default is InterpolationMode.BILINEAR.</p> <code>BILINEAR</code> <code>resize_antialiasing</code> <p>Boolean flag to apply antialiasing during resizing. Default is True.</p> <code>True</code> <code>reverse_tile</code> <p>Boolean flag indicating whether to apply reverse tiling to the image. Default is False.</p> <code>False</code> <code>reverse_tile_size</code> <p>Kernel size for the reverse tiling operation. Must be an odd number &gt;= 3. Default is 3.</p> <code>3</code> <code>save_keys_path</code> <p>Optional file path to save the list of dataset keys.</p> <code>None</code> <code>classification_map</code> <p>String specifying the classification mapping to use (\"land-cover\" or \"crops\"). Default is \"land-cover\".</p> <code>'land-cover'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>input_bands</code> is provided without specifying <code>dataset_bands</code>.</p> <code>ValueError</code> <p>If an invalid <code>classification_map</code> is provided.</p> Source code in <code>terratorch/datasets/sen4map.py</code> <pre><code>def __init__(\n        self,\n        h5py_file_object:h5py.File,\n        h5data_keys = None,\n        crop_size:None|int = None,\n        dataset_bands:list[HLSBands|int]|None = None,\n        input_bands:list[HLSBands|int]|None = None,\n        resize = False,\n        resize_to = [224, 224],\n        resize_interpolation = InterpolationMode.BILINEAR,\n        resize_antialiasing = True,\n        reverse_tile = False,\n        reverse_tile_size = 3,\n        save_keys_path = None,\n        classification_map = \"land-cover\"\n        ):\n    \"\"\"Initialize a new instance of Sen4MapDatasetMonthlyComposites.\n\n    This dataset loads data from an HDF5 file object containing multi-temporal satellite data and computes\n    monthly composite images by aggregating acquisitions (via median).\n\n    Args:\n        h5py_file_object: An open h5py.File object containing the dataset.\n        h5data_keys: Optional list of keys to select a subset of data samples from the HDF5 file.\n            If None, all keys are used.\n        crop_size: Optional integer specifying the square crop size for the output image.\n        dataset_bands: Optional list of bands available in the dataset.\n        input_bands: Optional list of bands to be used as input channels.\n            Must be provided along with `dataset_bands`.\n        resize: Boolean flag indicating whether the image should be resized. Default is False.\n        resize_to: Target dimensions [height, width] for resizing. Default is [224, 224].\n        resize_interpolation: Interpolation mode used for resizing. Default is InterpolationMode.BILINEAR.\n        resize_antialiasing: Boolean flag to apply antialiasing during resizing. Default is True.\n        reverse_tile: Boolean flag indicating whether to apply reverse tiling to the image. Default is False.\n        reverse_tile_size: Kernel size for the reverse tiling operation. Must be an odd number &gt;= 3. Default is 3.\n        save_keys_path: Optional file path to save the list of dataset keys.\n        classification_map: String specifying the classification mapping to use (\"land-cover\" or \"crops\").\n            Default is \"land-cover\".\n\n    Raises:\n        ValueError: If `input_bands` is provided without specifying `dataset_bands`.\n        ValueError: If an invalid `classification_map` is provided.\n    \"\"\"\n    self.h5data = h5py_file_object\n    if h5data_keys is None:\n        if classification_map == \"crops\": print(f\"Crop classification task chosen but no keys supplied. Will fail unless dataset hdf5 files have been filtered. Either filter dataset files or create a filtered set of keys.\")\n        self.h5data_keys = list(self.h5data.keys())\n        if save_keys_path is not None:\n            with open(save_keys_path, \"wb\") as file:\n                pickle.dump(self.h5data_keys, file)\n    else:\n        self.h5data_keys = h5data_keys\n    self.crop_size = crop_size\n    if input_bands and not dataset_bands:\n        raise ValueError(f\"input_bands was provided without specifying the dataset_bands\")\n    # self.dataset_bands = dataset_bands\n    # self.input_bands = input_bands\n    if input_bands and dataset_bands:\n        self.input_channels = [dataset_bands.index(band_ind) for band_ind in input_bands if band_ind in dataset_bands]\n    else: self.input_channels = None\n\n    classification_maps = {\"land-cover\": Sen4MapDatasetMonthlyComposites.land_cover_classification_map,\n                           \"crops\": Sen4MapDatasetMonthlyComposites.crop_classification_map}\n    if classification_map not in classification_maps.keys():\n        raise ValueError(f\"Provided classification_map of: {classification_map}, is not from the list of valid ones: {classification_maps}\")\n    self.classification_map = classification_maps[classification_map]\n\n    self.resize = resize\n    self.resize_to = resize_to\n    self.resize_interpolation = resize_interpolation\n    self.resize_antialiasing = resize_antialiasing\n\n    self.reverse_tile = reverse_tile\n    self.reverse_tile_size = reverse_tile_size\n</code></pre>"},{"location":"package/datasets/#terratorch.datasets.sen4map.Sen4MapDatasetMonthlyComposites.reverse_tiling_pytorch","title":"<code>reverse_tiling_pytorch(img_tensor, kernel_size=3)</code>","text":"<p>Upscales an image where every pixel is expanded into <code>kernel_size</code>*<code>kernel_size</code> pixels. Used to test whether the benefit of resizing images to the pre-trained size comes from the bilnearly interpolated pixels, or if the same would be realized with no interpolated pixels.</p> Source code in <code>terratorch/datasets/sen4map.py</code> <pre><code>def reverse_tiling_pytorch(self, img_tensor: torch.Tensor, kernel_size: int=3):\n    \"\"\"\n    Upscales an image where every pixel is expanded into `kernel_size`*`kernel_size` pixels.\n    Used to test whether the benefit of resizing images to the pre-trained size comes from the bilnearly interpolated pixels,\n    or if the same would be realized with no interpolated pixels.\n    \"\"\"\n    assert kernel_size % 2 == 1\n    assert kernel_size &gt;= 3\n    padding = (kernel_size - 1) // 2\n    # img_tensor shape: (batch_size, channels, H, W)\n    batch_size, channels, H, W = img_tensor.shape\n    # Unfold: Extract 3x3 patches with padding of 1 to cover borders\n    img_tensor = F.pad(img_tensor, pad=(padding,padding,padding,padding), mode=\"replicate\")\n    patches = F.unfold(img_tensor, kernel_size=kernel_size, padding=0)  # Shape: (batch_size, channels*9, H*W)\n    # Reshape to organize the 9 values from each 3x3 neighborhood\n    patches = patches.view(batch_size, channels, kernel_size*kernel_size, H, W)  # Shape: (batch_size, channels, 9, H, W)\n    # Rearrange the patches into (batch_size, channels, 3, 3, H, W)\n    patches = patches.view(batch_size, channels, kernel_size, kernel_size, H, W)\n    # Permute to have the spatial dimensions first and unfold them\n    patches = patches.permute(0, 1, 4, 2, 5, 3)  # Shape: (batch_size, channels, H, 3, W, 3)\n    # Reshape to get the final expanded image of shape (batch_size, channels, H*3, W*3)\n    expanded_img = patches.reshape(batch_size, channels, H * kernel_size, W * kernel_size)\n    return expanded_img\n</code></pre>"},{"location":"package/decoders/","title":"Decoder","text":"<p>Tip</p> <p>The <code>IdentityDecoder</code> is typically used for classification tasks while the <code>UNetDecoder</code> is suited for pixel-wise segmentation or regression tasks.</p>"},{"location":"package/decoders/#terratorch.models.decoders.identity_decoder.IdentityDecoder","title":"<code>terratorch.models.decoders.identity_decoder.IdentityDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Identity decoder. Useful to pass the feature straight to the head.</p> Source code in <code>terratorch/models/decoders/identity_decoder.py</code> <pre><code>@TERRATORCH_DECODER_REGISTRY.register\nclass IdentityDecoder(nn.Module):\n    \"\"\"Identity decoder. Useful to pass the feature straight to the head.\"\"\"\n\n    def __init__(self, embed_dim: int, out_index=-1) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            embed_dim (int): Input embedding dimension\n            out_index (int, optional): Index of the input list to take.. Defaults to -1.\n        \"\"\"\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.dim = out_index\n\n    @property\n    def out_channels(self):\n        return self.embed_dim[self.dim]\n\n    def forward(self, x: list[Tensor]):\n        return x[self.dim]\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.identity_decoder.IdentityDecoder.__init__","title":"<code>__init__(embed_dim, out_index=-1)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Input embedding dimension</p> required <code>out_index</code> <code>int</code> <p>Index of the input list to take.. Defaults to -1.</p> <code>-1</code> Source code in <code>terratorch/models/decoders/identity_decoder.py</code> <pre><code>def __init__(self, embed_dim: int, out_index=-1) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        embed_dim (int): Input embedding dimension\n        out_index (int, optional): Index of the input list to take.. Defaults to -1.\n    \"\"\"\n    super().__init__()\n    self.embed_dim = embed_dim\n    self.dim = out_index\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.unet_decoder.UNetDecoder","title":"<code>terratorch.models.decoders.unet_decoder.UNetDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>UNetDecoder. Wrapper around UNetDecoder from segmentation_models_pytorch to avoid ignoring the first layer.</p> Source code in <code>terratorch/models/decoders/unet_decoder.py</code> <pre><code>@TERRATORCH_DECODER_REGISTRY.register\nclass UNetDecoder(nn.Module):\n    \"\"\"UNetDecoder. Wrapper around UNetDecoder from segmentation_models_pytorch to avoid ignoring the first layer.\"\"\"\n\n    def __init__(\n        self, embed_dim: list[int], channels: list[int], use_batchnorm: bool = True, attention_type: str | None = None\n    ):\n        \"\"\"Constructor\n\n        Args:\n            embed_dim (list[int]): Input embedding dimension for each input.\n            channels (list[int]): Channels used in the decoder.\n            use_batchnorm (bool, optional): Whether to use batchnorm. Defaults to True.\n            attention_type (str | None, optional): Attention type to use. Defaults to None\n        \"\"\"\n        if len(embed_dim) != len(channels):\n            msg = \"channels should have the same length as embed_dim\"\n            raise ValueError(msg)\n        super().__init__()\n        self.decoder = UnetDecoder(\n            encoder_channels=[embed_dim[0], *embed_dim],\n            decoder_channels=channels,\n            n_blocks=len(channels),\n            use_batchnorm=use_batchnorm,\n            center=False,\n            attention_type=attention_type,\n        )\n        initialize_decoder(self.decoder)\n        self.out_channels = channels[-1]\n\n    def forward(self, x: list[torch.Tensor]) -&gt; torch.Tensor:\n        # The first layer is ignored in the original UnetDecoder, so we need to duplicate the first layer\n        x = [x[0].clone(), *x]\n        return self.decoder(*x)\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.unet_decoder.UNetDecoder.__init__","title":"<code>__init__(embed_dim, channels, use_batchnorm=True, attention_type=None)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>list[int]</code> <p>Input embedding dimension for each input.</p> required <code>channels</code> <code>list[int]</code> <p>Channels used in the decoder.</p> required <code>use_batchnorm</code> <code>bool</code> <p>Whether to use batchnorm. Defaults to True.</p> <code>True</code> <code>attention_type</code> <code>str | None</code> <p>Attention type to use. Defaults to None</p> <code>None</code> Source code in <code>terratorch/models/decoders/unet_decoder.py</code> <pre><code>def __init__(\n    self, embed_dim: list[int], channels: list[int], use_batchnorm: bool = True, attention_type: str | None = None\n):\n    \"\"\"Constructor\n\n    Args:\n        embed_dim (list[int]): Input embedding dimension for each input.\n        channels (list[int]): Channels used in the decoder.\n        use_batchnorm (bool, optional): Whether to use batchnorm. Defaults to True.\n        attention_type (str | None, optional): Attention type to use. Defaults to None\n    \"\"\"\n    if len(embed_dim) != len(channels):\n        msg = \"channels should have the same length as embed_dim\"\n        raise ValueError(msg)\n    super().__init__()\n    self.decoder = UnetDecoder(\n        encoder_channels=[embed_dim[0], *embed_dim],\n        decoder_channels=channels,\n        n_blocks=len(channels),\n        use_batchnorm=use_batchnorm,\n        center=False,\n        attention_type=attention_type,\n    )\n    initialize_decoder(self.decoder)\n    self.out_channels = channels[-1]\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.upernet_decoder.UperNetDecoder","title":"<code>terratorch.models.decoders.upernet_decoder.UperNetDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>UperNetDecoder. Adapted from MMSegmentation.</p> Source code in <code>terratorch/models/decoders/upernet_decoder.py</code> <pre><code>@TERRATORCH_DECODER_REGISTRY.register\nclass UperNetDecoder(nn.Module):\n    \"\"\"UperNetDecoder. Adapted from MMSegmentation.\"\"\"\n\n    def __init__(\n        self,\n        embed_dim: list[int],\n        pool_scales: tuple[int] = (1, 2, 3, 6),\n        channels: int = 256,\n        align_corners: bool = True,  # noqa: FBT001, FBT002\n        scale_modules: bool = False,\n    ):\n        \"\"\"Constructor\n\n        Args:\n            embed_dim (list[int]): Input embedding dimension for each input.\n            pool_scales (tuple[int], optional): Pooling scales used in Pooling Pyramid\n                Module applied on the last feature. Default: (1, 2, 3, 6).\n            channels (int, optional): Channels used in the decoder. Defaults to 256.\n            align_corners (bool, optional): Wheter to align corners in rescaling. Defaults to True.\n            scale_modules (bool, optional): Whether to apply scale modules to the inputs. Needed for plain ViT.\n                Defaults to False.\n        \"\"\"\n        super().__init__()\n        if scale_modules:\n            # TODO: remove scale_modules before v1?\n            warnings.warn(\n                \"DeprecationWarning: scale_modules is deprecated and will be removed in future versions. \"\n                \"Use LearnedInterpolateToPyramidal neck instead.\",\n                stacklevel=1,\n            )\n\n        self.scale_modules = scale_modules\n        if scale_modules:\n            self.fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim[0],\n                                embed_dim[0] // 2, 2, 2),\n                nn.BatchNorm2d(embed_dim[0] // 2),\n                nn.GELU(),\n                nn.ConvTranspose2d(embed_dim[0] // 2,\n                                embed_dim[0] // 4, 2, 2))\n            self.fpn2 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim[1],\n                                embed_dim[1] // 2, 2, 2))\n            self.fpn3 = nn.Sequential(nn.Identity())\n            self.fpn4 = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2))\n            self.embed_dim = [embed_dim[0] // 4, embed_dim[1] // 2, embed_dim[2], embed_dim[3]]\n        else:\n            self.embed_dim = embed_dim\n\n        self.out_channels = channels\n        self.channels = channels\n        self.align_corners = align_corners\n        # PSP Module\n        self.psp_modules = PPM(\n            pool_scales,\n            self.embed_dim[-1],\n            self.channels,\n            align_corners=self.align_corners,\n        )\n        self.bottleneck = ConvModule(\n            self.embed_dim[-1] + len(pool_scales) * self.channels, self.channels, 3, padding=1, inplace=True\n        )\n        # FPN Module\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n        for embed_dim in self.embed_dim[:-1]:  # skip the top layer\n            l_conv = ConvModule(\n                embed_dim,\n                self.channels,\n                1,\n                inplace=False,\n            )\n            fpn_conv = ConvModule(\n                self.channels,\n                self.channels,\n                3,\n                padding=1,\n                inplace=False,\n            )\n            self.lateral_convs.append(l_conv)\n            self.fpn_convs.append(fpn_conv)\n\n        self.fpn_bottleneck = ConvModule(len(self.embed_dim) * self.channels, self.channels, 3, padding=1, inplace=True)\n\n    def psp_forward(self, inputs):\n        \"\"\"Forward function of PSP module.\"\"\"\n        x = inputs[-1]\n        psp_outs = [x]\n        psp_outs.extend(self.psp_modules(x))\n        psp_outs = torch.cat(psp_outs, dim=1)\n        output = self.bottleneck(psp_outs)\n\n        return output\n\n    def forward(self, inputs):\n        \"\"\"Forward function for feature maps before classifying each pixel with\n        Args:\n            inputs (list[Tensor]): List of multi-level img features.\n\n        Returns:\n            feats (Tensor): A tensor of shape (batch_size, self.channels,\n                H, W) which is feature map for last layer of decoder head.\n        \"\"\"\n\n        if self.scale_modules:\n            scaled_inputs = []\n            scaled_inputs.append(self.fpn1(inputs[0]))\n            scaled_inputs.append(self.fpn2(inputs[1]))\n            scaled_inputs.append(self.fpn3(inputs[2]))\n            scaled_inputs.append(self.fpn4(inputs[3]))\n            inputs = scaled_inputs\n        # build laterals\n        laterals = [lateral_conv(inputs[i]) for i, lateral_conv in enumerate(self.lateral_convs)]\n        laterals.append(self.psp_forward(inputs))\n\n        # build top-down path\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            prev_shape = laterals[i - 1].shape[2:]\n            laterals[i - 1] = laterals[i - 1] + torch.nn.functional.interpolate(\n                laterals[i], size=prev_shape, mode=\"bilinear\", align_corners=self.align_corners\n            )\n\n        # build outputs\n        fpn_outs = [self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels - 1)]\n        # append psp feature\n        fpn_outs.append(laterals[-1])\n\n        for i in range(used_backbone_levels - 1, 0, -1):\n            fpn_outs[i] = torch.nn.functional.interpolate(\n                fpn_outs[i], size=fpn_outs[0].shape[2:], mode=\"bilinear\", align_corners=self.align_corners\n            )\n        fpn_outs = torch.cat(fpn_outs, dim=1)\n        feats = self.fpn_bottleneck(fpn_outs)\n        return feats\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.upernet_decoder.UperNetDecoder.__init__","title":"<code>__init__(embed_dim, pool_scales=(1, 2, 3, 6), channels=256, align_corners=True, scale_modules=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>list[int]</code> <p>Input embedding dimension for each input.</p> required <code>pool_scales</code> <code>tuple[int]</code> <p>Pooling scales used in Pooling Pyramid Module applied on the last feature. Default: (1, 2, 3, 6).</p> <code>(1, 2, 3, 6)</code> <code>channels</code> <code>int</code> <p>Channels used in the decoder. Defaults to 256.</p> <code>256</code> <code>align_corners</code> <code>bool</code> <p>Wheter to align corners in rescaling. Defaults to True.</p> <code>True</code> <code>scale_modules</code> <code>bool</code> <p>Whether to apply scale modules to the inputs. Needed for plain ViT. Defaults to False.</p> <code>False</code> Source code in <code>terratorch/models/decoders/upernet_decoder.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: list[int],\n    pool_scales: tuple[int] = (1, 2, 3, 6),\n    channels: int = 256,\n    align_corners: bool = True,  # noqa: FBT001, FBT002\n    scale_modules: bool = False,\n):\n    \"\"\"Constructor\n\n    Args:\n        embed_dim (list[int]): Input embedding dimension for each input.\n        pool_scales (tuple[int], optional): Pooling scales used in Pooling Pyramid\n            Module applied on the last feature. Default: (1, 2, 3, 6).\n        channels (int, optional): Channels used in the decoder. Defaults to 256.\n        align_corners (bool, optional): Wheter to align corners in rescaling. Defaults to True.\n        scale_modules (bool, optional): Whether to apply scale modules to the inputs. Needed for plain ViT.\n            Defaults to False.\n    \"\"\"\n    super().__init__()\n    if scale_modules:\n        # TODO: remove scale_modules before v1?\n        warnings.warn(\n            \"DeprecationWarning: scale_modules is deprecated and will be removed in future versions. \"\n            \"Use LearnedInterpolateToPyramidal neck instead.\",\n            stacklevel=1,\n        )\n\n    self.scale_modules = scale_modules\n    if scale_modules:\n        self.fpn1 = nn.Sequential(\n            nn.ConvTranspose2d(embed_dim[0],\n                            embed_dim[0] // 2, 2, 2),\n            nn.BatchNorm2d(embed_dim[0] // 2),\n            nn.GELU(),\n            nn.ConvTranspose2d(embed_dim[0] // 2,\n                            embed_dim[0] // 4, 2, 2))\n        self.fpn2 = nn.Sequential(\n            nn.ConvTranspose2d(embed_dim[1],\n                            embed_dim[1] // 2, 2, 2))\n        self.fpn3 = nn.Sequential(nn.Identity())\n        self.fpn4 = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2))\n        self.embed_dim = [embed_dim[0] // 4, embed_dim[1] // 2, embed_dim[2], embed_dim[3]]\n    else:\n        self.embed_dim = embed_dim\n\n    self.out_channels = channels\n    self.channels = channels\n    self.align_corners = align_corners\n    # PSP Module\n    self.psp_modules = PPM(\n        pool_scales,\n        self.embed_dim[-1],\n        self.channels,\n        align_corners=self.align_corners,\n    )\n    self.bottleneck = ConvModule(\n        self.embed_dim[-1] + len(pool_scales) * self.channels, self.channels, 3, padding=1, inplace=True\n    )\n    # FPN Module\n    self.lateral_convs = nn.ModuleList()\n    self.fpn_convs = nn.ModuleList()\n    for embed_dim in self.embed_dim[:-1]:  # skip the top layer\n        l_conv = ConvModule(\n            embed_dim,\n            self.channels,\n            1,\n            inplace=False,\n        )\n        fpn_conv = ConvModule(\n            self.channels,\n            self.channels,\n            3,\n            padding=1,\n            inplace=False,\n        )\n        self.lateral_convs.append(l_conv)\n        self.fpn_convs.append(fpn_conv)\n\n    self.fpn_bottleneck = ConvModule(len(self.embed_dim) * self.channels, self.channels, 3, padding=1, inplace=True)\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.upernet_decoder.UperNetDecoder.forward","title":"<code>forward(inputs)</code>","text":"<p>Forward function for feature maps before classifying each pixel with Args:     inputs (list[Tensor]): List of multi-level img features.</p> <p>Returns:</p> Name Type Description <code>feats</code> <code>Tensor</code> <p>A tensor of shape (batch_size, self.channels, H, W) which is feature map for last layer of decoder head.</p> Source code in <code>terratorch/models/decoders/upernet_decoder.py</code> <pre><code>def forward(self, inputs):\n    \"\"\"Forward function for feature maps before classifying each pixel with\n    Args:\n        inputs (list[Tensor]): List of multi-level img features.\n\n    Returns:\n        feats (Tensor): A tensor of shape (batch_size, self.channels,\n            H, W) which is feature map for last layer of decoder head.\n    \"\"\"\n\n    if self.scale_modules:\n        scaled_inputs = []\n        scaled_inputs.append(self.fpn1(inputs[0]))\n        scaled_inputs.append(self.fpn2(inputs[1]))\n        scaled_inputs.append(self.fpn3(inputs[2]))\n        scaled_inputs.append(self.fpn4(inputs[3]))\n        inputs = scaled_inputs\n    # build laterals\n    laterals = [lateral_conv(inputs[i]) for i, lateral_conv in enumerate(self.lateral_convs)]\n    laterals.append(self.psp_forward(inputs))\n\n    # build top-down path\n    used_backbone_levels = len(laterals)\n    for i in range(used_backbone_levels - 1, 0, -1):\n        prev_shape = laterals[i - 1].shape[2:]\n        laterals[i - 1] = laterals[i - 1] + torch.nn.functional.interpolate(\n            laterals[i], size=prev_shape, mode=\"bilinear\", align_corners=self.align_corners\n        )\n\n    # build outputs\n    fpn_outs = [self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels - 1)]\n    # append psp feature\n    fpn_outs.append(laterals[-1])\n\n    for i in range(used_backbone_levels - 1, 0, -1):\n        fpn_outs[i] = torch.nn.functional.interpolate(\n            fpn_outs[i], size=fpn_outs[0].shape[2:], mode=\"bilinear\", align_corners=self.align_corners\n        )\n    fpn_outs = torch.cat(fpn_outs, dim=1)\n    feats = self.fpn_bottleneck(fpn_outs)\n    return feats\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.upernet_decoder.UperNetDecoder.psp_forward","title":"<code>psp_forward(inputs)</code>","text":"<p>Forward function of PSP module.</p> Source code in <code>terratorch/models/decoders/upernet_decoder.py</code> <pre><code>def psp_forward(self, inputs):\n    \"\"\"Forward function of PSP module.\"\"\"\n    x = inputs[-1]\n    psp_outs = [x]\n    psp_outs.extend(self.psp_modules(x))\n    psp_outs = torch.cat(psp_outs, dim=1)\n    output = self.bottleneck(psp_outs)\n\n    return output\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.fcn_decoder.FCNDecoder","title":"<code>terratorch.models.decoders.fcn_decoder.FCNDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Fully Convolutional Decoder</p> Source code in <code>terratorch/models/decoders/fcn_decoder.py</code> <pre><code>@TERRATORCH_DECODER_REGISTRY.register\nclass FCNDecoder(nn.Module):\n    \"\"\"Fully Convolutional Decoder\"\"\"\n\n    def __init__(self, embed_dim: int, channels: int = 256, num_convs: int = 4, in_index: int = -1) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            embed_dim (_type_): Input embedding dimension\n            channels (int, optional): Number of channels for each conv. Defaults to 256.\n            num_convs (int, optional): Number of convs. Defaults to 4.\n            in_index (int, optional): Index of the input list to take. Defaults to -1.\n        \"\"\"\n        super().__init__()\n        kernel_size = 2\n        stride = 2\n        dilation = 1\n        padding = 0\n        output_padding = 0\n        self.channels = channels\n        self.num_convs = num_convs\n        self.in_index = in_index\n        self.embed_dim = embed_dim[in_index]\n        if num_convs &lt; 1:\n            msg = \"num_convs must be &gt;= 1\"\n            raise Exception(msg)\n\n        convs = []\n\n        for i in range(num_convs):\n            in_channels = self.embed_dim if i == 0 else self.channels\n            convs.append(\n                _conv_upscale_block(in_channels, self.channels, kernel_size, stride, dilation, padding, output_padding)\n            )\n\n        self.convs = nn.Sequential(*convs)\n\n    @property\n    def out_channels(self):\n        return self.channels\n\n    def forward(self, x: list[Tensor]):\n        x = x[self.in_index]\n        decoded = self.convs(x)\n        return decoded\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.fcn_decoder.FCNDecoder.__init__","title":"<code>__init__(embed_dim, channels=256, num_convs=4, in_index=-1)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>_type_</code> <p>Input embedding dimension</p> required <code>channels</code> <code>int</code> <p>Number of channels for each conv. Defaults to 256.</p> <code>256</code> <code>num_convs</code> <code>int</code> <p>Number of convs. Defaults to 4.</p> <code>4</code> <code>in_index</code> <code>int</code> <p>Index of the input list to take. Defaults to -1.</p> <code>-1</code> Source code in <code>terratorch/models/decoders/fcn_decoder.py</code> <pre><code>def __init__(self, embed_dim: int, channels: int = 256, num_convs: int = 4, in_index: int = -1) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        embed_dim (_type_): Input embedding dimension\n        channels (int, optional): Number of channels for each conv. Defaults to 256.\n        num_convs (int, optional): Number of convs. Defaults to 4.\n        in_index (int, optional): Index of the input list to take. Defaults to -1.\n    \"\"\"\n    super().__init__()\n    kernel_size = 2\n    stride = 2\n    dilation = 1\n    padding = 0\n    output_padding = 0\n    self.channels = channels\n    self.num_convs = num_convs\n    self.in_index = in_index\n    self.embed_dim = embed_dim[in_index]\n    if num_convs &lt; 1:\n        msg = \"num_convs must be &gt;= 1\"\n        raise Exception(msg)\n\n    convs = []\n\n    for i in range(num_convs):\n        in_channels = self.embed_dim if i == 0 else self.channels\n        convs.append(\n            _conv_upscale_block(in_channels, self.channels, kernel_size, stride, dilation, padding, output_padding)\n        )\n\n    self.convs = nn.Sequential(*convs)\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.linear_decoder.LinearDecoder","title":"<code>terratorch.models.decoders.linear_decoder.LinearDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>A linear decoder using a transposed convolution layer for upsampling.</p> Source code in <code>terratorch/models/decoders/linear_decoder.py</code> <pre><code>@TERRATORCH_DECODER_REGISTRY.register\nclass LinearDecoder(nn.Module):\n    \"\"\"\n    A linear decoder using a transposed convolution layer for upsampling.\n    \"\"\"\n    includes_head: bool = True\n\n    def __init__(self, embed_dim: list[int], num_classes: int, upsampling_size: int, in_index: int = -1) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            embed_dim (list[int]): A list of embedding dimensions for different feature maps.\n            num_classes (int): Number of output classes.\n            upsampling_size (int): Kernel and stride size for transposed convolution.\n            in_index (int, optional): Index of the input feature map to use. Defaults to -1.\"\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.in_index = in_index\n        self.embed_dim = embed_dim[in_index]\n\n        self.conv = nn.ConvTranspose2d(\n            in_channels=self.embed_dim,\n            out_channels=self.num_classes,\n            kernel_size=upsampling_size,\n            stride=upsampling_size,\n            padding=0,\n            output_padding=0,\n        )\n\n    @property\n    def out_channels(self) -&gt; int:\n        return self.num_classes\n\n    def forward(self, x: list[Tensor]) -&gt; Tensor:\n        return self.conv(x[self.in_index])\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.linear_decoder.LinearDecoder.__init__","title":"<code>__init__(embed_dim, num_classes, upsampling_size, in_index=-1)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>list[int]</code> <p>A list of embedding dimensions for different feature maps.</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> required <code>upsampling_size</code> <code>int</code> <p>Kernel and stride size for transposed convolution.</p> required <code>in_index</code> <code>int</code> <p>Index of the input feature map to use. Defaults to -1.\"</p> <code>-1</code> Source code in <code>terratorch/models/decoders/linear_decoder.py</code> <pre><code>def __init__(self, embed_dim: list[int], num_classes: int, upsampling_size: int, in_index: int = -1) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        embed_dim (list[int]): A list of embedding dimensions for different feature maps.\n        num_classes (int): Number of output classes.\n        upsampling_size (int): Kernel and stride size for transposed convolution.\n        in_index (int, optional): Index of the input feature map to use. Defaults to -1.\"\n    \"\"\"\n    super().__init__()\n    self.num_classes = num_classes\n    self.in_index = in_index\n    self.embed_dim = embed_dim[in_index]\n\n    self.conv = nn.ConvTranspose2d(\n        in_channels=self.embed_dim,\n        out_channels=self.num_classes,\n        kernel_size=upsampling_size,\n        stride=upsampling_size,\n        padding=0,\n        output_padding=0,\n    )\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.mlp_decoder.MLPDecoder","title":"<code>terratorch.models.decoders.mlp_decoder.MLPDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Identity decoder. Useful to pass the feature straight to the head.</p> Source code in <code>terratorch/models/decoders/mlp_decoder.py</code> <pre><code>@TERRATORCH_DECODER_REGISTRY.register\nclass MLPDecoder(nn.Module):\n    \"\"\"Identity decoder. Useful to pass the feature straight to the head.\"\"\"\n\n    def __init__(self, embed_dim: int, channels: int = 100, out_dim:int = 100, activation: str = \"ReLU\", out_index=-1) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            embed_dim (int): Input embedding dimension\n            out_index (int, optional): Index of the input list to take.. Defaults to -1.\n        \"\"\"\n\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.channels = channels\n        self.dim = out_index\n        self.n_inputs = len(self.embed_dim)\n        self.out_channels = self.embed_dim[self.dim]\n        self.hidden_layer = torch.nn.Linear(self.out_channels*self.n_inputs, self.out_channels)\n        self.activation = getattr(nn, activation)()\n\n    def forward(self, x: list[Tensor]):\n\n        data_ = torch.cat(x, axis=1)\n        data_ = data_.permute(0, 2, 3, 1)\n        data_ = self.activation(self.hidden_layer(data_))\n        data_ = data_.permute(0, 3, 1, 2)\n\n        return data_ \n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.mlp_decoder.MLPDecoder.__init__","title":"<code>__init__(embed_dim, channels=100, out_dim=100, activation='ReLU', out_index=-1)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Input embedding dimension</p> required <code>out_index</code> <code>int</code> <p>Index of the input list to take.. Defaults to -1.</p> <code>-1</code> Source code in <code>terratorch/models/decoders/mlp_decoder.py</code> <pre><code>def __init__(self, embed_dim: int, channels: int = 100, out_dim:int = 100, activation: str = \"ReLU\", out_index=-1) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        embed_dim (int): Input embedding dimension\n        out_index (int, optional): Index of the input list to take.. Defaults to -1.\n    \"\"\"\n\n    super().__init__()\n    self.embed_dim = embed_dim\n    self.channels = channels\n    self.dim = out_index\n    self.n_inputs = len(self.embed_dim)\n    self.out_channels = self.embed_dim[self.dim]\n    self.hidden_layer = torch.nn.Linear(self.out_channels*self.n_inputs, self.out_channels)\n    self.activation = getattr(nn, activation)()\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.aspp_head","title":"<code>terratorch.models.decoders.aspp_head</code>","text":""},{"location":"package/decoders/#terratorch.models.decoders.aspp_head.ASPPHead","title":"<code>ASPPHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Rethinking Atrous Convolution for Semantic Image Segmentation.</p> <p>This head is the implementation of <code>DeepLabV3 &lt;https://arxiv.org/abs/1706.05587&gt;</code>_.</p> <p>Parameters:</p> Name Type Description Default <code>dilations</code> <code>tuple[int]</code> <p>Dilation rates for ASPP module. Default: (1, 6, 12, 18).</p> <code>(1, 6, 12, 18)</code> Source code in <code>terratorch/models/decoders/aspp_head.py</code> <pre><code>@TERRATORCH_DECODER_REGISTRY.register\nclass ASPPHead(nn.Module):\n    \"\"\"Rethinking Atrous Convolution for Semantic Image Segmentation.\n\n    This head is the implementation of `DeepLabV3\n    &lt;https://arxiv.org/abs/1706.05587&gt;`_.\n\n    Args:\n        dilations (tuple[int]): Dilation rates for ASPP module.\n            Default: (1, 6, 12, 18).\n    \"\"\"\n\n    def __init__(self, dilations:list | tuple =(1, 6, 12, 18), \n                 in_channels:int=None, \n                 channels:int=None,\n                 out_dim:int=3,\n                 align_corners=False,\n                 head_dropout_ratio:float=0.3,\n                 input_transform: str = None,\n                 in_index: int = -1,\n                 **kwargs):\n\n        super(ASPPHead, self).__init__(**kwargs)\n\n        self.dilations = dilations\n        self.in_channels = in_channels\n        self.channels = channels\n        self.out_dim = out_dim\n\n        self.align_corners = align_corners\n        self.input_transform = input_transform\n        self.in_index = in_index \n\n        if 'conv_cfg' not in kwargs:\n            self.conv_cfg = self._default_conv_cfg\n\n        if 'norm_cfg' not in kwargs:\n            self.norm_cfg = self._default_norm_cfg\n\n        if 'act_cfg' not in kwargs:\n            self.act_cfg = self._default_act_cfg\n\n        self.image_pool = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            ConvModule(\n                self.in_channels,\n                self.channels,\n                1,\n\n                # TODO Extend it to support more possible configurations\n                # for convolution, normalization and activation.\n                #self.conv_cfg,\n                #norm_cfg=self.norm_cfg,\n                #act_cfg=self.act_cfg))\n                ))\n\n        self.aspp_modules = ASPPModule(\n            dilations,\n            self.in_channels,\n            self.channels,\n            conv_cfg=self.conv_cfg,\n            norm_cfg=self.norm_cfg,\n            act_cfg=self.act_cfg)\n\n        self.bottleneck = ConvModule(\n            (len(dilations) + 1) * self.channels,\n            self.channels,\n            padding=1,)\n            # TODO Extend it to support more possible configurations\n            # for convolution, normalization and activation.\n            #conv_cfg=self.conv_cfg,\n            #norm_cfg=self.norm_cfg,\n            #act_cfg=self.act_cfg)\n\n        if head_dropout_ratio &gt; 0:\n            self.dropout = nn.Dropout2d(head_dropout_ratio)\n\n    @property\n    def _default_conv_cfg(self):\n        return {\"kernel_size\": 3, \"padding\": 0, \"bias\": False}\n\n    @property\n    def _default_norm_cfg(self):\n        return {}\n\n    @property\n    def _default_act_cfg(self):\n        return {}\n\n    def _transform_inputs(self, inputs):\n        \"\"\"Transform inputs for decoder.\n\n        Args:\n            inputs (list[Tensor]): List of multi-level img features.\n\n        Returns:\n            Tensor: The transformed inputs\n        \"\"\"\n\n        if self.input_transform == 'resize_concat':\n            inputs = [inputs[i] for i in self.in_index]\n            upsampled_inputs = [\n                resize(\n                    input=x,\n                    size=inputs[0].shape[2:],\n                    mode='bilinear',\n                    align_corners=self.align_corners) for x in inputs\n            ]\n            inputs = torch.cat(upsampled_inputs, dim=1)\n        elif self.input_transform == 'multiple_select':\n            inputs = [inputs[i] for i in self.in_index]\n        else:\n            inputs = inputs[self.in_index]\n\n        return inputs\n\n    def _forward_feature(self, inputs):\n        \"\"\"Forward function.\n\n        Args:\n            inputs (list[Tensor]): List of multi-level img features.\n\n        Returns:\n            feats (Tensor): A tensor of shape (batch_size, self.channels,\n                H, W) which is feature map for last layer of decoder head.\n        \"\"\"\n        inputs = self._transform_inputs(inputs)\n\n        aspp_outs = [\n            resize(\n                self.image_pool(inputs),\n                size=inputs.size()[2:],\n                mode='bilinear',\n                align_corners=self.align_corners)\n        ]\n\n        aspp_outs.extend(self.aspp_modules(inputs))\n        aspp_outs = torch.cat(aspp_outs, dim=1)\n        feats = self.bottleneck(aspp_outs)\n\n        return feats\n\n    def forward(self, inputs):\n\n        output = self._forward_feature(inputs)\n\n        return output\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.aspp_head.ASPPSegmentationHead","title":"<code>ASPPSegmentationHead</code>","text":"<p>               Bases: <code>ASPPHead</code></p> <p>Rethinking Atrous Convolution for Semantic Image Segmentation.</p> <p>This head is the implementation of <code>DeepLabV3 &lt;https://arxiv.org/abs/1706.05587&gt;</code>_.</p> <p>Parameters:</p> Name Type Description Default <code>dilations</code> <code>tuple[int]</code> <p>Dilation rates for ASPP module. Default: (1, 6, 12, 18).</p> <code>(1, 6, 12, 18)</code> Source code in <code>terratorch/models/decoders/aspp_head.py</code> <pre><code>@TERRATORCH_DECODER_REGISTRY.register\nclass ASPPSegmentationHead(ASPPHead):\n    \"\"\"Rethinking Atrous Convolution for Semantic Image Segmentation.\n\n    This head is the implementation of `DeepLabV3\n    &lt;https://arxiv.org/abs/1706.05587&gt;`_.\n\n    Args:\n        dilations (tuple[int]): Dilation rates for ASPP module.\n            Default: (1, 6, 12, 18).\n    \"\"\"\n\n    def __init__(self, channel_list,\n                 dilations:list | tuple =(1, 6, 12, 18), \n                 in_channels:int=None, \n                 channels:int=None,\n                 num_classes:int=2,\n                 align_corners=False,\n                 head_dropout_ratio:float=0.3,\n                 input_transform: str = None,\n                 in_index: int = -1,\n                 **kwargs):\n\n        super(ASPPSegmentationHead, self).__init__(\n                 dilations=dilations, \n                 in_channels=in_channels, \n                 channels=channels,\n                 align_corners=align_corners,\n                 head_dropout_ratio=head_dropout_ratio,\n                 input_transform=input_transform,\n                 in_index=in_index,\n                **kwargs)\n\n        self.num_classes = num_classes\n        self.conv_seg = nn.Conv2d(self.channels, self.num_classes, kernel_size=1)\n\n        if head_dropout_ratio &gt; 0:\n            self.dropout = nn.Dropout2d(head_dropout_ratio)\n\n    def segmentation_head(self, features):\n\n        \"\"\"PixelWise classification\"\"\"\n\n        if self.dropout is not None:\n            features = self.dropout(features)\n        output = self.conv_seg(features)\n        return output\n\n    def forward(self, inputs):\n\n        output = self._forward_feature(inputs)\n        output = self.segmentation_head(output)\n\n        return output\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.aspp_head.ASPPSegmentationHead.segmentation_head","title":"<code>segmentation_head(features)</code>","text":"<p>PixelWise classification</p> Source code in <code>terratorch/models/decoders/aspp_head.py</code> <pre><code>def segmentation_head(self, features):\n\n    \"\"\"PixelWise classification\"\"\"\n\n    if self.dropout is not None:\n        features = self.dropout(features)\n    output = self.conv_seg(features)\n    return output\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.aspp_head.ASPPRegressionHead","title":"<code>ASPPRegressionHead</code>","text":"<p>               Bases: <code>ASPPHead</code></p> <p>Rethinking Atrous Convolution for regression.</p> <p>This head is the implementation of <code>DeepLabV3 &lt;https://arxiv.org/abs/1706.05587&gt;</code>_.</p> <p>Parameters:</p> Name Type Description Default <code>dilations</code> <code>tuple[int]</code> <p>Dilation rates for ASPP module. Default: (1, 6, 12, 18).</p> <code>(1, 6, 12, 18)</code> Source code in <code>terratorch/models/decoders/aspp_head.py</code> <pre><code>@TERRATORCH_DECODER_REGISTRY.register\nclass ASPPRegressionHead(ASPPHead):\n    \"\"\"Rethinking Atrous Convolution for regression.\n\n    This head is the implementation of `DeepLabV3\n    &lt;https://arxiv.org/abs/1706.05587&gt;`_.\n\n    Args:\n        dilations (tuple[int]): Dilation rates for ASPP module.\n            Default: (1, 6, 12, 18).\n    \"\"\"\n\n    def __init__(self, channel_list,\n                 dilations:list | tuple =(1, 6, 12, 18), \n                 in_channels:int=None, \n                 channels:int=None,\n                 out_channels:int=1,\n                 align_corners=False,\n                 head_dropout_ratio:float=0.3,\n                 input_transform: str = None,\n                 in_index: int = -1,\n                 **kwargs):\n\n        super(ASPPRegressionHead, self).__init__(\n                 dilations=dilations, \n                 in_channels=in_channels, \n                 channels=channels,\n                 out_dim=out_channels,\n                 align_corners=align_corners,\n                 head_dropout_ratio=head_dropout_ratio,\n                 input_transform=input_transform,\n                 in_index=in_index,\n                **kwargs)\n\n        self.out_channels = out_channels\n        self.conv_reg = nn.Conv2d(self.channels, self.out_channels, kernel_size=1)\n\n    def regression_head(self, features):\n\n        \"\"\"PixelWise regression\"\"\"\n        if self.dropout is not None:\n            features = self.dropout(features)\n        output = self.conv_reg(features)\n        return output\n\n\n    def forward(self, inputs):\n\n        output = self._forward_feature(inputs)\n        output = self.regression_head(output)\n\n        return output\n</code></pre>"},{"location":"package/decoders/#terratorch.models.decoders.aspp_head.ASPPRegressionHead.regression_head","title":"<code>regression_head(features)</code>","text":"<p>PixelWise regression</p> Source code in <code>terratorch/models/decoders/aspp_head.py</code> <pre><code>def regression_head(self, features):\n\n    \"\"\"PixelWise regression\"\"\"\n    if self.dropout is not None:\n        features = self.dropout(features)\n    output = self.conv_reg(features)\n    return output\n</code></pre>"},{"location":"package/extra_model_structures/","title":"Extra Model Structures","text":""},{"location":"package/extra_model_structures/#terratorch.models.model.Model","title":"<code>terratorch.models.model.Model</code>","text":"<p>               Bases: <code>ABC</code>, <code>Module</code></p> Source code in <code>terratorch/models/model.py</code> <pre><code>class Model(ABC, nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n    @abstractmethod\n    def freeze_encoder(self):\n        pass\n\n    @abstractmethod\n    def freeze_decoder(self):\n        pass\n\n    @abstractmethod\n    def forward(self, *args, **kwargs) -&gt; ModelOutput:\n        pass\n</code></pre>"},{"location":"package/extra_model_structures/#terratorch.models.model.AuxiliaryHead","title":"<code>terratorch.models.model.AuxiliaryHead</code>  <code>dataclass</code>","text":"<p>Class containing all information to create auxiliary heads.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the head. Should match the name given to the auxiliary loss.</p> required <code>decoder</code> <code>str</code> <p>Name of the decoder class to be used.</p> required <code>decoder_args</code> <code>dict | None</code> <p>parameters to be passed to the decoder constructor. Parameters for the decoder should be prefixed with <code>decoder_</code>. Parameters for the head should be prefixed with <code>head_</code>.</p> required Source code in <code>terratorch/models/model.py</code> <pre><code>@dataclass\nclass AuxiliaryHead:\n    \"\"\"Class containing all information to create auxiliary heads.\n\n    Args:\n        name (str): Name of the head. Should match the name given to the auxiliary loss.\n        decoder (str): Name of the decoder class to be used.\n        decoder_args (dict | None): parameters to be passed to the decoder constructor.\n            Parameters for the decoder should be prefixed with `decoder_`.\n            Parameters for the head should be prefixed with `head_`.\n    \"\"\"\n\n    name: str\n    decoder: str\n    decoder_args: dict | None\n</code></pre>"},{"location":"package/extra_model_structures/#terratorch.models.model.ModelOutput","title":"<code>terratorch.models.model.ModelOutput</code>  <code>dataclass</code>","text":"Source code in <code>terratorch/models/model.py</code> <pre><code>@dataclass\nclass ModelOutput:\n    output: Tensor\n    auxiliary_heads: dict[str, Tensor] = None\n</code></pre>"},{"location":"package/extra_model_structures/#terratorch.registry.registry.MultiSourceRegistry","title":"<code>terratorch.registry.registry.MultiSourceRegistry</code>","text":"<p>               Bases: <code>Mapping[str, T]</code>, <code>Generic[T]</code></p> <p>Registry that searches in multiple sources</p> <p>Correct functioning of this class depends on registries raising a KeyError when the model is not found.</p> Source code in <code>terratorch/registry/registry.py</code> <pre><code>class MultiSourceRegistry(Mapping[str, T], typing.Generic[T]):\n    \"\"\"Registry that searches in multiple sources\n\n        Correct functioning of this class depends on registries raising a KeyError when the model is not found.\n    \"\"\"\n    def __init__(self, **sources) -&gt; None:\n        self._sources: OrderedDict[str, T] = OrderedDict(sources)\n\n    def _parse_prefix(self, name) -&gt; tuple[str, str] | None:\n        split = name.split(\"_\")\n        if len(split) &gt; 1 and split[0] in self._sources:\n            prefix = split[0]\n            name_without_prefix = \"_\".join(split[1:])\n            return prefix, name_without_prefix\n        return None\n\n    def find_registry(self, name: str) -&gt; T:\n        parsed_prefix = self._parse_prefix(name)\n        if parsed_prefix:\n            prefix, name_without_prefix = parsed_prefix\n            registry = self._sources[prefix]\n            return registry\n\n        # if no prefix is given, go through all sources in order\n        for registry in self._sources.values():\n            if name in registry:\n                return registry\n        msg = f\"Model {name} not found in any registry\"\n        raise KeyError(msg)\n\n    def find_class(self, name: str) -&gt; type:\n        parsed_prefix = self._parse_prefix(name)\n        registry = self.find_registry(name)\n        if parsed_prefix:\n            prefix, name_without_prefix = parsed_prefix\n            return registry[name_without_prefix]\n        return registry[name]\n\n    def build(self, name: str, *constructor_args, **constructor_kwargs):\n        parsed_prefix = self._parse_prefix(name)\n        if parsed_prefix:\n            prefix, name_without_prefix = parsed_prefix\n            registry = self._sources[prefix]\n            return registry.build(name_without_prefix, *constructor_args, **constructor_kwargs)\n\n        # if no prefix, try to build in order\n        for source in self._sources.values():\n            with suppress(KeyError):\n                return source.build(name, *constructor_args, **constructor_kwargs)\n\n        msg = f\"Could not instantiate model {name} not from any source.\"\n        raise KeyError(msg)\n\n    def register_source(self, prefix: str, registry: T) -&gt; None:\n        \"\"\"Register a source in the registry\"\"\"\n        if prefix in self._sources:\n            msg = f\"Source for prefix {prefix} already exists.\"\n            raise KeyError(msg)\n        self._sources[prefix] = registry\n\n    def __iter__(self):\n        for prefix in self._sources:\n            for element in self._sources[prefix]:\n                yield prefix + \"_\" + element\n\n    def __len__(self):\n        return sum(len(source) for source in self._sources.values())\n\n    def __getitem__(self, name):\n        return self._sources[name]\n\n    def __contains__(self, name):\n        parsed_prefix = self._parse_prefix(name)\n        if parsed_prefix:\n            prefix, name_without_prefix = parsed_prefix\n            return name_without_prefix in self._sources[prefix]\n        return any(name in source for source in self._sources.values())\n\n    @_recursive_repr()\n    def __repr__(self):\n        args = [f\"{name}={source!r}\" for name, source in self._sources.items()]\n        return f'{self.__class__.__name__}({\", \".join(args)})'\n\n    def __str__(self):\n        sources_str = str(\" | \".join([f\"{prefix}: {source!s}\" for prefix, source in self._sources.items()]))\n        return f\"Multi source registry with {len(self)} items: {sources_str}\"\n\n    def keys(self):\n        return self._sources.keys()\n</code></pre>"},{"location":"package/extra_model_structures/#terratorch.registry.registry.MultiSourceRegistry.register_source","title":"<code>register_source(prefix, registry)</code>","text":"<p>Register a source in the registry</p> Source code in <code>terratorch/registry/registry.py</code> <pre><code>def register_source(self, prefix: str, registry: T) -&gt; None:\n    \"\"\"Register a source in the registry\"\"\"\n    if prefix in self._sources:\n        msg = f\"Source for prefix {prefix} already exists.\"\n        raise KeyError(msg)\n    self._sources[prefix] = registry\n</code></pre>"},{"location":"package/extra_model_structures/#terratorch.registry.registry.Registry","title":"<code>terratorch.registry.registry.Registry</code>","text":"<p>               Bases: <code>Set</code></p> <p>Registry holding model constructors and multiple additional sources.</p> <p>This registry behaves as a set of strings, which are model names, to model classes or functions which instantiate model classes.</p> <p>In addition, it can instantiate models with the build method.</p> <p>Add constructors to the registry by annotating them with @registry.register. <pre><code>&gt;&gt;&gt; registry = Registry()\n&gt;&gt;&gt; @registry.register\n... def model(*args, **kwargs):\n...     return object()\n&gt;&gt;&gt; \"model\" in registry\nTrue\n&gt;&gt;&gt; model_instance = registry.build(\"model\")\n</code></pre></p> Source code in <code>terratorch/registry/registry.py</code> <pre><code>class Registry(Set):\n    \"\"\"Registry holding model constructors and multiple additional sources.\n\n    This registry behaves as a set of strings, which are model names,\n    to model classes or functions which instantiate model classes.\n\n    In addition, it can instantiate models with the build method.\n\n    Add constructors to the registry by annotating them with @registry.register.\n    ```\n    &gt;&gt;&gt; registry = Registry()\n    &gt;&gt;&gt; @registry.register\n    ... def model(*args, **kwargs):\n    ...     return object()\n    &gt;&gt;&gt; \"model\" in registry\n    True\n    &gt;&gt;&gt; model_instance = registry.build(\"model\")\n    ```\n    \"\"\"\n\n    def __init__(self, **elements) -&gt; None:\n        self._registry: dict[str, Callable] = dict(elements)\n\n    def register(self, constructor: Callable | type) -&gt; Callable:\n        \"\"\"Register a component in the registry. Used as a decorator.\n\n        Args:\n            constructor (Callable | type): Function or class to be decorated with @register.\n        \"\"\"\n        if not callable(constructor):\n            msg = f\"Invalid argument. Decorate a function or class with @{self.__class__.__name__}.register\"\n            raise TypeError(msg)\n        self._registry[constructor.__name__] = constructor\n        return constructor\n\n    def build(self, name: str, *constructor_args, **constructor_kwargs):\n        \"\"\"Build and return the component.\n        Use prefixes ending with _ to forward to a specific source\n        \"\"\"\n        return self._registry[name](*constructor_args, **constructor_kwargs)\n\n    def __iter__(self):\n        return iter(self._registry)\n\n    def __getitem__(self, key):\n        return self._registry[key]\n\n    def __len__(self):\n        return len(self._registry)\n\n    def __contains__(self, key):\n        return key in self._registry\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self._registry!r})\"\n\n    def __str__(self):\n        return f\"Registry with {len(self)} registered items\"\n</code></pre>"},{"location":"package/extra_model_structures/#terratorch.registry.registry.Registry.build","title":"<code>build(name, *constructor_args, **constructor_kwargs)</code>","text":"<p>Build and return the component. Use prefixes ending with _ to forward to a specific source</p> Source code in <code>terratorch/registry/registry.py</code> <pre><code>def build(self, name: str, *constructor_args, **constructor_kwargs):\n    \"\"\"Build and return the component.\n    Use prefixes ending with _ to forward to a specific source\n    \"\"\"\n    return self._registry[name](*constructor_args, **constructor_kwargs)\n</code></pre>"},{"location":"package/extra_model_structures/#terratorch.registry.registry.Registry.register","title":"<code>register(constructor)</code>","text":"<p>Register a component in the registry. Used as a decorator.</p> <p>Parameters:</p> Name Type Description Default <code>constructor</code> <code>Callable | type</code> <p>Function or class to be decorated with @register.</p> required Source code in <code>terratorch/registry/registry.py</code> <pre><code>def register(self, constructor: Callable | type) -&gt; Callable:\n    \"\"\"Register a component in the registry. Used as a decorator.\n\n    Args:\n        constructor (Callable | type): Function or class to be decorated with @register.\n    \"\"\"\n    if not callable(constructor):\n        msg = f\"Invalid argument. Decorate a function or class with @{self.__class__.__name__}.register\"\n        raise TypeError(msg)\n    self._registry[constructor.__name__] = constructor\n    return constructor\n</code></pre>"},{"location":"package/generic_datamodules/","title":"Generic Datamodules","text":""},{"location":"package/generic_datamodules/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoSegmentationDataModule","title":"<code>terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoSegmentationDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoSegmentationDatasets</p> Source code in <code>terratorch/datamodules/generic_pixel_wise_data_module.py</code> <pre><code>class GenericNonGeoSegmentationDataModule(NonGeoDataModule):\n    \"\"\"\n    This is a generic datamodule class for instantiating data modules at runtime.\n    Composes several [GenericNonGeoSegmentationDatasets][terratorch.datasets.GenericNonGeoSegmentationDataset]\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        num_workers: int,\n        train_data_root: Path,\n        val_data_root: Path,\n        test_data_root: Path,\n        means: list[float] | str,\n        stds: list[float] | str,\n        num_classes: int,\n        img_grep: str = \"*\",\n        label_grep: str = \"*\",\n        predict_data_root: Path | None = None,\n        train_label_data_root: Path | None = None,\n        val_label_data_root: Path | None = None,\n        test_label_data_root: Path | None = None,\n        train_split: Path | None = None,\n        val_split: Path | None = None,\n        test_split: Path | None = None,\n        ignore_split_file_extensions: bool = True,\n        allow_substring_split_file: bool = True,\n        dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        predict_dataset_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n        predict_output_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n        constant_scale: float = 1,\n        rgb_indices: list[int] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = None,\n        drop_last: bool = True,\n        pin_memory: bool = False,\n        check_stackability: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            batch_size (int): _description_\n            num_workers (int): _description_\n            train_data_root (Path): _description_\n            val_data_root (Path): _description_\n            test_data_root (Path): _description_\n            predict_data_root (Path): _description_\n            img_grep (str): _description_\n            label_grep (str): _description_\n            means (list[float]): _description_\n            stds (list[float]): _description_\n            num_classes (int): _description_\n            train_label_data_root (Path | None, optional): _description_. Defaults to None.\n            val_label_data_root (Path | None, optional): _description_. Defaults to None.\n            test_label_data_root (Path | None, optional): _description_. Defaults to None.\n            train_split (Path | None, optional): _description_. Defaults to None.\n            val_split (Path | None, optional): _description_. Defaults to None.\n            test_split (Path | None, optional): _description_. Defaults to None.\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\". Defaults to True.\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None.\n            output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n                Naming must match that of dataset_bands. Defaults to None.\n            predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands\n                with this value at predict time.\n                Defaults to None, which does not overwrite.\n            predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands\n                with this value at predict time. Defaults to None, which does not overwrite.\n            constant_scale (float, optional): _description_. Defaults to 1.\n            rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n            train_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            val_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            test_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n            no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n            drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n            pin_memory (bool): If ``True``, the data loader will copy Tensors\n            into device/CUDA pinned memory before returning them. Defaults to False.\n            check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked.\n        \"\"\"\n        super().__init__(GenericNonGeoSegmentationDataset, batch_size, num_workers, **kwargs)\n        self.num_classes = num_classes\n        self.img_grep = img_grep\n        self.label_grep = label_grep\n        self.train_root = train_data_root\n        self.val_root = val_data_root\n        self.test_root = test_data_root\n        self.predict_root = predict_data_root\n        self.train_split = train_split\n        self.val_split = val_split\n        self.test_split = test_split\n        self.ignore_split_file_extensions = ignore_split_file_extensions\n        self.allow_substring_split_file = allow_substring_split_file\n        self.constant_scale = constant_scale\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.drop_last = drop_last\n        self.pin_memory = pin_memory\n\n        self.train_label_data_root = train_label_data_root\n        self.val_label_data_root = val_label_data_root\n        self.test_label_data_root = test_label_data_root\n\n        self.dataset_bands = dataset_bands\n        self.predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands\n        self.predict_output_bands = predict_output_bands if predict_output_bands else output_bands\n        self.output_bands = output_bands\n        self.rgb_indices = rgb_indices\n        self.expand_temporal_dimension = expand_temporal_dimension\n        self.reduce_zero_label = reduce_zero_label\n\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n\n        # self.aug = AugmentationSequential(\n        #     K.Normalize(means, stds),\n        #     data_keys=[\"image\"],\n        # )\n        means = load_from_file_or_attribute(means)\n        stds = load_from_file_or_attribute(stds)\n\n        self.aug = Normalize(means, stds)\n\n        # self.aug = Normalize(means, stds)\n        # self.collate_fn = collate_fn_list_dicts\n\n        self.check_stackability = check_stackability\n\n    def setup(self, stage: str) -&gt; None:\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                self.train_root,\n                self.num_classes,\n                image_grep=self.img_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.train_label_data_root,\n                split=self.train_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.train_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                self.val_root,\n                self.num_classes,\n                image_grep=self.img_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.val_label_data_root,\n                split=self.val_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.val_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                self.test_root,\n                self.num_classes,\n                image_grep=self.img_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.test_label_data_root,\n                split=self.test_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n        if stage in [\"predict\"] and self.predict_root:\n            self.predict_dataset = self.dataset_class(\n                self.predict_root,\n                self.num_classes,\n                image_grep=self.img_grep,\n                label_grep=self.label_grep,\n                dataset_bands=self.predict_dataset_bands,\n                output_bands=self.predict_output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n\n    def _dataloader_factory(self, split: str) -&gt; DataLoader[dict[str, Tensor]]:\n        \"\"\"Implement one or more PyTorch DataLoaders.\n\n        Args:\n            split: Either 'train', 'val', 'test', or 'predict'.\n\n        Returns:\n            A collection of data loaders specifying samples.\n\n        Raises:\n            MisconfigurationException: If :meth:`setup` does not define a\n                dataset or sampler, or if the dataset or sampler has length 0.\n        \"\"\"\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n\n        if self.check_stackability:\n            logger.info(f\"Checking stackability for {split} split.\")\n            batch_size = check_dataset_stackability(dataset, batch_size)\n\n        return DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=split == \"train\",\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            drop_last=split == \"train\" and self.drop_last,\n            pin_memory=self.pin_memory,\n        )\n</code></pre>"},{"location":"package/generic_datamodules/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoSegmentationDataModule.__init__","title":"<code>__init__(batch_size, num_workers, train_data_root, val_data_root, test_data_root, means, stds, num_classes, img_grep='*', label_grep='*', predict_data_root=None, train_label_data_root=None, val_label_data_root=None, test_label_data_root=None, train_split=None, val_split=None, test_split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, dataset_bands=None, output_bands=None, predict_dataset_bands=None, predict_output_bands=None, constant_scale=1, rgb_indices=None, train_transform=None, val_transform=None, test_transform=None, expand_temporal_dimension=False, reduce_zero_label=False, no_data_replace=None, no_label_replace=None, drop_last=True, pin_memory=False, check_stackability=True, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>description</p> required <code>num_workers</code> <code>int</code> <p>description</p> required <code>train_data_root</code> <code>Path</code> <p>description</p> required <code>val_data_root</code> <code>Path</code> <p>description</p> required <code>test_data_root</code> <code>Path</code> <p>description</p> required <code>predict_data_root</code> <code>Path</code> <p>description</p> <code>None</code> <code>img_grep</code> <code>str</code> <p>description</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>description</p> <code>'*'</code> <code>means</code> <code>list[float]</code> <p>description</p> required <code>stds</code> <code>list[float]</code> <p>description</p> required <code>num_classes</code> <code>int</code> <p>description</p> required <code>train_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None.</p> <code>None</code> <code>predict_dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>predict_output_bands</code> <code>list[HLSBands | int] | None</code> <p>Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>description. Defaults to 1.</p> <code>1</code> <code>rgb_indices</code> <code>list[int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>val_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>test_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Drop the last batch if it is not complete. Defaults to True.</p> <code>True</code> <code>pin_memory</code> <code>bool</code> <p>If <code>True</code>, the data loader will copy Tensors</p> <code>False</code> <code>check_stackability</code> <code>bool</code> <p>Check if all the files in the dataset has the same size and can be stacked.</p> <code>True</code> Source code in <code>terratorch/datamodules/generic_pixel_wise_data_module.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    num_workers: int,\n    train_data_root: Path,\n    val_data_root: Path,\n    test_data_root: Path,\n    means: list[float] | str,\n    stds: list[float] | str,\n    num_classes: int,\n    img_grep: str = \"*\",\n    label_grep: str = \"*\",\n    predict_data_root: Path | None = None,\n    train_label_data_root: Path | None = None,\n    val_label_data_root: Path | None = None,\n    test_label_data_root: Path | None = None,\n    train_split: Path | None = None,\n    val_split: Path | None = None,\n    test_split: Path | None = None,\n    ignore_split_file_extensions: bool = True,\n    allow_substring_split_file: bool = True,\n    dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    predict_dataset_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n    predict_output_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n    constant_scale: float = 1,\n    rgb_indices: list[int] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = None,\n    drop_last: bool = True,\n    pin_memory: bool = False,\n    check_stackability: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        batch_size (int): _description_\n        num_workers (int): _description_\n        train_data_root (Path): _description_\n        val_data_root (Path): _description_\n        test_data_root (Path): _description_\n        predict_data_root (Path): _description_\n        img_grep (str): _description_\n        label_grep (str): _description_\n        means (list[float]): _description_\n        stds (list[float]): _description_\n        num_classes (int): _description_\n        train_label_data_root (Path | None, optional): _description_. Defaults to None.\n        val_label_data_root (Path | None, optional): _description_. Defaults to None.\n        test_label_data_root (Path | None, optional): _description_. Defaults to None.\n        train_split (Path | None, optional): _description_. Defaults to None.\n        val_split (Path | None, optional): _description_. Defaults to None.\n        test_split (Path | None, optional): _description_. Defaults to None.\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\". Defaults to True.\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None.\n        output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n            Naming must match that of dataset_bands. Defaults to None.\n        predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands\n            with this value at predict time.\n            Defaults to None, which does not overwrite.\n        predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands\n            with this value at predict time. Defaults to None, which does not overwrite.\n        constant_scale (float, optional): _description_. Defaults to 1.\n        rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n        train_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        val_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        test_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n        no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n        drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n        pin_memory (bool): If ``True``, the data loader will copy Tensors\n        into device/CUDA pinned memory before returning them. Defaults to False.\n        check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked.\n    \"\"\"\n    super().__init__(GenericNonGeoSegmentationDataset, batch_size, num_workers, **kwargs)\n    self.num_classes = num_classes\n    self.img_grep = img_grep\n    self.label_grep = label_grep\n    self.train_root = train_data_root\n    self.val_root = val_data_root\n    self.test_root = test_data_root\n    self.predict_root = predict_data_root\n    self.train_split = train_split\n    self.val_split = val_split\n    self.test_split = test_split\n    self.ignore_split_file_extensions = ignore_split_file_extensions\n    self.allow_substring_split_file = allow_substring_split_file\n    self.constant_scale = constant_scale\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.drop_last = drop_last\n    self.pin_memory = pin_memory\n\n    self.train_label_data_root = train_label_data_root\n    self.val_label_data_root = val_label_data_root\n    self.test_label_data_root = test_label_data_root\n\n    self.dataset_bands = dataset_bands\n    self.predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands\n    self.predict_output_bands = predict_output_bands if predict_output_bands else output_bands\n    self.output_bands = output_bands\n    self.rgb_indices = rgb_indices\n    self.expand_temporal_dimension = expand_temporal_dimension\n    self.reduce_zero_label = reduce_zero_label\n\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n\n    # self.aug = AugmentationSequential(\n    #     K.Normalize(means, stds),\n    #     data_keys=[\"image\"],\n    # )\n    means = load_from_file_or_attribute(means)\n    stds = load_from_file_or_attribute(stds)\n\n    self.aug = Normalize(means, stds)\n\n    # self.aug = Normalize(means, stds)\n    # self.collate_fn = collate_fn_list_dicts\n\n    self.check_stackability = check_stackability\n</code></pre>"},{"location":"package/generic_datamodules/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoPixelwiseRegressionDataModule","title":"<code>terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoPixelwiseRegressionDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoPixelwiseRegressionDataset</p> Source code in <code>terratorch/datamodules/generic_pixel_wise_data_module.py</code> <pre><code>class GenericNonGeoPixelwiseRegressionDataModule(NonGeoDataModule):\n    \"\"\"This is a generic datamodule class for instantiating data modules at runtime.\n    Composes several\n    [GenericNonGeoPixelwiseRegressionDataset][terratorch.datasets.GenericNonGeoPixelwiseRegressionDataset]\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        num_workers: int,\n        train_data_root: Path,\n        val_data_root: Path,\n        test_data_root: Path,\n        means: list[float] | str,\n        stds: list[float] | str,\n        predict_data_root: Path | None = None,\n        img_grep: str | None = \"*\",\n        label_grep: str | None = \"*\",\n        train_label_data_root: Path | None = None,\n        val_label_data_root: Path | None = None,\n        test_label_data_root: Path | None = None,\n        train_split: Path | None = None,\n        val_split: Path | None = None,\n        test_split: Path | None = None,\n        ignore_split_file_extensions: bool = True,\n        allow_substring_split_file: bool = True,\n        dataset_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n        output_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n        predict_dataset_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n        predict_output_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n        constant_scale: float = 1,\n        rgb_indices: list[int] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = None,\n        drop_last: bool = True,\n        pin_memory: bool = False,\n        check_stackability: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            batch_size (int): _description_\n            num_workers (int): _description_\n            train_data_root (Path): _description_\n            val_data_root (Path): _description_\n            test_data_root (Path): _description_\n            predict_data_root (Path): _description_\n            img_grep (str): _description_\n            label_grep (str): _description_\n            means (list[float]): _description_\n            stds (list[float]): _description_\n            train_label_data_root (Path | None, optional): _description_. Defaults to None.\n            val_label_data_root (Path | None, optional): _description_. Defaults to None.\n            test_label_data_root (Path | None, optional): _description_. Defaults to None.\n            train_split (Path | None, optional): _description_. Defaults to None.\n            val_split (Path | None, optional): _description_. Defaults to None.\n            test_split (Path | None, optional): _description_. Defaults to None.\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\". Defaults to True.\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None.\n            output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n                Naming must match that of dataset_bands. Defaults to None.\n            predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands\n                with this value at predict time.\n                Defaults to None, which does not overwrite.\n            predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands\n                with this value at predict time. Defaults to None, which does not overwrite.\n            constant_scale (float, optional): _description_. Defaults to 1.\n            rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n            train_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            val_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            test_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n            no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n            drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n            pin_memory (bool): If ``True``, the data loader will copy Tensors\n            into device/CUDA pinned memory before returning them. Defaults to False.\n            check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked.\n        \"\"\"\n        super().__init__(GenericNonGeoPixelwiseRegressionDataset, batch_size, num_workers, **kwargs)\n        self.img_grep = img_grep\n        self.label_grep = label_grep\n        self.train_root = train_data_root\n        self.val_root = val_data_root\n        self.test_root = test_data_root\n        self.predict_root = predict_data_root\n        self.train_split = train_split\n        self.val_split = val_split\n        self.test_split = test_split\n        self.ignore_split_file_extensions = ignore_split_file_extensions\n        self.allow_substring_split_file = allow_substring_split_file\n        self.drop_last = drop_last\n        self.pin_memory = pin_memory\n        self.expand_temporal_dimension = expand_temporal_dimension\n        self.reduce_zero_label = reduce_zero_label\n\n        self.train_label_data_root = train_label_data_root\n        self.val_label_data_root = val_label_data_root\n        self.test_label_data_root = test_label_data_root\n\n        self.constant_scale = constant_scale\n\n        self.dataset_bands = dataset_bands\n        self.predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands\n        self.predict_output_bands = predict_output_bands if predict_output_bands else output_bands\n        self.output_bands = output_bands\n        self.rgb_indices = rgb_indices\n\n        # self.aug = AugmentationSequential(\n        #     K.Normalize(means, stds),\n        #     data_keys=[\"image\"],\n        # )\n        means = load_from_file_or_attribute(means)\n        stds = load_from_file_or_attribute(stds)\n\n        self.aug = Normalize(means, stds)\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n\n        self.check_stackability = check_stackability\n\n    def setup(self, stage: str) -&gt; None:\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                self.train_root,\n                image_grep=self.img_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.train_label_data_root,\n                split=self.train_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.train_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                self.val_root,\n                image_grep=self.img_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.val_label_data_root,\n                split=self.val_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.val_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                self.test_root,\n                image_grep=self.img_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.test_label_data_root,\n                split=self.test_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n\n        if stage in [\"predict\"] and self.predict_root:\n            self.predict_dataset = self.dataset_class(\n                self.predict_root,\n                image_grep=self.img_grep,\n                dataset_bands=self.predict_dataset_bands,\n                output_bands=self.predict_output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n\n    def _dataloader_factory(self, split: str) -&gt; DataLoader[dict[str, Tensor]]:\n        \"\"\"Implement one or more PyTorch DataLoaders.\n\n        Args:\n            split: Either 'train', 'val', 'test', or 'predict'.\n\n        Returns:\n            A collection of data loaders specifying samples.\n\n        Raises:\n            MisconfigurationException: If :meth:`setup` does not define a\n                dataset or sampler, or if the dataset or sampler has length 0.\n        \"\"\"\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n\n        if self.check_stackability:\n            logger.info(\"Checking stackability.\")\n            batch_size = check_dataset_stackability(dataset, batch_size)\n\n        return DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=split == \"train\",\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            drop_last=split == \"train\" and self.drop_last,\n            pin_memory=self.pin_memory,\n        )\n</code></pre>"},{"location":"package/generic_datamodules/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoPixelwiseRegressionDataModule.__init__","title":"<code>__init__(batch_size, num_workers, train_data_root, val_data_root, test_data_root, means, stds, predict_data_root=None, img_grep='*', label_grep='*', train_label_data_root=None, val_label_data_root=None, test_label_data_root=None, train_split=None, val_split=None, test_split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, dataset_bands=None, output_bands=None, predict_dataset_bands=None, predict_output_bands=None, constant_scale=1, rgb_indices=None, train_transform=None, val_transform=None, test_transform=None, expand_temporal_dimension=False, reduce_zero_label=False, no_data_replace=None, no_label_replace=None, drop_last=True, pin_memory=False, check_stackability=True, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>description</p> required <code>num_workers</code> <code>int</code> <p>description</p> required <code>train_data_root</code> <code>Path</code> <p>description</p> required <code>val_data_root</code> <code>Path</code> <p>description</p> required <code>test_data_root</code> <code>Path</code> <p>description</p> required <code>predict_data_root</code> <code>Path</code> <p>description</p> <code>None</code> <code>img_grep</code> <code>str</code> <p>description</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>description</p> <code>'*'</code> <code>means</code> <code>list[float]</code> <p>description</p> required <code>stds</code> <code>list[float]</code> <p>description</p> required <code>train_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None.</p> <code>None</code> <code>predict_dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>predict_output_bands</code> <code>list[HLSBands | int] | None</code> <p>Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>description. Defaults to 1.</p> <code>1</code> <code>rgb_indices</code> <code>list[int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>val_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>test_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Drop the last batch if it is not complete. Defaults to True.</p> <code>True</code> <code>pin_memory</code> <code>bool</code> <p>If <code>True</code>, the data loader will copy Tensors</p> <code>False</code> <code>check_stackability</code> <code>bool</code> <p>Check if all the files in the dataset has the same size and can be stacked.</p> <code>True</code> Source code in <code>terratorch/datamodules/generic_pixel_wise_data_module.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    num_workers: int,\n    train_data_root: Path,\n    val_data_root: Path,\n    test_data_root: Path,\n    means: list[float] | str,\n    stds: list[float] | str,\n    predict_data_root: Path | None = None,\n    img_grep: str | None = \"*\",\n    label_grep: str | None = \"*\",\n    train_label_data_root: Path | None = None,\n    val_label_data_root: Path | None = None,\n    test_label_data_root: Path | None = None,\n    train_split: Path | None = None,\n    val_split: Path | None = None,\n    test_split: Path | None = None,\n    ignore_split_file_extensions: bool = True,\n    allow_substring_split_file: bool = True,\n    dataset_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n    output_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n    predict_dataset_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n    predict_output_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n    constant_scale: float = 1,\n    rgb_indices: list[int] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = None,\n    drop_last: bool = True,\n    pin_memory: bool = False,\n    check_stackability: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        batch_size (int): _description_\n        num_workers (int): _description_\n        train_data_root (Path): _description_\n        val_data_root (Path): _description_\n        test_data_root (Path): _description_\n        predict_data_root (Path): _description_\n        img_grep (str): _description_\n        label_grep (str): _description_\n        means (list[float]): _description_\n        stds (list[float]): _description_\n        train_label_data_root (Path | None, optional): _description_. Defaults to None.\n        val_label_data_root (Path | None, optional): _description_. Defaults to None.\n        test_label_data_root (Path | None, optional): _description_. Defaults to None.\n        train_split (Path | None, optional): _description_. Defaults to None.\n        val_split (Path | None, optional): _description_. Defaults to None.\n        test_split (Path | None, optional): _description_. Defaults to None.\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\". Defaults to True.\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None.\n        output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n            Naming must match that of dataset_bands. Defaults to None.\n        predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands\n            with this value at predict time.\n            Defaults to None, which does not overwrite.\n        predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands\n            with this value at predict time. Defaults to None, which does not overwrite.\n        constant_scale (float, optional): _description_. Defaults to 1.\n        rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n        train_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        val_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        test_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n        no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n        drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n        pin_memory (bool): If ``True``, the data loader will copy Tensors\n        into device/CUDA pinned memory before returning them. Defaults to False.\n        check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked.\n    \"\"\"\n    super().__init__(GenericNonGeoPixelwiseRegressionDataset, batch_size, num_workers, **kwargs)\n    self.img_grep = img_grep\n    self.label_grep = label_grep\n    self.train_root = train_data_root\n    self.val_root = val_data_root\n    self.test_root = test_data_root\n    self.predict_root = predict_data_root\n    self.train_split = train_split\n    self.val_split = val_split\n    self.test_split = test_split\n    self.ignore_split_file_extensions = ignore_split_file_extensions\n    self.allow_substring_split_file = allow_substring_split_file\n    self.drop_last = drop_last\n    self.pin_memory = pin_memory\n    self.expand_temporal_dimension = expand_temporal_dimension\n    self.reduce_zero_label = reduce_zero_label\n\n    self.train_label_data_root = train_label_data_root\n    self.val_label_data_root = val_label_data_root\n    self.test_label_data_root = test_label_data_root\n\n    self.constant_scale = constant_scale\n\n    self.dataset_bands = dataset_bands\n    self.predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands\n    self.predict_output_bands = predict_output_bands if predict_output_bands else output_bands\n    self.output_bands = output_bands\n    self.rgb_indices = rgb_indices\n\n    # self.aug = AugmentationSequential(\n    #     K.Normalize(means, stds),\n    #     data_keys=[\"image\"],\n    # )\n    means = load_from_file_or_attribute(means)\n    stds = load_from_file_or_attribute(stds)\n\n    self.aug = Normalize(means, stds)\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n\n    self.check_stackability = check_stackability\n</code></pre>"},{"location":"package/generic_datamodules/#terratorch.datamodules.generic_scalar_label_data_module.GenericNonGeoClassificationDataModule","title":"<code>terratorch.datamodules.generic_scalar_label_data_module.GenericNonGeoClassificationDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoClassificationDatasets</p> Source code in <code>terratorch/datamodules/generic_scalar_label_data_module.py</code> <pre><code>class GenericNonGeoClassificationDataModule(NonGeoDataModule):\n    \"\"\"\n    This is a generic datamodule class for instantiating data modules at runtime.\n    Composes several [GenericNonGeoClassificationDatasets][terratorch.datasets.GenericNonGeoClassificationDataset]\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        num_workers: int,\n        train_data_root: Path,\n        val_data_root: Path,\n        test_data_root: Path,\n        means: list[float] | str,\n        stds: list[float] | str,\n        num_classes: int,\n        predict_data_root: Path | None = None,\n        train_split: Path | None = None,\n        val_split: Path | None = None,\n        test_split: Path | None = None,\n        ignore_split_file_extensions: bool = True,\n        allow_substring_split_file: bool = True,\n        dataset_bands: list[HLSBands | int] | None = None,\n        predict_dataset_bands: list[HLSBands | int] | None = None,\n        output_bands: list[HLSBands | int] | None = None,\n        constant_scale: float = 1,\n        rgb_indices: list[int] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        expand_temporal_dimension: bool = False,\n        no_data_replace: float = 0,\n        drop_last: bool = True,\n        check_stackability: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            batch_size (int): _description_\n            num_workers (int): _description_\n            train_data_root (Path): _description_\n            val_data_root (Path): _description_\n            test_data_root (Path): _description_\n            means (list[float]): _description_\n            stds (list[float]): _description_\n            num_classes (int): _description_\n            predict_data_root (Path): _description_\n            train_split (Path | None, optional): _description_. Defaults to None.\n            val_split (Path | None, optional): _description_. Defaults to None.\n            test_split (Path | None, optional): _description_. Defaults to None.\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\".\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None.\n            predict_dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None.\n            output_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None.\n            constant_scale (float, optional): _description_. Defaults to 1.\n            rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n            train_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            val_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            test_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float): Replace nan values in input images with this value. Defaults to 0.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n            drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n            check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked.\n        \"\"\"\n        super().__init__(GenericNonGeoClassificationDataset, batch_size, num_workers, **kwargs)\n        self.num_classes = num_classes\n        self.train_root = train_data_root\n        self.val_root = val_data_root\n        self.test_root = test_data_root\n        self.predict_root = predict_data_root\n        self.train_split = train_split\n        self.val_split = val_split\n        self.test_split = test_split\n        self.ignore_split_file_extensions = ignore_split_file_extensions\n        self.allow_substring_split_file = allow_substring_split_file\n        self.constant_scale = constant_scale\n        self.no_data_replace = no_data_replace\n        self.drop_last = drop_last\n\n        self.dataset_bands = dataset_bands\n        self.predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands\n        self.output_bands = output_bands\n        self.rgb_indices = rgb_indices\n        self.expand_temporal_dimension = expand_temporal_dimension\n\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n\n        # self.aug = AugmentationSequential(\n        #     K.Normalize(means, stds),\n        #     data_keys=[\"image\"],\n        # )\n\n        means = load_from_file_or_attribute(means)\n        stds = load_from_file_or_attribute(stds)\n\n        self.aug = Normalize(means, stds)\n\n        # self.aug = Normalize(means, stds)\n        # self.collate_fn = collate_fn_list_dicts\n\n        self.check_stackability = check_stackability\n\n    def setup(self, stage: str) -&gt; None:\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                self.train_root,\n                self.num_classes,\n                split=self.train_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.train_transform,\n                no_data_replace=self.no_data_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                self.val_root,\n                self.num_classes,\n                split=self.val_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.val_transform,\n                no_data_replace=self.no_data_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                self.test_root,\n                self.num_classes,\n                split=self.test_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n            )\n        if stage in [\"predict\"] and self.predict_root:\n            self.predict_dataset = self.dataset_class(\n                self.predict_root,\n                self.num_classes,\n                dataset_bands=self.predict_dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n            )\n\n    def _dataloader_factory(self, split: str) -&gt; DataLoader[dict[str, Tensor]]:\n        \"\"\"Implement one or more PyTorch DataLoaders.\n\n        Args:\n            split: Either 'train', 'val', 'test', or 'predict'.\n\n        Returns:\n            A collection of data loaders specifying samples.\n\n        Raises:\n            MisconfigurationException: If :meth:`setup` does not define a\n                dataset or sampler, or if the dataset or sampler has length 0.\n        \"\"\"\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n\n        if self.check_stackability:\n            logger.info(\"Checking stackability.\")\n            batch_size = check_dataset_stackability(dataset, batch_size)\n\n        return DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=split == \"train\",\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            drop_last=split == \"train\" and self.drop_last,\n        )\n</code></pre>"},{"location":"package/generic_datamodules/#terratorch.datamodules.generic_scalar_label_data_module.GenericNonGeoClassificationDataModule.__init__","title":"<code>__init__(batch_size, num_workers, train_data_root, val_data_root, test_data_root, means, stds, num_classes, predict_data_root=None, train_split=None, val_split=None, test_split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, dataset_bands=None, predict_dataset_bands=None, output_bands=None, constant_scale=1, rgb_indices=None, train_transform=None, val_transform=None, test_transform=None, expand_temporal_dimension=False, no_data_replace=0, drop_last=True, check_stackability=True, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>description</p> required <code>num_workers</code> <code>int</code> <p>description</p> required <code>train_data_root</code> <code>Path</code> <p>description</p> required <code>val_data_root</code> <code>Path</code> <p>description</p> required <code>test_data_root</code> <code>Path</code> <p>description</p> required <code>means</code> <code>list[float]</code> <p>description</p> required <code>stds</code> <code>list[float]</code> <p>description</p> required <code>num_classes</code> <code>int</code> <p>description</p> required <code>predict_data_root</code> <code>Path</code> <p>description</p> <code>None</code> <code>train_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\".</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>predict_dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>description. Defaults to 1.</p> <code>1</code> <code>rgb_indices</code> <code>list[int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>val_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>test_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float</code> <p>Replace nan values in input images with this value. Defaults to 0.</p> <code>0</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Drop the last batch if it is not complete. Defaults to True.</p> <code>True</code> <code>check_stackability</code> <code>bool</code> <p>Check if all the files in the dataset has the same size and can be stacked.</p> <code>True</code> Source code in <code>terratorch/datamodules/generic_scalar_label_data_module.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    num_workers: int,\n    train_data_root: Path,\n    val_data_root: Path,\n    test_data_root: Path,\n    means: list[float] | str,\n    stds: list[float] | str,\n    num_classes: int,\n    predict_data_root: Path | None = None,\n    train_split: Path | None = None,\n    val_split: Path | None = None,\n    test_split: Path | None = None,\n    ignore_split_file_extensions: bool = True,\n    allow_substring_split_file: bool = True,\n    dataset_bands: list[HLSBands | int] | None = None,\n    predict_dataset_bands: list[HLSBands | int] | None = None,\n    output_bands: list[HLSBands | int] | None = None,\n    constant_scale: float = 1,\n    rgb_indices: list[int] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    expand_temporal_dimension: bool = False,\n    no_data_replace: float = 0,\n    drop_last: bool = True,\n    check_stackability: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        batch_size (int): _description_\n        num_workers (int): _description_\n        train_data_root (Path): _description_\n        val_data_root (Path): _description_\n        test_data_root (Path): _description_\n        means (list[float]): _description_\n        stds (list[float]): _description_\n        num_classes (int): _description_\n        predict_data_root (Path): _description_\n        train_split (Path | None, optional): _description_. Defaults to None.\n        val_split (Path | None, optional): _description_. Defaults to None.\n        test_split (Path | None, optional): _description_. Defaults to None.\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\".\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None.\n        predict_dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None.\n        output_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None.\n        constant_scale (float, optional): _description_. Defaults to 1.\n        rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n        train_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        val_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        test_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float): Replace nan values in input images with this value. Defaults to 0.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n        drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n        check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked.\n    \"\"\"\n    super().__init__(GenericNonGeoClassificationDataset, batch_size, num_workers, **kwargs)\n    self.num_classes = num_classes\n    self.train_root = train_data_root\n    self.val_root = val_data_root\n    self.test_root = test_data_root\n    self.predict_root = predict_data_root\n    self.train_split = train_split\n    self.val_split = val_split\n    self.test_split = test_split\n    self.ignore_split_file_extensions = ignore_split_file_extensions\n    self.allow_substring_split_file = allow_substring_split_file\n    self.constant_scale = constant_scale\n    self.no_data_replace = no_data_replace\n    self.drop_last = drop_last\n\n    self.dataset_bands = dataset_bands\n    self.predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands\n    self.output_bands = output_bands\n    self.rgb_indices = rgb_indices\n    self.expand_temporal_dimension = expand_temporal_dimension\n\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n\n    # self.aug = AugmentationSequential(\n    #     K.Normalize(means, stds),\n    #     data_keys=[\"image\"],\n    # )\n\n    means = load_from_file_or_attribute(means)\n    stds = load_from_file_or_attribute(stds)\n\n    self.aug = Normalize(means, stds)\n\n    # self.aug = Normalize(means, stds)\n    # self.collate_fn = collate_fn_list_dicts\n\n    self.check_stackability = check_stackability\n</code></pre>"},{"location":"package/generic_datamodules/#terratorch.datamodules.generic_multimodal_data_module.GenericMultiModalDataModule","title":"<code>terratorch.datamodules.generic_multimodal_data_module.GenericMultiModalDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoSegmentationDatasets</p> Source code in <code>terratorch/datamodules/generic_multimodal_data_module.py</code> <pre><code>class GenericMultiModalDataModule(NonGeoDataModule):\n    \"\"\"\n    This is a generic datamodule class for instantiating data modules at runtime.\n    Composes several [GenericNonGeoSegmentationDatasets][terratorch.datasets.GenericNonGeoSegmentationDataset]\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        modalities: list[str],\n        train_data_root: dict[str, Path],\n        val_data_root: dict[str, Path],\n        test_data_root: dict[str, Path],\n        means: dict[str, list],\n        stds: dict[str, list],\n        task: str | None = None,\n        num_classes: int | None = None,\n        image_grep: str | dict[str, str] | None = None,\n        label_grep: str | None = None,\n        train_label_data_root: Path | str | None = None,\n        val_label_data_root: Path | str | None = None,\n        test_label_data_root: Path | str | None = None,\n        predict_data_root: dict[str, Path] | str | None = None,\n        train_split: Path | str | None = None,\n        val_split: Path | str | None = None,\n        test_split: Path| str | None = None,\n        dataset_bands: dict[str, list] | None = None,\n        output_bands: dict[str, list] | None = None,\n        predict_dataset_bands: dict[str, list] | None = None,\n        predict_output_bands: dict[str, list] | None = None,\n        image_modalities: list[str] | None = None,\n        rgb_modality: str | None = None,\n        rgb_indices: list[int] | None = None,\n        allow_substring_file_names: bool = True,\n        class_names: list[str] | None = None,\n        constant_scale: dict[float] = None,\n        train_transform: dict | A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: dict | A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: dict | A.Compose | None | list[A.BasicTransform] = None,\n        shared_transforms: list | bool = True,\n        expand_temporal_dimension: bool = False,\n        no_data_replace: float | None = None,\n        no_label_replace: float | None = -1,\n        reduce_zero_label: bool = False,\n        drop_last: bool = True,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n        data_with_sample_dim: bool = False,\n        sample_num_modalities: int | None = None,\n        sample_replace: bool = False,\n        channel_position: int = -3,\n        concat_bands: bool = False,\n        check_stackability: bool = True,\n        img_grep: str | dict[str, str] | None = None,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            batch_size (int): Number of samples in per batch.\n            modalities (list[str]): List of modalities.\n            train_data_root (dict[Path]): Dictionary of paths to training data root directory or csv/parquet files with \n                image-level data, with modalities as keys.\n            val_data_root (dict[Path]): Dictionary of paths to validation data root directory or csv/parquet files with \n                image-level data, with modalities as keys.\n            test_data_root (dict[Path]): Dictionary of paths to test data root directory or csv/parquet files with \n                image-level data, with modalities as keys.\n            means (dict[list]): Dictionary of mean values as lists with modalities as keys.\n            stds (dict[list]): Dictionary of std values as lists with modalities as keys.\n            task (str, optional): Selected task form segmentation, regression (pixel-wise), classification,\n                multilabel_classification, scalar_regression, scalar (custom image-level task), or None (no targets).\n                Defaults to None.\n            num_classes (int, optional): Number of classes in classification or segmentation tasks.\n            predict_data_root (dict[Path], optional): Dictionary of paths to data root directory or csv/parquet files\n                with image-level data, with modalities as keys.\n            image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n                images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            label_grep (str, optional): Regular expression appended to label_data_root to find labels or mask files.\n                Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            train_label_data_root (Path | None, optional): Path to data root directory with training labels or\n                csv/parquet files with labels. Required for supervised tasks.\n            val_label_data_root (Path | None, optional): Path to data root directory with validation labels or\n                csv/parquet files with labels. Required for supervised tasks.\n            test_label_data_root (Path | None, optional): Path to data root directory with test labels or\n                csv/parquet files with labels. Required for supervised tasks.\n            train_split (Path, optional): Path to file containing training samples prefixes to be used for this split.\n                The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n                sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n                files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n                If not specified, search samples based on files in data_root. Defaults to None.\n            val_split (Path, optional): Path to file containing validation samples prefixes to be used for this split.\n                The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n                sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n                files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n                If not specified, search samples based on files in data_root. Defaults to None.\n            test_split (Path, optional): Path to file containing test samples prefixes to be used for this split.\n                The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n                sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n                files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n                If not specified, search samples based on files in data_root. Defaults to None.\n            dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n                as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n                that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n                of all modalities. Defaults to None.\n            output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n                provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n            predict_dataset_bands (list[dict], optional): Overwrites dataset_bands with this value at predict time.\n                Defaults to None, which does not overwrite.\n            predict_output_bands (list[dict], optional): Overwrites output_bands with this value at predict time.\n                Defaults to None, which does not overwrite.\n            image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n                The difference between all modalities and image_modalities are non-image modalities which are treated\n                differently during the transforms and are not modified but only converted into a tensor if possible.\n            rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n            rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n            allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n                image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n                If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n                Defaults to True.\n            class_names (list[str], optional): Names of the classes. Defaults to None.\n            constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n                keys. Can be subset of all modalities. Defaults to None.\n            train_transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n                modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to \n                non-image data, which is only converted to tensors if possible. If dict, can include separate transforms \n                per modality (no shared parameters between modalities). \n                Defaults to None, which simply applies ToTensorV2().\n            val_transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n                modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to \n                non-image data, which is only converted to tensors if possible. If dict, can include separate transforms \n                per modality (no shared parameters between modalities). \n                Defaults to None, which simply applies ToTensorV2().\n            test_transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n                modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to \n                non-image data, which is only converted to tensors if possible. If dict, can include separate transforms \n                per modality (no shared parameters between modalities). \n                Defaults to None, which simply applies ToTensorV2().\n            shared_transforms (bool): transforms are shared between all image modalities (e.g., similar crop). \n                This setting is ignored if transforms are defined per modality. Defaults to True.  \n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n                Defaults to False.\n            no_data_replace (float | None): Replace nan values in input images with this value. If none, does no \n                replacement. Defaults to None.\n            no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. \n                Defaults to None.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n            drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n            num_workers (int): Number of parallel workers. Defaults to 0 for single threaded process.\n            pin_memory (bool): If ``True``, the data loader will copy Tensors into device/CUDA pinned memory before \n                returning them. Defaults to False.\n            data_with_sample_dim (bool): Use a specific collate function to concatenate samples along a existing sample\n                dimension instead of stacking the samples. Defaults to False.\n            sample_num_modalities (int, optional): Load only a subset of modalities per batch. Defaults to None.\n            sample_replace (bool): If sample_num_modalities is set, sample modalities with replacement.\n                Defaults to False.\n            channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n            concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n                that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n                Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n            check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked.\n        \"\"\"\n\n        if task == \"segmentation\":\n            dataset_class = GenericMultimodalSegmentationDataset\n        elif task == \"regression\":\n            dataset_class = GenericMultimodalPixelwiseRegressionDataset\n        elif task in [\"classification\", \"multilabel_classification\", \"scalar_regression\", \"scalar\"]:\n            dataset_class = GenericMultimodalScalarDataset\n            task = \"scalar\"\n        elif task is None:\n            dataset_class = GenericMultimodalDataset\n        else:\n            raise ValueError(f\"Unknown task {task}, only segmentation and regression are supported.\")\n\n        super().__init__(dataset_class, batch_size, num_workers)\n        self.num_classes = num_classes\n        self.class_names = class_names\n        self.modalities = modalities\n        self.image_modalities = image_modalities or modalities\n        self.non_image_modalities = list(set(self.modalities) - set(self.image_modalities))\n        if task == \"scalar\":\n            self.non_image_modalities += [\"label\"]\n\n        if img_grep is not None:\n            warnings.warn(f'img_grep was renamed to image_grep and will be removed in a future version.',\n                          DeprecationWarning)\n            image_grep = img_grep\n\n        if isinstance(image_grep, dict):\n            self.image_grep = {m: image_grep[m] if m in image_grep else \"*\" for m in modalities}\n        else:\n            self.image_grep = {m: image_grep or \"*\" for m in modalities}\n        self.label_grep = label_grep or \"*\"\n        self.train_root = train_data_root\n        self.val_root = val_data_root\n        self.test_root = test_data_root\n        self.train_label_data_root = train_label_data_root\n        self.val_label_data_root = val_label_data_root\n        self.test_label_data_root = test_label_data_root\n        self.predict_root = predict_data_root\n\n        assert not train_data_root or all(m in train_data_root for m in modalities), \\\n            f\"predict_data_root is missing paths to some modalities {modalities}: {train_data_root}\"\n        assert not val_data_root or all(m in val_data_root for m in modalities), \\\n            f\"predict_data_root is missing paths to some modalities {modalities}: {val_data_root}\"\n        assert not test_data_root or all(m in test_data_root for m in modalities), \\\n            f\"predict_data_root is missing paths to some modalities {modalities}: {test_data_root}\"\n        assert not predict_data_root or all(m in predict_data_root for m in modalities), \\\n            f\"predict_data_root is missing paths to some modalities {modalities}: {predict_data_root}\"\n\n        self.train_split = train_split\n        self.val_split = val_split\n        self.test_split = test_split\n        self.allow_substring_file_names = allow_substring_file_names\n        self.constant_scale = constant_scale\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.drop_last = drop_last\n        self.pin_memory = pin_memory\n        self.sample_num_modalities = sample_num_modalities\n        self.sample_replace = sample_replace        \n\n        self.dataset_bands = dataset_bands\n        self.output_bands = output_bands\n        self.predict_dataset_bands = predict_dataset_bands\n        self.predict_output_bands = predict_output_bands\n\n        self.rgb_modality = rgb_modality or modalities[0]\n        self.rgb_indices = rgb_indices\n        self.expand_temporal_dimension = expand_temporal_dimension\n        self.reduce_zero_label = reduce_zero_label\n        self.channel_position = channel_position\n        self.concat_bands = concat_bands\n        if not concat_bands and check_stackability:\n            logger.debug(f\"Cannot check stackability if bands are not concatenated.\")\n        self.check_stackability = check_stackability\n\n        if isinstance(train_transform, dict):\n            self.train_transform = {m: wrap_in_compose_is_list(train_transform[m]) if m in train_transform else None\n                                    for m in modalities}\n        elif shared_transforms:\n            self.train_transform = wrap_in_compose_is_list(train_transform,\n                                                           image_modalities=self.image_modalities,\n                                                           non_image_modalities=self.non_image_modalities)\n        else:\n            self.train_transform = {m: wrap_in_compose_is_list(train_transform)\n                                    for m in modalities}\n\n        if isinstance(val_transform, dict):\n            self.val_transform = {m: wrap_in_compose_is_list(val_transform[m]) if m in val_transform else None\n                                    for m in modalities}\n        elif shared_transforms:\n            self.val_transform = wrap_in_compose_is_list(val_transform,\n                                                         image_modalities=self.image_modalities,\n                                                         non_image_modalities=self.non_image_modalities)\n        else:\n            self.val_transform = {m: wrap_in_compose_is_list(val_transform)\n                                    for m in modalities}\n\n        if isinstance(test_transform, dict):\n            self.test_transform = {m: wrap_in_compose_is_list(test_transform[m]) if m in test_transform else None\n                                    for m in modalities}\n        elif shared_transforms:\n            self.test_transform = wrap_in_compose_is_list(test_transform,\n                                                          image_modalities=self.image_modalities,\n                                                          non_image_modalities=self.non_image_modalities,                                                          \n                                                          )\n        else:\n            self.test_transform = {m: wrap_in_compose_is_list(test_transform)\n                                    for m in modalities}\n\n        if self.concat_bands:\n            # Concatenate mean and std values\n            means = load_from_file_or_attribute(np.concatenate([means[m] for m in self.image_modalities]).tolist())\n            stds = load_from_file_or_attribute(np.concatenate([stds[m] for m in self.image_modalities]).tolist())\n\n            self.aug = Normalize(means, stds)\n        else:\n            # Apply standardization per modality\n            means = {m: load_from_file_or_attribute(means[m]) for m in means.keys()}\n            stds = {m: load_from_file_or_attribute(stds[m]) for m in stds.keys()}\n\n            self.aug = MultimodalNormalize(means, stds)\n\n        self.data_with_sample_dim = data_with_sample_dim\n\n        self.collate_fn = collate_chunk_dicts if data_with_sample_dim else collate_samples\n\n\n    def setup(self, stage: str) -&gt; None:\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                data_root=self.train_root,\n                num_classes=self.num_classes,\n                image_grep=self.image_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.train_label_data_root,\n                split=self.train_split,\n                allow_substring_file_names=self.allow_substring_file_names,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                image_modalities=self.image_modalities,\n                rgb_modality=self.rgb_modality,\n                rgb_indices=self.rgb_indices,\n                transform=self.train_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n                channel_position=self.channel_position,\n                data_with_sample_dim = self.data_with_sample_dim,\n                concat_bands=self.concat_bands,\n            )\n            logger.info(f\"Train dataset: {len(self.train_dataset)}\")\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                data_root=self.val_root,\n                num_classes=self.num_classes,\n                image_grep=self.image_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.val_label_data_root,\n                split=self.val_split,\n                allow_substring_file_names=self.allow_substring_file_names,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                image_modalities=self.image_modalities,\n                rgb_modality=self.rgb_modality,\n                rgb_indices=self.rgb_indices,\n                transform=self.val_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n                channel_position=self.channel_position,\n                data_with_sample_dim = self.data_with_sample_dim,\n                concat_bands=self.concat_bands,\n            )\n            logger.info(f\"Val dataset: {len(self.val_dataset)}\")\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                data_root=self.test_root,\n                num_classes=self.num_classes,\n                image_grep=self.image_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.test_label_data_root,\n                split=self.test_split,\n                allow_substring_file_names=self.allow_substring_file_names,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                image_modalities=self.image_modalities,\n                rgb_modality=self.rgb_modality,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n                channel_position=self.channel_position,\n                data_with_sample_dim = self.data_with_sample_dim,\n                concat_bands=self.concat_bands,\n            )\n            logger.info(f\"Test dataset: {len(self.test_dataset)}\")\n        if stage in [\"predict\"] and self.predict_root:\n            self.predict_dataset = self.dataset_class(\n                data_root=self.predict_root,\n                num_classes=self.num_classes,\n                image_grep=self.image_grep,\n                label_grep=self.label_grep,\n                allow_missing_modalities=True,\n                allow_substring_file_names=self.allow_substring_file_names,\n                dataset_bands=self.predict_dataset_bands,\n                output_bands=self.predict_output_bands,\n                constant_scale=self.constant_scale,\n                image_modalities=self.image_modalities,\n                rgb_modality=self.rgb_modality,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n                channel_position=self.channel_position,\n                data_with_sample_dim=self.data_with_sample_dim,\n                concat_bands=self.concat_bands,\n                prediction_mode=True,\n            )\n            logger.info(f\"Predict dataset: {len(self.predict_dataset)}\")\n\n    def _dataloader_factory(self, split: str) -&gt; DataLoader[dict[str, Tensor]]:\n        \"\"\"Implement one or more PyTorch DataLoaders.\n\n        Args:\n            split: Either \"train\", \"val\", \"test\", or \"predict\".\n\n        Returns:\n            A collection of data loaders specifying samples.\n\n        Raises:\n            MisconfigurationException: If :meth:`setup` does not define a\n                dataset or sampler, or if the dataset or sampler has length 0.\n        \"\"\"\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n\n        if self.check_stackability:\n            logger.info(f'Checking dataset stackability for {split} split')\n            if self.concat_bands:\n                batch_size = check_dataset_stackability(dataset, batch_size)\n            else:\n                batch_size = check_dataset_stackability_dict(dataset, batch_size)\n\n        if self.sample_num_modalities:\n            # Custom batch sampler for sampling modalities per batch\n            batch_sampler = MultiModalBatchSampler(\n                self.modalities, self.sample_num_modalities, self.sample_replace,\n                RandomSampler(dataset) if split == \"train\" else SequentialSampler(dataset),\n                batch_size=batch_size,\n                drop_last=split == \"train\" and self.drop_last\n            )\n        else:\n            batch_sampler = BatchSampler(\n                RandomSampler(dataset) if split == \"train\" else SequentialSampler(dataset),\n                batch_size=batch_size,\n                drop_last=split == \"train\" and self.drop_last\n            )\n\n        return DataLoader(\n            dataset=dataset,\n            batch_sampler=batch_sampler,\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            pin_memory=self.pin_memory,\n        )\n</code></pre>"},{"location":"package/generic_datamodules/#terratorch.datamodules.generic_multimodal_data_module.GenericMultiModalDataModule.__init__","title":"<code>__init__(batch_size, modalities, train_data_root, val_data_root, test_data_root, means, stds, task=None, num_classes=None, image_grep=None, label_grep=None, train_label_data_root=None, val_label_data_root=None, test_label_data_root=None, predict_data_root=None, train_split=None, val_split=None, test_split=None, dataset_bands=None, output_bands=None, predict_dataset_bands=None, predict_output_bands=None, image_modalities=None, rgb_modality=None, rgb_indices=None, allow_substring_file_names=True, class_names=None, constant_scale=None, train_transform=None, val_transform=None, test_transform=None, shared_transforms=True, expand_temporal_dimension=False, no_data_replace=None, no_label_replace=-1, reduce_zero_label=False, drop_last=True, num_workers=0, pin_memory=False, data_with_sample_dim=False, sample_num_modalities=None, sample_replace=False, channel_position=-3, concat_bands=False, check_stackability=True, img_grep=None)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of samples in per batch.</p> required <code>modalities</code> <code>list[str]</code> <p>List of modalities.</p> required <code>train_data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to training data root directory or csv/parquet files with  image-level data, with modalities as keys.</p> required <code>val_data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to validation data root directory or csv/parquet files with  image-level data, with modalities as keys.</p> required <code>test_data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to test data root directory or csv/parquet files with  image-level data, with modalities as keys.</p> required <code>means</code> <code>dict[list]</code> <p>Dictionary of mean values as lists with modalities as keys.</p> required <code>stds</code> <code>dict[list]</code> <p>Dictionary of std values as lists with modalities as keys.</p> required <code>task</code> <code>str</code> <p>Selected task form segmentation, regression (pixel-wise), classification, multilabel_classification, scalar_regression, scalar (custom image-level task), or None (no targets). Defaults to None.</p> <code>None</code> <code>num_classes</code> <code>int</code> <p>Number of classes in classification or segmentation tasks.</p> <code>None</code> <code>predict_data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to data root directory or csv/parquet files with image-level data, with modalities as keys.</p> <code>None</code> <code>image_grep</code> <code>dict[str]</code> <p>Dictionary with regular expression appended to data_root to find input images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>None</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to label_data_root to find labels or mask files. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>None</code> <code>train_label_data_root</code> <code>Path | None</code> <p>Path to data root directory with training labels or csv/parquet files with labels. Required for supervised tasks.</p> <code>None</code> <code>val_label_data_root</code> <code>Path | None</code> <p>Path to data root directory with validation labels or csv/parquet files with labels. Required for supervised tasks.</p> <code>None</code> <code>test_label_data_root</code> <code>Path | None</code> <p>Path to data root directory with test labels or csv/parquet files with labels. Required for supervised tasks.</p> <code>None</code> <code>train_split</code> <code>Path</code> <p>Path to file containing training samples prefixes to be used for this split. The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise, files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]). If not specified, search samples based on files in data_root. Defaults to None.</p> <code>None</code> <code>val_split</code> <code>Path</code> <p>Path to file containing validation samples prefixes to be used for this split. The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise, files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]). If not specified, search samples based on files in data_root. Defaults to None.</p> <code>None</code> <code>test_split</code> <code>Path</code> <p>Path to file containing test samples prefixes to be used for this split. The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise, files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]). If not specified, search samples based on files in data_root. Defaults to None.</p> <code>None</code> <code>dataset_bands</code> <code>dict[list]</code> <p>Bands present in the dataset, provided in a dictionary with modalities as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset of all modalities. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>dict[list]</code> <p>Bands that should be output by the dataset as named by dataset_bands, provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.</p> <code>None</code> <code>predict_dataset_bands</code> <code>list[dict]</code> <p>Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>predict_output_bands</code> <code>list[dict]</code> <p>Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>image_modalities(list[str],</code> <code>optional</code> <p>List of pixel-level raster modalities. Defaults to data_root.keys(). The difference between all modalities and image_modalities are non-image modalities which are treated differently during the transforms and are not modified but only converted into a tensor if possible.</p> required <code>rgb_modality</code> <code>str</code> <p>Modality used for RGB plots. Defaults to first modality in data_root.keys().</p> <code>None</code> <code>rgb_indices</code> <code>list[int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>allow_substring_file_names</code> <code>bool</code> <p>Allow substrings during sample identification by adding image or label grep to the sample prefixes. If False, treats sample prefixes as full file names. If True and no split file is provided, considers the file stem as prefix, otherwise the full file name. Defaults to True.</p> <code>True</code> <code>class_names</code> <code>list[str]</code> <p>Names of the classes. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>dict[float]</code> <p>Factor to multiply data values by, provided as a dictionary with modalities as keys. Can be subset of all modalities. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | dict | None</code> <p>Albumentations transform to be applied to all image modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to  non-image data, which is only converted to tensors if possible. If dict, can include separate transforms  per modality (no shared parameters between modalities).  Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>val_transform</code> <code>Compose | dict | None</code> <p>Albumentations transform to be applied to all image modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to  non-image data, which is only converted to tensors if possible. If dict, can include separate transforms  per modality (no shared parameters between modalities).  Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>test_transform</code> <code>Compose | dict | None</code> <p>Albumentations transform to be applied to all image modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to  non-image data, which is only converted to tensors if possible. If dict, can include separate transforms  per modality (no shared parameters between modalities).  Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>shared_transforms</code> <code>bool</code> <p>transforms are shared between all image modalities (e.g., similar crop).  This setting is ignored if transforms are defined per modality. Defaults to True.  </p> <code>True</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Only works with image modalities. Is only applied to modalities with defined dataset_bands. Defaults to False.</p> <code>False</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no  replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement.  Defaults to None.</p> <code>-1</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Drop the last batch if it is not complete. Defaults to True.</p> <code>True</code> <code>num_workers</code> <code>int</code> <p>Number of parallel workers. Defaults to 0 for single threaded process.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>If <code>True</code>, the data loader will copy Tensors into device/CUDA pinned memory before  returning them. Defaults to False.</p> <code>False</code> <code>data_with_sample_dim</code> <code>bool</code> <p>Use a specific collate function to concatenate samples along a existing sample dimension instead of stacking the samples. Defaults to False.</p> <code>False</code> <code>sample_num_modalities</code> <code>int</code> <p>Load only a subset of modalities per batch. Defaults to None.</p> <code>None</code> <code>sample_replace</code> <code>bool</code> <p>If sample_num_modalities is set, sample modalities with replacement. Defaults to False.</p> <code>False</code> <code>channel_position</code> <code>int</code> <p>Position of the channel dimension in the image modalities. Defaults to -3.</p> <code>-3</code> <code>concat_bands</code> <code>bool</code> <p>Concatenate all image modalities along the band dimension into a single \"image\", so that it can be processed by single-modal models. Concatenate in the order of provided modalities. Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.</p> <code>False</code> <code>check_stackability</code> <code>bool</code> <p>Check if all the files in the dataset has the same size and can be stacked.</p> <code>True</code> Source code in <code>terratorch/datamodules/generic_multimodal_data_module.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    modalities: list[str],\n    train_data_root: dict[str, Path],\n    val_data_root: dict[str, Path],\n    test_data_root: dict[str, Path],\n    means: dict[str, list],\n    stds: dict[str, list],\n    task: str | None = None,\n    num_classes: int | None = None,\n    image_grep: str | dict[str, str] | None = None,\n    label_grep: str | None = None,\n    train_label_data_root: Path | str | None = None,\n    val_label_data_root: Path | str | None = None,\n    test_label_data_root: Path | str | None = None,\n    predict_data_root: dict[str, Path] | str | None = None,\n    train_split: Path | str | None = None,\n    val_split: Path | str | None = None,\n    test_split: Path| str | None = None,\n    dataset_bands: dict[str, list] | None = None,\n    output_bands: dict[str, list] | None = None,\n    predict_dataset_bands: dict[str, list] | None = None,\n    predict_output_bands: dict[str, list] | None = None,\n    image_modalities: list[str] | None = None,\n    rgb_modality: str | None = None,\n    rgb_indices: list[int] | None = None,\n    allow_substring_file_names: bool = True,\n    class_names: list[str] | None = None,\n    constant_scale: dict[float] = None,\n    train_transform: dict | A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: dict | A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: dict | A.Compose | None | list[A.BasicTransform] = None,\n    shared_transforms: list | bool = True,\n    expand_temporal_dimension: bool = False,\n    no_data_replace: float | None = None,\n    no_label_replace: float | None = -1,\n    reduce_zero_label: bool = False,\n    drop_last: bool = True,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    data_with_sample_dim: bool = False,\n    sample_num_modalities: int | None = None,\n    sample_replace: bool = False,\n    channel_position: int = -3,\n    concat_bands: bool = False,\n    check_stackability: bool = True,\n    img_grep: str | dict[str, str] | None = None,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        batch_size (int): Number of samples in per batch.\n        modalities (list[str]): List of modalities.\n        train_data_root (dict[Path]): Dictionary of paths to training data root directory or csv/parquet files with \n            image-level data, with modalities as keys.\n        val_data_root (dict[Path]): Dictionary of paths to validation data root directory or csv/parquet files with \n            image-level data, with modalities as keys.\n        test_data_root (dict[Path]): Dictionary of paths to test data root directory or csv/parquet files with \n            image-level data, with modalities as keys.\n        means (dict[list]): Dictionary of mean values as lists with modalities as keys.\n        stds (dict[list]): Dictionary of std values as lists with modalities as keys.\n        task (str, optional): Selected task form segmentation, regression (pixel-wise), classification,\n            multilabel_classification, scalar_regression, scalar (custom image-level task), or None (no targets).\n            Defaults to None.\n        num_classes (int, optional): Number of classes in classification or segmentation tasks.\n        predict_data_root (dict[Path], optional): Dictionary of paths to data root directory or csv/parquet files\n            with image-level data, with modalities as keys.\n        image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n            images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        label_grep (str, optional): Regular expression appended to label_data_root to find labels or mask files.\n            Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        train_label_data_root (Path | None, optional): Path to data root directory with training labels or\n            csv/parquet files with labels. Required for supervised tasks.\n        val_label_data_root (Path | None, optional): Path to data root directory with validation labels or\n            csv/parquet files with labels. Required for supervised tasks.\n        test_label_data_root (Path | None, optional): Path to data root directory with test labels or\n            csv/parquet files with labels. Required for supervised tasks.\n        train_split (Path, optional): Path to file containing training samples prefixes to be used for this split.\n            The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n            sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n            files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n            If not specified, search samples based on files in data_root. Defaults to None.\n        val_split (Path, optional): Path to file containing validation samples prefixes to be used for this split.\n            The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n            sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n            files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n            If not specified, search samples based on files in data_root. Defaults to None.\n        test_split (Path, optional): Path to file containing test samples prefixes to be used for this split.\n            The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n            sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n            files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n            If not specified, search samples based on files in data_root. Defaults to None.\n        dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n            as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n            that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n            of all modalities. Defaults to None.\n        output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n            provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n        predict_dataset_bands (list[dict], optional): Overwrites dataset_bands with this value at predict time.\n            Defaults to None, which does not overwrite.\n        predict_output_bands (list[dict], optional): Overwrites output_bands with this value at predict time.\n            Defaults to None, which does not overwrite.\n        image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n            The difference between all modalities and image_modalities are non-image modalities which are treated\n            differently during the transforms and are not modified but only converted into a tensor if possible.\n        rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n        rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n        allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n            image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n            If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n            Defaults to True.\n        class_names (list[str], optional): Names of the classes. Defaults to None.\n        constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n            keys. Can be subset of all modalities. Defaults to None.\n        train_transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n            modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to \n            non-image data, which is only converted to tensors if possible. If dict, can include separate transforms \n            per modality (no shared parameters between modalities). \n            Defaults to None, which simply applies ToTensorV2().\n        val_transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n            modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to \n            non-image data, which is only converted to tensors if possible. If dict, can include separate transforms \n            per modality (no shared parameters between modalities). \n            Defaults to None, which simply applies ToTensorV2().\n        test_transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n            modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to \n            non-image data, which is only converted to tensors if possible. If dict, can include separate transforms \n            per modality (no shared parameters between modalities). \n            Defaults to None, which simply applies ToTensorV2().\n        shared_transforms (bool): transforms are shared between all image modalities (e.g., similar crop). \n            This setting is ignored if transforms are defined per modality. Defaults to True.  \n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n            Defaults to False.\n        no_data_replace (float | None): Replace nan values in input images with this value. If none, does no \n            replacement. Defaults to None.\n        no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. \n            Defaults to None.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n        drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n        num_workers (int): Number of parallel workers. Defaults to 0 for single threaded process.\n        pin_memory (bool): If ``True``, the data loader will copy Tensors into device/CUDA pinned memory before \n            returning them. Defaults to False.\n        data_with_sample_dim (bool): Use a specific collate function to concatenate samples along a existing sample\n            dimension instead of stacking the samples. Defaults to False.\n        sample_num_modalities (int, optional): Load only a subset of modalities per batch. Defaults to None.\n        sample_replace (bool): If sample_num_modalities is set, sample modalities with replacement.\n            Defaults to False.\n        channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n        concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n            that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n            Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n        check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked.\n    \"\"\"\n\n    if task == \"segmentation\":\n        dataset_class = GenericMultimodalSegmentationDataset\n    elif task == \"regression\":\n        dataset_class = GenericMultimodalPixelwiseRegressionDataset\n    elif task in [\"classification\", \"multilabel_classification\", \"scalar_regression\", \"scalar\"]:\n        dataset_class = GenericMultimodalScalarDataset\n        task = \"scalar\"\n    elif task is None:\n        dataset_class = GenericMultimodalDataset\n    else:\n        raise ValueError(f\"Unknown task {task}, only segmentation and regression are supported.\")\n\n    super().__init__(dataset_class, batch_size, num_workers)\n    self.num_classes = num_classes\n    self.class_names = class_names\n    self.modalities = modalities\n    self.image_modalities = image_modalities or modalities\n    self.non_image_modalities = list(set(self.modalities) - set(self.image_modalities))\n    if task == \"scalar\":\n        self.non_image_modalities += [\"label\"]\n\n    if img_grep is not None:\n        warnings.warn(f'img_grep was renamed to image_grep and will be removed in a future version.',\n                      DeprecationWarning)\n        image_grep = img_grep\n\n    if isinstance(image_grep, dict):\n        self.image_grep = {m: image_grep[m] if m in image_grep else \"*\" for m in modalities}\n    else:\n        self.image_grep = {m: image_grep or \"*\" for m in modalities}\n    self.label_grep = label_grep or \"*\"\n    self.train_root = train_data_root\n    self.val_root = val_data_root\n    self.test_root = test_data_root\n    self.train_label_data_root = train_label_data_root\n    self.val_label_data_root = val_label_data_root\n    self.test_label_data_root = test_label_data_root\n    self.predict_root = predict_data_root\n\n    assert not train_data_root or all(m in train_data_root for m in modalities), \\\n        f\"predict_data_root is missing paths to some modalities {modalities}: {train_data_root}\"\n    assert not val_data_root or all(m in val_data_root for m in modalities), \\\n        f\"predict_data_root is missing paths to some modalities {modalities}: {val_data_root}\"\n    assert not test_data_root or all(m in test_data_root for m in modalities), \\\n        f\"predict_data_root is missing paths to some modalities {modalities}: {test_data_root}\"\n    assert not predict_data_root or all(m in predict_data_root for m in modalities), \\\n        f\"predict_data_root is missing paths to some modalities {modalities}: {predict_data_root}\"\n\n    self.train_split = train_split\n    self.val_split = val_split\n    self.test_split = test_split\n    self.allow_substring_file_names = allow_substring_file_names\n    self.constant_scale = constant_scale\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.drop_last = drop_last\n    self.pin_memory = pin_memory\n    self.sample_num_modalities = sample_num_modalities\n    self.sample_replace = sample_replace        \n\n    self.dataset_bands = dataset_bands\n    self.output_bands = output_bands\n    self.predict_dataset_bands = predict_dataset_bands\n    self.predict_output_bands = predict_output_bands\n\n    self.rgb_modality = rgb_modality or modalities[0]\n    self.rgb_indices = rgb_indices\n    self.expand_temporal_dimension = expand_temporal_dimension\n    self.reduce_zero_label = reduce_zero_label\n    self.channel_position = channel_position\n    self.concat_bands = concat_bands\n    if not concat_bands and check_stackability:\n        logger.debug(f\"Cannot check stackability if bands are not concatenated.\")\n    self.check_stackability = check_stackability\n\n    if isinstance(train_transform, dict):\n        self.train_transform = {m: wrap_in_compose_is_list(train_transform[m]) if m in train_transform else None\n                                for m in modalities}\n    elif shared_transforms:\n        self.train_transform = wrap_in_compose_is_list(train_transform,\n                                                       image_modalities=self.image_modalities,\n                                                       non_image_modalities=self.non_image_modalities)\n    else:\n        self.train_transform = {m: wrap_in_compose_is_list(train_transform)\n                                for m in modalities}\n\n    if isinstance(val_transform, dict):\n        self.val_transform = {m: wrap_in_compose_is_list(val_transform[m]) if m in val_transform else None\n                                for m in modalities}\n    elif shared_transforms:\n        self.val_transform = wrap_in_compose_is_list(val_transform,\n                                                     image_modalities=self.image_modalities,\n                                                     non_image_modalities=self.non_image_modalities)\n    else:\n        self.val_transform = {m: wrap_in_compose_is_list(val_transform)\n                                for m in modalities}\n\n    if isinstance(test_transform, dict):\n        self.test_transform = {m: wrap_in_compose_is_list(test_transform[m]) if m in test_transform else None\n                                for m in modalities}\n    elif shared_transforms:\n        self.test_transform = wrap_in_compose_is_list(test_transform,\n                                                      image_modalities=self.image_modalities,\n                                                      non_image_modalities=self.non_image_modalities,                                                          \n                                                      )\n    else:\n        self.test_transform = {m: wrap_in_compose_is_list(test_transform)\n                                for m in modalities}\n\n    if self.concat_bands:\n        # Concatenate mean and std values\n        means = load_from_file_or_attribute(np.concatenate([means[m] for m in self.image_modalities]).tolist())\n        stds = load_from_file_or_attribute(np.concatenate([stds[m] for m in self.image_modalities]).tolist())\n\n        self.aug = Normalize(means, stds)\n    else:\n        # Apply standardization per modality\n        means = {m: load_from_file_or_attribute(means[m]) for m in means.keys()}\n        stds = {m: load_from_file_or_attribute(stds[m]) for m in stds.keys()}\n\n        self.aug = MultimodalNormalize(means, stds)\n\n    self.data_with_sample_dim = data_with_sample_dim\n\n    self.collate_fn = collate_chunk_dicts if data_with_sample_dim else collate_samples\n</code></pre>"},{"location":"package/generic_datasets/","title":"Generic Datasets","text":""},{"location":"package/generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoSegmentationDataset","title":"<code>terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoSegmentationDataset</code>","text":"<p>               Bases: <code>GenericPixelWiseDataset</code></p> <p>GenericNonGeoSegmentationDataset</p> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>class GenericNonGeoSegmentationDataset(GenericPixelWiseDataset):\n    \"\"\"GenericNonGeoSegmentationDataset\"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        num_classes: int,\n        label_data_root: Path | None = None,\n        image_grep: str | None = \"*\",\n        label_grep: str | None = \"*\",\n        split: Path | None = None,\n        ignore_split_file_extensions: bool = True,\n        allow_substring_split_file: bool = True,\n        rgb_indices: list[str] | None = None,\n        dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        class_names: list[str] | None = None,\n        constant_scale: float = 1,\n        transform: A.Compose | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = None,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (Path): Path to data root directory\n            num_classes (int): Number of classes in the dataset\n            label_data_root (Path, optional): Path to data root directory with labels.\n                If not specified, will use the same as for images.\n            image_grep (str, optional): Regular expression appended to data_root to find input images.\n                Defaults to \"*\".\n            label_grep (str, optional): Regular expression appended to data_root to find ground truth masks.\n                Defaults to \"*\".\n            split (Path, optional): Path to file containing files to be used for this split.\n                The file should be a new-line separated prefixes contained in the desired files.\n                Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\". Defaults to True\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            dataset_bands (list[HLSBands | int] | None): Bands present in the dataset.\n            output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n            class_names (list[str], optional): Class names. Defaults to None.\n            constant_scale (float): Factor to multiply image values by. Defaults to 1.\n            transform (Albumentations.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n            no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n        \"\"\"\n        super().__init__(\n            data_root,\n            label_data_root=label_data_root,\n            image_grep=image_grep,\n            label_grep=label_grep,\n            split=split,\n            ignore_split_file_extensions=ignore_split_file_extensions,\n            allow_substring_split_file=allow_substring_split_file,\n            rgb_indices=rgb_indices,\n            dataset_bands=dataset_bands,\n            output_bands=output_bands,\n            constant_scale=constant_scale,\n            transform=transform,\n            no_data_replace=no_data_replace,\n            no_label_replace=no_label_replace,\n            expand_temporal_dimension=expand_temporal_dimension,\n            reduce_zero_label=reduce_zero_label,\n        )\n        self.num_classes = num_classes\n        self.class_names = class_names\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        item = super().__getitem__(index)\n        if \"mask\" in item:\n            item[\"mask\"] = item[\"mask\"].long()\n        return item\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None, show_axes: bool | None = False) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            suptitle: optional string to use as a suptitle\n            show_axes: whether to show axes or not\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n\n        .. versionadded:: 0.2\n        \"\"\"\n        image = sample[\"image\"]\n        if len(image.shape) == 5:\n            return\n        if isinstance(image, Tensor):\n            image = image.numpy()\n        image = image.take(self.rgb_indices, axis=0)\n        image = np.transpose(image, (1, 2, 0))\n        image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n        image = np.clip(image, 0, 1)\n\n        label_mask = sample[\"mask\"]\n        if isinstance(label_mask, Tensor):\n            label_mask = label_mask.numpy()\n\n        showing_predictions = \"prediction\" in sample\n        if showing_predictions:\n            prediction_mask = sample[\"prediction\"]\n            if isinstance(prediction_mask, Tensor):\n                prediction_mask = prediction_mask.numpy()\n\n        return self._plot_sample(\n            image,\n            label_mask,\n            self.num_classes,\n            prediction=prediction_mask if showing_predictions else None,\n            suptitle=suptitle,\n            class_names=self.class_names,\n            show_axes=show_axes,\n        )\n\n    @staticmethod\n    def _plot_sample(image, label, num_classes, prediction=None, suptitle=None, class_names=None, show_axes=False):\n        num_images = 5 if prediction is not None else 4\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 10), layout=\"compressed\")\n        axes_visibility = \"on\" if show_axes else \"off\"\n\n        # for legend\n        ax[0].axis(\"off\")\n\n        norm = mpl.colors.Normalize(vmin=0, vmax=num_classes - 1)\n        ax[1].axis(axes_visibility)\n        ax[1].title.set_text(\"Image\")\n        ax[1].imshow(image)\n\n        ax[2].axis(axes_visibility)\n        ax[2].title.set_text(\"Ground Truth Mask\")\n        ax[2].imshow(label, cmap=\"jet\", norm=norm)\n\n        ax[3].axis(axes_visibility)\n        ax[3].title.set_text(\"GT Mask on Image\")\n        ax[3].imshow(image)\n        ax[3].imshow(label, cmap=\"jet\", alpha=0.3, norm=norm)\n\n        if prediction is not None:\n            ax[4].axis(axes_visibility)\n            ax[4].title.set_text(\"Predicted Mask\")\n            ax[4].imshow(prediction, cmap=\"jet\", norm=norm)\n\n        cmap = plt.get_cmap(\"jet\")\n        legend_data = []\n        for i, _ in enumerate(range(num_classes)):\n            class_name = class_names[i] if class_names else str(i)\n            data = [i, cmap(norm(i)), class_name]\n            legend_data.append(data)\n        handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n        labels = [n for k, c, n in legend_data]\n        ax[0].legend(handles, labels, loc=\"center\")\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n        return fig\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoSegmentationDataset.__init__","title":"<code>__init__(data_root, num_classes, label_data_root=None, image_grep='*', label_grep='*', split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, class_names=None, constant_scale=1, transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=False, reduce_zero_label=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>num_classes</code> <code>int</code> <p>Number of classes in the dataset</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with labels. If not specified, will use the same as for images.</p> <code>None</code> <code>image_grep</code> <code>str</code> <p>Regular expression appended to data_root to find input images. Defaults to \"*\".</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to data_root to find ground truth masks. Defaults to \"*\".</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset.</p> <code>None</code> <code>class_names</code> <code>list[str]</code> <p>Class names. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    num_classes: int,\n    label_data_root: Path | None = None,\n    image_grep: str | None = \"*\",\n    label_grep: str | None = \"*\",\n    split: Path | None = None,\n    ignore_split_file_extensions: bool = True,\n    allow_substring_split_file: bool = True,\n    rgb_indices: list[str] | None = None,\n    dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    class_names: list[str] | None = None,\n    constant_scale: float = 1,\n    transform: A.Compose | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = None,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (Path): Path to data root directory\n        num_classes (int): Number of classes in the dataset\n        label_data_root (Path, optional): Path to data root directory with labels.\n            If not specified, will use the same as for images.\n        image_grep (str, optional): Regular expression appended to data_root to find input images.\n            Defaults to \"*\".\n        label_grep (str, optional): Regular expression appended to data_root to find ground truth masks.\n            Defaults to \"*\".\n        split (Path, optional): Path to file containing files to be used for this split.\n            The file should be a new-line separated prefixes contained in the desired files.\n            Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\". Defaults to True\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        dataset_bands (list[HLSBands | int] | None): Bands present in the dataset.\n        output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n        class_names (list[str], optional): Class names. Defaults to None.\n        constant_scale (float): Factor to multiply image values by. Defaults to 1.\n        transform (Albumentations.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n        no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n    \"\"\"\n    super().__init__(\n        data_root,\n        label_data_root=label_data_root,\n        image_grep=image_grep,\n        label_grep=label_grep,\n        split=split,\n        ignore_split_file_extensions=ignore_split_file_extensions,\n        allow_substring_split_file=allow_substring_split_file,\n        rgb_indices=rgb_indices,\n        dataset_bands=dataset_bands,\n        output_bands=output_bands,\n        constant_scale=constant_scale,\n        transform=transform,\n        no_data_replace=no_data_replace,\n        no_label_replace=no_label_replace,\n        expand_temporal_dimension=expand_temporal_dimension,\n        reduce_zero_label=reduce_zero_label,\n    )\n    self.num_classes = num_classes\n    self.class_names = class_names\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoSegmentationDataset.plot","title":"<code>plot(sample, suptitle=None, show_axes=False)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <code>show_axes</code> <code>bool | None</code> <p>whether to show axes or not</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> <p>.. versionadded:: 0.2</p> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>def plot(self, sample: dict[str, Tensor], suptitle: str | None = None, show_axes: bool | None = False) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        suptitle: optional string to use as a suptitle\n        show_axes: whether to show axes or not\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n\n    .. versionadded:: 0.2\n    \"\"\"\n    image = sample[\"image\"]\n    if len(image.shape) == 5:\n        return\n    if isinstance(image, Tensor):\n        image = image.numpy()\n    image = image.take(self.rgb_indices, axis=0)\n    image = np.transpose(image, (1, 2, 0))\n    image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n    image = np.clip(image, 0, 1)\n\n    label_mask = sample[\"mask\"]\n    if isinstance(label_mask, Tensor):\n        label_mask = label_mask.numpy()\n\n    showing_predictions = \"prediction\" in sample\n    if showing_predictions:\n        prediction_mask = sample[\"prediction\"]\n        if isinstance(prediction_mask, Tensor):\n            prediction_mask = prediction_mask.numpy()\n\n    return self._plot_sample(\n        image,\n        label_mask,\n        self.num_classes,\n        prediction=prediction_mask if showing_predictions else None,\n        suptitle=suptitle,\n        class_names=self.class_names,\n        show_axes=show_axes,\n    )\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoPixelwiseRegressionDataset","title":"<code>terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoPixelwiseRegressionDataset</code>","text":"<p>               Bases: <code>GenericPixelWiseDataset</code></p> <p>GenericNonGeoPixelwiseRegressionDataset</p> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>class GenericNonGeoPixelwiseRegressionDataset(GenericPixelWiseDataset):\n    \"\"\"GenericNonGeoPixelwiseRegressionDataset\"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        label_data_root: Path | None = None,\n        image_grep: str | None = \"*\",\n        label_grep: str | None = \"*\",\n        split: Path | None = None,\n        ignore_split_file_extensions: bool = True,\n        allow_substring_split_file: bool = True,\n        rgb_indices: list[int] | None = None,\n        dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        constant_scale: float = 1,\n        transform: A.Compose | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = None,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (Path): Path to data root directory\n            label_data_root (Path, optional): Path to data root directory with labels.\n                If not specified, will use the same as for images.\n            image_grep (str, optional): Regular expression appended to data_root to find input images.\n                Defaults to \"*\".\n            label_grep (str, optional): Regular expression appended to data_root to find ground truth masks.\n                Defaults to \"*\".\n            split (Path, optional): Path to file containing files to be used for this split.\n                The file should be a new-line separated prefixes contained in the desired files.\n                Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\". Defaults to True.\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            dataset_bands (list[HLSBands | int] | None): Bands present in the dataset.\n            output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n            constant_scale (float): Factor to multiply image values by. Defaults to 1.\n            transform (Albumentations.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n            no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n        \"\"\"\n        super().__init__(\n            data_root,\n            label_data_root=label_data_root,\n            image_grep=image_grep,\n            label_grep=label_grep,\n            split=split,\n            ignore_split_file_extensions=ignore_split_file_extensions,\n            allow_substring_split_file=allow_substring_split_file,\n            rgb_indices=rgb_indices,\n            dataset_bands=dataset_bands,\n            output_bands=output_bands,\n            constant_scale=constant_scale,\n            transform=transform,\n            no_data_replace=no_data_replace,\n            no_label_replace=no_label_replace,\n            expand_temporal_dimension=expand_temporal_dimension,\n            reduce_zero_label=reduce_zero_label,\n        )\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        item = super().__getitem__(index)\n        if \"mask\" in item:\n            item[\"mask\"] = item[\"mask\"].float()\n        return item\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None, show_axes: bool | None = False) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__`\n            suptitle (str|None): optional string to use as a suptitle\n            show_axes (bool|None): whether to show axes or not\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n\n        .. versionadded:: 0.2\n        \"\"\"\n        image = sample[\"image\"]\n        if len(image.shape) == 5:\n            return\n        if isinstance(image, Tensor):\n            image = image.numpy()\n        image = image.take(self.rgb_indices, axis=0)\n        image = np.transpose(image, (1, 2, 0))\n        image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n        image = np.clip(image, 0, 1)\n\n        label_mask = sample[\"mask\"]\n        if isinstance(label_mask, Tensor):\n            label_mask = label_mask.numpy()\n\n        showing_predictions = \"prediction\" in sample\n        if showing_predictions:\n            prediction_mask = sample[\"prediction\"]\n            if isinstance(prediction_mask, Tensor):\n                prediction_mask = prediction_mask.numpy()\n\n        return self._plot_sample(\n            image,\n            label_mask,\n            prediction=prediction_mask if showing_predictions else None,\n            suptitle=suptitle,\n            show_axes=show_axes,\n        )\n\n    @staticmethod\n    def _plot_sample(image, label, prediction=None, suptitle=None, show_axes=False):\n        num_images = 4 if prediction is not None else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 10), layout=\"compressed\")\n        axes_visibility = \"on\" if show_axes else \"off\"\n\n        norm = mpl.colors.Normalize(vmin=label.min(), vmax=label.max())\n        ax[0].axis(axes_visibility)\n        ax[0].title.set_text(\"Image\")\n        ax[0].imshow(image)\n\n        ax[1].axis(axes_visibility)\n        ax[1].title.set_text(\"Ground Truth Mask\")\n        ax[1].imshow(label, cmap=\"Greens\", norm=norm)\n\n        ax[2].axis(axes_visibility)\n        ax[2].title.set_text(\"GT Mask on Image\")\n        ax[2].imshow(image)\n        ax[2].imshow(label, cmap=\"Greens\", alpha=0.3, norm=norm)\n        # ax[2].legend()\n\n        if prediction is not None:\n            ax[3].axis(axes_visibility)\n            ax[3].title.set_text(\"Predicted Mask\")\n            ax[3].imshow(prediction, cmap=\"Greens\", norm=norm)\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n        return fig\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoPixelwiseRegressionDataset.__init__","title":"<code>__init__(data_root, label_data_root=None, image_grep='*', label_grep='*', split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, constant_scale=1, transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=False, reduce_zero_label=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with labels. If not specified, will use the same as for images.</p> <code>None</code> <code>image_grep</code> <code>str</code> <p>Regular expression appended to data_root to find input images. Defaults to \"*\".</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to data_root to find ground truth masks. Defaults to \"*\".</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    label_data_root: Path | None = None,\n    image_grep: str | None = \"*\",\n    label_grep: str | None = \"*\",\n    split: Path | None = None,\n    ignore_split_file_extensions: bool = True,\n    allow_substring_split_file: bool = True,\n    rgb_indices: list[int] | None = None,\n    dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    constant_scale: float = 1,\n    transform: A.Compose | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = None,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (Path): Path to data root directory\n        label_data_root (Path, optional): Path to data root directory with labels.\n            If not specified, will use the same as for images.\n        image_grep (str, optional): Regular expression appended to data_root to find input images.\n            Defaults to \"*\".\n        label_grep (str, optional): Regular expression appended to data_root to find ground truth masks.\n            Defaults to \"*\".\n        split (Path, optional): Path to file containing files to be used for this split.\n            The file should be a new-line separated prefixes contained in the desired files.\n            Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\". Defaults to True.\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        dataset_bands (list[HLSBands | int] | None): Bands present in the dataset.\n        output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n        constant_scale (float): Factor to multiply image values by. Defaults to 1.\n        transform (Albumentations.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n        no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n    \"\"\"\n    super().__init__(\n        data_root,\n        label_data_root=label_data_root,\n        image_grep=image_grep,\n        label_grep=label_grep,\n        split=split,\n        ignore_split_file_extensions=ignore_split_file_extensions,\n        allow_substring_split_file=allow_substring_split_file,\n        rgb_indices=rgb_indices,\n        dataset_bands=dataset_bands,\n        output_bands=output_bands,\n        constant_scale=constant_scale,\n        transform=transform,\n        no_data_replace=no_data_replace,\n        no_label_replace=no_label_replace,\n        expand_temporal_dimension=expand_temporal_dimension,\n        reduce_zero_label=reduce_zero_label,\n    )\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoPixelwiseRegressionDataset.plot","title":"<code>plot(sample, suptitle=None, show_axes=False)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <code>show_axes</code> <code>bool | None</code> <p>whether to show axes or not</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> <p>.. versionadded:: 0.2</p> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>def plot(self, sample: dict[str, Tensor], suptitle: str | None = None, show_axes: bool | None = False) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__`\n        suptitle (str|None): optional string to use as a suptitle\n        show_axes (bool|None): whether to show axes or not\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n\n    .. versionadded:: 0.2\n    \"\"\"\n    image = sample[\"image\"]\n    if len(image.shape) == 5:\n        return\n    if isinstance(image, Tensor):\n        image = image.numpy()\n    image = image.take(self.rgb_indices, axis=0)\n    image = np.transpose(image, (1, 2, 0))\n    image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n    image = np.clip(image, 0, 1)\n\n    label_mask = sample[\"mask\"]\n    if isinstance(label_mask, Tensor):\n        label_mask = label_mask.numpy()\n\n    showing_predictions = \"prediction\" in sample\n    if showing_predictions:\n        prediction_mask = sample[\"prediction\"]\n        if isinstance(prediction_mask, Tensor):\n            prediction_mask = prediction_mask.numpy()\n\n    return self._plot_sample(\n        image,\n        label_mask,\n        prediction=prediction_mask if showing_predictions else None,\n        suptitle=suptitle,\n        show_axes=show_axes,\n    )\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericPixelWiseDataset","title":"<code>terratorch.datasets.generic_pixel_wise_dataset.GenericPixelWiseDataset</code>","text":"<p>               Bases: <code>NonGeoDataset</code>, <code>ABC</code></p> <p>This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset.</p> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>class GenericPixelWiseDataset(NonGeoDataset, ABC):\n    \"\"\"\n    This is a generic dataset class to be used for instantiating datasets from arguments.\n    Ideally, one would create a dataset class specific to a dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        label_data_root: Path | None = None,\n        image_grep: str | None = \"*\",\n        label_grep: str | None = \"*\",\n        split: Path | None = None,\n        ignore_split_file_extensions: bool = True,\n        allow_substring_split_file: bool = True,\n        rgb_indices: list[int] | None = None,\n        dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        constant_scale: float = 1,\n        transform: A.Compose | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = None,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (Path): Path to data root directory\n            label_data_root (Path, optional): Path to data root directory with labels.\n                If not specified, will use the same as for images.\n            image_grep (str, optional): Regular expression appended to data_root to find input images.\n                Defaults to \"*\".\n            label_grep (str, optional): Regular expression appended to data_root to find ground truth masks.\n                Defaults to \"*\".\n            split (Path, optional): Path to file containing files to be used for this split.\n                The file should be a new-line separated prefixes contained in the desired files.\n                Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\". Defaults to True.\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be refered to by output_bands. Defaults to None.\n            output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the dataset as named by dataset_bands.\n            constant_scale (float): Factor to multiply image values by. Defaults to 1.\n            transform (Albumentations.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n            no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n        \"\"\"\n        super().__init__()\n\n        self.split_file = split\n\n        label_data_root = label_data_root if label_data_root is not None else data_root\n        self.image_files = sorted(glob.glob(os.path.join(data_root, image_grep)))\n        self.constant_scale = constant_scale\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.segmentation_mask_files = sorted(glob.glob(os.path.join(label_data_root, label_grep)))\n        self.reduce_zero_label = reduce_zero_label\n        self.expand_temporal_dimension = expand_temporal_dimension\n\n        if self.expand_temporal_dimension and output_bands is None:\n            msg = \"Please provide output_bands when expand_temporal_dimension is True\"\n            raise Exception(msg)\n        if self.split_file is not None:\n            with open(self.split_file) as f:\n                split = f.readlines()\n            valid_files = {rf\"{substring.strip()}\" for substring in split}\n            self.image_files = filter_valid_files(\n                self.image_files,\n                valid_files=valid_files,\n                ignore_extensions=ignore_split_file_extensions,\n                allow_substring=allow_substring_split_file,\n            )\n            self.segmentation_mask_files = filter_valid_files(\n                self.segmentation_mask_files,\n                valid_files=valid_files,\n                ignore_extensions=ignore_split_file_extensions,\n                allow_substring=allow_substring_split_file,\n            )\n\n        # We don't define a split file for prediction\n        if not self.split_file:\n            # When prediction is enabled, we don't have mask files, so\n            # we need to provide a way to run the dataloder in these cases.\n            if not self.segmentation_mask_files:\n                self.segmentation_mask_files = self.image_files\n                # The masks can be `None` since they won't be used in fact. \n\n        self.rgb_indices = [0, 1, 2] if rgb_indices is None else rgb_indices\n\n        self.dataset_bands = generate_bands_intervals(dataset_bands)\n        self.output_bands = generate_bands_intervals(output_bands)\n\n        if self.output_bands and not self.dataset_bands:\n            msg = \"If output bands provided, dataset_bands must also be provided\"\n            return Exception(msg)  # noqa: PLE0101\n\n        # There is a special condition if the bands are defined as simple strings.\n        if self.output_bands:\n            if len(set(self.output_bands) &amp; set(self.dataset_bands)) != len(self.output_bands):\n                msg = \"Output bands must be a subset of dataset bands\"\n                raise Exception(msg)\n\n            self.filter_indices = [self.dataset_bands.index(band) for band in self.output_bands]\n\n        else:\n            self.filter_indices = None\n\n        # If no transform is given, apply only to transform to torch tensor\n        self.transform = transform if transform else default_transform\n        # self.transform = transform if transform else ToTensorV2()\n\n        import warnings\n\n        import rasterio\n\n        warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        image = self._load_file(self.image_files[index], nan_replace=self.no_data_replace).to_numpy()\n        # to channels last\n        if self.expand_temporal_dimension:\n            image = rearrange(image, \"(channels time) h w -&gt; channels time h w\", channels=len(self.output_bands))\n        image = np.moveaxis(image, 0, -1)\n\n        if self.filter_indices:\n            image = image[..., self.filter_indices]\n        output = {\n            \"image\": image.astype(np.float32) * self.constant_scale,\n        }\n        if self.segmentation_mask_files:\n            mask = self._load_file(self.segmentation_mask_files[index], nan_replace=self.no_label_replace)\n            output[\"mask\"] = mask.to_numpy()[0]\n            if self.reduce_zero_label:\n                output[\"mask\"] -= 1\n        if self.transform:\n            output = self.transform(**output)\n        output[\"filename\"] = self.image_files[index]\n\n        return output\n\n    def _load_file(self, path, nan_replace: int | float | None = None) -&gt; xr.DataArray:\n        data = rioxarray.open_rasterio(path, masked=True)\n        if nan_replace is not None:\n            data = data.fillna(nan_replace)\n        return data\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericPixelWiseDataset.__init__","title":"<code>__init__(data_root, label_data_root=None, image_grep='*', label_grep='*', split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, constant_scale=1, transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=False, reduce_zero_label=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with labels. If not specified, will use the same as for images.</p> <code>None</code> <code>image_grep</code> <code>str</code> <p>Regular expression appended to data_root to find input images. Defaults to \"*\".</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to data_root to find ground truth masks. Defaults to \"*\".</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int | tuple[int, int] | str] | None</code> <p>Bands present in the dataset. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be refered to by output_bands. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int | tuple[int, int] | str] | None</code> <p>Bands that should be output by the dataset as named by dataset_bands.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    label_data_root: Path | None = None,\n    image_grep: str | None = \"*\",\n    label_grep: str | None = \"*\",\n    split: Path | None = None,\n    ignore_split_file_extensions: bool = True,\n    allow_substring_split_file: bool = True,\n    rgb_indices: list[int] | None = None,\n    dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    constant_scale: float = 1,\n    transform: A.Compose | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = None,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (Path): Path to data root directory\n        label_data_root (Path, optional): Path to data root directory with labels.\n            If not specified, will use the same as for images.\n        image_grep (str, optional): Regular expression appended to data_root to find input images.\n            Defaults to \"*\".\n        label_grep (str, optional): Regular expression appended to data_root to find ground truth masks.\n            Defaults to \"*\".\n        split (Path, optional): Path to file containing files to be used for this split.\n            The file should be a new-line separated prefixes contained in the desired files.\n            Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\". Defaults to True.\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be refered to by output_bands. Defaults to None.\n        output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the dataset as named by dataset_bands.\n        constant_scale (float): Factor to multiply image values by. Defaults to 1.\n        transform (Albumentations.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n        no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n    \"\"\"\n    super().__init__()\n\n    self.split_file = split\n\n    label_data_root = label_data_root if label_data_root is not None else data_root\n    self.image_files = sorted(glob.glob(os.path.join(data_root, image_grep)))\n    self.constant_scale = constant_scale\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.segmentation_mask_files = sorted(glob.glob(os.path.join(label_data_root, label_grep)))\n    self.reduce_zero_label = reduce_zero_label\n    self.expand_temporal_dimension = expand_temporal_dimension\n\n    if self.expand_temporal_dimension and output_bands is None:\n        msg = \"Please provide output_bands when expand_temporal_dimension is True\"\n        raise Exception(msg)\n    if self.split_file is not None:\n        with open(self.split_file) as f:\n            split = f.readlines()\n        valid_files = {rf\"{substring.strip()}\" for substring in split}\n        self.image_files = filter_valid_files(\n            self.image_files,\n            valid_files=valid_files,\n            ignore_extensions=ignore_split_file_extensions,\n            allow_substring=allow_substring_split_file,\n        )\n        self.segmentation_mask_files = filter_valid_files(\n            self.segmentation_mask_files,\n            valid_files=valid_files,\n            ignore_extensions=ignore_split_file_extensions,\n            allow_substring=allow_substring_split_file,\n        )\n\n    # We don't define a split file for prediction\n    if not self.split_file:\n        # When prediction is enabled, we don't have mask files, so\n        # we need to provide a way to run the dataloder in these cases.\n        if not self.segmentation_mask_files:\n            self.segmentation_mask_files = self.image_files\n            # The masks can be `None` since they won't be used in fact. \n\n    self.rgb_indices = [0, 1, 2] if rgb_indices is None else rgb_indices\n\n    self.dataset_bands = generate_bands_intervals(dataset_bands)\n    self.output_bands = generate_bands_intervals(output_bands)\n\n    if self.output_bands and not self.dataset_bands:\n        msg = \"If output bands provided, dataset_bands must also be provided\"\n        return Exception(msg)  # noqa: PLE0101\n\n    # There is a special condition if the bands are defined as simple strings.\n    if self.output_bands:\n        if len(set(self.output_bands) &amp; set(self.dataset_bands)) != len(self.output_bands):\n            msg = \"Output bands must be a subset of dataset bands\"\n            raise Exception(msg)\n\n        self.filter_indices = [self.dataset_bands.index(band) for band in self.output_bands]\n\n    else:\n        self.filter_indices = None\n\n    # If no transform is given, apply only to transform to torch tensor\n    self.transform = transform if transform else default_transform\n    # self.transform = transform if transform else ToTensorV2()\n\n    import warnings\n\n    import rasterio\n\n    warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_scalar_label_dataset.GenericNonGeoClassificationDataset","title":"<code>terratorch.datasets.generic_scalar_label_dataset.GenericNonGeoClassificationDataset</code>","text":"<p>               Bases: <code>GenericScalarLabelDataset</code></p> <p>GenericNonGeoClassificationDataset</p> Source code in <code>terratorch/datasets/generic_scalar_label_dataset.py</code> <pre><code>class GenericNonGeoClassificationDataset(GenericScalarLabelDataset):\n    \"\"\"GenericNonGeoClassificationDataset\"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        num_classes: int,\n        split: Path | None = None,\n        ignore_split_file_extensions: bool = True,  # noqa: FBT001, FBT002\n        allow_substring_split_file: bool = True,  # noqa: FBT001, FBT002\n        rgb_indices: list[str] | None = None,\n        dataset_bands: list[HLSBands | int] | None = None,\n        output_bands: list[HLSBands | int] | None = None,\n        class_names: list[str] | None = None,\n        constant_scale: float = 1,\n        transform: A.Compose | None = None,\n        no_data_replace: float = 0,\n        expand_temporal_dimension: bool = False,  # noqa: FBT001, FBT002\n    ) -&gt; None:\n        \"\"\"A generic Non-Geo dataset for classification.\n\n        Args:\n            data_root (Path): Path to data root directory\n            num_classes (int): Number of classes in the dataset\n            split (Path, optional): Path to file containing files to be used for this split.\n                The file should be a new-line separated prefixes contained in the desired files.\n                Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\". Defaults to True.\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            dataset_bands (list[HLSBands | int] | None): Bands present in the dataset.\n            output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n            class_names (list[str], optional): Class names. Defaults to None.\n            constant_scale (float): Factor to multiply image values by. Defaults to 1.\n            transform (Albumentations.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float): Replace nan values in input images with this value. Defaults to 0.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n        \"\"\"\n        super().__init__(\n            data_root,\n            split=split,\n            ignore_split_file_extensions=ignore_split_file_extensions,\n            allow_substring_split_file=allow_substring_split_file,\n            rgb_indices=rgb_indices,\n            dataset_bands=dataset_bands,\n            output_bands=output_bands,\n            constant_scale=constant_scale,\n            transform=transform,\n            no_data_replace=no_data_replace,\n            expand_temporal_dimension=expand_temporal_dimension,\n        )\n        self.num_classes = num_classes\n        self.class_names = class_names\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        item = super().__getitem__(index)\n        item[\"label\"] = torch.tensor(item[\"label\"]).long()\n        return item\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n        pass\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_scalar_label_dataset.GenericNonGeoClassificationDataset.__init__","title":"<code>__init__(data_root, num_classes, split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, class_names=None, constant_scale=1, transform=None, no_data_replace=0, expand_temporal_dimension=False)</code>","text":"<p>A generic Non-Geo dataset for classification.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>num_classes</code> <code>int</code> <p>Number of classes in the dataset</p> required <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset.</p> <code>None</code> <code>class_names</code> <code>list[str]</code> <p>Class names. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float</code> <p>Replace nan values in input images with this value. Defaults to 0.</p> <code>0</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_scalar_label_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    num_classes: int,\n    split: Path | None = None,\n    ignore_split_file_extensions: bool = True,  # noqa: FBT001, FBT002\n    allow_substring_split_file: bool = True,  # noqa: FBT001, FBT002\n    rgb_indices: list[str] | None = None,\n    dataset_bands: list[HLSBands | int] | None = None,\n    output_bands: list[HLSBands | int] | None = None,\n    class_names: list[str] | None = None,\n    constant_scale: float = 1,\n    transform: A.Compose | None = None,\n    no_data_replace: float = 0,\n    expand_temporal_dimension: bool = False,  # noqa: FBT001, FBT002\n) -&gt; None:\n    \"\"\"A generic Non-Geo dataset for classification.\n\n    Args:\n        data_root (Path): Path to data root directory\n        num_classes (int): Number of classes in the dataset\n        split (Path, optional): Path to file containing files to be used for this split.\n            The file should be a new-line separated prefixes contained in the desired files.\n            Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\". Defaults to True.\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        dataset_bands (list[HLSBands | int] | None): Bands present in the dataset.\n        output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n        class_names (list[str], optional): Class names. Defaults to None.\n        constant_scale (float): Factor to multiply image values by. Defaults to 1.\n        transform (Albumentations.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float): Replace nan values in input images with this value. Defaults to 0.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n    \"\"\"\n    super().__init__(\n        data_root,\n        split=split,\n        ignore_split_file_extensions=ignore_split_file_extensions,\n        allow_substring_split_file=allow_substring_split_file,\n        rgb_indices=rgb_indices,\n        dataset_bands=dataset_bands,\n        output_bands=output_bands,\n        constant_scale=constant_scale,\n        transform=transform,\n        no_data_replace=no_data_replace,\n        expand_temporal_dimension=expand_temporal_dimension,\n    )\n    self.num_classes = num_classes\n    self.class_names = class_names\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_scalar_label_dataset.GenericScalarLabelDataset","title":"<code>terratorch.datasets.generic_scalar_label_dataset.GenericScalarLabelDataset</code>","text":"<p>               Bases: <code>NonGeoDataset</code>, <code>ImageFolder</code>, <code>ABC</code></p> <p>This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset.</p> Source code in <code>terratorch/datasets/generic_scalar_label_dataset.py</code> <pre><code>class GenericScalarLabelDataset(NonGeoDataset, ImageFolder, ABC):\n    \"\"\"\n    This is a generic dataset class to be used for instantiating datasets from arguments.\n    Ideally, one would create a dataset class specific to a dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        split: Path | None = None,\n        ignore_split_file_extensions: bool = True,  # noqa: FBT001, FBT002\n        allow_substring_split_file: bool = True,  # noqa: FBT001, FBT002\n        rgb_indices: list[int] | None = None,\n        dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        constant_scale: float = 1,\n        transform: A.Compose | None = None,\n        no_data_replace: float = 0,\n        expand_temporal_dimension: bool = False,  # noqa: FBT001, FBT002\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (Path): Path to data root directory\n            split (Path, optional): Path to file containing files to be used for this split.\n                The file should be a new-line separated prefixes contained in the desired files.\n                Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\". Defaults to True.\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This\n                parameter gives identifiers to input channels (bands) so that they can then be refered to by\n                output_bands. Can use the HLSBands enum, ints, int ranges, or strings. Defaults to None.\n            output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the\n                dataset as named by dataset_bands.\n            constant_scale (float): Factor to multiply image values by. Defaults to 1.\n            transform (Albumentations.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float): Replace nan values in input images with this value. Defaults to 0.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n        \"\"\"\n        self.split_file = split\n\n        self.image_files = sorted(glob.glob(os.path.join(data_root, \"**\"), recursive=True))\n        self.image_files = [f for f in self.image_files if not os.path.isdir(f)]\n        self.constant_scale = constant_scale\n        self.no_data_replace = no_data_replace\n        self.expand_temporal_dimension = expand_temporal_dimension\n        if self.expand_temporal_dimension and output_bands is None:\n            msg = \"Please provide output_bands when expand_temporal_dimension is True\"\n            raise Exception(msg)\n        if self.split_file is not None:\n            with open(self.split_file) as f:\n                split = f.readlines()\n            valid_files = {rf\"{substring.strip()}\" for substring in split}\n            self.image_files = filter_valid_files(\n                self.image_files,\n                valid_files=valid_files,\n                ignore_extensions=ignore_split_file_extensions,\n                allow_substring=allow_substring_split_file,\n            )\n\n            def is_valid_file(x):\n                return x in self.image_files\n\n        else:\n\n            def is_valid_file(x):\n                return True\n\n        super().__init__(\n            root=data_root, transform=None, target_transform=None, loader=rasterio_loader, is_valid_file=is_valid_file\n        )\n\n        self.rgb_indices = [0, 1, 2] if rgb_indices is None else rgb_indices\n\n        self.dataset_bands = generate_bands_intervals(dataset_bands)\n        self.output_bands = generate_bands_intervals(output_bands)\n\n        if self.output_bands and not self.dataset_bands:\n            msg = \"If output bands provided, dataset_bands must also be provided\"\n            return Exception(msg)  # noqa: PLE0101\n\n        # There is a special condition if the bands are defined as simple strings.\n        if self.output_bands:\n            if len(set(self.output_bands) &amp; set(self.dataset_bands)) != len(self.output_bands):\n                msg = \"Output bands must be a subset of dataset bands\"\n                raise Exception(msg)\n\n            self.filter_indices = [self.dataset_bands.index(band) for band in self.output_bands]\n\n        else:\n            self.filter_indices = None\n        # If no transform is given, apply only to transform to torch tensor\n        self.transforms = transform if transform else default_transform\n        # self.transform = transform if transform else ToTensorV2()\n\n        import warnings\n\n        import rasterio\n        warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def _loader(self, path: str | Path) -&gt; Image.Image:\n\n        try:\n            with open(path, \"rb\") as f:\n                img = np.asarray(Image.open(f))\n        except PIL.UnidentifiedImageError:\n            # TIFF files containing floating-point values should be handled in\n            # another way.\n            if path.endswith(\".tif\") or path.endswith(\".tiff\"):\n                img = tifffile.imread(path)\n            else:\n                raise IOError(f\"Could not open {path}. Unsupported format or configuration.\")\n        return img\n\n    def __base_getitem__(self, index: int) -&gt; Tuple[Any, Any]:\n\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target class.\n        \"\"\"\n        path, target = self.samples[index]\n        sample = self._loader(path)\n\n        return sample, target\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n\n        image, label = self.__base_getitem__(index)\n\n        if self.expand_temporal_dimension:\n            image = rearrange(image, \"h w (channels time) -&gt; time h w channels\", channels=len(self.output_bands))\n        if self.filter_indices:\n            image = image[..., self.filter_indices]\n\n        image = image.astype(np.float32) * self.constant_scale\n\n        if self.transforms:\n            image = self.transforms(image=image)[\"image\"]  # albumentations returns dict\n\n        output = {\n            \"image\": image,\n            \"label\": label,  # samples is an attribute of ImageFolder. Contains a tuple of (Path, Target)\n            \"filename\": self.image_files[index]\n        }\n\n        return output\n\n    def _generate_bands_intervals(self, bands_intervals: list[int | str | HLSBands | tuple[int]] | None = None):\n        if bands_intervals is None:\n            return None\n        bands = []\n        for element in bands_intervals:\n            # if its an interval\n            if isinstance(element, tuple):\n                if len(element) != 2:  # noqa: PLR2004\n                    msg = \"When defining an interval, a tuple of two integers should be passed,\\\n                    defining start and end indices inclusive\"\n                    raise Exception(msg)\n                expanded_element = list(range(element[0], element[1] + 1))\n                bands.extend(expanded_element)\n            else:\n                bands.append(element)\n        return bands\n\n    def _load_file(self, path) -&gt; xr.DataArray:\n        data = rioxarray.open_rasterio(path, masked=True)\n        data = data.fillna(self.no_data_replace)\n        return data\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_scalar_label_dataset.GenericScalarLabelDataset.__base_getitem__","title":"<code>__base_getitem__(index)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Any, Any]</code> <p>(sample, target) where target is class_index of the target class.</p> Source code in <code>terratorch/datasets/generic_scalar_label_dataset.py</code> <pre><code>def __base_getitem__(self, index: int) -&gt; Tuple[Any, Any]:\n\n    \"\"\"\n    Args:\n        index (int): Index\n\n    Returns:\n        tuple: (sample, target) where target is class_index of the target class.\n    \"\"\"\n    path, target = self.samples[index]\n    sample = self._loader(path)\n\n    return sample, target\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_scalar_label_dataset.GenericScalarLabelDataset.__init__","title":"<code>__init__(data_root, split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, constant_scale=1, transform=None, no_data_replace=0, expand_temporal_dimension=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int | tuple[int, int] | str] | None</code> <p>Bands present in the dataset. This parameter gives identifiers to input channels (bands) so that they can then be refered to by output_bands. Can use the HLSBands enum, ints, int ranges, or strings. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int | tuple[int, int] | str] | None</code> <p>Bands that should be output by the dataset as named by dataset_bands.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float</code> <p>Replace nan values in input images with this value. Defaults to 0.</p> <code>0</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_scalar_label_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    split: Path | None = None,\n    ignore_split_file_extensions: bool = True,  # noqa: FBT001, FBT002\n    allow_substring_split_file: bool = True,  # noqa: FBT001, FBT002\n    rgb_indices: list[int] | None = None,\n    dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    constant_scale: float = 1,\n    transform: A.Compose | None = None,\n    no_data_replace: float = 0,\n    expand_temporal_dimension: bool = False,  # noqa: FBT001, FBT002\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (Path): Path to data root directory\n        split (Path, optional): Path to file containing files to be used for this split.\n            The file should be a new-line separated prefixes contained in the desired files.\n            Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\". Defaults to True.\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This\n            parameter gives identifiers to input channels (bands) so that they can then be refered to by\n            output_bands. Can use the HLSBands enum, ints, int ranges, or strings. Defaults to None.\n        output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the\n            dataset as named by dataset_bands.\n        constant_scale (float): Factor to multiply image values by. Defaults to 1.\n        transform (Albumentations.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float): Replace nan values in input images with this value. Defaults to 0.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n    \"\"\"\n    self.split_file = split\n\n    self.image_files = sorted(glob.glob(os.path.join(data_root, \"**\"), recursive=True))\n    self.image_files = [f for f in self.image_files if not os.path.isdir(f)]\n    self.constant_scale = constant_scale\n    self.no_data_replace = no_data_replace\n    self.expand_temporal_dimension = expand_temporal_dimension\n    if self.expand_temporal_dimension and output_bands is None:\n        msg = \"Please provide output_bands when expand_temporal_dimension is True\"\n        raise Exception(msg)\n    if self.split_file is not None:\n        with open(self.split_file) as f:\n            split = f.readlines()\n        valid_files = {rf\"{substring.strip()}\" for substring in split}\n        self.image_files = filter_valid_files(\n            self.image_files,\n            valid_files=valid_files,\n            ignore_extensions=ignore_split_file_extensions,\n            allow_substring=allow_substring_split_file,\n        )\n\n        def is_valid_file(x):\n            return x in self.image_files\n\n    else:\n\n        def is_valid_file(x):\n            return True\n\n    super().__init__(\n        root=data_root, transform=None, target_transform=None, loader=rasterio_loader, is_valid_file=is_valid_file\n    )\n\n    self.rgb_indices = [0, 1, 2] if rgb_indices is None else rgb_indices\n\n    self.dataset_bands = generate_bands_intervals(dataset_bands)\n    self.output_bands = generate_bands_intervals(output_bands)\n\n    if self.output_bands and not self.dataset_bands:\n        msg = \"If output bands provided, dataset_bands must also be provided\"\n        return Exception(msg)  # noqa: PLE0101\n\n    # There is a special condition if the bands are defined as simple strings.\n    if self.output_bands:\n        if len(set(self.output_bands) &amp; set(self.dataset_bands)) != len(self.output_bands):\n            msg = \"Output bands must be a subset of dataset bands\"\n            raise Exception(msg)\n\n        self.filter_indices = [self.dataset_bands.index(band) for band in self.output_bands]\n\n    else:\n        self.filter_indices = None\n    # If no transform is given, apply only to transform to torch tensor\n    self.transforms = transform if transform else default_transform\n    # self.transform = transform if transform else ToTensorV2()\n\n    import warnings\n\n    import rasterio\n    warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalSegmentationDataset","title":"<code>terratorch.datasets.generic_multimodal_dataset.GenericMultimodalSegmentationDataset</code>","text":"<p>               Bases: <code>GenericMultimodalDataset</code></p> <p>GenericNonGeoSegmentationDataset</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>class GenericMultimodalSegmentationDataset(GenericMultimodalDataset):\n    \"\"\"GenericNonGeoSegmentationDataset\"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        num_classes: int,\n        label_data_root: Path | None = None,\n        image_grep: str | None = \"*\",\n        label_grep: str | None = \"*\",\n        split: Path | None = None,\n        image_modalities: list[str] | None = None,\n        rgb_modality: str | None = None,\n        rgb_indices: list[str] | None = None,\n        allow_missing_modalities: bool = False,\n        allow_substring_file_names: bool = False,\n        dataset_bands: dict[list] | None = None,\n        output_bands: dict[list] | None = None,\n        class_names: list[str] | None = None,\n        constant_scale: dict[float] = 1.0,\n        transform: A.Compose | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = -1,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n        channel_position: int = -3,\n        concat_bands: bool = False,\n        prediction_mode: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n                data, with modalities as keys.\n            num_classes (int): Number of classes.\n            label_data_root (Path): Path to data root directory with mask files.\n            image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n                images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            label_grep (str, optional): Regular expression appended to label_data_root to find mask files.\n                Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            split (Path, optional): Path to file containing samples prefixes to be used for this split.\n                The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n                sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n                files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n                If not specified, search samples based on files in data_root. Defaults to None.\n            image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n                The difference between all modalities and image_modalities are non-image modalities which are treated\n                differently during the transforms and are not modified but only converted into a tensor if possible.\n            rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n                TODO: Currently not implemented on a data module level!\n            allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n                image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n                If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n                Defaults to True.\n            dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n                as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n                that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n                of all modalities. Defaults to None.\n            output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n                provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n            class_names (list[str], optional): Names of the classes. Defaults to None.\n            constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n                keys. Can be subset of all modalities. Defaults to None.\n            transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n                modalities (transformation are shared between image modalities, e.g., similar crop or rotation).\n                Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization.\n                Not supported for multi-temporal data. The transform is not applied to non-image data, which is only\n                converted to tensors if possible. If dict, can include multiple transforms per modality which are\n                applied separately (no shared parameters between modalities).\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input data with this value.\n                If None, does no replacement. Defaults to None.\n            no_label_replace (float | None): Replace nan values in label with this value.\n                If none, does no replacement. Defaults to -1.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n            channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n            concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n                that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n                Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n            prediction_mode (bool): Used to deactivate the checking for a label when it is not necessary.\n        \"\"\"\n\n        super().__init__(\n            data_root,\n            label_data_root=label_data_root,\n            image_grep=image_grep,\n            label_grep=label_grep,\n            split=split,\n            image_modalities=image_modalities,\n            rgb_modality=rgb_modality,\n            rgb_indices=rgb_indices,\n            allow_missing_modalities=allow_missing_modalities,\n            allow_substring_file_names=allow_substring_file_names,\n            dataset_bands=dataset_bands,\n            output_bands=output_bands,\n            constant_scale=constant_scale,\n            transform=transform,\n            no_data_replace=no_data_replace,\n            no_label_replace=no_label_replace,\n            expand_temporal_dimension=expand_temporal_dimension,\n            reduce_zero_label=reduce_zero_label,\n            channel_position=channel_position,\n            concat_bands=concat_bands,\n            prediction_mode=prediction_mode,\n            *args,\n            **kwargs,\n        )\n        self.num_classes = num_classes\n        self.class_names = class_names\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        item = super().__getitem__(index)\n\n        if not self.prediction_mode:\n            item[\"mask\"] = item[\"mask\"].long()\n\n        return item\n\n    def plot(\n        self, sample: dict[str, torch.Tensor], suptitle: str | None = None, show_axes: bool | None = False\n    ) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            suptitle: optional string to use as a suptitle\n            show_axes: whether to show axes or not\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n\n        .. versionadded:: 0.2\n        \"\"\"\n        image = sample[\"image\"]\n        if isinstance(image, dict):\n            image = image[self.rgb_modality]\n        if isinstance(image, torch.Tensor):\n            image = image.numpy()\n        image = image.take(self.rgb_indices, axis=0)\n        image = np.transpose(image, (1, 2, 0))\n        image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n        image = np.clip(image, 0, 1)\n\n        label_mask = sample[\"mask\"]\n        if isinstance(label_mask, torch.Tensor):\n            label_mask = label_mask.numpy()\n\n        showing_predictions = \"prediction\" in sample\n        if showing_predictions:\n            prediction_mask = sample[\"prediction\"]\n            if isinstance(prediction_mask, torch.Tensor):\n                prediction_mask = prediction_mask.numpy()\n\n        return self._plot_sample(\n            image,\n            label_mask,\n            self.num_classes,\n            prediction=prediction_mask if showing_predictions else None,\n            suptitle=suptitle,\n            class_names=self.class_names,\n            show_axes=show_axes,\n        )\n\n    @staticmethod\n    def _plot_sample(image, label, num_classes, prediction=None, suptitle=None, class_names=None, show_axes=False):\n        num_images = 5 if prediction is not None else 4\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 10), layout=\"compressed\")\n        axes_visibility = \"on\" if show_axes else \"off\"\n\n        # for legend\n        ax[0].axis(\"off\")\n\n        norm = mpl.colors.Normalize(vmin=0, vmax=num_classes - 1)\n        ax[1].axis(axes_visibility)\n        ax[1].title.set_text(\"Image\")\n        ax[1].imshow(image)\n\n        ax[2].axis(axes_visibility)\n        ax[2].title.set_text(\"Ground Truth Mask\")\n        ax[2].imshow(label, cmap=\"jet\", norm=norm)\n\n        ax[3].axis(axes_visibility)\n        ax[3].title.set_text(\"GT Mask on Image\")\n        ax[3].imshow(image)\n        ax[3].imshow(label, cmap=\"jet\", alpha=0.3, norm=norm)\n\n        if prediction is not None:\n            ax[4].axis(axes_visibility)\n            ax[4].title.set_text(\"Predicted Mask\")\n            ax[4].imshow(prediction, cmap=\"jet\", norm=norm)\n\n        cmap = plt.get_cmap(\"jet\")\n        legend_data = []\n        for i, _ in enumerate(range(num_classes)):\n            class_name = class_names[i] if class_names else str(i)\n            data = [i, cmap(norm(i)), class_name]\n            legend_data.append(data)\n        handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n        labels = [n for k, c, n in legend_data]\n        ax[0].legend(handles, labels, loc=\"center\")\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n        return fig\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalSegmentationDataset.__init__","title":"<code>__init__(data_root, num_classes, label_data_root=None, image_grep='*', label_grep='*', split=None, image_modalities=None, rgb_modality=None, rgb_indices=None, allow_missing_modalities=False, allow_substring_file_names=False, dataset_bands=None, output_bands=None, class_names=None, constant_scale=1.0, transform=None, no_data_replace=None, no_label_replace=-1, expand_temporal_dimension=False, reduce_zero_label=False, channel_position=-3, concat_bands=False, prediction_mode=False, *args, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to data root directory or csv/parquet files with image-level data, with modalities as keys.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes.</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with mask files.</p> <code>None</code> <code>image_grep</code> <code>dict[str]</code> <p>Dictionary with regular expression appended to data_root to find input images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to label_data_root to find mask files. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing samples prefixes to be used for this split. The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise, files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]). If not specified, search samples based on files in data_root. Defaults to None.</p> <code>None</code> <code>image_modalities(list[str],</code> <code>optional</code> <p>List of pixel-level raster modalities. Defaults to data_root.keys(). The difference between all modalities and image_modalities are non-image modalities which are treated differently during the transforms and are not modified but only converted into a tensor if possible.</p> required <code>rgb_modality</code> <code>str</code> <p>Modality used for RGB plots. Defaults to first modality in data_root.keys().</p> <code>None</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>allow_missing_modalities</code> <code>bool</code> <p>Allow missing modalities during data loading. Defaults to False. TODO: Currently not implemented on a data module level!</p> <code>False</code> <code>allow_substring_file_names</code> <code>bool</code> <p>Allow substrings during sample identification by adding image or label grep to the sample prefixes. If False, treats sample prefixes as full file names. If True and no split file is provided, considers the file stem as prefix, otherwise the full file name. Defaults to True.</p> <code>False</code> <code>dataset_bands</code> <code>dict[list]</code> <p>Bands present in the dataset, provided in a dictionary with modalities as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset of all modalities. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>dict[list]</code> <p>Bands that should be output by the dataset as named by dataset_bands, provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.</p> <code>None</code> <code>class_names</code> <code>list[str]</code> <p>Names of the classes. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>dict[float]</code> <p>Factor to multiply data values by, provided as a dictionary with modalities as keys. Can be subset of all modalities. Defaults to None.</p> <code>1.0</code> <code>transform</code> <code>Compose | dict | None</code> <p>Albumentations transform to be applied to all image modalities (transformation are shared between image modalities, e.g., similar crop or rotation). Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. The transform is not applied to non-image data, which is only converted to tensors if possible. If dict, can include multiple transforms per modality which are applied separately (no shared parameters between modalities). Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input data with this value. If None, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>float | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>-1</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Only works with image modalities. Is only applied to modalities with defined dataset_bands. Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>channel_position</code> <code>int</code> <p>Position of the channel dimension in the image modalities. Defaults to -3.</p> <code>-3</code> <code>concat_bands</code> <code>bool</code> <p>Concatenate all image modalities along the band dimension into a single \"image\", so that it can be processed by single-modal models. Concatenate in the order of provided modalities. Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.</p> <code>False</code> <code>prediction_mode</code> <code>bool</code> <p>Used to deactivate the checking for a label when it is not necessary.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    num_classes: int,\n    label_data_root: Path | None = None,\n    image_grep: str | None = \"*\",\n    label_grep: str | None = \"*\",\n    split: Path | None = None,\n    image_modalities: list[str] | None = None,\n    rgb_modality: str | None = None,\n    rgb_indices: list[str] | None = None,\n    allow_missing_modalities: bool = False,\n    allow_substring_file_names: bool = False,\n    dataset_bands: dict[list] | None = None,\n    output_bands: dict[list] | None = None,\n    class_names: list[str] | None = None,\n    constant_scale: dict[float] = 1.0,\n    transform: A.Compose | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = -1,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n    channel_position: int = -3,\n    concat_bands: bool = False,\n    prediction_mode: bool = False,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n            data, with modalities as keys.\n        num_classes (int): Number of classes.\n        label_data_root (Path): Path to data root directory with mask files.\n        image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n            images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        label_grep (str, optional): Regular expression appended to label_data_root to find mask files.\n            Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        split (Path, optional): Path to file containing samples prefixes to be used for this split.\n            The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n            sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n            files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n            If not specified, search samples based on files in data_root. Defaults to None.\n        image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n            The difference between all modalities and image_modalities are non-image modalities which are treated\n            differently during the transforms and are not modified but only converted into a tensor if possible.\n        rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n            TODO: Currently not implemented on a data module level!\n        allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n            image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n            If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n            Defaults to True.\n        dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n            as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n            that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n            of all modalities. Defaults to None.\n        output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n            provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n        class_names (list[str], optional): Names of the classes. Defaults to None.\n        constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n            keys. Can be subset of all modalities. Defaults to None.\n        transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n            modalities (transformation are shared between image modalities, e.g., similar crop or rotation).\n            Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization.\n            Not supported for multi-temporal data. The transform is not applied to non-image data, which is only\n            converted to tensors if possible. If dict, can include multiple transforms per modality which are\n            applied separately (no shared parameters between modalities).\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input data with this value.\n            If None, does no replacement. Defaults to None.\n        no_label_replace (float | None): Replace nan values in label with this value.\n            If none, does no replacement. Defaults to -1.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n        channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n        concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n            that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n            Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n        prediction_mode (bool): Used to deactivate the checking for a label when it is not necessary.\n    \"\"\"\n\n    super().__init__(\n        data_root,\n        label_data_root=label_data_root,\n        image_grep=image_grep,\n        label_grep=label_grep,\n        split=split,\n        image_modalities=image_modalities,\n        rgb_modality=rgb_modality,\n        rgb_indices=rgb_indices,\n        allow_missing_modalities=allow_missing_modalities,\n        allow_substring_file_names=allow_substring_file_names,\n        dataset_bands=dataset_bands,\n        output_bands=output_bands,\n        constant_scale=constant_scale,\n        transform=transform,\n        no_data_replace=no_data_replace,\n        no_label_replace=no_label_replace,\n        expand_temporal_dimension=expand_temporal_dimension,\n        reduce_zero_label=reduce_zero_label,\n        channel_position=channel_position,\n        concat_bands=concat_bands,\n        prediction_mode=prediction_mode,\n        *args,\n        **kwargs,\n    )\n    self.num_classes = num_classes\n    self.class_names = class_names\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalSegmentationDataset.plot","title":"<code>plot(sample, suptitle=None, show_axes=False)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <code>show_axes</code> <code>bool | None</code> <p>whether to show axes or not</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> <p>.. versionadded:: 0.2</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def plot(\n    self, sample: dict[str, torch.Tensor], suptitle: str | None = None, show_axes: bool | None = False\n) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        suptitle: optional string to use as a suptitle\n        show_axes: whether to show axes or not\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n\n    .. versionadded:: 0.2\n    \"\"\"\n    image = sample[\"image\"]\n    if isinstance(image, dict):\n        image = image[self.rgb_modality]\n    if isinstance(image, torch.Tensor):\n        image = image.numpy()\n    image = image.take(self.rgb_indices, axis=0)\n    image = np.transpose(image, (1, 2, 0))\n    image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n    image = np.clip(image, 0, 1)\n\n    label_mask = sample[\"mask\"]\n    if isinstance(label_mask, torch.Tensor):\n        label_mask = label_mask.numpy()\n\n    showing_predictions = \"prediction\" in sample\n    if showing_predictions:\n        prediction_mask = sample[\"prediction\"]\n        if isinstance(prediction_mask, torch.Tensor):\n            prediction_mask = prediction_mask.numpy()\n\n    return self._plot_sample(\n        image,\n        label_mask,\n        self.num_classes,\n        prediction=prediction_mask if showing_predictions else None,\n        suptitle=suptitle,\n        class_names=self.class_names,\n        show_axes=show_axes,\n    )\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalPixelwiseRegressionDataset","title":"<code>terratorch.datasets.generic_multimodal_dataset.GenericMultimodalPixelwiseRegressionDataset</code>","text":"<p>               Bases: <code>GenericMultimodalDataset</code></p> <p>GenericNonGeoPixelwiseRegressionDataset</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>class GenericMultimodalPixelwiseRegressionDataset(GenericMultimodalDataset):\n    \"\"\"GenericNonGeoPixelwiseRegressionDataset\"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        label_data_root: Path | None = None,\n        image_grep: str | None = \"*\",\n        label_grep: str | None = \"*\",\n        split: Path | None = None,\n        image_modalities: list[str] | None = None,\n        rgb_modality: str | None = None,\n        rgb_indices: list[int] | None = None,\n        allow_missing_modalities: bool = False,\n        allow_substring_file_names: bool = False,\n        dataset_bands: dict[list] | None = None,\n        output_bands: dict[list] | None = None,\n        constant_scale: dict[float] = 1.0,\n        transform: A.Compose | dict | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: float | None = None,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n        channel_position: int = -3,\n        concat_bands: bool = False,\n        prediction_mode: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n                data, with modalities as keys.\n            label_data_root (Path): Path to data root directory with ground truth files.\n            image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n                images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            label_grep (str, optional): Regular expression appended to label_data_root to find ground truth files.\n                Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            split (Path, optional): Path to file containing samples prefixes to be used for this split.\n                The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n                sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n                files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n                If not specified, search samples based on files in data_root. Defaults to None.\n            image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n                The difference between all modalities and image_modalities are non-image modalities which are treated\n                differently during the transforms and are not modified but only converted into a tensor if possible.\n            rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n                TODO: Currently not implemented on a data module level!\n            allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n                image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n                If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n                Defaults to True.\n            dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n                as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n                that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n                of all modalities. Defaults to None.\n            output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n                provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n            constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n                keys. Can be subset of all modalities. Defaults to None.\n            transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n                modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to\n                non-image data, which is only converted to tensors if possible. If dict, can include separate transforms\n                per modality (no shared parameters between modalities).\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input data with this value.\n                If None, does no replacement. Defaults to None.\n            no_label_replace (float | None): Replace nan values in label with this value.\n                If none, does no replacement. Defaults to None.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n            channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n            concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n                that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n                Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n            prediction_mode (bool): Used to deactivate the checking for a label when it is not necessary.\n        \"\"\"\n\n        super().__init__(\n            data_root,\n            label_data_root=label_data_root,\n            image_grep=image_grep,\n            label_grep=label_grep,\n            split=split,\n            image_modalities=image_modalities,\n            rgb_modality=rgb_modality,\n            rgb_indices=rgb_indices,\n            allow_missing_modalities=allow_missing_modalities,\n            allow_substring_file_names=allow_substring_file_names,\n            dataset_bands=dataset_bands,\n            output_bands=output_bands,\n            constant_scale=constant_scale,\n            transform=transform,\n            no_data_replace=no_data_replace,\n            no_label_replace=no_label_replace,\n            expand_temporal_dimension=expand_temporal_dimension,\n            reduce_zero_label=reduce_zero_label,\n            channel_position=channel_position,\n            concat_bands=concat_bands,\n            prediction_mode=prediction_mode,\n            *args,\n            **kwargs,\n        )\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        item = super().__getitem__(index)\n\n        if not self.prediction_mode:\n            item[\"mask\"] = item[\"mask\"].float()\n\n        return item\n\n    def plot(\n        self, sample: dict[str, torch.Tensor], suptitle: str | None = None, show_axes: bool | None = False\n    ) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__`\n            suptitle (str|None): optional string to use as a suptitle\n            show_axes (bool|None): whether to show axes or not\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n\n        .. versionadded:: 0.2\n        \"\"\"\n\n        image = sample[\"image\"]\n        if isinstance(image, dict):\n            image = image[self.rgb_modality]\n        if isinstance(image, torch.Tensor):\n            image = image.numpy()\n        image = image.take(self.rgb_indices, axis=0)\n        image = np.transpose(image, (1, 2, 0))\n        image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n        image = np.clip(image, 0, 1)\n\n        label_mask = sample[\"mask\"]\n        if isinstance(label_mask, torch.Tensor):\n            label_mask = label_mask.numpy()\n\n        showing_predictions = \"prediction\" in sample\n        if showing_predictions:\n            prediction_mask = sample[\"prediction\"]\n            if isinstance(prediction_mask, torch.Tensor):\n                prediction_mask = prediction_mask.numpy()\n\n        return self._plot_sample(\n            image,\n            label_mask,\n            prediction=prediction_mask if showing_predictions else None,\n            suptitle=suptitle,\n            show_axes=show_axes,\n        )\n\n    @staticmethod\n    def _plot_sample(image, label, prediction=None, suptitle=None, show_axes=False):\n        num_images = 4 if prediction is not None else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 10), layout=\"compressed\")\n        axes_visibility = \"on\" if show_axes else \"off\"\n\n        norm = mpl.colors.Normalize(vmin=label.min(), vmax=label.max())\n        ax[0].axis(axes_visibility)\n        ax[0].title.set_text(\"Image\")\n        ax[0].imshow(image)\n\n        ax[1].axis(axes_visibility)\n        ax[1].title.set_text(\"Ground Truth Mask\")\n        ax[1].imshow(label, cmap=\"Greens\", norm=norm)\n\n        ax[2].axis(axes_visibility)\n        ax[2].title.set_text(\"GT Mask on Image\")\n        ax[2].imshow(image)\n        ax[2].imshow(label, cmap=\"Greens\", alpha=0.3, norm=norm)\n        # ax[2].legend()\n\n        if prediction is not None:\n            ax[3].axis(axes_visibility)\n            ax[3].title.set_text(\"Predicted Mask\")\n            ax[3].imshow(prediction, cmap=\"Greens\", norm=norm)\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n        return fig\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalPixelwiseRegressionDataset.__init__","title":"<code>__init__(data_root, label_data_root=None, image_grep='*', label_grep='*', split=None, image_modalities=None, rgb_modality=None, rgb_indices=None, allow_missing_modalities=False, allow_substring_file_names=False, dataset_bands=None, output_bands=None, constant_scale=1.0, transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=False, reduce_zero_label=False, channel_position=-3, concat_bands=False, prediction_mode=False, *args, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to data root directory or csv/parquet files with image-level data, with modalities as keys.</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with ground truth files.</p> <code>None</code> <code>image_grep</code> <code>dict[str]</code> <p>Dictionary with regular expression appended to data_root to find input images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to label_data_root to find ground truth files. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing samples prefixes to be used for this split. The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise, files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]). If not specified, search samples based on files in data_root. Defaults to None.</p> <code>None</code> <code>image_modalities(list[str],</code> <code>optional</code> <p>List of pixel-level raster modalities. Defaults to data_root.keys(). The difference between all modalities and image_modalities are non-image modalities which are treated differently during the transforms and are not modified but only converted into a tensor if possible.</p> required <code>rgb_modality</code> <code>str</code> <p>Modality used for RGB plots. Defaults to first modality in data_root.keys().</p> <code>None</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>allow_missing_modalities</code> <code>bool</code> <p>Allow missing modalities during data loading. Defaults to False. TODO: Currently not implemented on a data module level!</p> <code>False</code> <code>allow_substring_file_names</code> <code>bool</code> <p>Allow substrings during sample identification by adding image or label grep to the sample prefixes. If False, treats sample prefixes as full file names. If True and no split file is provided, considers the file stem as prefix, otherwise the full file name. Defaults to True.</p> <code>False</code> <code>dataset_bands</code> <code>dict[list]</code> <p>Bands present in the dataset, provided in a dictionary with modalities as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset of all modalities. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>dict[list]</code> <p>Bands that should be output by the dataset as named by dataset_bands, provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>dict[float]</code> <p>Factor to multiply data values by, provided as a dictionary with modalities as keys. Can be subset of all modalities. Defaults to None.</p> <code>1.0</code> <code>transform</code> <code>Compose | dict | None</code> <p>Albumentations transform to be applied to all image modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to non-image data, which is only converted to tensors if possible. If dict, can include separate transforms per modality (no shared parameters between modalities). Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input data with this value. If None, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>float | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Only works with image modalities. Is only applied to modalities with defined dataset_bands. Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>channel_position</code> <code>int</code> <p>Position of the channel dimension in the image modalities. Defaults to -3.</p> <code>-3</code> <code>concat_bands</code> <code>bool</code> <p>Concatenate all image modalities along the band dimension into a single \"image\", so that it can be processed by single-modal models. Concatenate in the order of provided modalities. Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.</p> <code>False</code> <code>prediction_mode</code> <code>bool</code> <p>Used to deactivate the checking for a label when it is not necessary.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    label_data_root: Path | None = None,\n    image_grep: str | None = \"*\",\n    label_grep: str | None = \"*\",\n    split: Path | None = None,\n    image_modalities: list[str] | None = None,\n    rgb_modality: str | None = None,\n    rgb_indices: list[int] | None = None,\n    allow_missing_modalities: bool = False,\n    allow_substring_file_names: bool = False,\n    dataset_bands: dict[list] | None = None,\n    output_bands: dict[list] | None = None,\n    constant_scale: dict[float] = 1.0,\n    transform: A.Compose | dict | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: float | None = None,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n    channel_position: int = -3,\n    concat_bands: bool = False,\n    prediction_mode: bool = False,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n            data, with modalities as keys.\n        label_data_root (Path): Path to data root directory with ground truth files.\n        image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n            images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        label_grep (str, optional): Regular expression appended to label_data_root to find ground truth files.\n            Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        split (Path, optional): Path to file containing samples prefixes to be used for this split.\n            The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n            sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n            files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n            If not specified, search samples based on files in data_root. Defaults to None.\n        image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n            The difference between all modalities and image_modalities are non-image modalities which are treated\n            differently during the transforms and are not modified but only converted into a tensor if possible.\n        rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n            TODO: Currently not implemented on a data module level!\n        allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n            image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n            If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n            Defaults to True.\n        dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n            as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n            that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n            of all modalities. Defaults to None.\n        output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n            provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n        constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n            keys. Can be subset of all modalities. Defaults to None.\n        transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n            modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to\n            non-image data, which is only converted to tensors if possible. If dict, can include separate transforms\n            per modality (no shared parameters between modalities).\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input data with this value.\n            If None, does no replacement. Defaults to None.\n        no_label_replace (float | None): Replace nan values in label with this value.\n            If none, does no replacement. Defaults to None.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n        channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n        concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n            that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n            Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n        prediction_mode (bool): Used to deactivate the checking for a label when it is not necessary.\n    \"\"\"\n\n    super().__init__(\n        data_root,\n        label_data_root=label_data_root,\n        image_grep=image_grep,\n        label_grep=label_grep,\n        split=split,\n        image_modalities=image_modalities,\n        rgb_modality=rgb_modality,\n        rgb_indices=rgb_indices,\n        allow_missing_modalities=allow_missing_modalities,\n        allow_substring_file_names=allow_substring_file_names,\n        dataset_bands=dataset_bands,\n        output_bands=output_bands,\n        constant_scale=constant_scale,\n        transform=transform,\n        no_data_replace=no_data_replace,\n        no_label_replace=no_label_replace,\n        expand_temporal_dimension=expand_temporal_dimension,\n        reduce_zero_label=reduce_zero_label,\n        channel_position=channel_position,\n        concat_bands=concat_bands,\n        prediction_mode=prediction_mode,\n        *args,\n        **kwargs,\n    )\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalPixelwiseRegressionDataset.plot","title":"<code>plot(sample, suptitle=None, show_axes=False)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <code>show_axes</code> <code>bool | None</code> <p>whether to show axes or not</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> <p>.. versionadded:: 0.2</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def plot(\n    self, sample: dict[str, torch.Tensor], suptitle: str | None = None, show_axes: bool | None = False\n) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__`\n        suptitle (str|None): optional string to use as a suptitle\n        show_axes (bool|None): whether to show axes or not\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n\n    .. versionadded:: 0.2\n    \"\"\"\n\n    image = sample[\"image\"]\n    if isinstance(image, dict):\n        image = image[self.rgb_modality]\n    if isinstance(image, torch.Tensor):\n        image = image.numpy()\n    image = image.take(self.rgb_indices, axis=0)\n    image = np.transpose(image, (1, 2, 0))\n    image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n    image = np.clip(image, 0, 1)\n\n    label_mask = sample[\"mask\"]\n    if isinstance(label_mask, torch.Tensor):\n        label_mask = label_mask.numpy()\n\n    showing_predictions = \"prediction\" in sample\n    if showing_predictions:\n        prediction_mask = sample[\"prediction\"]\n        if isinstance(prediction_mask, torch.Tensor):\n            prediction_mask = prediction_mask.numpy()\n\n    return self._plot_sample(\n        image,\n        label_mask,\n        prediction=prediction_mask if showing_predictions else None,\n        suptitle=suptitle,\n        show_axes=show_axes,\n    )\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalScalarDataset","title":"<code>terratorch.datasets.generic_multimodal_dataset.GenericMultimodalScalarDataset</code>","text":"<p>               Bases: <code>GenericMultimodalDataset</code></p> <p>GenericMultimodalClassificationDataset</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>class GenericMultimodalScalarDataset(GenericMultimodalDataset):\n    \"\"\"GenericMultimodalClassificationDataset\"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        num_classes: int,\n        label_data_root: Path | None = None,\n        image_grep: str | None = \"*\",\n        label_grep: str | None = \"*\",\n        split: Path | None = None,\n        image_modalities: list[str] | None = None,\n        rgb_modality: str | None = None,\n        rgb_indices: list[int] | None = None,\n        allow_missing_modalities: bool = False,\n        allow_substring_file_names: bool = False,\n        dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        class_names: list[str] | None = None,\n        constant_scale: dict[float] = 1.0,\n        transform: A.Compose | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = None,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n        channel_position: int = -3,\n        concat_bands: bool = False,\n        prediction_mode: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n                data, with modalities as keys.\n            num_classes (int): Number of classes.\n            label_data_root (Path, optional): Path to data root directory with labels or csv/parquet files with labels.\n            image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n                images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            label_grep (str, optional): Regular expression appended to label_data_root to find labels files.\n                Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            split (Path, optional): Path to file containing samples prefixes to be used for this split.\n                The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n                sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n                files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n                If not specified, search samples based on files in data_root. Defaults to None.\n            image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n                The difference between all modalities and image_modalities are non-image modalities which are treated\n                differently during the transforms and are not modified but only converted into a tensor if possible.\n            rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n                TODO: Currently not implemented on a data module level!\n            allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n                image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n                If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n                Defaults to True.\n            dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n                as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n                that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n                of all modalities. Defaults to None.\n            output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n                provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n            class_names (list[str], optional): Names of the classes. Defaults to None.\n            constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n                keys. Can be subset of all modalities. Defaults to None.\n            transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n                modalities (transformation are shared between image modalities, e.g., similar crop or rotation).\n                Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization.\n                Not supported for multi-temporal data. The transform is not applied to non-image data, which is only\n                converted to tensors if possible. If dict, can include multiple transforms per modality which are\n                applied separately (no shared parameters between modalities).\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input data with this value.\n                If None, does no replacement. Defaults to None.\n            no_label_replace (float | None): Replace nan values in label with this value.\n                If none, does no replacement. Defaults to -1.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n            channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n            concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n                that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n                Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n            prediction_mode (bool): Used to deactivate the checking for a label when it is not necessary.\n        \"\"\"\n\n        super().__init__(\n            data_root,\n            label_data_root=label_data_root,\n            image_grep=image_grep,\n            label_grep=label_grep,\n            split=split,\n            image_modalities=image_modalities,\n            rgb_modality=rgb_modality,\n            rgb_indices=rgb_indices,\n            allow_missing_modalities=allow_missing_modalities,\n            allow_substring_file_names=allow_substring_file_names,\n            dataset_bands=dataset_bands,\n            output_bands=output_bands,\n            constant_scale=constant_scale,\n            transform=transform,\n            no_data_replace=no_data_replace,\n            no_label_replace=no_label_replace,\n            expand_temporal_dimension=expand_temporal_dimension,\n            reduce_zero_label=reduce_zero_label,\n            channel_position=channel_position,\n            scalar_label=True,\n            concat_bands=concat_bands,\n            prediction_mode=prediction_mode,\n            *args,\n            **kwargs,\n        )\n\n        self.num_classes = num_classes\n        self.class_names = class_names\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        item = super().__getitem__(index)\n        return item\n\n    def plot(\n        self, sample: dict[str, torch.Tensor], suptitle: str | None = None, show_axes: bool | None = False\n    ) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__`\n            suptitle (str|None): optional string to use as a suptitle\n            show_axes (bool|None): whether to show axes or not\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n\n        .. versionadded:: 0.2\n        \"\"\"\n\n        # TODO: Check plotting code for classification tasks and add it to generic classification dataset as well\n        raise NotImplementedError\n\n        image = sample[\"image\"]\n        if isinstance(image, dict):\n            image = image[self.rgb_modality]\n        if isinstance(image, torch.Tensor):\n            image = image.numpy()\n        image = image.take(self.rgb_indices, axis=0)\n        image = np.transpose(image, (1, 2, 0))\n        image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n        image = np.clip(image, 0, 1)\n\n        label_mask = sample[\"mask\"]\n        if isinstance(label_mask, torch.Tensor):\n            label_mask = label_mask.numpy()\n\n        showing_predictions = \"prediction\" in sample\n        if showing_predictions:\n            prediction_mask = sample[\"prediction\"]\n            if isinstance(prediction_mask, torch.Tensor):\n                prediction_mask = prediction_mask.numpy()\n\n        return self._plot_sample(\n            image,\n            label_mask,\n            prediction=prediction_mask if showing_predictions else None,\n            suptitle=suptitle,\n            show_axes=show_axes,\n        )\n\n    @staticmethod\n    def _plot_sample(image, label, prediction=None, suptitle=None, show_axes=False):\n        num_images = 4 if prediction is not None else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 10), layout=\"compressed\")\n        axes_visibility = \"on\" if show_axes else \"off\"\n\n        norm = mpl.colors.Normalize(vmin=label.min(), vmax=label.max())\n        ax[0].axis(axes_visibility)\n        ax[0].title.set_text(\"Image\")\n        ax[0].imshow(image)\n\n        ax[1].axis(axes_visibility)\n        ax[1].title.set_text(\"Ground Truth Mask\")\n        ax[1].imshow(label, cmap=\"Greens\", norm=norm)\n\n        ax[2].axis(axes_visibility)\n        ax[2].title.set_text(\"GT Mask on Image\")\n        ax[2].imshow(image)\n        ax[2].imshow(label, cmap=\"Greens\", alpha=0.3, norm=norm)\n        # ax[2].legend()\n\n        if prediction is not None:\n            ax[3].axis(axes_visibility)\n            ax[3].title.set_text(\"Predicted Mask\")\n            ax[3].imshow(prediction, cmap=\"Greens\", norm=norm)\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n        return fig\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalScalarDataset.__init__","title":"<code>__init__(data_root, num_classes, label_data_root=None, image_grep='*', label_grep='*', split=None, image_modalities=None, rgb_modality=None, rgb_indices=None, allow_missing_modalities=False, allow_substring_file_names=False, dataset_bands=None, output_bands=None, class_names=None, constant_scale=1.0, transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=False, reduce_zero_label=False, channel_position=-3, concat_bands=False, prediction_mode=False, *args, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to data root directory or csv/parquet files with image-level data, with modalities as keys.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes.</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with labels or csv/parquet files with labels.</p> <code>None</code> <code>image_grep</code> <code>dict[str]</code> <p>Dictionary with regular expression appended to data_root to find input images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to label_data_root to find labels files. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing samples prefixes to be used for this split. The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise, files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]). If not specified, search samples based on files in data_root. Defaults to None.</p> <code>None</code> <code>image_modalities(list[str],</code> <code>optional</code> <p>List of pixel-level raster modalities. Defaults to data_root.keys(). The difference between all modalities and image_modalities are non-image modalities which are treated differently during the transforms and are not modified but only converted into a tensor if possible.</p> required <code>rgb_modality</code> <code>str</code> <p>Modality used for RGB plots. Defaults to first modality in data_root.keys().</p> <code>None</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>allow_missing_modalities</code> <code>bool</code> <p>Allow missing modalities during data loading. Defaults to False. TODO: Currently not implemented on a data module level!</p> <code>False</code> <code>allow_substring_file_names</code> <code>bool</code> <p>Allow substrings during sample identification by adding image or label grep to the sample prefixes. If False, treats sample prefixes as full file names. If True and no split file is provided, considers the file stem as prefix, otherwise the full file name. Defaults to True.</p> <code>False</code> <code>dataset_bands</code> <code>dict[list]</code> <p>Bands present in the dataset, provided in a dictionary with modalities as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset of all modalities. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>dict[list]</code> <p>Bands that should be output by the dataset as named by dataset_bands, provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.</p> <code>None</code> <code>class_names</code> <code>list[str]</code> <p>Names of the classes. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>dict[float]</code> <p>Factor to multiply data values by, provided as a dictionary with modalities as keys. Can be subset of all modalities. Defaults to None.</p> <code>1.0</code> <code>transform</code> <code>Compose | dict | None</code> <p>Albumentations transform to be applied to all image modalities (transformation are shared between image modalities, e.g., similar crop or rotation). Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. The transform is not applied to non-image data, which is only converted to tensors if possible. If dict, can include multiple transforms per modality which are applied separately (no shared parameters between modalities). Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input data with this value. If None, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>float | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Only works with image modalities. Is only applied to modalities with defined dataset_bands. Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>channel_position</code> <code>int</code> <p>Position of the channel dimension in the image modalities. Defaults to -3.</p> <code>-3</code> <code>concat_bands</code> <code>bool</code> <p>Concatenate all image modalities along the band dimension into a single \"image\", so that it can be processed by single-modal models. Concatenate in the order of provided modalities. Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.</p> <code>False</code> <code>prediction_mode</code> <code>bool</code> <p>Used to deactivate the checking for a label when it is not necessary.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    num_classes: int,\n    label_data_root: Path | None = None,\n    image_grep: str | None = \"*\",\n    label_grep: str | None = \"*\",\n    split: Path | None = None,\n    image_modalities: list[str] | None = None,\n    rgb_modality: str | None = None,\n    rgb_indices: list[int] | None = None,\n    allow_missing_modalities: bool = False,\n    allow_substring_file_names: bool = False,\n    dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    class_names: list[str] | None = None,\n    constant_scale: dict[float] = 1.0,\n    transform: A.Compose | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = None,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n    channel_position: int = -3,\n    concat_bands: bool = False,\n    prediction_mode: bool = False,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n            data, with modalities as keys.\n        num_classes (int): Number of classes.\n        label_data_root (Path, optional): Path to data root directory with labels or csv/parquet files with labels.\n        image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n            images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        label_grep (str, optional): Regular expression appended to label_data_root to find labels files.\n            Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        split (Path, optional): Path to file containing samples prefixes to be used for this split.\n            The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n            sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n            files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n            If not specified, search samples based on files in data_root. Defaults to None.\n        image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n            The difference between all modalities and image_modalities are non-image modalities which are treated\n            differently during the transforms and are not modified but only converted into a tensor if possible.\n        rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n            TODO: Currently not implemented on a data module level!\n        allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n            image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n            If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n            Defaults to True.\n        dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n            as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n            that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n            of all modalities. Defaults to None.\n        output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n            provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n        class_names (list[str], optional): Names of the classes. Defaults to None.\n        constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n            keys. Can be subset of all modalities. Defaults to None.\n        transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n            modalities (transformation are shared between image modalities, e.g., similar crop or rotation).\n            Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization.\n            Not supported for multi-temporal data. The transform is not applied to non-image data, which is only\n            converted to tensors if possible. If dict, can include multiple transforms per modality which are\n            applied separately (no shared parameters between modalities).\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input data with this value.\n            If None, does no replacement. Defaults to None.\n        no_label_replace (float | None): Replace nan values in label with this value.\n            If none, does no replacement. Defaults to -1.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n        channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n        concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n            that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n            Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n        prediction_mode (bool): Used to deactivate the checking for a label when it is not necessary.\n    \"\"\"\n\n    super().__init__(\n        data_root,\n        label_data_root=label_data_root,\n        image_grep=image_grep,\n        label_grep=label_grep,\n        split=split,\n        image_modalities=image_modalities,\n        rgb_modality=rgb_modality,\n        rgb_indices=rgb_indices,\n        allow_missing_modalities=allow_missing_modalities,\n        allow_substring_file_names=allow_substring_file_names,\n        dataset_bands=dataset_bands,\n        output_bands=output_bands,\n        constant_scale=constant_scale,\n        transform=transform,\n        no_data_replace=no_data_replace,\n        no_label_replace=no_label_replace,\n        expand_temporal_dimension=expand_temporal_dimension,\n        reduce_zero_label=reduce_zero_label,\n        channel_position=channel_position,\n        scalar_label=True,\n        concat_bands=concat_bands,\n        prediction_mode=prediction_mode,\n        *args,\n        **kwargs,\n    )\n\n    self.num_classes = num_classes\n    self.class_names = class_names\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalScalarDataset.plot","title":"<code>plot(sample, suptitle=None, show_axes=False)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <code>show_axes</code> <code>bool | None</code> <p>whether to show axes or not</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> <p>.. versionadded:: 0.2</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def plot(\n    self, sample: dict[str, torch.Tensor], suptitle: str | None = None, show_axes: bool | None = False\n) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__`\n        suptitle (str|None): optional string to use as a suptitle\n        show_axes (bool|None): whether to show axes or not\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n\n    .. versionadded:: 0.2\n    \"\"\"\n\n    # TODO: Check plotting code for classification tasks and add it to generic classification dataset as well\n    raise NotImplementedError\n\n    image = sample[\"image\"]\n    if isinstance(image, dict):\n        image = image[self.rgb_modality]\n    if isinstance(image, torch.Tensor):\n        image = image.numpy()\n    image = image.take(self.rgb_indices, axis=0)\n    image = np.transpose(image, (1, 2, 0))\n    image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n    image = np.clip(image, 0, 1)\n\n    label_mask = sample[\"mask\"]\n    if isinstance(label_mask, torch.Tensor):\n        label_mask = label_mask.numpy()\n\n    showing_predictions = \"prediction\" in sample\n    if showing_predictions:\n        prediction_mask = sample[\"prediction\"]\n        if isinstance(prediction_mask, torch.Tensor):\n            prediction_mask = prediction_mask.numpy()\n\n    return self._plot_sample(\n        image,\n        label_mask,\n        prediction=prediction_mask if showing_predictions else None,\n        suptitle=suptitle,\n        show_axes=show_axes,\n    )\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalDataset","title":"<code>terratorch.datasets.generic_multimodal_dataset.GenericMultimodalDataset</code>","text":"<p>               Bases: <code>NonGeoDataset</code>, <code>ABC</code></p> <p>This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset.</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>class GenericMultimodalDataset(NonGeoDataset, ABC):\n    \"\"\"\n    This is a generic dataset class to be used for instantiating datasets from arguments.\n    Ideally, one would create a dataset class specific to a dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_root: dict[str, Path | str],\n        label_data_root: Path | str | list[Path | str] | None = None,\n        image_grep: dict[str, str] | None = \"*\",\n        label_grep: str | None = \"*\",\n        split: Path | None = None,\n        image_modalities: list[str] | None = None,\n        rgb_modality: str | None = None,\n        rgb_indices: list[int] | None = None,\n        allow_missing_modalities: bool = False,\n        allow_substring_file_names: bool = True,\n        dataset_bands: dict[str, list] | None = None,\n        output_bands: dict[str, list] | None = None,\n        constant_scale: dict[str, float] = None,\n        transform: A.Compose | dict | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: float | None = -1,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n        channel_position: int = -3,\n        scalar_label: bool = False,\n        data_with_sample_dim: bool = False,\n        concat_bands: bool = False,\n        prediction_mode: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n                data, with modalities as keys.\n            label_data_root (Path, optional): Path to data root directory with labels or csv/parquet files with\n                image-level labels. Needs to be specified for supervised tasks.\n            image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n                images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            label_grep (str, optional): Regular expression appended to label_data_root to find labels or mask files.\n                Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            split (Path, optional): Path to file containing samples prefixes to be used for this split.\n                The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n                sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n                files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n                If not specified, search samples based on files in data_root. Defaults to None.\n            image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n                The difference between all modalities and image_modalities are non-image modalities which are treated\n                differently during the transforms and are not modified but only converted into a tensor if possible.\n            rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n                TODO: Currently not implemented on a data module level!\n            allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n                image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n                If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n                Defaults to True.\n            dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n                as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n                that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n                of all modalities. Defaults to None.\n            output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n                provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n            constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n                keys. Can be subset of all modalities. Defaults to None.\n            transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n                modalities (transformation are shared between image modalities, e.g., similar crop or rotation).\n                Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization.\n                Not supported for multi-temporal data. The transform is not applied to non-image data, which is only\n                converted to tensors if possible. If dict, can include multiple transforms per modality which are\n                applied separately (no shared parameters between modalities).\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input data with this value.\n                If None, does no replacement. Defaults to None.\n            no_label_replace (float | None): Replace nan values in label with this value.\n                If none, does no replacement. Defaults to -1.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n            channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n            scalar_label (bool): Returns a image mask if False or otherwise the raw labels. Defaults to False.\n            concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n                that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n                Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n            prediction_mode (bool): Used to deactivate the checking for a label when it is not necessary.\n        \"\"\"\n\n        if prediction_mode:\n            label_data_root = None\n        else:\n            label_data_root = label_data_root or data_root\n\n        super().__init__()\n\n        self.prediction_mode = prediction_mode\n        self.split_file = split\n        self.modalities = list(data_root.keys())\n        assert \"mask\" not in self.modalities, \"Modality cannot be called 'mask'.\"\n        self.image_modalities = image_modalities or self.modalities\n        self.non_image_modalities = list(set(self.modalities) - set(image_modalities))\n        self.modalities = self.image_modalities + self.non_image_modalities  # Ensure image modalities to be first\n\n        if scalar_label:\n            self.non_image_modalities += [\"label\"]\n\n        # Order by modalities and convert path strings to lists as the code expects a list of paths per modality\n        data_root = {m: data_root[m] for m in self.modalities}\n\n        self.constant_scale = constant_scale or {}\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.reduce_zero_label = reduce_zero_label\n        self.expand_temporal_dimension = expand_temporal_dimension\n        self.channel_position = channel_position\n        self.scalar_label = scalar_label\n        self.data_with_sample_dim = data_with_sample_dim\n        self.concat_bands = concat_bands\n        assert not self.concat_bands or len(self.non_image_modalities) == 0, (\n            f\"concat_bands can only be used with image modalities, \"\n            f\"but non-image modalities are given: {self.non_image_modalities}\"\n        )\n        assert (\n            not self.concat_bands or not allow_missing_modalities\n        ), \"concat_bands cannot be used with allow_missing_modalities.\"\n\n        if self.expand_temporal_dimension and dataset_bands is None:\n            msg = \"Please provide dataset_bands when expand_temporal_dimension is True\"\n            raise Exception(msg)\n\n        # Load samples based on split file\n        if self.split_file is not None:\n            if str(self.split_file).endswith(\".txt\"):\n                with open(self.split_file) as f:\n                    split = f.readlines()\n                valid_files = [rf\"{substring.strip()}\" for substring in split]\n            else:\n                valid_files = list(load_table_data(self.split_file).index)\n\n        else:\n            image_files = {}\n            for m, m_paths in data_root.items():\n                image_files[m] = sorted(glob.glob(os.path.join(m_paths, image_grep[m])))\n            if label_data_root is not None:\n                image_files[\"mask\"] = sorted(glob.glob(os.path.join(label_data_root, label_grep)))\n\n            def get_file_id(file_name, mod):\n                glob_as_regex = '^' + ''.join('(.*?)' if ch == '*' else re.escape(ch)\n                                              for ch in image_grep[mod]) + '$'\n                stem = re.match(glob_as_regex, file_name).group(1)\n                if allow_substring_file_names:\n                    # Remove file extensions\n                    stem = os.path.splitext(stem)[0]\n                # Remote folder structure\n                return os.path.basename(stem)\n\n            if allow_missing_modalities:\n                valid_files = list(set([get_file_id(file, mod)\n                                        for mod, files in image_files.items()\n                                        for file in files\n                                        ]))\n            else:\n                valid_files = [get_file_id(file, self.modalities[0]) for file in image_files[self.modalities[0]]]\n\n        self.samples = []\n        num_modalities = len(self.modalities) + int(label_data_root is not None)\n\n        # Check for parquet and csv files with modality data and read the file\n\n        for m, m_path in data_root.items():\n            if os.path.isfile(m_path):\n                data_root[m] = load_table_data(m_path)\n                # Check for some sample keys\n                if not any(f in data_root[m].index for f in valid_files[:100]):\n                    warnings.warn(f\"Sample key expected in table index (first column) for {m} (file: {m_path}). \"\n                                  f\"{valid_files[:3]+['...']} are not in index {list(data_root[m].index[:3])+['...']}.\")\n        if label_data_root is not None:\n            if os.path.isfile(label_data_root):\n                label_data_root = load_table_data(label_data_root)\n                # Check for some sample keys\n                if not any(f in label_data_root.index for f in valid_files[:100]):\n                    warnings.warn(f\"Keys expected in table index (first column) for labels (file: {label_data_root}). \"\n                                  f\"The keys {valid_files[:3] + ['...']} are not in the index.\")\n\n        # Iterate over all files in split\n        for file in valid_files:\n            sample = {}\n            # Iterate over all modalities\n            for m, m_path in data_root.items():\n                if isinstance(m_path, pd.DataFrame):\n                    # Add tabular data to sample\n                    sample[m] = m_path.loc[file].values\n                elif allow_substring_file_names:\n                    # Substring match with image_grep\n                    m_files = glob.glob(os.path.join(m_path, file + image_grep[m]))\n                    if m_files:\n                        sample[m] = m_files[0]\n                else:\n                    # Exact match\n                    file_path = os.path.join(m_path, file)\n                    if os.path.exists(file_path):\n                        sample[m] = file_path\n\n            if label_data_root is not None:\n                if isinstance(label_data_root, pd.DataFrame):\n                    # Add tabular data to sample\n                    sample[\"mask\"] = label_data_root.loc[file].values\n                elif allow_substring_file_names:\n                    # Substring match with label_grep\n                    l_files = glob.glob(os.path.join(label_data_root, file + label_grep))\n                    if l_files:\n                        sample[\"mask\"] = l_files[0]\n                else:\n                    # Exact match\n                    file_path = os.path.join(label_data_root, file)\n                    if os.path.exists(file_path):\n                        sample[\"mask\"] = file_path\n                if \"mask\" not in sample:\n                    # Only add sample if mask is present\n                    break\n\n            if len(sample) == num_modalities or allow_missing_modalities:\n                self.samples.append(sample)\n\n        self.rgb_modality = rgb_modality or self.modalities[0]\n        self.rgb_indices = rgb_indices or [0, 1, 2]\n\n        if dataset_bands is not None:\n            self.dataset_bands = {m: generate_bands_intervals(m_bands) for m, m_bands in dataset_bands.items()}\n        else:\n            self.dataset_bands = None\n        if output_bands is not None:\n            self.output_bands = {m: generate_bands_intervals(m_bands) for m, m_bands in output_bands.items()}\n            for modality in self.modalities:\n                if modality in self.output_bands and modality not in self.dataset_bands:\n                    msg = f\"If output bands are provided, dataset_bands must also be provided (modality: {modality})\"\n                    raise Exception(msg)  # noqa: PLE0101\n        else:\n            self.output_bands = {}\n\n        self.filter_indices = {}\n        if self.output_bands:\n            for m in self.output_bands.keys():\n                if m not in self.output_bands or self.output_bands[m] == self.dataset_bands[m]:\n                    continue\n                if len(set(self.output_bands[m]) &amp; set(self.dataset_bands[m])) != len(self.output_bands[m]):\n                    msg = f\"Output bands must be a subset of dataset bands (Modality: {m})\"\n                    raise Exception(msg)\n\n                self.filter_indices[m] = [self.dataset_bands[m].index(band) for band in self.output_bands[m]]\n\n            if not self.channel_position:\n                logger.warning(\n                    \"output_bands is defined but no channel_position is provided. \"\n                    \"Channels must be in the last dimension, otherwise provide channel_position.\"\n                )\n\n        # If no transform is given, apply only to transform to torch tensor\n        if isinstance(transform, A.Compose):\n            self.transform = MultimodalTransforms(transform,\n                                                  non_image_modalities=self.non_image_modalities + ['label']\n                                                  if scalar_label else self.non_image_modalities)\n        elif transform is None:\n            self.transform = MultimodalToTensor(self.modalities)\n        else:\n            # Modality-specific transforms\n            transform = {m: transform[m] if m in transform else default_transform for m in self.modalities}\n            self.transform = MultimodalTransforms(transform, shared=False)\n\n        # Ignore rasterio warning for not geo-referenced files\n        import rasterio\n\n        warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n\n    def __len__(self) -&gt; int:\n        return len(self.samples)\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        output = {}\n        if isinstance(index, tuple):\n            # Load only sampled modalities instead of all modalities\n            # (see sample_num_modalities in GenericMultiModalDataModule for details)\n            index, modalities = index\n            sample = {m: self.samples[index][m] for m in modalities}\n        else:\n            sample = self.samples[index]\n\n        for modality, file in sample.items():\n            data = self._load_file(\n                file,\n                nan_replace=self.no_label_replace if modality == \"mask\" else self.no_data_replace,\n                modality=modality,\n            )\n\n            # Expand temporal dim\n            if modality in self.filter_indices and self.expand_temporal_dimension:\n                data = rearrange(\n                    data, \"(channels time) h w -&gt; channels time h w\", channels=len(self.dataset_bands[modality])\n                )\n\n            if modality == \"mask\" and not self.scalar_label:\n                # tasks expect image masks without channel dim\n                data = data[0]\n\n            if modality in self.image_modalities and len(data.shape) &gt;= 3 and self.channel_position:\n                # to channels last (required by albumentations)\n                data = np.moveaxis(data, self.channel_position, -1)\n\n            if modality in self.filter_indices:\n                data = data[..., self.filter_indices[modality]]\n\n            if modality in self.constant_scale:\n                data = data.astype(np.float32) * self.constant_scale[modality]\n\n            output[modality] = data\n\n        if \"mask\" in output:\n            if self.reduce_zero_label:\n                output[\"mask\"] -= 1\n            if self.scalar_label:\n                output[\"label\"] = output.pop(\"mask\")\n\n        if self.transform:\n            output = self.transform(output)\n\n        if self.concat_bands:\n            # Concatenate bands of all image modalities\n            data = [output.pop(m) for m in self.image_modalities if m in output]\n            output[\"image\"] = torch.cat(data, dim=1 if self.data_with_sample_dim else 0)\n        else:\n            # Tasks expect data to be stored in \"image\", moving modalities to image dict\n            output[\"image\"] = {m: output.pop(m) for m in self.modalities if m in output}\n\n        output[\"filename\"] = self.samples[index]\n\n        return output\n\n    def _load_file(self, path, nan_replace: int | float | None = None, modality: str | None = None) -&gt; xr.DataArray:\n        if isinstance(path, np.ndarray):\n            # data was loaded from table and is saved in memory\n            data = path\n        elif path.endswith(\".zarr\") or path.endswith(\".zarr.zip\"):\n            data = xr.open_zarr(path, mask_and_scale=True)\n            data_var = modality if modality in data.data_vars else list(data.data_vars)[0]\n            data = data[data_var].to_numpy()\n        elif path.endswith(\".npy\"):\n            data = np.load(path)\n        else:\n            data = rioxarray.open_rasterio(path, masked=True).to_numpy()\n\n        if nan_replace is not None:\n            data = np.nan_to_num(data, nan=nan_replace)\n        return data\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            suptitle: optional string to use as a suptitle\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n\n        .. versionadded:: 0.2\n        \"\"\"\n        image = sample[\"image\"]\n        if isinstance(image, dict):\n            image = image[self.rgb_modality]\n        if isinstance(image, torch.Tensor):\n            image = image.numpy()\n        image = image.take(self.rgb_indices, axis=0)\n        image = np.transpose(image, (1, 2, 0))\n        image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n        image = np.clip(image, 0, 1)\n\n        if \"mask\" in sample:\n            mask = sample[\"mask\"]\n            if isinstance(mask, torch.Tensor):\n                mask = mask.numpy()\n            if mask.ndim == 2:\n                mask = np.expand_dims(mask, axis=-1)\n            # Convert masked regions to 0.\n            mask = mask * -1 + 1\n        else:\n            mask = None\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"]\n            if isinstance(image, dict):\n                prediction = prediction[self.rgb_modality]\n            if isinstance(prediction, torch.Tensor):\n                prediction = prediction.numpy()\n            # Assuming reconstructed image\n            prediction = prediction.take(self.rgb_indices, axis=0)\n            prediction = np.transpose(prediction, (1, 2, 0))\n            prediction = (prediction - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n            prediction = np.clip(prediction, 0, 1)\n        else:\n            prediction = None\n\n        return self._plot_sample(\n            image,\n            mask=mask,\n            prediction=prediction,\n            suptitle=suptitle,\n        )\n\n    @staticmethod\n    def _plot_sample(image, mask=None, prediction=None, suptitle=None):\n        num_images = 1 + int(mask is not None) + int(prediction is not None)\n        fig, ax = plt.subplots(1, num_images, figsize=(5*num_images, 5), layout=\"compressed\")\n\n        ax[0].axis(\"off\")\n        ax[0].imshow(image)\n\n        if mask is not None:\n            ax[1].axis(\"off\")\n            ax[1].imshow(image * mask)\n\n        if prediction is not None:\n            ax[num_images-1].axis(\"off\")\n            ax[num_images-1].imshow(prediction)\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n        return fig\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalDataset.__init__","title":"<code>__init__(data_root, label_data_root=None, image_grep='*', label_grep='*', split=None, image_modalities=None, rgb_modality=None, rgb_indices=None, allow_missing_modalities=False, allow_substring_file_names=True, dataset_bands=None, output_bands=None, constant_scale=None, transform=None, no_data_replace=None, no_label_replace=-1, expand_temporal_dimension=False, reduce_zero_label=False, channel_position=-3, scalar_label=False, data_with_sample_dim=False, concat_bands=False, prediction_mode=False, *args, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to data root directory or csv/parquet files with image-level data, with modalities as keys.</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with labels or csv/parquet files with image-level labels. Needs to be specified for supervised tasks.</p> <code>None</code> <code>image_grep</code> <code>dict[str]</code> <p>Dictionary with regular expression appended to data_root to find input images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to label_data_root to find labels or mask files. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing samples prefixes to be used for this split. The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise, files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]). If not specified, search samples based on files in data_root. Defaults to None.</p> <code>None</code> <code>image_modalities(list[str],</code> <code>optional</code> <p>List of pixel-level raster modalities. Defaults to data_root.keys(). The difference between all modalities and image_modalities are non-image modalities which are treated differently during the transforms and are not modified but only converted into a tensor if possible.</p> required <code>rgb_modality</code> <code>str</code> <p>Modality used for RGB plots. Defaults to first modality in data_root.keys().</p> <code>None</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>allow_missing_modalities</code> <code>bool</code> <p>Allow missing modalities during data loading. Defaults to False. TODO: Currently not implemented on a data module level!</p> <code>False</code> <code>allow_substring_file_names</code> <code>bool</code> <p>Allow substrings during sample identification by adding image or label grep to the sample prefixes. If False, treats sample prefixes as full file names. If True and no split file is provided, considers the file stem as prefix, otherwise the full file name. Defaults to True.</p> <code>True</code> <code>dataset_bands</code> <code>dict[list]</code> <p>Bands present in the dataset, provided in a dictionary with modalities as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset of all modalities. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>dict[list]</code> <p>Bands that should be output by the dataset as named by dataset_bands, provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>dict[float]</code> <p>Factor to multiply data values by, provided as a dictionary with modalities as keys. Can be subset of all modalities. Defaults to None.</p> <code>None</code> <code>transform</code> <code>Compose | dict | None</code> <p>Albumentations transform to be applied to all image modalities (transformation are shared between image modalities, e.g., similar crop or rotation). Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. The transform is not applied to non-image data, which is only converted to tensors if possible. If dict, can include multiple transforms per modality which are applied separately (no shared parameters between modalities). Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input data with this value. If None, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>float | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>-1</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Only works with image modalities. Is only applied to modalities with defined dataset_bands. Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>channel_position</code> <code>int</code> <p>Position of the channel dimension in the image modalities. Defaults to -3.</p> <code>-3</code> <code>scalar_label</code> <code>bool</code> <p>Returns a image mask if False or otherwise the raw labels. Defaults to False.</p> <code>False</code> <code>concat_bands</code> <code>bool</code> <p>Concatenate all image modalities along the band dimension into a single \"image\", so that it can be processed by single-modal models. Concatenate in the order of provided modalities. Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.</p> <code>False</code> <code>prediction_mode</code> <code>bool</code> <p>Used to deactivate the checking for a label when it is not necessary.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: dict[str, Path | str],\n    label_data_root: Path | str | list[Path | str] | None = None,\n    image_grep: dict[str, str] | None = \"*\",\n    label_grep: str | None = \"*\",\n    split: Path | None = None,\n    image_modalities: list[str] | None = None,\n    rgb_modality: str | None = None,\n    rgb_indices: list[int] | None = None,\n    allow_missing_modalities: bool = False,\n    allow_substring_file_names: bool = True,\n    dataset_bands: dict[str, list] | None = None,\n    output_bands: dict[str, list] | None = None,\n    constant_scale: dict[str, float] = None,\n    transform: A.Compose | dict | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: float | None = -1,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n    channel_position: int = -3,\n    scalar_label: bool = False,\n    data_with_sample_dim: bool = False,\n    concat_bands: bool = False,\n    prediction_mode: bool = False,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n            data, with modalities as keys.\n        label_data_root (Path, optional): Path to data root directory with labels or csv/parquet files with\n            image-level labels. Needs to be specified for supervised tasks.\n        image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n            images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        label_grep (str, optional): Regular expression appended to label_data_root to find labels or mask files.\n            Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        split (Path, optional): Path to file containing samples prefixes to be used for this split.\n            The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n            sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n            files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n            If not specified, search samples based on files in data_root. Defaults to None.\n        image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n            The difference between all modalities and image_modalities are non-image modalities which are treated\n            differently during the transforms and are not modified but only converted into a tensor if possible.\n        rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n            TODO: Currently not implemented on a data module level!\n        allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n            image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n            If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n            Defaults to True.\n        dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n            as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n            that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n            of all modalities. Defaults to None.\n        output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n            provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n        constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n            keys. Can be subset of all modalities. Defaults to None.\n        transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n            modalities (transformation are shared between image modalities, e.g., similar crop or rotation).\n            Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization.\n            Not supported for multi-temporal data. The transform is not applied to non-image data, which is only\n            converted to tensors if possible. If dict, can include multiple transforms per modality which are\n            applied separately (no shared parameters between modalities).\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input data with this value.\n            If None, does no replacement. Defaults to None.\n        no_label_replace (float | None): Replace nan values in label with this value.\n            If none, does no replacement. Defaults to -1.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n        channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n        scalar_label (bool): Returns a image mask if False or otherwise the raw labels. Defaults to False.\n        concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n            that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n            Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n        prediction_mode (bool): Used to deactivate the checking for a label when it is not necessary.\n    \"\"\"\n\n    if prediction_mode:\n        label_data_root = None\n    else:\n        label_data_root = label_data_root or data_root\n\n    super().__init__()\n\n    self.prediction_mode = prediction_mode\n    self.split_file = split\n    self.modalities = list(data_root.keys())\n    assert \"mask\" not in self.modalities, \"Modality cannot be called 'mask'.\"\n    self.image_modalities = image_modalities or self.modalities\n    self.non_image_modalities = list(set(self.modalities) - set(image_modalities))\n    self.modalities = self.image_modalities + self.non_image_modalities  # Ensure image modalities to be first\n\n    if scalar_label:\n        self.non_image_modalities += [\"label\"]\n\n    # Order by modalities and convert path strings to lists as the code expects a list of paths per modality\n    data_root = {m: data_root[m] for m in self.modalities}\n\n    self.constant_scale = constant_scale or {}\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.reduce_zero_label = reduce_zero_label\n    self.expand_temporal_dimension = expand_temporal_dimension\n    self.channel_position = channel_position\n    self.scalar_label = scalar_label\n    self.data_with_sample_dim = data_with_sample_dim\n    self.concat_bands = concat_bands\n    assert not self.concat_bands or len(self.non_image_modalities) == 0, (\n        f\"concat_bands can only be used with image modalities, \"\n        f\"but non-image modalities are given: {self.non_image_modalities}\"\n    )\n    assert (\n        not self.concat_bands or not allow_missing_modalities\n    ), \"concat_bands cannot be used with allow_missing_modalities.\"\n\n    if self.expand_temporal_dimension and dataset_bands is None:\n        msg = \"Please provide dataset_bands when expand_temporal_dimension is True\"\n        raise Exception(msg)\n\n    # Load samples based on split file\n    if self.split_file is not None:\n        if str(self.split_file).endswith(\".txt\"):\n            with open(self.split_file) as f:\n                split = f.readlines()\n            valid_files = [rf\"{substring.strip()}\" for substring in split]\n        else:\n            valid_files = list(load_table_data(self.split_file).index)\n\n    else:\n        image_files = {}\n        for m, m_paths in data_root.items():\n            image_files[m] = sorted(glob.glob(os.path.join(m_paths, image_grep[m])))\n        if label_data_root is not None:\n            image_files[\"mask\"] = sorted(glob.glob(os.path.join(label_data_root, label_grep)))\n\n        def get_file_id(file_name, mod):\n            glob_as_regex = '^' + ''.join('(.*?)' if ch == '*' else re.escape(ch)\n                                          for ch in image_grep[mod]) + '$'\n            stem = re.match(glob_as_regex, file_name).group(1)\n            if allow_substring_file_names:\n                # Remove file extensions\n                stem = os.path.splitext(stem)[0]\n            # Remote folder structure\n            return os.path.basename(stem)\n\n        if allow_missing_modalities:\n            valid_files = list(set([get_file_id(file, mod)\n                                    for mod, files in image_files.items()\n                                    for file in files\n                                    ]))\n        else:\n            valid_files = [get_file_id(file, self.modalities[0]) for file in image_files[self.modalities[0]]]\n\n    self.samples = []\n    num_modalities = len(self.modalities) + int(label_data_root is not None)\n\n    # Check for parquet and csv files with modality data and read the file\n\n    for m, m_path in data_root.items():\n        if os.path.isfile(m_path):\n            data_root[m] = load_table_data(m_path)\n            # Check for some sample keys\n            if not any(f in data_root[m].index for f in valid_files[:100]):\n                warnings.warn(f\"Sample key expected in table index (first column) for {m} (file: {m_path}). \"\n                              f\"{valid_files[:3]+['...']} are not in index {list(data_root[m].index[:3])+['...']}.\")\n    if label_data_root is not None:\n        if os.path.isfile(label_data_root):\n            label_data_root = load_table_data(label_data_root)\n            # Check for some sample keys\n            if not any(f in label_data_root.index for f in valid_files[:100]):\n                warnings.warn(f\"Keys expected in table index (first column) for labels (file: {label_data_root}). \"\n                              f\"The keys {valid_files[:3] + ['...']} are not in the index.\")\n\n    # Iterate over all files in split\n    for file in valid_files:\n        sample = {}\n        # Iterate over all modalities\n        for m, m_path in data_root.items():\n            if isinstance(m_path, pd.DataFrame):\n                # Add tabular data to sample\n                sample[m] = m_path.loc[file].values\n            elif allow_substring_file_names:\n                # Substring match with image_grep\n                m_files = glob.glob(os.path.join(m_path, file + image_grep[m]))\n                if m_files:\n                    sample[m] = m_files[0]\n            else:\n                # Exact match\n                file_path = os.path.join(m_path, file)\n                if os.path.exists(file_path):\n                    sample[m] = file_path\n\n        if label_data_root is not None:\n            if isinstance(label_data_root, pd.DataFrame):\n                # Add tabular data to sample\n                sample[\"mask\"] = label_data_root.loc[file].values\n            elif allow_substring_file_names:\n                # Substring match with label_grep\n                l_files = glob.glob(os.path.join(label_data_root, file + label_grep))\n                if l_files:\n                    sample[\"mask\"] = l_files[0]\n            else:\n                # Exact match\n                file_path = os.path.join(label_data_root, file)\n                if os.path.exists(file_path):\n                    sample[\"mask\"] = file_path\n            if \"mask\" not in sample:\n                # Only add sample if mask is present\n                break\n\n        if len(sample) == num_modalities or allow_missing_modalities:\n            self.samples.append(sample)\n\n    self.rgb_modality = rgb_modality or self.modalities[0]\n    self.rgb_indices = rgb_indices or [0, 1, 2]\n\n    if dataset_bands is not None:\n        self.dataset_bands = {m: generate_bands_intervals(m_bands) for m, m_bands in dataset_bands.items()}\n    else:\n        self.dataset_bands = None\n    if output_bands is not None:\n        self.output_bands = {m: generate_bands_intervals(m_bands) for m, m_bands in output_bands.items()}\n        for modality in self.modalities:\n            if modality in self.output_bands and modality not in self.dataset_bands:\n                msg = f\"If output bands are provided, dataset_bands must also be provided (modality: {modality})\"\n                raise Exception(msg)  # noqa: PLE0101\n    else:\n        self.output_bands = {}\n\n    self.filter_indices = {}\n    if self.output_bands:\n        for m in self.output_bands.keys():\n            if m not in self.output_bands or self.output_bands[m] == self.dataset_bands[m]:\n                continue\n            if len(set(self.output_bands[m]) &amp; set(self.dataset_bands[m])) != len(self.output_bands[m]):\n                msg = f\"Output bands must be a subset of dataset bands (Modality: {m})\"\n                raise Exception(msg)\n\n            self.filter_indices[m] = [self.dataset_bands[m].index(band) for band in self.output_bands[m]]\n\n        if not self.channel_position:\n            logger.warning(\n                \"output_bands is defined but no channel_position is provided. \"\n                \"Channels must be in the last dimension, otherwise provide channel_position.\"\n            )\n\n    # If no transform is given, apply only to transform to torch tensor\n    if isinstance(transform, A.Compose):\n        self.transform = MultimodalTransforms(transform,\n                                              non_image_modalities=self.non_image_modalities + ['label']\n                                              if scalar_label else self.non_image_modalities)\n    elif transform is None:\n        self.transform = MultimodalToTensor(self.modalities)\n    else:\n        # Modality-specific transforms\n        transform = {m: transform[m] if m in transform else default_transform for m in self.modalities}\n        self.transform = MultimodalTransforms(transform, shared=False)\n\n    # Ignore rasterio warning for not geo-referenced files\n    import rasterio\n\n    warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n</code></pre>"},{"location":"package/generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalDataset.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> <p>.. versionadded:: 0.2</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        suptitle: optional string to use as a suptitle\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n\n    .. versionadded:: 0.2\n    \"\"\"\n    image = sample[\"image\"]\n    if isinstance(image, dict):\n        image = image[self.rgb_modality]\n    if isinstance(image, torch.Tensor):\n        image = image.numpy()\n    image = image.take(self.rgb_indices, axis=0)\n    image = np.transpose(image, (1, 2, 0))\n    image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n    image = np.clip(image, 0, 1)\n\n    if \"mask\" in sample:\n        mask = sample[\"mask\"]\n        if isinstance(mask, torch.Tensor):\n            mask = mask.numpy()\n        if mask.ndim == 2:\n            mask = np.expand_dims(mask, axis=-1)\n        # Convert masked regions to 0.\n        mask = mask * -1 + 1\n    else:\n        mask = None\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"]\n        if isinstance(image, dict):\n            prediction = prediction[self.rgb_modality]\n        if isinstance(prediction, torch.Tensor):\n            prediction = prediction.numpy()\n        # Assuming reconstructed image\n        prediction = prediction.take(self.rgb_indices, axis=0)\n        prediction = np.transpose(prediction, (1, 2, 0))\n        prediction = (prediction - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n        prediction = np.clip(prediction, 0, 1)\n    else:\n        prediction = None\n\n    return self._plot_sample(\n        image,\n        mask=mask,\n        prediction=prediction,\n        suptitle=suptitle,\n    )\n</code></pre>"},{"location":"package/heads/","title":"Heads","text":"<p>Info</p> <p>Heads are linear layers or MLPs that are applied on top of the decoder and produce the final output. Parameters like <code>in_channels</code> and <code>num_classes</code> are normally selected by the model factories and do not need to be defined manually. </p>"},{"location":"package/heads/#terratorch.models.heads.regression_head.RegressionHead","title":"<code>terratorch.models.heads.regression_head.RegressionHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Regression head</p> Source code in <code>terratorch/models/heads/regression_head.py</code> <pre><code>class RegressionHead(nn.Module):\n    \"\"\"Regression head\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        final_act: nn.Module | str | None = None,\n        learned_upscale_layers: int = 0,\n        channel_list: list[int] | None = None,\n        batch_norm: bool = True,\n        dropout: float = 0,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            in_channels (int): Number of input channels\n            final_act (nn.Module | None, optional): Final activation to be applied. Defaults to None.\n            learned_upscale_layers (int, optional): Number of Pixelshuffle layers to create. Each upscales 2x.\n                Defaults to 0.\n            channel_list (list[int] | None, optional): List with number of channels for each Conv\n                layer to be created. Defaults to None.\n            batch_norm (bool, optional): Whether to apply batch norm. Defaults to True.\n            dropout (float, optional): Dropout value to apply. Defaults to 0.\n\n        \"\"\"\n        super().__init__()\n        self.learned_upscale_layers = learned_upscale_layers\n        self.final_act = final_act if final_act else nn.Identity()\n        if isinstance(final_act, str):\n            module_name, class_name = final_act.rsplit(\".\", 1)\n            target_class = getattr(importlib.import_module(module_name), class_name)\n            self.final_act = target_class()\n        pre_layers = []\n        if learned_upscale_layers != 0:\n            learned_upscale = nn.Sequential(\n                *[PixelShuffleUpscale(in_channels) for _ in range(self.learned_upscale_layers)]\n            )\n            pre_layers.append(learned_upscale)\n\n        if channel_list is None:\n            pre_head = nn.Identity()\n        else:\n\n            def block(in_channels, out_channels):\n                return nn.Sequential(\n                    nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=False),\n                    nn.BatchNorm2d(out_channels),\n                    nn.ReLU(inplace=True),\n                )\n\n            channel_list = [in_channels, *channel_list]\n            pre_head = nn.Sequential(\n                *[block(channel_list[i], channel_list[i + 1]) for i in range(len(channel_list) - 1)]\n            )\n            in_channels = channel_list[-1]\n            pre_layers.append(pre_head)\n        dropout = nn.Dropout2d(dropout)\n        final_layer = nn.Conv2d(in_channels=in_channels, out_channels=1, kernel_size=1)\n        self.head = nn.Sequential(*[*pre_layers, dropout, final_layer])\n\n    def forward(self, x):\n        output = self.head(x)\n        return self.final_act(output)\n</code></pre>"},{"location":"package/heads/#terratorch.models.heads.regression_head.RegressionHead.__init__","title":"<code>__init__(in_channels, final_act=None, learned_upscale_layers=0, channel_list=None, batch_norm=True, dropout=0)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>final_act</code> <code>Module | None</code> <p>Final activation to be applied. Defaults to None.</p> <code>None</code> <code>learned_upscale_layers</code> <code>int</code> <p>Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0.</p> <code>0</code> <code>channel_list</code> <code>list[int] | None</code> <p>List with number of channels for each Conv layer to be created. Defaults to None.</p> <code>None</code> <code>batch_norm</code> <code>bool</code> <p>Whether to apply batch norm. Defaults to True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout value to apply. Defaults to 0.</p> <code>0</code> Source code in <code>terratorch/models/heads/regression_head.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    final_act: nn.Module | str | None = None,\n    learned_upscale_layers: int = 0,\n    channel_list: list[int] | None = None,\n    batch_norm: bool = True,\n    dropout: float = 0,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        in_channels (int): Number of input channels\n        final_act (nn.Module | None, optional): Final activation to be applied. Defaults to None.\n        learned_upscale_layers (int, optional): Number of Pixelshuffle layers to create. Each upscales 2x.\n            Defaults to 0.\n        channel_list (list[int] | None, optional): List with number of channels for each Conv\n            layer to be created. Defaults to None.\n        batch_norm (bool, optional): Whether to apply batch norm. Defaults to True.\n        dropout (float, optional): Dropout value to apply. Defaults to 0.\n\n    \"\"\"\n    super().__init__()\n    self.learned_upscale_layers = learned_upscale_layers\n    self.final_act = final_act if final_act else nn.Identity()\n    if isinstance(final_act, str):\n        module_name, class_name = final_act.rsplit(\".\", 1)\n        target_class = getattr(importlib.import_module(module_name), class_name)\n        self.final_act = target_class()\n    pre_layers = []\n    if learned_upscale_layers != 0:\n        learned_upscale = nn.Sequential(\n            *[PixelShuffleUpscale(in_channels) for _ in range(self.learned_upscale_layers)]\n        )\n        pre_layers.append(learned_upscale)\n\n    if channel_list is None:\n        pre_head = nn.Identity()\n    else:\n\n        def block(in_channels, out_channels):\n            return nn.Sequential(\n                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True),\n            )\n\n        channel_list = [in_channels, *channel_list]\n        pre_head = nn.Sequential(\n            *[block(channel_list[i], channel_list[i + 1]) for i in range(len(channel_list) - 1)]\n        )\n        in_channels = channel_list[-1]\n        pre_layers.append(pre_head)\n    dropout = nn.Dropout2d(dropout)\n    final_layer = nn.Conv2d(in_channels=in_channels, out_channels=1, kernel_size=1)\n    self.head = nn.Sequential(*[*pre_layers, dropout, final_layer])\n</code></pre>"},{"location":"package/heads/#terratorch.models.heads.segmentation_head.SegmentationHead","title":"<code>terratorch.models.heads.segmentation_head.SegmentationHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Segmentation head</p> Source code in <code>terratorch/models/heads/segmentation_head.py</code> <pre><code>class SegmentationHead(nn.Module):\n    \"\"\"Segmentation head\"\"\"\n\n    def __init__(\n        self, in_channels: int, num_classes: int, channel_list: list[int] | None = None, dropout: float = 0\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            in_channels (int): Number of input channels\n            num_classes (int): Number of output classes\n            channel_list (list[int] | None, optional):  List with number of channels for each Conv\n                layer to be created. Defaults to None.\n            dropout (float, optional): Dropout value to apply. Defaults to 0.\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        if channel_list is None:\n            pre_head = nn.Identity()\n        else:\n\n            def block(in_channels, out_channels):\n                return nn.Sequential(\n                    nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1), nn.ReLU()\n                )\n\n            channel_list = [in_channels, *channel_list]\n            pre_head = nn.Sequential(\n                *[block(channel_list[i], channel_list[i + 1]) for i in range(len(channel_list) - 1)]\n            )\n            in_channels = channel_list[-1]\n        dropout = nn.Identity() if dropout == 0 else nn.Dropout(dropout)\n        self.head = nn.Sequential(\n            pre_head,\n            dropout,\n            nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=num_classes,\n                kernel_size=1,\n            ),\n        )\n\n    def forward(self, x):\n        return self.head(x)\n</code></pre>"},{"location":"package/heads/#terratorch.models.heads.segmentation_head.SegmentationHead.__init__","title":"<code>__init__(in_channels, num_classes, channel_list=None, dropout=0)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes</p> required <code>channel_list</code> <code>list[int] | None</code> <p>List with number of channels for each Conv layer to be created. Defaults to None.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout value to apply. Defaults to 0.</p> <code>0</code> Source code in <code>terratorch/models/heads/segmentation_head.py</code> <pre><code>def __init__(\n    self, in_channels: int, num_classes: int, channel_list: list[int] | None = None, dropout: float = 0\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        in_channels (int): Number of input channels\n        num_classes (int): Number of output classes\n        channel_list (list[int] | None, optional):  List with number of channels for each Conv\n            layer to be created. Defaults to None.\n        dropout (float, optional): Dropout value to apply. Defaults to 0.\n    \"\"\"\n    super().__init__()\n    self.num_classes = num_classes\n    if channel_list is None:\n        pre_head = nn.Identity()\n    else:\n\n        def block(in_channels, out_channels):\n            return nn.Sequential(\n                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1), nn.ReLU()\n            )\n\n        channel_list = [in_channels, *channel_list]\n        pre_head = nn.Sequential(\n            *[block(channel_list[i], channel_list[i + 1]) for i in range(len(channel_list) - 1)]\n        )\n        in_channels = channel_list[-1]\n    dropout = nn.Identity() if dropout == 0 else nn.Dropout(dropout)\n    self.head = nn.Sequential(\n        pre_head,\n        dropout,\n        nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=num_classes,\n            kernel_size=1,\n        ),\n    )\n</code></pre>"},{"location":"package/heads/#terratorch.models.heads.classification_head.ClassificationHead","title":"<code>terratorch.models.heads.classification_head.ClassificationHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Classification head</p> Source code in <code>terratorch/models/heads/classification_head.py</code> <pre><code>class ClassificationHead(nn.Module):\n    \"\"\"Classification head\"\"\"\n\n    # how to allow cls token?\n    def __init__(\n        self,\n        in_dim: int,\n        num_classes: int,\n        dim_list: list[int] | None = None,\n        dropout: float = 0,\n        linear_after_pool: bool = False,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            in_dim (int): Input dimensionality\n            num_classes (int): Number of output classes\n            dim_list (list[int] | None, optional):  List with number of dimensions for each Linear\n                layer to be created. Defaults to None.\n            dropout (float, optional): Dropout value to apply. Defaults to 0.\n            linear_after_pool (bool, optional): Apply pooling first, then apply the linear layer. Defaults to False\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.linear_after_pool = linear_after_pool\n        if dim_list is None:\n            pre_head = nn.Identity()\n        else:\n\n            def block(in_dim, out_dim):\n                return nn.Sequential(nn.Linear(in_features=in_dim, out_features=out_dim), nn.ReLU())\n\n            dim_list = [in_dim, *dim_list]\n            pre_head = nn.Sequential(*[block(dim_list[i], dim_list[i + 1]) for i in range(len(dim_list) - 1)])\n            in_dim = dim_list[-1]\n        dropout = nn.Identity() if dropout == 0 else nn.Dropout(dropout)\n        self.head = nn.Sequential(\n            pre_head,\n            dropout,\n            nn.Linear(in_features=in_dim, out_features=num_classes),\n        )\n\n    def forward(self, x: Tensor):\n        x = x.reshape(x.shape[0], x.shape[1], -1).permute(0, 2, 1)\n\n        if self.linear_after_pool:\n            x = x.mean(axis=1)\n            out = self.head(x)\n        else:\n            x = self.head(x)\n            out = x.mean(axis=1)\n        return out\n</code></pre>"},{"location":"package/heads/#terratorch.models.heads.classification_head.ClassificationHead.__init__","title":"<code>__init__(in_dim, num_classes, dim_list=None, dropout=0, linear_after_pool=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input dimensionality</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes</p> required <code>dim_list</code> <code>list[int] | None</code> <p>List with number of dimensions for each Linear layer to be created. Defaults to None.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout value to apply. Defaults to 0.</p> <code>0</code> <code>linear_after_pool</code> <code>bool</code> <p>Apply pooling first, then apply the linear layer. Defaults to False</p> <code>False</code> Source code in <code>terratorch/models/heads/classification_head.py</code> <pre><code>def __init__(\n    self,\n    in_dim: int,\n    num_classes: int,\n    dim_list: list[int] | None = None,\n    dropout: float = 0,\n    linear_after_pool: bool = False,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        in_dim (int): Input dimensionality\n        num_classes (int): Number of output classes\n        dim_list (list[int] | None, optional):  List with number of dimensions for each Linear\n            layer to be created. Defaults to None.\n        dropout (float, optional): Dropout value to apply. Defaults to 0.\n        linear_after_pool (bool, optional): Apply pooling first, then apply the linear layer. Defaults to False\n    \"\"\"\n    super().__init__()\n    self.num_classes = num_classes\n    self.linear_after_pool = linear_after_pool\n    if dim_list is None:\n        pre_head = nn.Identity()\n    else:\n\n        def block(in_dim, out_dim):\n            return nn.Sequential(nn.Linear(in_features=in_dim, out_features=out_dim), nn.ReLU())\n\n        dim_list = [in_dim, *dim_list]\n        pre_head = nn.Sequential(*[block(dim_list[i], dim_list[i + 1]) for i in range(len(dim_list) - 1)])\n        in_dim = dim_list[-1]\n    dropout = nn.Identity() if dropout == 0 else nn.Dropout(dropout)\n    self.head = nn.Sequential(\n        pre_head,\n        dropout,\n        nn.Linear(in_features=in_dim, out_features=num_classes),\n    )\n</code></pre>"},{"location":"package/loss/","title":"Loss handler","text":""},{"location":"package/loss/#terratorch.tasks.loss_handler.LossHandler","title":"<code>terratorch.tasks.loss_handler.LossHandler</code>","text":"<p>Class to help handle the computation and logging of loss</p> Source code in <code>terratorch/tasks/loss_handler.py</code> <pre><code>class LossHandler:\n    \"\"\"Class to help handle the computation and logging of loss\"\"\"\n\n    def __init__(self, loss_prefix: str) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            loss_prefix (str): Prefix to be prepended to all the metrics (e.g. training).\n        \"\"\"\n        self.loss_prefix = loss_prefix\n\n    def compute_loss(\n        self,\n        model_output: ModelOutput,\n        ground_truth: Tensor,\n        criterion: Callable,\n        aux_loss_weights: dict[str, float] | None,\n    ) -&gt; dict[str, Tensor]:\n        \"\"\"Compute the loss for the mean decode head as well as other heads\n\n        Args:\n            model_output (ModelOutput): Output from the model\n            ground_truth (Tensor): Tensor with labels\n            criterion (Callable): Loss function to be applied\n            aux_loss_weights (Union[dict[str, float], None]): Dictionary of names of model auxiliary\n                heads and their weights\n\n        Raises:\n            Exception: If the keys in aux_loss_weights and the model output do not match, will raise an exception.\n\n        Returns:\n            dict[str, Tensor]: Dictionary of computed losses. Total loss is returned under the key \"loss\".\n                If there are auxiliary heads, the main decode head is returned under the key \"decode_head\".\n                All other heads are returned with the same key as their name.\n        \"\"\"\n\n        loss = self._compute_loss(model_output.output, ground_truth, criterion)\n        if not model_output.auxiliary_heads:\n            return {\"loss\": loss}\n\n        if aux_loss_weights is None:\n            msg = \"Auxiliary heads given with no aux_loss_weights\"\n            raise Exception(msg)\n        all_losses = {}\n        all_losses[\"decode_head\"] = loss\n        total_loss = loss.clone()\n        # incorporate aux heads\n        model_output_names = set(model_output.auxiliary_heads.keys())\n        aux_loss_names = set(aux_loss_weights.keys())\n        if aux_loss_names != model_output_names:\n            msg = f\"Found difference in declared auxiliary losses and model outputs.\\n \\\n                Found in declared losses but not in model output: {aux_loss_names - model_output_names}. \\n \\\n                Found in model output but not in delcared losses: {model_output_names - aux_loss_names}\"\n            raise Exception(msg)\n\n        for loss_name, loss_weight in aux_loss_weights.items():\n            output = model_output.auxiliary_heads[loss_name]\n            loss_value: Tensor = self._compute_loss(output, ground_truth, criterion)\n            all_losses[loss_name] = loss_value\n            total_loss = total_loss + loss_value * loss_weight\n\n        all_losses[\"loss\"] = total_loss\n        return all_losses\n\n    def _compute_loss(self, y_hat: Tensor, ground_truth: Tensor, criterion: Callable):\n        loss: Tensor = criterion(y_hat, ground_truth)\n        return loss\n\n    def log_loss(\n        self, log_function: Callable, loss_dict: dict[str, Tensor] | None = None, batch_size: int | None = None\n    ) -&gt; None:\n        \"\"\"Log the loss. If auxiliary heads exist, log the full loss suffix \"loss\", and then all other losses.\n\n        Args:\n            log_function (Callable): _description_\n            loss_dict (dict[str, Tensor], optional): _description_. Defaults to None.\n        \"\"\"\n\n        # dont alter passed dict\n        all_losses = dict(loss_dict)\n        full_loss = all_losses.pop(\"loss\")\n        log_function(f\"{self.loss_prefix}loss\", full_loss, sync_dist=True, batch_size=batch_size)\n\n        for loss_name, loss_value in all_losses.items():\n            log_function(\n                f\"{self.loss_prefix}{loss_name}\",\n                loss_value,\n                on_epoch=True,\n                on_step=True,\n                sync_dist=True,\n                batch_size=batch_size,\n            )\n</code></pre>"},{"location":"package/loss/#terratorch.tasks.loss_handler.LossHandler.__init__","title":"<code>__init__(loss_prefix)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>loss_prefix</code> <code>str</code> <p>Prefix to be prepended to all the metrics (e.g. training).</p> required Source code in <code>terratorch/tasks/loss_handler.py</code> <pre><code>def __init__(self, loss_prefix: str) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        loss_prefix (str): Prefix to be prepended to all the metrics (e.g. training).\n    \"\"\"\n    self.loss_prefix = loss_prefix\n</code></pre>"},{"location":"package/loss/#terratorch.tasks.loss_handler.LossHandler.compute_loss","title":"<code>compute_loss(model_output, ground_truth, criterion, aux_loss_weights)</code>","text":"<p>Compute the loss for the mean decode head as well as other heads</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>ModelOutput</code> <p>Output from the model</p> required <code>ground_truth</code> <code>Tensor</code> <p>Tensor with labels</p> required <code>criterion</code> <code>Callable</code> <p>Loss function to be applied</p> required <code>aux_loss_weights</code> <code>Union[dict[str, float], None]</code> <p>Dictionary of names of model auxiliary heads and their weights</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If the keys in aux_loss_weights and the model output do not match, will raise an exception.</p> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>dict[str, Tensor]: Dictionary of computed losses. Total loss is returned under the key \"loss\". If there are auxiliary heads, the main decode head is returned under the key \"decode_head\". All other heads are returned with the same key as their name.</p> Source code in <code>terratorch/tasks/loss_handler.py</code> <pre><code>def compute_loss(\n    self,\n    model_output: ModelOutput,\n    ground_truth: Tensor,\n    criterion: Callable,\n    aux_loss_weights: dict[str, float] | None,\n) -&gt; dict[str, Tensor]:\n    \"\"\"Compute the loss for the mean decode head as well as other heads\n\n    Args:\n        model_output (ModelOutput): Output from the model\n        ground_truth (Tensor): Tensor with labels\n        criterion (Callable): Loss function to be applied\n        aux_loss_weights (Union[dict[str, float], None]): Dictionary of names of model auxiliary\n            heads and their weights\n\n    Raises:\n        Exception: If the keys in aux_loss_weights and the model output do not match, will raise an exception.\n\n    Returns:\n        dict[str, Tensor]: Dictionary of computed losses. Total loss is returned under the key \"loss\".\n            If there are auxiliary heads, the main decode head is returned under the key \"decode_head\".\n            All other heads are returned with the same key as their name.\n    \"\"\"\n\n    loss = self._compute_loss(model_output.output, ground_truth, criterion)\n    if not model_output.auxiliary_heads:\n        return {\"loss\": loss}\n\n    if aux_loss_weights is None:\n        msg = \"Auxiliary heads given with no aux_loss_weights\"\n        raise Exception(msg)\n    all_losses = {}\n    all_losses[\"decode_head\"] = loss\n    total_loss = loss.clone()\n    # incorporate aux heads\n    model_output_names = set(model_output.auxiliary_heads.keys())\n    aux_loss_names = set(aux_loss_weights.keys())\n    if aux_loss_names != model_output_names:\n        msg = f\"Found difference in declared auxiliary losses and model outputs.\\n \\\n            Found in declared losses but not in model output: {aux_loss_names - model_output_names}. \\n \\\n            Found in model output but not in delcared losses: {model_output_names - aux_loss_names}\"\n        raise Exception(msg)\n\n    for loss_name, loss_weight in aux_loss_weights.items():\n        output = model_output.auxiliary_heads[loss_name]\n        loss_value: Tensor = self._compute_loss(output, ground_truth, criterion)\n        all_losses[loss_name] = loss_value\n        total_loss = total_loss + loss_value * loss_weight\n\n    all_losses[\"loss\"] = total_loss\n    return all_losses\n</code></pre>"},{"location":"package/loss/#terratorch.tasks.loss_handler.LossHandler.log_loss","title":"<code>log_loss(log_function, loss_dict=None, batch_size=None)</code>","text":"<p>Log the loss. If auxiliary heads exist, log the full loss suffix \"loss\", and then all other losses.</p> <p>Parameters:</p> Name Type Description Default <code>log_function</code> <code>Callable</code> <p>description</p> required <code>loss_dict</code> <code>dict[str, Tensor]</code> <p>description. Defaults to None.</p> <code>None</code> Source code in <code>terratorch/tasks/loss_handler.py</code> <pre><code>def log_loss(\n    self, log_function: Callable, loss_dict: dict[str, Tensor] | None = None, batch_size: int | None = None\n) -&gt; None:\n    \"\"\"Log the loss. If auxiliary heads exist, log the full loss suffix \"loss\", and then all other losses.\n\n    Args:\n        log_function (Callable): _description_\n        loss_dict (dict[str, Tensor], optional): _description_. Defaults to None.\n    \"\"\"\n\n    # dont alter passed dict\n    all_losses = dict(loss_dict)\n    full_loss = all_losses.pop(\"loss\")\n    log_function(f\"{self.loss_prefix}loss\", full_loss, sync_dist=True, batch_size=batch_size)\n\n    for loss_name, loss_value in all_losses.items():\n        log_function(\n            f\"{self.loss_prefix}{loss_name}\",\n            loss_value,\n            on_epoch=True,\n            on_step=True,\n            sync_dist=True,\n            batch_size=batch_size,\n        )\n</code></pre>"},{"location":"package/meta_models/","title":"Meta Models","text":""},{"location":"package/meta_models/#terratorch.models.pixel_wise_model.PixelWiseModel","title":"<code>terratorch.models.pixel_wise_model.PixelWiseModel</code>","text":"<p>               Bases: <code>Model</code>, <code>SegmentationModel</code></p> <p>Model that encapsulates encoder and decoder and heads Expects decoder to have a \"forward_features\" method, an embed_dims property and optionally a \"prepare_features_for_image_model\" method.</p> Source code in <code>terratorch/models/pixel_wise_model.py</code> <pre><code>class PixelWiseModel(Model, SegmentationModel):\n    \"\"\"Model that encapsulates encoder and decoder and heads\n    Expects decoder to have a \"forward_features\" method, an embed_dims property\n    and optionally a \"prepare_features_for_image_model\" method.\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        encoder: nn.Module,\n        decoder: nn.Module,\n        head_kwargs: dict,\n        patch_size: int = None, \n        padding: str = None,\n        decoder_includes_head: bool = False,\n        auxiliary_heads: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None = None,\n        neck: nn.Module | None = None,\n        rescale: bool = True,  # noqa: FBT002, FBT001\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            task (str): Task to be performed. One of segmentation or regression.\n            encoder (nn.Module): Encoder to be used\n            decoder (nn.Module): Decoder to be used\n            head_kwargs (dict): Arguments to be passed at instantiation of the head.\n            decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.\n            auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of\n                AuxiliaryHeads with heads to be instantiated. Defaults to None.\n            neck (nn.Module | None): Module applied between backbone and decoder.\n                Defaults to None, which applies the identity.\n            rescale (bool, optional): Rescale the output of the model if it has a different size than the ground truth.\n                Uses bilinear interpolation. Defaults to True.\n        \"\"\"\n        super().__init__()\n\n        self.task = task\n        self.encoder = encoder\n        self.decoder = decoder\n        self.head = (\n            self._get_head(task, decoder.out_channels, head_kwargs) if not decoder_includes_head else nn.Identity()\n        )\n\n        if auxiliary_heads is not None:\n            aux_heads = {}\n            for aux_head_to_be_instantiated in auxiliary_heads:\n                aux_head: nn.Module = self._get_head(\n                    task, aux_head_to_be_instantiated.decoder.out_channels, head_kwargs\n                ) if not aux_head_to_be_instantiated.decoder_includes_head else nn.Identity()\n                aux_head = nn.Sequential(aux_head_to_be_instantiated.decoder, aux_head)\n                aux_heads[aux_head_to_be_instantiated.name] = aux_head\n        else:\n            aux_heads = {}\n        self.aux_heads = nn.ModuleDict(aux_heads)\n\n        self.neck = neck\n        self.rescale = rescale\n        self.patch_size = patch_size\n        self.padding = padding\n\n    def freeze_encoder(self):\n        if hasattr(self.encoder, \"freeze\"):\n            self.encoder.freeze()\n        else:\n            freeze_module(self.encoder)\n\n    def freeze_decoder(self):\n        freeze_module(self.decoder)\n\n    def freeze_head(self):\n        freeze_module(self.head)\n\n    # TODO: do this properly\n    def check_input_shape(self, x: torch.Tensor) -&gt; bool:  # noqa: ARG002\n        return True\n\n    @staticmethod\n    def _check_for_single_channel_and_squeeze(x):\n        if x.shape[1] == 1:\n            x = x.squeeze(1)\n        return x\n\n    def forward(self, x: torch.Tensor, **kwargs) -&gt; ModelOutput:\n        \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\"\n\n        def _get_size(x):\n            if isinstance(x, torch.Tensor):\n                return x.shape[-2:]\n            elif isinstance(x, dict):\n                # Multimodal input in passed as dict (Assuming first modality to be an image)\n                return list(x.values())[0].shape[-2:]\n            elif hasattr(kwargs, 'image_size'):\n                return kwargs['image_size']\n            else:\n                ValueError('Could not infer image shape.')\n\n        image_size = _get_size(x)\n        if isinstance(x, torch.Tensor) and self.patch_size:\n            # Only works for single image modalities\n            x = pad_images(x, self.patch_size, self.padding)\n        input_size = _get_size(x)\n\n        features = self.encoder(x, **kwargs)\n\n        # only for backwards compatibility with pre-neck times.\n        if self.neck:\n            prepare = self.neck\n        else:\n            # for backwards compatibility, if this is defined in the encoder, use it\n            prepare = getattr(self.encoder, \"prepare_features_for_image_model\", lambda x: x)\n\n        features = prepare(features)\n\n        decoder_output = self.decoder([f.clone() for f in features])\n        mask = self.head(decoder_output)\n        if self.rescale and mask.shape[-2:] != input_size:\n            mask = F.interpolate(mask, size=input_size, mode=\"bilinear\")\n        mask = self._check_for_single_channel_and_squeeze(mask)\n        mask = mask[..., :image_size[0], :image_size[1]]\n\n        aux_outputs = {}\n        for name, decoder in self.aux_heads.items():\n            aux_output = decoder([f.clone() for f in features])\n            if self.rescale and aux_output.shape[-2:] != input_size:\n                aux_output = F.interpolate(aux_output, size=input_size, mode=\"bilinear\")\n            aux_output = self._check_for_single_channel_and_squeeze(aux_output)\n            aux_output = aux_output[..., :image_size[0], :image_size[1]]\n            aux_outputs[name] = aux_output\n\n\n        return ModelOutput(output=mask, auxiliary_heads=aux_outputs)\n\n    def _get_head(self, task: str, input_embed_dim: int, head_kwargs):\n        if task == \"segmentation\":\n            if \"num_classes\" not in head_kwargs:\n                msg = \"num_classes must be defined for segmentation task\"\n                raise Exception(msg)\n            return SegmentationHead(input_embed_dim, **head_kwargs)\n        if task == \"regression\":\n            return RegressionHead(input_embed_dim, **head_kwargs)\n        msg = \"Task must be one of segmentation or regression.\"\n        raise Exception(msg)\n</code></pre>"},{"location":"package/meta_models/#terratorch.models.pixel_wise_model.PixelWiseModel.__init__","title":"<code>__init__(task, encoder, decoder, head_kwargs, patch_size=None, padding=None, decoder_includes_head=False, auxiliary_heads=None, neck=None, rescale=True)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. One of segmentation or regression.</p> required <code>encoder</code> <code>Module</code> <p>Encoder to be used</p> required <code>decoder</code> <code>Module</code> <p>Decoder to be used</p> required <code>head_kwargs</code> <code>dict</code> <p>Arguments to be passed at instantiation of the head.</p> required <code>decoder_includes_head</code> <code>bool</code> <p>Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.</p> <code>False</code> <code>auxiliary_heads</code> <code>list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None</code> <p>List of AuxiliaryHeads with heads to be instantiated. Defaults to None.</p> <code>None</code> <code>neck</code> <code>Module | None</code> <p>Module applied between backbone and decoder. Defaults to None, which applies the identity.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Rescale the output of the model if it has a different size than the ground truth. Uses bilinear interpolation. Defaults to True.</p> <code>True</code> Source code in <code>terratorch/models/pixel_wise_model.py</code> <pre><code>def __init__(\n    self,\n    task: str,\n    encoder: nn.Module,\n    decoder: nn.Module,\n    head_kwargs: dict,\n    patch_size: int = None, \n    padding: str = None,\n    decoder_includes_head: bool = False,\n    auxiliary_heads: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None = None,\n    neck: nn.Module | None = None,\n    rescale: bool = True,  # noqa: FBT002, FBT001\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        task (str): Task to be performed. One of segmentation or regression.\n        encoder (nn.Module): Encoder to be used\n        decoder (nn.Module): Decoder to be used\n        head_kwargs (dict): Arguments to be passed at instantiation of the head.\n        decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.\n        auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of\n            AuxiliaryHeads with heads to be instantiated. Defaults to None.\n        neck (nn.Module | None): Module applied between backbone and decoder.\n            Defaults to None, which applies the identity.\n        rescale (bool, optional): Rescale the output of the model if it has a different size than the ground truth.\n            Uses bilinear interpolation. Defaults to True.\n    \"\"\"\n    super().__init__()\n\n    self.task = task\n    self.encoder = encoder\n    self.decoder = decoder\n    self.head = (\n        self._get_head(task, decoder.out_channels, head_kwargs) if not decoder_includes_head else nn.Identity()\n    )\n\n    if auxiliary_heads is not None:\n        aux_heads = {}\n        for aux_head_to_be_instantiated in auxiliary_heads:\n            aux_head: nn.Module = self._get_head(\n                task, aux_head_to_be_instantiated.decoder.out_channels, head_kwargs\n            ) if not aux_head_to_be_instantiated.decoder_includes_head else nn.Identity()\n            aux_head = nn.Sequential(aux_head_to_be_instantiated.decoder, aux_head)\n            aux_heads[aux_head_to_be_instantiated.name] = aux_head\n    else:\n        aux_heads = {}\n    self.aux_heads = nn.ModuleDict(aux_heads)\n\n    self.neck = neck\n    self.rescale = rescale\n    self.patch_size = patch_size\n    self.padding = padding\n</code></pre>"},{"location":"package/meta_models/#terratorch.models.pixel_wise_model.PixelWiseModel.forward","title":"<code>forward(x, **kwargs)</code>","text":"<p>Sequentially pass <code>x</code> through model`s encoder, decoder and heads</p> Source code in <code>terratorch/models/pixel_wise_model.py</code> <pre><code>def forward(self, x: torch.Tensor, **kwargs) -&gt; ModelOutput:\n    \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\"\n\n    def _get_size(x):\n        if isinstance(x, torch.Tensor):\n            return x.shape[-2:]\n        elif isinstance(x, dict):\n            # Multimodal input in passed as dict (Assuming first modality to be an image)\n            return list(x.values())[0].shape[-2:]\n        elif hasattr(kwargs, 'image_size'):\n            return kwargs['image_size']\n        else:\n            ValueError('Could not infer image shape.')\n\n    image_size = _get_size(x)\n    if isinstance(x, torch.Tensor) and self.patch_size:\n        # Only works for single image modalities\n        x = pad_images(x, self.patch_size, self.padding)\n    input_size = _get_size(x)\n\n    features = self.encoder(x, **kwargs)\n\n    # only for backwards compatibility with pre-neck times.\n    if self.neck:\n        prepare = self.neck\n    else:\n        # for backwards compatibility, if this is defined in the encoder, use it\n        prepare = getattr(self.encoder, \"prepare_features_for_image_model\", lambda x: x)\n\n    features = prepare(features)\n\n    decoder_output = self.decoder([f.clone() for f in features])\n    mask = self.head(decoder_output)\n    if self.rescale and mask.shape[-2:] != input_size:\n        mask = F.interpolate(mask, size=input_size, mode=\"bilinear\")\n    mask = self._check_for_single_channel_and_squeeze(mask)\n    mask = mask[..., :image_size[0], :image_size[1]]\n\n    aux_outputs = {}\n    for name, decoder in self.aux_heads.items():\n        aux_output = decoder([f.clone() for f in features])\n        if self.rescale and aux_output.shape[-2:] != input_size:\n            aux_output = F.interpolate(aux_output, size=input_size, mode=\"bilinear\")\n        aux_output = self._check_for_single_channel_and_squeeze(aux_output)\n        aux_output = aux_output[..., :image_size[0], :image_size[1]]\n        aux_outputs[name] = aux_output\n\n\n    return ModelOutput(output=mask, auxiliary_heads=aux_outputs)\n</code></pre>"},{"location":"package/meta_models/#terratorch.models.scalar_output_model.ScalarOutputModel","title":"<code>terratorch.models.scalar_output_model.ScalarOutputModel</code>","text":"<p>               Bases: <code>Model</code>, <code>SegmentationModel</code></p> <p>Model that encapsulates encoder and decoder and heads for a scalar output Expects decoder to have a \"forward_features\" method, an embed_dims property and optionally a \"prepare_features_for_image_model\" method.</p> Source code in <code>terratorch/models/scalar_output_model.py</code> <pre><code>class ScalarOutputModel(Model, SegmentationModel):\n    \"\"\"Model that encapsulates encoder and decoder and heads for a scalar output\n    Expects decoder to have a \"forward_features\" method, an embed_dims property\n    and optionally a \"prepare_features_for_image_model\" method.\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        encoder: nn.Module,\n        decoder: nn.Module,\n        head_kwargs: dict,\n        patch_size: int = None,\n        padding: str = None,\n        decoder_includes_head: bool = False,\n        auxiliary_heads: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None = None,\n        neck: nn.Module | None = None,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            task (str): Task to be performed. Must be \"classification\".\n            encoder (nn.Module): Encoder to be used\n            decoder (nn.Module): Decoder to be used\n            head_kwargs (dict): Arguments to be passed at instantiation of the head.\n            decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.\n            auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of\n                AuxiliaryHeads with heads to be instantiated. Defaults to None.\n            neck (nn.Module | None): Module applied between backbone and decoder.\n                Defaults to None, which applies the identity.\n        \"\"\"\n        super().__init__()\n        self.task = task\n        self.encoder = encoder\n        self.decoder = decoder\n        self.head = (\n            self._get_head(task, decoder.out_channels, head_kwargs) if not decoder_includes_head else nn.Identity()\n        )\n\n        if auxiliary_heads is not None:\n            aux_heads = {}\n            for aux_head_to_be_instantiated in auxiliary_heads:\n                aux_head: nn.Module = self._get_head(\n                    task, aux_head_to_be_instantiated.decoder.out_channels, head_kwargs\n                ) if not aux_head_to_be_instantiated.decoder_includes_head else nn.Identity()\n                aux_head = nn.Sequential(aux_head_to_be_instantiated.decoder, aux_head)\n                aux_heads[aux_head_to_be_instantiated.name] = aux_head\n        else:\n            aux_heads = {}\n        self.aux_heads = nn.ModuleDict(aux_heads)\n\n        self.neck = neck\n        self.patch_size = patch_size\n        self.padding = padding\n\n    def freeze_encoder(self):\n        freeze_module(self.encoder)\n\n    def freeze_decoder(self):\n        freeze_module(self.decoder)\n\n    def freeze_head(self):\n        freeze_module(self.head)\n\n    def forward(self, x: torch.Tensor, **kwargs) -&gt; ModelOutput:\n        \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\"\n\n        if isinstance(x, torch.Tensor) and self.patch_size:\n            # Only works for single image modalities\n            x = pad_images(x, self.patch_size, self.padding)\n        features = self.encoder(x, **kwargs)\n\n        # only for backwards compatibility with pre-neck times.\n        if self.neck:\n            prepare = self.neck\n        else:\n            # for backwards compatibility, if this is defined in the encoder, use it\n            prepare = getattr(self.encoder, \"prepare_features_for_image_model\", lambda x: x)\n\n        features = prepare(features)\n\n        decoder_output = self.decoder([f.clone() for f in features])\n        mask = self.head(decoder_output)\n\n        aux_outputs = {}\n        for name, decoder in self.aux_heads.items():\n            aux_output = decoder([f.clone() for f in features])\n            aux_outputs[name] = aux_output\n\n        return ModelOutput(output=mask, auxiliary_heads=aux_outputs)\n\n    def _get_head(self, task: str, input_embed_dim: int, head_kwargs: dict):\n        if task == \"classification\":\n            if \"num_classes\" not in head_kwargs:\n                msg = \"num_classes must be defined for classification task\"\n                raise Exception(msg)\n            return ClassificationHead(input_embed_dim, **head_kwargs)\n        msg = \"Task must be classification.\"\n        raise Exception(msg)\n</code></pre>"},{"location":"package/meta_models/#terratorch.models.scalar_output_model.ScalarOutputModel.__init__","title":"<code>__init__(task, encoder, decoder, head_kwargs, patch_size=None, padding=None, decoder_includes_head=False, auxiliary_heads=None, neck=None)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Must be \"classification\".</p> required <code>encoder</code> <code>Module</code> <p>Encoder to be used</p> required <code>decoder</code> <code>Module</code> <p>Decoder to be used</p> required <code>head_kwargs</code> <code>dict</code> <p>Arguments to be passed at instantiation of the head.</p> required <code>decoder_includes_head</code> <code>bool</code> <p>Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.</p> <code>False</code> <code>auxiliary_heads</code> <code>list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None</code> <p>List of AuxiliaryHeads with heads to be instantiated. Defaults to None.</p> <code>None</code> <code>neck</code> <code>Module | None</code> <p>Module applied between backbone and decoder. Defaults to None, which applies the identity.</p> <code>None</code> Source code in <code>terratorch/models/scalar_output_model.py</code> <pre><code>def __init__(\n    self,\n    task: str,\n    encoder: nn.Module,\n    decoder: nn.Module,\n    head_kwargs: dict,\n    patch_size: int = None,\n    padding: str = None,\n    decoder_includes_head: bool = False,\n    auxiliary_heads: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None = None,\n    neck: nn.Module | None = None,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        task (str): Task to be performed. Must be \"classification\".\n        encoder (nn.Module): Encoder to be used\n        decoder (nn.Module): Decoder to be used\n        head_kwargs (dict): Arguments to be passed at instantiation of the head.\n        decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.\n        auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of\n            AuxiliaryHeads with heads to be instantiated. Defaults to None.\n        neck (nn.Module | None): Module applied between backbone and decoder.\n            Defaults to None, which applies the identity.\n    \"\"\"\n    super().__init__()\n    self.task = task\n    self.encoder = encoder\n    self.decoder = decoder\n    self.head = (\n        self._get_head(task, decoder.out_channels, head_kwargs) if not decoder_includes_head else nn.Identity()\n    )\n\n    if auxiliary_heads is not None:\n        aux_heads = {}\n        for aux_head_to_be_instantiated in auxiliary_heads:\n            aux_head: nn.Module = self._get_head(\n                task, aux_head_to_be_instantiated.decoder.out_channels, head_kwargs\n            ) if not aux_head_to_be_instantiated.decoder_includes_head else nn.Identity()\n            aux_head = nn.Sequential(aux_head_to_be_instantiated.decoder, aux_head)\n            aux_heads[aux_head_to_be_instantiated.name] = aux_head\n    else:\n        aux_heads = {}\n    self.aux_heads = nn.ModuleDict(aux_heads)\n\n    self.neck = neck\n    self.patch_size = patch_size\n    self.padding = padding\n</code></pre>"},{"location":"package/meta_models/#terratorch.models.scalar_output_model.ScalarOutputModel.forward","title":"<code>forward(x, **kwargs)</code>","text":"<p>Sequentially pass <code>x</code> through model`s encoder, decoder and heads</p> Source code in <code>terratorch/models/scalar_output_model.py</code> <pre><code>def forward(self, x: torch.Tensor, **kwargs) -&gt; ModelOutput:\n    \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\"\n\n    if isinstance(x, torch.Tensor) and self.patch_size:\n        # Only works for single image modalities\n        x = pad_images(x, self.patch_size, self.padding)\n    features = self.encoder(x, **kwargs)\n\n    # only for backwards compatibility with pre-neck times.\n    if self.neck:\n        prepare = self.neck\n    else:\n        # for backwards compatibility, if this is defined in the encoder, use it\n        prepare = getattr(self.encoder, \"prepare_features_for_image_model\", lambda x: x)\n\n    features = prepare(features)\n\n    decoder_output = self.decoder([f.clone() for f in features])\n    mask = self.head(decoder_output)\n\n    aux_outputs = {}\n    for name, decoder in self.aux_heads.items():\n        aux_output = decoder([f.clone() for f in features])\n        aux_outputs[name] = aux_output\n\n    return ModelOutput(output=mask, auxiliary_heads=aux_outputs)\n</code></pre>"},{"location":"package/model_factories/","title":"Model Factories","text":"<p>Model factories build the model that is fine-tuned by TerraTorch. Specifically, a backbone is used an encoder and combined with a task-specific decoder and head.  Necks are using to reshape the encoder output to be compatible with the decoder input.</p> <p>Tip</p> <p>The <code>EncoderDecoderFactory</code> is the default factory for segmentation, pixel-wise regression, and classification tasks.</p> <p>Other commonly used factories are the <code>ObjectDetectionModelFactory</code> for object detection tasks and sometimes the <code>FullModelFactory</code> if a model is registered in the <code>FULL_MODEL_REGISTRY</code> and can be directly applied to a specific task. </p>"},{"location":"package/model_factories/#terratorch.models.encoder_decoder_factory.EncoderDecoderFactory","title":"<code>terratorch.models.encoder_decoder_factory.EncoderDecoderFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/encoder_decoder_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass EncoderDecoderFactory(ModelFactory):\n    def build_model(\n        self,\n        task: str,\n        backbone: str | nn.Module,\n        decoder: str | nn.Module,\n        backbone_kwargs: dict | None = None,\n        decoder_kwargs: dict | None = None,\n        head_kwargs: dict | None = None,\n        num_classes: int | None = None,\n        necks: list[dict] | None = None,\n        aux_decoders: list[AuxiliaryHead] | None = None,\n        rescale: bool = True,  # noqa: FBT002, FBT001\n        peft_config: dict | None = None,\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"Generic model factory that combines an encoder and decoder, together with a head, for a specific task.\n\n        Further arguments to be passed to the backbone, decoder or head. They should be prefixed with\n        `backbone_`, `decoder_` and `head_` respectively.\n\n        Args:\n            task (str): Task to be performed. Currently supports \"segmentation\", \"regression\" and \"classification\".\n            backbone (str, nn.Module): Backbone to be used. If a string, will look for such models in the different\n                registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it\n                directly. The backbone should have and `out_channels` attribute and its `forward` should return a list[Tensor].\n            decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model.\n                    If a string, will look for such decoders in the different\n                    registries supported (internal terratorch registry, smp, ...).\n                    If an nn.Module, we expect it to expose a property `decoder.out_channels`.\n                    Pixel wise tasks will be concatenated with a Conv2d for the final convolution.\n                    Defaults to \"FCNDecoder\".\n            backbone_kwargs (dict, optional) : Arguments to be passed to instantiate the backbone.\n            decoder_kwargs (dict, optional) : Arguments to be passed to instantiate the decoder.\n            head_kwargs (dict, optional) : Arguments to be passed to the head network. \n            num_classes (int, optional): Number of classes. None for regression tasks.\n            necks (list[dict]): nn.Modules to be called in succession on encoder features\n                before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry.\n                Expects each one to have a key \"name\" and subsequent keys for arguments, if any.\n                Defaults to None, which applies the identity function.\n            aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead decoders to be added to the model.\n                These decoders take the input from the encoder as well.\n            rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n                is different from the ground truth. Only applicable to pixel wise models\n                (e.g. segmentation, pixel wise regression). Defaults to True.\n            peft_config (dict): Configuration options for using [PEFT](https://huggingface.co/docs/peft/index).\n                The dictionary should have the following keys:\n\n                - \"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available [here](https://huggingface.co/docs/peft/package_reference/peft_types#peft.PeftType).\n                - \"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep.\n                  This should be used when the qkv matrices are merged together in a single linear layer and the PEFT\n                  method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in\n                  Q and V matrices). e.g. If using Prithvi this should be \"qkv\"\n                - \"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to [PeftConfig](https://huggingface.co/docs/peft/package_reference/config#peft.PeftConfig)\n\n\n        Returns:\n            nn.Module: Full model with encoder, decoder and head.\n        \"\"\"\n        task = task.lower()\n        if task not in SUPPORTED_TASKS:\n            msg = f\"Task {task} not supported. Please choose one of {SUPPORTED_TASKS}\"\n            raise NotImplementedError(msg)\n\n        if not backbone_kwargs:\n            backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n\n        backbone = _get_backbone(backbone, **backbone_kwargs)\n\n        # If patch size is not provided in the config or by the model, it might lead to errors due to irregular images.\n        patch_size = backbone_kwargs.get(\"patch_size\", None)\n\n        if patch_size is None:\n            # Infer patch size from model by checking all backbone modules\n            for module in backbone.modules():\n                if hasattr(module, \"patch_size\"):\n                    patch_size = module.patch_size\n                    break\n        padding = backbone_kwargs.get(\"padding\", \"reflect\")\n\n        if peft_config is not None:\n            if not backbone_kwargs.get(\"pretrained\", False):\n                msg = (\n                    \"You are using PEFT without a pretrained backbone. If you are loading a checkpoint afterwards \"\n                    \"this is probably fine, but if you are training a model check the backbone_pretrained parameter.\"\n                )\n                warnings.warn(msg, stacklevel=1)\n\n            backbone = get_peft_backbone(peft_config, backbone)\n\n        try:\n            out_channels = backbone.out_channels\n        except AttributeError as e:\n            msg = \"backbone must have out_channels attribute\"\n            raise AttributeError(msg) from e\n\n        if necks is None:\n            necks = []\n        neck_list, channel_list = build_neck_list(necks, out_channels)\n\n        # some decoders already include a head\n        # for these, we pass the num_classes to them\n        # others dont include a head\n        # for those, we dont pass num_classes\n        if not decoder_kwargs:\n            decoder_kwargs, kwargs = extract_prefix_keys(kwargs, \"decoder_\")\n\n        if not head_kwargs:\n            head_kwargs, kwargs = extract_prefix_keys(kwargs, \"head_\")\n\n        decoder, head_kwargs, decoder_includes_head = _get_decoder_and_head_kwargs(\n            decoder, channel_list, decoder_kwargs, head_kwargs, num_classes=num_classes\n        )\n\n        if aux_decoders is None:\n            _check_all_args_used(kwargs)\n            return _build_appropriate_model(\n                task,\n                backbone,\n                decoder,\n                head_kwargs,\n                patch_size=patch_size,\n                padding=padding,\n                necks=neck_list,\n                decoder_includes_head=decoder_includes_head,\n                rescale=rescale,\n            )\n\n        to_be_aux_decoders: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] = []\n        for aux_decoder in aux_decoders:\n            args = aux_decoder.decoder_args if aux_decoder.decoder_args else {}\n            aux_decoder_kwargs, args = extract_prefix_keys(args, \"decoder_\")\n            aux_head_kwargs, args = extract_prefix_keys(args, \"head_\")\n            aux_decoder_instance, aux_head_kwargs, aux_decoder_includes_head = _get_decoder_and_head_kwargs(\n                aux_decoder.decoder, channel_list, aux_decoder_kwargs, aux_head_kwargs, num_classes=num_classes\n            )\n            to_be_aux_decoders.append(\n                AuxiliaryHeadWithDecoderWithoutInstantiatedHead(aux_decoder.name, aux_decoder_instance, aux_head_kwargs)\n            )\n            _check_all_args_used(args)\n\n        _check_all_args_used(kwargs)\n\n        return _build_appropriate_model(\n            task,\n            backbone,\n            decoder,\n            head_kwargs,\n            patch_size=patch_size,\n            padding=padding,\n            necks=neck_list,\n            decoder_includes_head=decoder_includes_head,\n            rescale=rescale,\n            auxiliary_heads=to_be_aux_decoders,\n        )\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.encoder_decoder_factory.EncoderDecoderFactory.build_model","title":"<code>build_model(task, backbone, decoder, backbone_kwargs=None, decoder_kwargs=None, head_kwargs=None, num_classes=None, necks=None, aux_decoders=None, rescale=True, peft_config=None, **kwargs)</code>","text":"<p>Generic model factory that combines an encoder and decoder, together with a head, for a specific task.</p> <p>Further arguments to be passed to the backbone, decoder or head. They should be prefixed with <code>backbone_</code>, <code>decoder_</code> and <code>head_</code> respectively.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Currently supports \"segmentation\", \"regression\" and \"classification\".</p> required <code>backbone</code> <code>(str, Module)</code> <p>Backbone to be used. If a string, will look for such models in the different registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it directly. The backbone should have and <code>out_channels</code> attribute and its <code>forward</code> should return a list[Tensor].</p> required <code>decoder</code> <code>Union[str, Module]</code> <p>Decoder to be used for the segmentation model.     If a string, will look for such decoders in the different     registries supported (internal terratorch registry, smp, ...).     If an nn.Module, we expect it to expose a property <code>decoder.out_channels</code>.     Pixel wise tasks will be concatenated with a Conv2d for the final convolution.     Defaults to \"FCNDecoder\".</p> required <code>backbone_kwargs</code> <code>dict, optional) </code> <p>Arguments to be passed to instantiate the backbone.</p> <code>None</code> <code>decoder_kwargs</code> <code>dict, optional) </code> <p>Arguments to be passed to instantiate the decoder.</p> <code>None</code> <code>head_kwargs</code> <code>dict, optional) </code> <p>Arguments to be passed to the head network. </p> <code>None</code> <code>num_classes</code> <code>int</code> <p>Number of classes. None for regression tasks.</p> <code>None</code> <code>necks</code> <code>list[dict]</code> <p>nn.Modules to be called in succession on encoder features before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry. Expects each one to have a key \"name\" and subsequent keys for arguments, if any. Defaults to None, which applies the identity function.</p> <code>None</code> <code>aux_decoders</code> <code>list[AuxiliaryHead] | None</code> <p>List of AuxiliaryHead decoders to be added to the model. These decoders take the input from the encoder as well.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.</p> <code>True</code> <code>peft_config</code> <code>dict</code> <p>Configuration options for using PEFT. The dictionary should have the following keys:</p> <ul> <li>\"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available here.</li> <li>\"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep.   This should be used when the qkv matrices are merged together in a single linear layer and the PEFT   method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in   Q and V matrices). e.g. If using Prithvi this should be \"qkv\"</li> <li>\"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to PeftConfig</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: Full model with encoder, decoder and head.</p> Source code in <code>terratorch/models/encoder_decoder_factory.py</code> <pre><code>def build_model(\n    self,\n    task: str,\n    backbone: str | nn.Module,\n    decoder: str | nn.Module,\n    backbone_kwargs: dict | None = None,\n    decoder_kwargs: dict | None = None,\n    head_kwargs: dict | None = None,\n    num_classes: int | None = None,\n    necks: list[dict] | None = None,\n    aux_decoders: list[AuxiliaryHead] | None = None,\n    rescale: bool = True,  # noqa: FBT002, FBT001\n    peft_config: dict | None = None,\n    **kwargs,\n) -&gt; Model:\n    \"\"\"Generic model factory that combines an encoder and decoder, together with a head, for a specific task.\n\n    Further arguments to be passed to the backbone, decoder or head. They should be prefixed with\n    `backbone_`, `decoder_` and `head_` respectively.\n\n    Args:\n        task (str): Task to be performed. Currently supports \"segmentation\", \"regression\" and \"classification\".\n        backbone (str, nn.Module): Backbone to be used. If a string, will look for such models in the different\n            registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it\n            directly. The backbone should have and `out_channels` attribute and its `forward` should return a list[Tensor].\n        decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model.\n                If a string, will look for such decoders in the different\n                registries supported (internal terratorch registry, smp, ...).\n                If an nn.Module, we expect it to expose a property `decoder.out_channels`.\n                Pixel wise tasks will be concatenated with a Conv2d for the final convolution.\n                Defaults to \"FCNDecoder\".\n        backbone_kwargs (dict, optional) : Arguments to be passed to instantiate the backbone.\n        decoder_kwargs (dict, optional) : Arguments to be passed to instantiate the decoder.\n        head_kwargs (dict, optional) : Arguments to be passed to the head network. \n        num_classes (int, optional): Number of classes. None for regression tasks.\n        necks (list[dict]): nn.Modules to be called in succession on encoder features\n            before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry.\n            Expects each one to have a key \"name\" and subsequent keys for arguments, if any.\n            Defaults to None, which applies the identity function.\n        aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead decoders to be added to the model.\n            These decoders take the input from the encoder as well.\n        rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n            is different from the ground truth. Only applicable to pixel wise models\n            (e.g. segmentation, pixel wise regression). Defaults to True.\n        peft_config (dict): Configuration options for using [PEFT](https://huggingface.co/docs/peft/index).\n            The dictionary should have the following keys:\n\n            - \"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available [here](https://huggingface.co/docs/peft/package_reference/peft_types#peft.PeftType).\n            - \"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep.\n              This should be used when the qkv matrices are merged together in a single linear layer and the PEFT\n              method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in\n              Q and V matrices). e.g. If using Prithvi this should be \"qkv\"\n            - \"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to [PeftConfig](https://huggingface.co/docs/peft/package_reference/config#peft.PeftConfig)\n\n\n    Returns:\n        nn.Module: Full model with encoder, decoder and head.\n    \"\"\"\n    task = task.lower()\n    if task not in SUPPORTED_TASKS:\n        msg = f\"Task {task} not supported. Please choose one of {SUPPORTED_TASKS}\"\n        raise NotImplementedError(msg)\n\n    if not backbone_kwargs:\n        backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n\n    backbone = _get_backbone(backbone, **backbone_kwargs)\n\n    # If patch size is not provided in the config or by the model, it might lead to errors due to irregular images.\n    patch_size = backbone_kwargs.get(\"patch_size\", None)\n\n    if patch_size is None:\n        # Infer patch size from model by checking all backbone modules\n        for module in backbone.modules():\n            if hasattr(module, \"patch_size\"):\n                patch_size = module.patch_size\n                break\n    padding = backbone_kwargs.get(\"padding\", \"reflect\")\n\n    if peft_config is not None:\n        if not backbone_kwargs.get(\"pretrained\", False):\n            msg = (\n                \"You are using PEFT without a pretrained backbone. If you are loading a checkpoint afterwards \"\n                \"this is probably fine, but if you are training a model check the backbone_pretrained parameter.\"\n            )\n            warnings.warn(msg, stacklevel=1)\n\n        backbone = get_peft_backbone(peft_config, backbone)\n\n    try:\n        out_channels = backbone.out_channels\n    except AttributeError as e:\n        msg = \"backbone must have out_channels attribute\"\n        raise AttributeError(msg) from e\n\n    if necks is None:\n        necks = []\n    neck_list, channel_list = build_neck_list(necks, out_channels)\n\n    # some decoders already include a head\n    # for these, we pass the num_classes to them\n    # others dont include a head\n    # for those, we dont pass num_classes\n    if not decoder_kwargs:\n        decoder_kwargs, kwargs = extract_prefix_keys(kwargs, \"decoder_\")\n\n    if not head_kwargs:\n        head_kwargs, kwargs = extract_prefix_keys(kwargs, \"head_\")\n\n    decoder, head_kwargs, decoder_includes_head = _get_decoder_and_head_kwargs(\n        decoder, channel_list, decoder_kwargs, head_kwargs, num_classes=num_classes\n    )\n\n    if aux_decoders is None:\n        _check_all_args_used(kwargs)\n        return _build_appropriate_model(\n            task,\n            backbone,\n            decoder,\n            head_kwargs,\n            patch_size=patch_size,\n            padding=padding,\n            necks=neck_list,\n            decoder_includes_head=decoder_includes_head,\n            rescale=rescale,\n        )\n\n    to_be_aux_decoders: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] = []\n    for aux_decoder in aux_decoders:\n        args = aux_decoder.decoder_args if aux_decoder.decoder_args else {}\n        aux_decoder_kwargs, args = extract_prefix_keys(args, \"decoder_\")\n        aux_head_kwargs, args = extract_prefix_keys(args, \"head_\")\n        aux_decoder_instance, aux_head_kwargs, aux_decoder_includes_head = _get_decoder_and_head_kwargs(\n            aux_decoder.decoder, channel_list, aux_decoder_kwargs, aux_head_kwargs, num_classes=num_classes\n        )\n        to_be_aux_decoders.append(\n            AuxiliaryHeadWithDecoderWithoutInstantiatedHead(aux_decoder.name, aux_decoder_instance, aux_head_kwargs)\n        )\n        _check_all_args_used(args)\n\n    _check_all_args_used(kwargs)\n\n    return _build_appropriate_model(\n        task,\n        backbone,\n        decoder,\n        head_kwargs,\n        patch_size=patch_size,\n        padding=padding,\n        necks=neck_list,\n        decoder_includes_head=decoder_includes_head,\n        rescale=rescale,\n        auxiliary_heads=to_be_aux_decoders,\n    )\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.object_detection_model_factory.ObjectDetectionModelFactory","title":"<code>terratorch.models.object_detection_model_factory.ObjectDetectionModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/object_detection_model_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass ObjectDetectionModelFactory(ModelFactory):\n    def build_model(\n        self,\n        task: str,\n        backbone: str | nn.Module,\n        framework: str,\n        num_classes: int | None = None,\n        necks: list[dict] | None = None,\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"\n        Generic model factory that combines an encoder and necks with the detection models, called framework, in torchvision.detection.\n\n        Further arguments to be passed to the backbone_ and framework_.\n\n        Args:\n            task (str): Task to be performed. Currently supports \"object_detection\".\n            backbone (str, nn.Module): Backbone to be used. If a string, will look for such models in the different\n                registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it\n                directly. The backbone should have and `out_channels` attribute and its `forward` should return a list[Tensor].\n            framework (str): object detection framework to be used between \"faster-rcnn\", \"fcos\", \"retinanet\" for object detection and \"mask-rcnn\" for instance segmentation.\n            num_classes (int, optional): Number of classes. None for regression tasks.\n            necks (list[dict]): nn.Modules to be called in succession on encoder features\n                before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry.\n                Expects each one to have a key \"name\" and subsequent keys for arguments, if any.\n                Defaults to None, which applies the identity function.\n\n        Returns:\n            nn.Module: Full torchvision detection model.\n        \"\"\"\n        task = task.lower()\n        if task not in SUPPORTED_TASKS:\n            msg = f\"Task {task} not supported. Please choose one of {SUPPORTED_TASKS}\"\n            raise NotImplementedError(msg)\n        backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n        framework_kwargs, kwargs = extract_prefix_keys(kwargs, \"framework_\")\n\n        backbone = _get_backbone(backbone, **backbone_kwargs)\n        if 'in_channels' in kwargs.keys():\n            in_channels = kwargs['in_channels']\n        else:\n            in_channels = len(backbone_kwargs[\"model_bands\"]) if \"model_bands\" in backbone_kwargs.keys() else len(backbone_kwargs[\"bands\"])\n\n        try:\n            out_channels = backbone.out_channels\n        except AttributeError as e:\n            msg = \"backbone must have out_channels attribute\"\n            raise AttributeError(msg) from e\n        # pdb.set_trace()\n        if necks is None:\n            necks = []\n        neck_list, channel_list = build_neck_list(necks, out_channels)\n\n        neck_module = nn.Sequential(*neck_list)\n\n        combined_backbone = BackboneWrapper(backbone, neck_module, channel_list)\n        # pdb.set_trace()\n\n        if framework == 'faster-rcnn':\n\n            sizes = ((32), (64), (128), (256), (512))\n            sizes = sizes[:len(combined_backbone.channel_list)]\n            aspect_ratios = ((0.5, 1.0, 2.0),) * len(sizes)\n            anchor_generator = AnchorGenerator(sizes=sizes, aspect_ratios=aspect_ratios)\n\n            roi_pooler = MultiScaleRoIAlign(\n                featmap_names=['feat0', 'feat1', 'feat2', 'feat3'], output_size=7, sampling_ratio=2\n            )\n\n            model = torchvision.models.detection.FasterRCNN(\n                combined_backbone,\n                num_classes,\n                rpn_anchor_generator=anchor_generator,\n                box_roi_pool=roi_pooler,\n                _skip_resize=True,\n                image_mean = np.repeat(0, in_channels),\n                image_std = np.repeat(1, in_channels),\n                **framework_kwargs\n            )\n        elif framework == 'fcos':\n\n            sizes = ((8,), (16,), (32,), (64,), (128,), (256,))\n            sizes=sizes[:len(combined_backbone.channel_list)]\n            aspect_ratios = ((1.0,), (1.0,), (1.0,), (1.0,), (1.0,), (1.0,)) * len(sizes)\n            anchor_generator = AnchorGenerator(\n                sizes=sizes,\n                aspect_ratios=aspect_ratios,\n            )\n\n            model = torchvision.models.detection.FCOS(\n                combined_backbone, \n                num_classes,\n                anchor_generator=anchor_generator, \n                _skip_resize=True,\n                image_mean = np.repeat(0, in_channels),\n                image_std = np.repeat(1, in_channels),\n                **framework_kwargs\n\n            )\n        elif framework == 'retinanet':\n\n            sizes = (\n                (16, 20, 25),\n                (32, 40, 50),\n                (64, 80, 101),\n                (128, 161, 203),\n                (256, 322, 406),\n                (512, 645, 812),\n            )\n            sizes=sizes[:len(combined_backbone.channel_list)]\n            aspect_ratios = ((0.5, 1.0, 2.0),) * len(sizes)\n            anchor_generator = AnchorGenerator(sizes, aspect_ratios)\n            head = RetinaNetHead(\n                combined_backbone.out_channels,\n                anchor_generator.num_anchors_per_location()[0],\n                num_classes,\n                norm_layer=partial(torch.nn.GroupNorm, 32),\n            )\n\n            model = torchvision.models.detection.RetinaNet(\n                combined_backbone,\n                num_classes,\n                anchor_generator=anchor_generator,\n                head=head,\n                _skip_resize=True,\n                image_mean=np.repeat(0, in_channels),\n                image_std=np.repeat(1, in_channels),\n                **framework_kwargs\n            )\n\n        elif framework == 'mask-rcnn':\n\n            sizes = ((32), (64), (128), (256), (512))\n            sizes = sizes[:len(combined_backbone.channel_list)]\n            aspect_ratios = ((0.5, 1.0, 2.0),) * len(sizes)\n            anchor_generator = AnchorGenerator(sizes=sizes, aspect_ratios=aspect_ratios)\n\n            rpn_head = torchvision.models.detection.faster_rcnn.RPNHead(combined_backbone.out_channels, anchor_generator.num_anchors_per_location()[0], conv_depth=2)\n            box_head = torchvision.models.detection.faster_rcnn.FastRCNNConvFCHead(\n                (combined_backbone.out_channels, 7, 7), [256, 256, 256, 256], [1024], norm_layer=nn.BatchNorm2d\n            )\n            mask_head = torchvision.models.detection.mask_rcnn.MaskRCNNHeads(combined_backbone.out_channels, [256, 256, 256, 256], 1, norm_layer=nn.BatchNorm2d)\n            roi_pooler = MultiScaleRoIAlign(\n                featmap_names=['feat0', 'feat1', 'feat2', 'feat3'], output_size=7, sampling_ratio=2\n            )\n\n            model = torchvision.models.detection.MaskRCNN(\n                combined_backbone,\n                num_classes=num_classes,\n                rpn_anchor_generator=anchor_generator,\n                rpn_head=rpn_head,\n                box_head=box_head,\n                box_roi_pool=roi_pooler,\n                mask_roi_pool=roi_pooler,\n                mask_head=mask_head,\n                _skip_resize=True,\n                image_mean=np.repeat(0, in_channels),\n                image_std=np.repeat(1, in_channels),\n                **framework_kwargs\n            )\n\n        else:\n            raise ValueError(f\"Framework type '{framework}' is not valid.\")\n\n        # some decoders already include a head\n        # for these, we pass the num_classes to them\n        # others dont include a head\n        # for those, we dont pass num_classes\n        # model.transform = IdentityTransform()\n\n        return ObjectDetectionModel(model, framework)\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.object_detection_model_factory.ObjectDetectionModelFactory.build_model","title":"<code>build_model(task, backbone, framework, num_classes=None, necks=None, **kwargs)</code>","text":"<p>Generic model factory that combines an encoder and necks with the detection models, called framework, in torchvision.detection.</p> <p>Further arguments to be passed to the backbone_ and framework_.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Currently supports \"object_detection\".</p> required <code>backbone</code> <code>(str, Module)</code> <p>Backbone to be used. If a string, will look for such models in the different registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it directly. The backbone should have and <code>out_channels</code> attribute and its <code>forward</code> should return a list[Tensor].</p> required <code>framework</code> <code>str</code> <p>object detection framework to be used between \"faster-rcnn\", \"fcos\", \"retinanet\" for object detection and \"mask-rcnn\" for instance segmentation.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes. None for regression tasks.</p> <code>None</code> <code>necks</code> <code>list[dict]</code> <p>nn.Modules to be called in succession on encoder features before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry. Expects each one to have a key \"name\" and subsequent keys for arguments, if any. Defaults to None, which applies the identity function.</p> <code>None</code> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: Full torchvision detection model.</p> Source code in <code>terratorch/models/object_detection_model_factory.py</code> <pre><code>def build_model(\n    self,\n    task: str,\n    backbone: str | nn.Module,\n    framework: str,\n    num_classes: int | None = None,\n    necks: list[dict] | None = None,\n    **kwargs,\n) -&gt; Model:\n    \"\"\"\n    Generic model factory that combines an encoder and necks with the detection models, called framework, in torchvision.detection.\n\n    Further arguments to be passed to the backbone_ and framework_.\n\n    Args:\n        task (str): Task to be performed. Currently supports \"object_detection\".\n        backbone (str, nn.Module): Backbone to be used. If a string, will look for such models in the different\n            registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it\n            directly. The backbone should have and `out_channels` attribute and its `forward` should return a list[Tensor].\n        framework (str): object detection framework to be used between \"faster-rcnn\", \"fcos\", \"retinanet\" for object detection and \"mask-rcnn\" for instance segmentation.\n        num_classes (int, optional): Number of classes. None for regression tasks.\n        necks (list[dict]): nn.Modules to be called in succession on encoder features\n            before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry.\n            Expects each one to have a key \"name\" and subsequent keys for arguments, if any.\n            Defaults to None, which applies the identity function.\n\n    Returns:\n        nn.Module: Full torchvision detection model.\n    \"\"\"\n    task = task.lower()\n    if task not in SUPPORTED_TASKS:\n        msg = f\"Task {task} not supported. Please choose one of {SUPPORTED_TASKS}\"\n        raise NotImplementedError(msg)\n    backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n    framework_kwargs, kwargs = extract_prefix_keys(kwargs, \"framework_\")\n\n    backbone = _get_backbone(backbone, **backbone_kwargs)\n    if 'in_channels' in kwargs.keys():\n        in_channels = kwargs['in_channels']\n    else:\n        in_channels = len(backbone_kwargs[\"model_bands\"]) if \"model_bands\" in backbone_kwargs.keys() else len(backbone_kwargs[\"bands\"])\n\n    try:\n        out_channels = backbone.out_channels\n    except AttributeError as e:\n        msg = \"backbone must have out_channels attribute\"\n        raise AttributeError(msg) from e\n    # pdb.set_trace()\n    if necks is None:\n        necks = []\n    neck_list, channel_list = build_neck_list(necks, out_channels)\n\n    neck_module = nn.Sequential(*neck_list)\n\n    combined_backbone = BackboneWrapper(backbone, neck_module, channel_list)\n    # pdb.set_trace()\n\n    if framework == 'faster-rcnn':\n\n        sizes = ((32), (64), (128), (256), (512))\n        sizes = sizes[:len(combined_backbone.channel_list)]\n        aspect_ratios = ((0.5, 1.0, 2.0),) * len(sizes)\n        anchor_generator = AnchorGenerator(sizes=sizes, aspect_ratios=aspect_ratios)\n\n        roi_pooler = MultiScaleRoIAlign(\n            featmap_names=['feat0', 'feat1', 'feat2', 'feat3'], output_size=7, sampling_ratio=2\n        )\n\n        model = torchvision.models.detection.FasterRCNN(\n            combined_backbone,\n            num_classes,\n            rpn_anchor_generator=anchor_generator,\n            box_roi_pool=roi_pooler,\n            _skip_resize=True,\n            image_mean = np.repeat(0, in_channels),\n            image_std = np.repeat(1, in_channels),\n            **framework_kwargs\n        )\n    elif framework == 'fcos':\n\n        sizes = ((8,), (16,), (32,), (64,), (128,), (256,))\n        sizes=sizes[:len(combined_backbone.channel_list)]\n        aspect_ratios = ((1.0,), (1.0,), (1.0,), (1.0,), (1.0,), (1.0,)) * len(sizes)\n        anchor_generator = AnchorGenerator(\n            sizes=sizes,\n            aspect_ratios=aspect_ratios,\n        )\n\n        model = torchvision.models.detection.FCOS(\n            combined_backbone, \n            num_classes,\n            anchor_generator=anchor_generator, \n            _skip_resize=True,\n            image_mean = np.repeat(0, in_channels),\n            image_std = np.repeat(1, in_channels),\n            **framework_kwargs\n\n        )\n    elif framework == 'retinanet':\n\n        sizes = (\n            (16, 20, 25),\n            (32, 40, 50),\n            (64, 80, 101),\n            (128, 161, 203),\n            (256, 322, 406),\n            (512, 645, 812),\n        )\n        sizes=sizes[:len(combined_backbone.channel_list)]\n        aspect_ratios = ((0.5, 1.0, 2.0),) * len(sizes)\n        anchor_generator = AnchorGenerator(sizes, aspect_ratios)\n        head = RetinaNetHead(\n            combined_backbone.out_channels,\n            anchor_generator.num_anchors_per_location()[0],\n            num_classes,\n            norm_layer=partial(torch.nn.GroupNorm, 32),\n        )\n\n        model = torchvision.models.detection.RetinaNet(\n            combined_backbone,\n            num_classes,\n            anchor_generator=anchor_generator,\n            head=head,\n            _skip_resize=True,\n            image_mean=np.repeat(0, in_channels),\n            image_std=np.repeat(1, in_channels),\n            **framework_kwargs\n        )\n\n    elif framework == 'mask-rcnn':\n\n        sizes = ((32), (64), (128), (256), (512))\n        sizes = sizes[:len(combined_backbone.channel_list)]\n        aspect_ratios = ((0.5, 1.0, 2.0),) * len(sizes)\n        anchor_generator = AnchorGenerator(sizes=sizes, aspect_ratios=aspect_ratios)\n\n        rpn_head = torchvision.models.detection.faster_rcnn.RPNHead(combined_backbone.out_channels, anchor_generator.num_anchors_per_location()[0], conv_depth=2)\n        box_head = torchvision.models.detection.faster_rcnn.FastRCNNConvFCHead(\n            (combined_backbone.out_channels, 7, 7), [256, 256, 256, 256], [1024], norm_layer=nn.BatchNorm2d\n        )\n        mask_head = torchvision.models.detection.mask_rcnn.MaskRCNNHeads(combined_backbone.out_channels, [256, 256, 256, 256], 1, norm_layer=nn.BatchNorm2d)\n        roi_pooler = MultiScaleRoIAlign(\n            featmap_names=['feat0', 'feat1', 'feat2', 'feat3'], output_size=7, sampling_ratio=2\n        )\n\n        model = torchvision.models.detection.MaskRCNN(\n            combined_backbone,\n            num_classes=num_classes,\n            rpn_anchor_generator=anchor_generator,\n            rpn_head=rpn_head,\n            box_head=box_head,\n            box_roi_pool=roi_pooler,\n            mask_roi_pool=roi_pooler,\n            mask_head=mask_head,\n            _skip_resize=True,\n            image_mean=np.repeat(0, in_channels),\n            image_std=np.repeat(1, in_channels),\n            **framework_kwargs\n        )\n\n    else:\n        raise ValueError(f\"Framework type '{framework}' is not valid.\")\n\n    # some decoders already include a head\n    # for these, we pass the num_classes to them\n    # others dont include a head\n    # for those, we dont pass num_classes\n    # model.transform = IdentityTransform()\n\n    return ObjectDetectionModel(model, framework)\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.full_model_factory.FullModelFactory","title":"<code>terratorch.models.full_model_factory.FullModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/full_model_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass FullModelFactory(ModelFactory):\n    def build_model(\n        self,\n        model: str | nn.Module,\n        rescale: bool = True,  # noqa: FBT002, FBT001\n        padding: str = \"reflect\",\n        peft_config: dict | None = None,\n        **kwargs,\n    ) -&gt; nn.Module:\n        \"\"\"Generic model factory that wraps any model.\n\n        All kwargs are passed to the model.\n\n        Args:\n            task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\".\n            model (str, nn.Module): Model to be used. If a string, will look for such models in the different\n                registries supported (internal terratorch registry, ...). If a torch nn.Module, will use it\n                directly.\n            rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n                is different from the ground truth. Only applicable to pixel wise models\n                (e.g. segmentation, pixel wise regression, reconstruction). Defaults to True.\n            padding (str): Padding method used if images are not divisible by the patch size. Defaults to \"reflect\".\n            peft_config (dict): Configuration options for using [PEFT](https://huggingface.co/docs/peft/index).\n                The dictionary should have the following keys:\n                - \"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available [here](https://huggingface.co/docs/peft/package_reference/peft_types#peft.PeftType).\n                - \"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep.\n                  This should be used when the qkv matrices are merged together in a single linear layer and the PEFT\n                  method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in\n                  Q and V matrices). e.g. If using Prithvi this should be \"qkv\"\n                - \"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to [PeftConfig](https://huggingface.co/docs/peft/package_reference/config#peft.PeftConfig)\n\n\n        Returns:\n            nn.Module: Full model.\n        \"\"\"\n\n        model = _get_model(model, **kwargs)\n\n        # If patch size is not provided in the config or by the model, it might lead to errors due to irregular images.\n        patch_size = kwargs.get(\"patch_size\", None)\n\n        if patch_size is None:\n            # Infer patch size from model by checking all backbone modules\n            for module in model.modules():\n                if hasattr(module, \"patch_size\"):\n                    patch_size = module.patch_size\n                    break\n\n        if peft_config is not None:\n            if not kwargs.get(\"pretrained\", False):\n                msg = (\n                    \"You are using PEFT without a pretrained backbone. If you are loading a checkpoint afterwards \"\n                    \"this is probably fine, but if you are training a model check the backbone_pretrained parameter.\"\n                )\n                warnings.warn(msg, stacklevel=1)\n\n            model = get_peft_backbone(peft_config, model)\n\n        return model\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.full_model_factory.FullModelFactory.build_model","title":"<code>build_model(model, rescale=True, padding='reflect', peft_config=None, **kwargs)</code>","text":"<p>Generic model factory that wraps any model.</p> <p>All kwargs are passed to the model.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Currently supports \"segmentation\" and \"regression\".</p> required <code>model</code> <code>(str, Module)</code> <p>Model to be used. If a string, will look for such models in the different registries supported (internal terratorch registry, ...). If a torch nn.Module, will use it directly.</p> required <code>rescale</code> <code>bool</code> <p>Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression, reconstruction). Defaults to True.</p> <code>True</code> <code>padding</code> <code>str</code> <p>Padding method used if images are not divisible by the patch size. Defaults to \"reflect\".</p> <code>'reflect'</code> <code>peft_config</code> <code>dict</code> <p>Configuration options for using PEFT. The dictionary should have the following keys: - \"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available here. - \"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep.   This should be used when the qkv matrices are merged together in a single linear layer and the PEFT   method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in   Q and V matrices). e.g. If using Prithvi this should be \"qkv\" - \"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to PeftConfig</p> <code>None</code> <p>Returns:</p> Type Description <code>Module</code> <p>nn.Module: Full model.</p> Source code in <code>terratorch/models/full_model_factory.py</code> <pre><code>def build_model(\n    self,\n    model: str | nn.Module,\n    rescale: bool = True,  # noqa: FBT002, FBT001\n    padding: str = \"reflect\",\n    peft_config: dict | None = None,\n    **kwargs,\n) -&gt; nn.Module:\n    \"\"\"Generic model factory that wraps any model.\n\n    All kwargs are passed to the model.\n\n    Args:\n        task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\".\n        model (str, nn.Module): Model to be used. If a string, will look for such models in the different\n            registries supported (internal terratorch registry, ...). If a torch nn.Module, will use it\n            directly.\n        rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n            is different from the ground truth. Only applicable to pixel wise models\n            (e.g. segmentation, pixel wise regression, reconstruction). Defaults to True.\n        padding (str): Padding method used if images are not divisible by the patch size. Defaults to \"reflect\".\n        peft_config (dict): Configuration options for using [PEFT](https://huggingface.co/docs/peft/index).\n            The dictionary should have the following keys:\n            - \"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available [here](https://huggingface.co/docs/peft/package_reference/peft_types#peft.PeftType).\n            - \"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep.\n              This should be used when the qkv matrices are merged together in a single linear layer and the PEFT\n              method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in\n              Q and V matrices). e.g. If using Prithvi this should be \"qkv\"\n            - \"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to [PeftConfig](https://huggingface.co/docs/peft/package_reference/config#peft.PeftConfig)\n\n\n    Returns:\n        nn.Module: Full model.\n    \"\"\"\n\n    model = _get_model(model, **kwargs)\n\n    # If patch size is not provided in the config or by the model, it might lead to errors due to irregular images.\n    patch_size = kwargs.get(\"patch_size\", None)\n\n    if patch_size is None:\n        # Infer patch size from model by checking all backbone modules\n        for module in model.modules():\n            if hasattr(module, \"patch_size\"):\n                patch_size = module.patch_size\n                break\n\n    if peft_config is not None:\n        if not kwargs.get(\"pretrained\", False):\n            msg = (\n                \"You are using PEFT without a pretrained backbone. If you are loading a checkpoint afterwards \"\n                \"this is probably fine, but if you are training a model check the backbone_pretrained parameter.\"\n            )\n            warnings.warn(msg, stacklevel=1)\n\n        model = get_peft_backbone(peft_config, model)\n\n    return model\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.smp_model_factory.SMPModelFactory","title":"<code>terratorch.models.smp_model_factory.SMPModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/smp_model_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass SMPModelFactory(ModelFactory):\n    def build_model(\n        self,\n        task: str,\n        backbone: str,\n        model: str,\n        bands: list[HLSBands | int],\n        in_channels: int | None = None,\n        num_classes: int = 1,\n        pretrained: str | bool | None = True,  # noqa: FBT002\n        prepare_features_for_image_model: Callable | None = None,\n        regression_relu: bool = False,  # noqa: FBT001, FBT002\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"\n        Factory class for creating SMP (Segmentation Models Pytorch) based models with optional customization.\n\n        This factory handles the instantiation of segmentation and regression models using specified\n        encoders and decoders from the SMP library, along with custom modifications and extensions such\n        as auxiliary decoders or modified encoders.\n\n        Attributes:\n            task (str): Specifies the task for which the model is being built. Supported tasks are\n                        \"segmentation\".\n            backbone (str): Specifies the backbone model to be used.\n            decoder (str): Specifies the decoder to be used for constructing the\n                        segmentation model.\n            bands (list[terratorch.datasets.HLSBands | int]): A list specifying the bands that the model\n                        will operate on. These are expected to be from terratorch.datasets.HLSBands.\n            in_channels (int, optional): Specifies the number of input channels. Defaults to None.\n            num_classes (int, optional): The number of output classes for the model.\n            pretrained (bool | Path, optional): Indicates whether to load pretrained weights for the\n                        backbone. Can also specify a path to weights. Defaults to True.\n            num_frames (int, optional): Specifies the number of timesteps the model should handle. Useful\n                        for temporal models.\n            regression_relu (bool): Whether to apply ReLU activation in the case of regression tasks.\n            **kwargs: Additional arguments that might be passed to further customize the backbone, decoder,\n                        or any auxiliary heads. These should be prefixed appropriately\n\n        Raises:\n            ValueError: If the specified decoder is not supported by SMP.\n            Exception: If the specified task is not \"segmentation\"\n\n        Returns:\n            nn.Module: A model instance wrapped in SMPModelWrapper configured according to the specified\n                    parameters and tasks.\n        \"\"\"\n        if task != \"segmentation\":\n            msg = f\"SMP models can only perform segmentation, but got task {task}\"\n            raise Exception(msg)\n\n        bands = [HLSBands.try_convert_to_hls_bands_enum(b) for b in bands]\n        if in_channels is None:\n            in_channels = len(bands)\n\n        # Gets decoder module.\n        model_module = getattr(smp, model, None)\n        if model_module is None:\n            msg = f\"Decoder {model} is not supported in SMP.\"\n            raise ValueError(msg)\n\n        backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")  # Encoder params should be prefixed backbone_\n        smp_kwargs, kwargs = extract_prefix_keys(backbone_kwargs, \"smp_\")  # Smp model params should be prefixed smp_\n        aux_params, kwargs = extract_prefix_keys(backbone_kwargs, \"aux_\")  # Auxiliary head params should be prefixed aux_\n        aux_params = None if aux_params == {} else aux_params\n\n        if isinstance(pretrained, bool):\n            if pretrained:\n                pretrained = \"imagenet\"\n            else:\n                pretrained = None\n\n        # If encoder not currently supported by SMP (custom encoder).\n        if backbone not in smp_encoders:\n            # These params must be included in the config file with appropriate prefix.\n            required_params = {\n                \"encoder_depth\": smp_kwargs,\n                \"out_channels\": backbone_kwargs,\n                \"output_stride\": backbone_kwargs,\n            }\n\n            for param, config_dict in required_params.items():\n                if param not in config_dict:\n                    msg = f\"Config must include the '{param}' parameter\"\n                    raise ValueError(msg)\n\n            # Using new encoder.\n            backbone_class = make_smp_encoder(backbone)\n            backbone_kwargs[\"prepare_features_for_image_model\"] = prepare_features_for_image_model\n            # Registering custom encoder into SMP.\n            register_custom_encoder(backbone_class, backbone_kwargs, pretrained)\n\n            model_args = {\n                \"encoder_name\": \"SMPEncoderWrapperWithPFFIM\",\n                \"encoder_weights\": pretrained,\n                \"in_channels\": in_channels,\n                \"classes\": num_classes,\n                **smp_kwargs,\n            }\n        # Using SMP encoder.\n        else:\n            model_args = {\n                \"encoder_name\": backbone,\n                \"encoder_weights\": pretrained,\n                \"in_channels\": in_channels,\n                \"classes\": num_classes,\n                **smp_kwargs,\n            }\n\n        model = model_module(**model_args, aux_params=aux_params)\n\n        return SMPModelWrapper(\n            model, relu=task == \"regression\" and regression_relu, squeeze_single_class=task == \"regression\"\n        )\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.smp_model_factory.SMPModelFactory.build_model","title":"<code>build_model(task, backbone, model, bands, in_channels=None, num_classes=1, pretrained=True, prepare_features_for_image_model=None, regression_relu=False, **kwargs)</code>","text":"<p>Factory class for creating SMP (Segmentation Models Pytorch) based models with optional customization.</p> <p>This factory handles the instantiation of segmentation and regression models using specified encoders and decoders from the SMP library, along with custom modifications and extensions such as auxiliary decoders or modified encoders.</p> <p>Attributes:</p> Name Type Description <code>task</code> <code>str</code> <p>Specifies the task for which the model is being built. Supported tasks are         \"segmentation\".</p> <code>backbone</code> <code>str</code> <p>Specifies the backbone model to be used.</p> <code>decoder</code> <code>str</code> <p>Specifies the decoder to be used for constructing the         segmentation model.</p> <code>bands</code> <code>list[HLSBands | int]</code> <p>A list specifying the bands that the model         will operate on. These are expected to be from terratorch.datasets.HLSBands.</p> <code>in_channels</code> <code>int</code> <p>Specifies the number of input channels. Defaults to None.</p> <code>num_classes</code> <code>int</code> <p>The number of output classes for the model.</p> <code>pretrained</code> <code>bool | Path</code> <p>Indicates whether to load pretrained weights for the         backbone. Can also specify a path to weights. Defaults to True.</p> <code>num_frames</code> <code>int</code> <p>Specifies the number of timesteps the model should handle. Useful         for temporal models.</p> <code>regression_relu</code> <code>bool</code> <p>Whether to apply ReLU activation in the case of regression tasks.</p> <code>**kwargs</code> <code>bool</code> <p>Additional arguments that might be passed to further customize the backbone, decoder,         or any auxiliary heads. These should be prefixed appropriately</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified decoder is not supported by SMP.</p> <code>Exception</code> <p>If the specified task is not \"segmentation\"</p> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: A model instance wrapped in SMPModelWrapper configured according to the specified     parameters and tasks.</p> Source code in <code>terratorch/models/smp_model_factory.py</code> <pre><code>def build_model(\n    self,\n    task: str,\n    backbone: str,\n    model: str,\n    bands: list[HLSBands | int],\n    in_channels: int | None = None,\n    num_classes: int = 1,\n    pretrained: str | bool | None = True,  # noqa: FBT002\n    prepare_features_for_image_model: Callable | None = None,\n    regression_relu: bool = False,  # noqa: FBT001, FBT002\n    **kwargs,\n) -&gt; Model:\n    \"\"\"\n    Factory class for creating SMP (Segmentation Models Pytorch) based models with optional customization.\n\n    This factory handles the instantiation of segmentation and regression models using specified\n    encoders and decoders from the SMP library, along with custom modifications and extensions such\n    as auxiliary decoders or modified encoders.\n\n    Attributes:\n        task (str): Specifies the task for which the model is being built. Supported tasks are\n                    \"segmentation\".\n        backbone (str): Specifies the backbone model to be used.\n        decoder (str): Specifies the decoder to be used for constructing the\n                    segmentation model.\n        bands (list[terratorch.datasets.HLSBands | int]): A list specifying the bands that the model\n                    will operate on. These are expected to be from terratorch.datasets.HLSBands.\n        in_channels (int, optional): Specifies the number of input channels. Defaults to None.\n        num_classes (int, optional): The number of output classes for the model.\n        pretrained (bool | Path, optional): Indicates whether to load pretrained weights for the\n                    backbone. Can also specify a path to weights. Defaults to True.\n        num_frames (int, optional): Specifies the number of timesteps the model should handle. Useful\n                    for temporal models.\n        regression_relu (bool): Whether to apply ReLU activation in the case of regression tasks.\n        **kwargs: Additional arguments that might be passed to further customize the backbone, decoder,\n                    or any auxiliary heads. These should be prefixed appropriately\n\n    Raises:\n        ValueError: If the specified decoder is not supported by SMP.\n        Exception: If the specified task is not \"segmentation\"\n\n    Returns:\n        nn.Module: A model instance wrapped in SMPModelWrapper configured according to the specified\n                parameters and tasks.\n    \"\"\"\n    if task != \"segmentation\":\n        msg = f\"SMP models can only perform segmentation, but got task {task}\"\n        raise Exception(msg)\n\n    bands = [HLSBands.try_convert_to_hls_bands_enum(b) for b in bands]\n    if in_channels is None:\n        in_channels = len(bands)\n\n    # Gets decoder module.\n    model_module = getattr(smp, model, None)\n    if model_module is None:\n        msg = f\"Decoder {model} is not supported in SMP.\"\n        raise ValueError(msg)\n\n    backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")  # Encoder params should be prefixed backbone_\n    smp_kwargs, kwargs = extract_prefix_keys(backbone_kwargs, \"smp_\")  # Smp model params should be prefixed smp_\n    aux_params, kwargs = extract_prefix_keys(backbone_kwargs, \"aux_\")  # Auxiliary head params should be prefixed aux_\n    aux_params = None if aux_params == {} else aux_params\n\n    if isinstance(pretrained, bool):\n        if pretrained:\n            pretrained = \"imagenet\"\n        else:\n            pretrained = None\n\n    # If encoder not currently supported by SMP (custom encoder).\n    if backbone not in smp_encoders:\n        # These params must be included in the config file with appropriate prefix.\n        required_params = {\n            \"encoder_depth\": smp_kwargs,\n            \"out_channels\": backbone_kwargs,\n            \"output_stride\": backbone_kwargs,\n        }\n\n        for param, config_dict in required_params.items():\n            if param not in config_dict:\n                msg = f\"Config must include the '{param}' parameter\"\n                raise ValueError(msg)\n\n        # Using new encoder.\n        backbone_class = make_smp_encoder(backbone)\n        backbone_kwargs[\"prepare_features_for_image_model\"] = prepare_features_for_image_model\n        # Registering custom encoder into SMP.\n        register_custom_encoder(backbone_class, backbone_kwargs, pretrained)\n\n        model_args = {\n            \"encoder_name\": \"SMPEncoderWrapperWithPFFIM\",\n            \"encoder_weights\": pretrained,\n            \"in_channels\": in_channels,\n            \"classes\": num_classes,\n            **smp_kwargs,\n        }\n    # Using SMP encoder.\n    else:\n        model_args = {\n            \"encoder_name\": backbone,\n            \"encoder_weights\": pretrained,\n            \"in_channels\": in_channels,\n            \"classes\": num_classes,\n            **smp_kwargs,\n        }\n\n    model = model_module(**model_args, aux_params=aux_params)\n\n    return SMPModelWrapper(\n        model, relu=task == \"regression\" and regression_relu, squeeze_single_class=task == \"regression\"\n    )\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.timm_model_factory.TimmModelFactory","title":"<code>terratorch.models.timm_model_factory.TimmModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/timm_model_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass TimmModelFactory(ModelFactory):\n    def build_model(\n        self,\n        task: str,\n        backbone: str,\n        in_channels: int,\n        num_classes: int,\n        pretrained: str | bool = True,\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"Build a classifier from timm\n\n        Args:\n            task (str): Must be \"classification\".\n            backbone (str): Name of the backbone in timm.\n            in_channels (int): Number of input channels.\n            num_classes (int): Number of classes.\n\n        Returns:\n            Model: Timm model wrapped in TimmModelWrapper.\n        \"\"\"\n        if task != \"classification\":\n            msg = f\"timm models can only perform classification, but got task {task}\"\n            raise Exception(msg)\n        backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n        if isinstance(pretrained, bool):\n            model = create_model(\n                backbone, pretrained=pretrained, num_classes=num_classes, in_chans=in_channels, **backbone_kwargs\n            )\n        else:\n            model = create_model(backbone, num_classes=num_classes, in_chans=in_channels, **backbone_kwargs)\n\n        # Load weights\n        # Code adapted from geobench\n        if pretrained and pretrained is not True:\n            try:\n                weights = WeightsEnum(pretrained)\n                state_dict = weights.get_state_dict(progress=True)\n            except ValueError:\n                if os.path.exists(pretrained):\n                    _, state_dict = utils.extract_backbone(pretrained)\n                else:\n                    state_dict = get_weight(pretrained).get_state_dict(progress=True)\n            model = utils.load_state_dict(model, state_dict)\n\n        return TimmModelWrapper(model)\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.timm_model_factory.TimmModelFactory.build_model","title":"<code>build_model(task, backbone, in_channels, num_classes, pretrained=True, **kwargs)</code>","text":"<p>Build a classifier from timm</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Must be \"classification\".</p> required <code>backbone</code> <code>str</code> <p>Name of the backbone in timm.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>Timm model wrapped in TimmModelWrapper.</p> Source code in <code>terratorch/models/timm_model_factory.py</code> <pre><code>def build_model(\n    self,\n    task: str,\n    backbone: str,\n    in_channels: int,\n    num_classes: int,\n    pretrained: str | bool = True,\n    **kwargs,\n) -&gt; Model:\n    \"\"\"Build a classifier from timm\n\n    Args:\n        task (str): Must be \"classification\".\n        backbone (str): Name of the backbone in timm.\n        in_channels (int): Number of input channels.\n        num_classes (int): Number of classes.\n\n    Returns:\n        Model: Timm model wrapped in TimmModelWrapper.\n    \"\"\"\n    if task != \"classification\":\n        msg = f\"timm models can only perform classification, but got task {task}\"\n        raise Exception(msg)\n    backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n    if isinstance(pretrained, bool):\n        model = create_model(\n            backbone, pretrained=pretrained, num_classes=num_classes, in_chans=in_channels, **backbone_kwargs\n        )\n    else:\n        model = create_model(backbone, num_classes=num_classes, in_chans=in_channels, **backbone_kwargs)\n\n    # Load weights\n    # Code adapted from geobench\n    if pretrained and pretrained is not True:\n        try:\n            weights = WeightsEnum(pretrained)\n            state_dict = weights.get_state_dict(progress=True)\n        except ValueError:\n            if os.path.exists(pretrained):\n                _, state_dict = utils.extract_backbone(pretrained)\n            else:\n                state_dict = get_weight(pretrained).get_state_dict(progress=True)\n        model = utils.load_state_dict(model, state_dict)\n\n    return TimmModelWrapper(model)\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.generic_model_factory.GenericModelFactory","title":"<code>terratorch.models.generic_model_factory.GenericModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/generic_model_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass GenericModelFactory(ModelFactory):\n\n    def build_model(\n        self,\n        backbone: str | None = None,\n        in_channels: int = 6,\n        pretrained: str | bool | None = True,\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"Factory to create models from any custom module.\n\n        Args:\n            model (str): The name for the model class.\n            in_channels (int): Number of input channels.\n            pretrained(str | bool): Which weights to use for the backbone. If true, will use \"imagenet\". If false or None, random weights. Defaults to True.\n\n        Returns:\n            Model: A wrapped generic model.\n        \"\"\"\n\n        model_kwargs = _extract_prefix_keys(kwargs, \"backbone_\")\n\n        try:\n            model = BACKBONE_REGISTRY.build(backbone, **model_kwargs)\n\n        except KeyError:\n            raise KeyError(f\"Model {backbone} not found in the registry.\")\n\n        return GenericModelWrapper(model)\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.generic_model_factory.GenericModelFactory.build_model","title":"<code>build_model(backbone=None, in_channels=6, pretrained=True, **kwargs)</code>","text":"<p>Factory to create models from any custom module.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name for the model class.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> <code>6</code> <code>pretrained(str</code> <code>| bool</code> <p>Which weights to use for the backbone. If true, will use \"imagenet\". If false or None, random weights. Defaults to True.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>A wrapped generic model.</p> Source code in <code>terratorch/models/generic_model_factory.py</code> <pre><code>def build_model(\n    self,\n    backbone: str | None = None,\n    in_channels: int = 6,\n    pretrained: str | bool | None = True,\n    **kwargs,\n) -&gt; Model:\n    \"\"\"Factory to create models from any custom module.\n\n    Args:\n        model (str): The name for the model class.\n        in_channels (int): Number of input channels.\n        pretrained(str | bool): Which weights to use for the backbone. If true, will use \"imagenet\". If false or None, random weights. Defaults to True.\n\n    Returns:\n        Model: A wrapped generic model.\n    \"\"\"\n\n    model_kwargs = _extract_prefix_keys(kwargs, \"backbone_\")\n\n    try:\n        model = BACKBONE_REGISTRY.build(backbone, **model_kwargs)\n\n    except KeyError:\n        raise KeyError(f\"Model {backbone} not found in the registry.\")\n\n    return GenericModelWrapper(model)\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.clay_model_factory.ClayModelFactory","title":"<code>terratorch.models.clay_model_factory.ClayModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/clay_model_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass ClayModelFactory(ModelFactory):\n    def build_model(\n        self,\n        task: str,\n        backbone: str | nn.Module,\n        decoder: str | nn.Module,\n        in_channels: int,\n        bands: list[int] = [],\n        num_classes: int | None = None,\n        pretrained: bool = True,  # noqa: FBT001, FBT002\n        num_frames: int = 1,\n        prepare_features_for_image_model: Callable | None = None,\n        aux_decoders: list[AuxiliaryHead] | None = None,\n        rescale: bool = True,  # noqa: FBT002, FBT001\n        checkpoint_path: str = None,\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"Model factory for Clay models.\n\n        Further arguments to be passed to the backbone, decoder or head. They should be prefixed with\n        `backbone_`, `decoder_` and `head_` respectively.\n\n        Args:\n            task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\".\n            backbone (str, nn.Module): Backbone to be used. If string, should be able to be parsed\n                by the specified factory. Defaults to \"prithvi_100\".\n            decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model.\n                    If a string, it will be created from a class exposed in decoder.__init__.py with the same name.\n                    If an nn.Module, we expect it to expose a property `decoder.out_channels`.\n                    Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".\n            in_channels (int, optional): Number of input channels. Defaults to 3.\n            num_classes (int, optional): Number of classes. None for regression tasks.\n            pretrained (Union[bool, Path], optional): Whether to load pretrained weights for the backbone, if available.\n                Defaults to True.\n            num_frames (int, optional): Number of timesteps for the model to handle. Defaults to 1.\n            prepare_features_for_image_model (Callable | None): Function to be called on encoder features\n                before passing them to the decoder. Defaults to None, which applies the identity function.\n            aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead deciders to be added to the model.\n                These decoders take the input from the encoder as well.\n            rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n                is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.\n\n        Raises:\n            NotImplementedError: _description_\n            DecoderNotFoundException: _description_\n\n        Returns:\n            nn.Module: _description_\n        \"\"\"\n        if not torch.cuda.is_available():\n            self.CPU_ONLY = True\n        else:\n            self.CPU_ONLY = False\n\n        # Path for accessing the model source code.\n        self.syspath_kwarg = \"model_sys_path\"\n        backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n\n        # TODO: support auxiliary heads\n        if not isinstance(backbone, nn.Module):\n            if not \"clay\" in backbone:\n                msg = \"This class only handles models for `Clay` encoders\"\n                raise NotImplementedError(msg)\n\n            task = task.lower()\n            if task not in SUPPORTED_TASKS:\n                msg = f\"Task {task} not supported. Please choose one of {SUPPORTED_TASKS}\"\n                raise NotImplementedError(msg)\n\n            # Trying to find the model on HuggingFace.\n            try:\n                backbone: nn.Module = timm.create_model(\n                    backbone,\n                    pretrained=pretrained,\n                    in_chans=in_channels,\n                    bands=bands,\n                    num_frames=num_frames,\n                    features_only=True,\n                    **backbone_kwargs,\n                )\n            except Exception as e:\n                print(e, \"Error loading from HF. Trying to instantiate locally ...\")\n\n        else:\n            if checkpoint_path is None:\n                raise ValueError(\"A checkpoint (checkpoint_path) must be provided to restore the model.\")\n\n            backbone: nn.Module = Embedder(ckpt_path=checkpoint_path, **backbone_kwargs)\n            print(\"Model Clay was successfully restored.\")\n\n        # If patch size is not provided in the config or by the model, it might lead to errors due to irregular images.\n        patch_size = backbone_kwargs.get(\"patch_size\", None)\n        if patch_size is None:\n            # Infer patch size from model by checking all backbone modules\n            for module in backbone.modules():\n                if hasattr(module, \"patch_size\"):\n                    patch_size = module.patch_size\n                    break\n        padding = backbone_kwargs.get(\"padding\", \"reflect\")\n\n        # allow decoder to be a module passed directly\n        decoder_cls = _get_decoder(decoder)\n        decoder_kwargs, kwargs = extract_prefix_keys(kwargs, \"decoder_\")\n\n        # TODO: remove this\n        decoder: nn.Module = decoder_cls(\n            backbone.feature_info.channels(), **decoder_kwargs)\n        # decoder: nn.Module = decoder_cls([128, 256, 512, 1024], **decoder_kwargs)\n\n        head_kwargs, kwargs = extract_prefix_keys(kwargs, \"head_\")\n        if num_classes:\n            head_kwargs[\"num_classes\"] = num_classes\n        if aux_decoders is None:\n            return _build_appropriate_model(\n                task, backbone, decoder, head_kwargs, prepare_features_for_image_model, patch_size=patch_size, padding=padding, rescale=rescale\n            )\n\n        to_be_aux_decoders: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] = []\n\n        for aux_decoder in aux_decoders:\n            args = aux_decoder.decoder_args if aux_decoder.decoder_args else {}\n            aux_decoder_cls: nn.Module = _get_decoder(aux_decoder.decoder)\n\n            aux_decoder_kwargs, kwargs = extract_prefix_keys(args, \"decoder_\")\n            aux_decoder_instance = aux_decoder_cls(backbone.feature_info.channels(), **aux_decoder_kwargs)\n            # aux_decoder_instance = aux_decoder_cls([128, 256, 512, 1024], **decoder_kwargs)\n\n            aux_head_kwargs, kwargs = extract_prefix_keys(args, \"head_\")\n            if num_classes:\n                aux_head_kwargs[\"num_classes\"] = num_classes\n            # aux_head: nn.Module = _get_head(task, aux_decoder_instance, num_classes=num_classes, **head_kwargs)\n            # aux_decoder.decoder = nn.Sequential(aux_decoder_instance, aux_head)\n            to_be_aux_decoders.append(\n                AuxiliaryHeadWithDecoderWithoutInstantiatedHead(\n                    aux_decoder.name, aux_decoder_instance, aux_head_kwargs)\n            )\n\n        return _build_appropriate_model(\n            task,\n            backbone,\n            decoder,\n            head_kwargs,\n            prepare_features_for_image_model,\n            patch_size=patch_size,\n            padding=padding,\n            rescale=rescale,\n            auxiliary_heads=to_be_aux_decoders,\n        )\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.clay_model_factory.ClayModelFactory.build_model","title":"<code>build_model(task, backbone, decoder, in_channels, bands=[], num_classes=None, pretrained=True, num_frames=1, prepare_features_for_image_model=None, aux_decoders=None, rescale=True, checkpoint_path=None, **kwargs)</code>","text":"<p>Model factory for Clay models.</p> <p>Further arguments to be passed to the backbone, decoder or head. They should be prefixed with <code>backbone_</code>, <code>decoder_</code> and <code>head_</code> respectively.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Currently supports \"segmentation\" and \"regression\".</p> required <code>backbone</code> <code>(str, Module)</code> <p>Backbone to be used. If string, should be able to be parsed by the specified factory. Defaults to \"prithvi_100\".</p> required <code>decoder</code> <code>Union[str, Module]</code> <p>Decoder to be used for the segmentation model.     If a string, it will be created from a class exposed in decoder.init.py with the same name.     If an nn.Module, we expect it to expose a property <code>decoder.out_channels</code>.     Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes. None for regression tasks.</p> <code>None</code> <code>pretrained</code> <code>Union[bool, Path]</code> <p>Whether to load pretrained weights for the backbone, if available. Defaults to True.</p> <code>True</code> <code>num_frames</code> <code>int</code> <p>Number of timesteps for the model to handle. Defaults to 1.</p> <code>1</code> <code>prepare_features_for_image_model</code> <code>Callable | None</code> <p>Function to be called on encoder features before passing them to the decoder. Defaults to None, which applies the identity function.</p> <code>None</code> <code>aux_decoders</code> <code>list[AuxiliaryHead] | None</code> <p>List of AuxiliaryHead deciders to be added to the model. These decoders take the input from the encoder as well.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> <code>DecoderNotFoundException</code> <p>description</p> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: description</p> Source code in <code>terratorch/models/clay_model_factory.py</code> <pre><code>def build_model(\n    self,\n    task: str,\n    backbone: str | nn.Module,\n    decoder: str | nn.Module,\n    in_channels: int,\n    bands: list[int] = [],\n    num_classes: int | None = None,\n    pretrained: bool = True,  # noqa: FBT001, FBT002\n    num_frames: int = 1,\n    prepare_features_for_image_model: Callable | None = None,\n    aux_decoders: list[AuxiliaryHead] | None = None,\n    rescale: bool = True,  # noqa: FBT002, FBT001\n    checkpoint_path: str = None,\n    **kwargs,\n) -&gt; Model:\n    \"\"\"Model factory for Clay models.\n\n    Further arguments to be passed to the backbone, decoder or head. They should be prefixed with\n    `backbone_`, `decoder_` and `head_` respectively.\n\n    Args:\n        task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\".\n        backbone (str, nn.Module): Backbone to be used. If string, should be able to be parsed\n            by the specified factory. Defaults to \"prithvi_100\".\n        decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model.\n                If a string, it will be created from a class exposed in decoder.__init__.py with the same name.\n                If an nn.Module, we expect it to expose a property `decoder.out_channels`.\n                Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".\n        in_channels (int, optional): Number of input channels. Defaults to 3.\n        num_classes (int, optional): Number of classes. None for regression tasks.\n        pretrained (Union[bool, Path], optional): Whether to load pretrained weights for the backbone, if available.\n            Defaults to True.\n        num_frames (int, optional): Number of timesteps for the model to handle. Defaults to 1.\n        prepare_features_for_image_model (Callable | None): Function to be called on encoder features\n            before passing them to the decoder. Defaults to None, which applies the identity function.\n        aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead deciders to be added to the model.\n            These decoders take the input from the encoder as well.\n        rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n            is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.\n\n    Raises:\n        NotImplementedError: _description_\n        DecoderNotFoundException: _description_\n\n    Returns:\n        nn.Module: _description_\n    \"\"\"\n    if not torch.cuda.is_available():\n        self.CPU_ONLY = True\n    else:\n        self.CPU_ONLY = False\n\n    # Path for accessing the model source code.\n    self.syspath_kwarg = \"model_sys_path\"\n    backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n\n    # TODO: support auxiliary heads\n    if not isinstance(backbone, nn.Module):\n        if not \"clay\" in backbone:\n            msg = \"This class only handles models for `Clay` encoders\"\n            raise NotImplementedError(msg)\n\n        task = task.lower()\n        if task not in SUPPORTED_TASKS:\n            msg = f\"Task {task} not supported. Please choose one of {SUPPORTED_TASKS}\"\n            raise NotImplementedError(msg)\n\n        # Trying to find the model on HuggingFace.\n        try:\n            backbone: nn.Module = timm.create_model(\n                backbone,\n                pretrained=pretrained,\n                in_chans=in_channels,\n                bands=bands,\n                num_frames=num_frames,\n                features_only=True,\n                **backbone_kwargs,\n            )\n        except Exception as e:\n            print(e, \"Error loading from HF. Trying to instantiate locally ...\")\n\n    else:\n        if checkpoint_path is None:\n            raise ValueError(\"A checkpoint (checkpoint_path) must be provided to restore the model.\")\n\n        backbone: nn.Module = Embedder(ckpt_path=checkpoint_path, **backbone_kwargs)\n        print(\"Model Clay was successfully restored.\")\n\n    # If patch size is not provided in the config or by the model, it might lead to errors due to irregular images.\n    patch_size = backbone_kwargs.get(\"patch_size\", None)\n    if patch_size is None:\n        # Infer patch size from model by checking all backbone modules\n        for module in backbone.modules():\n            if hasattr(module, \"patch_size\"):\n                patch_size = module.patch_size\n                break\n    padding = backbone_kwargs.get(\"padding\", \"reflect\")\n\n    # allow decoder to be a module passed directly\n    decoder_cls = _get_decoder(decoder)\n    decoder_kwargs, kwargs = extract_prefix_keys(kwargs, \"decoder_\")\n\n    # TODO: remove this\n    decoder: nn.Module = decoder_cls(\n        backbone.feature_info.channels(), **decoder_kwargs)\n    # decoder: nn.Module = decoder_cls([128, 256, 512, 1024], **decoder_kwargs)\n\n    head_kwargs, kwargs = extract_prefix_keys(kwargs, \"head_\")\n    if num_classes:\n        head_kwargs[\"num_classes\"] = num_classes\n    if aux_decoders is None:\n        return _build_appropriate_model(\n            task, backbone, decoder, head_kwargs, prepare_features_for_image_model, patch_size=patch_size, padding=padding, rescale=rescale\n        )\n\n    to_be_aux_decoders: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] = []\n\n    for aux_decoder in aux_decoders:\n        args = aux_decoder.decoder_args if aux_decoder.decoder_args else {}\n        aux_decoder_cls: nn.Module = _get_decoder(aux_decoder.decoder)\n\n        aux_decoder_kwargs, kwargs = extract_prefix_keys(args, \"decoder_\")\n        aux_decoder_instance = aux_decoder_cls(backbone.feature_info.channels(), **aux_decoder_kwargs)\n        # aux_decoder_instance = aux_decoder_cls([128, 256, 512, 1024], **decoder_kwargs)\n\n        aux_head_kwargs, kwargs = extract_prefix_keys(args, \"head_\")\n        if num_classes:\n            aux_head_kwargs[\"num_classes\"] = num_classes\n        # aux_head: nn.Module = _get_head(task, aux_decoder_instance, num_classes=num_classes, **head_kwargs)\n        # aux_decoder.decoder = nn.Sequential(aux_decoder_instance, aux_head)\n        to_be_aux_decoders.append(\n            AuxiliaryHeadWithDecoderWithoutInstantiatedHead(\n                aux_decoder.name, aux_decoder_instance, aux_head_kwargs)\n        )\n\n    return _build_appropriate_model(\n        task,\n        backbone,\n        decoder,\n        head_kwargs,\n        prepare_features_for_image_model,\n        patch_size=patch_size,\n        padding=padding,\n        rescale=rescale,\n        auxiliary_heads=to_be_aux_decoders,\n    )\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.generic_unet_model_factory.GenericUnetModelFactory","title":"<code>terratorch.models.generic_unet_model_factory.GenericUnetModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/generic_unet_model_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass GenericUnetModelFactory(ModelFactory):\n    def _check_model_availability(self, model, builtin_engine, engine, **model_kwargs):\n\n        try:\n            print(f\"Using module {model} from terratorch.\")\n            if builtin_engine:\n                model_class = getattr(builtin_engine, model)\n            else:\n                model_class = None\n        except:\n            if _has_mmseg:\n                print(\"Module not available on terratorch.\")\n                print(f\"Using module {model} from mmseg.\")\n                if engine:\n                    model_class = getattr(engine, model)\n                else:\n                    model_class = None\n            else:\n                raise Exception(\"mmseg is not installed.\")\n\n        if model_class:\n            model = model_class(\n               **model_kwargs,\n            )\n        else:\n            model = None\n\n        return model \n\n    def build_model(\n        self,\n        task: str = \"segmentation\",\n        backbone: str | None = None,\n        decoder: str | None = None,\n        dilations: tuple[int] = (1, 6, 12, 18),\n        in_channels: int = 6,\n        pretrained: str | bool | None = True,\n        regression_relu: bool = False,\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"Factory to create model based on mmseg.\n\n        Args:\n            task (str): Must be \"segmentation\".\n            model (str): Decoder architecture. Currently only supports \"unet\".\n            in_channels (int): Number of input channels.\n            pretrained(str | bool): Which weights to use for the backbone. If true, will use \"imagenet\". If false or None, random weights. Defaults to True.\n            regression_relu (bool). Whether to apply a ReLU if task is regression. Defaults to False.\n\n        Returns:\n            Model: UNet model.\n        \"\"\"\n        if task not in [\"segmentation\", \"regression\"]:\n            msg = f\"This model can only perform pixel wise tasks, but got task {task}\"\n            raise Exception(msg)\n\n        builtin_engine_decoders = importlib.import_module(\"terratorch.models.decoders\")\n        builtin_engine_encoders = importlib.import_module(\"terratorch.models.backbones\")\n\n        # Default values\n        backbone_builtin_engine = None\n        decoder_builtin_engine = None\n        backbone_engine = None \n        decoder_engine = None \n        backbone_model_kwargs = {}\n        decoder_model_kwargs = {}\n\n        try:\n            engine_decoders = importlib.import_module(\"mmseg.models.decode_heads\")\n            engine_encoders = importlib.import_module(\"mmseg.models.backbones\")\n            _has_mmseg = True\n        except:\n            engine_decoders = None\n            engine_encoders = None\n            _has_mmseg = False\n            print(\"mmseg is not installed.\")\n\n        if backbone:\n            backbone_kwargs = _extract_prefix_keys(kwargs, \"backbone_\")\n            backbone_model_kwargs = backbone_kwargs\n            backbone_engine = engine_encoders\n            backbone_builtin_engine = builtin_engine_encoders\n        else:\n            backbone=None\n\n        if decoder: \n            decoder_kwargs = _extract_prefix_keys(kwargs, \"decoder_\")\n            decoder_model_kwargs = decoder_kwargs\n            decoder_engine = engine_decoders\n            decoder_builtin_engine = builtin_engine_decoders\n        else:\n            decoder = None \n\n        if not backbone and not decoder:\n            print(\"It is necessary to define a backbone and/or a decoder.\")\n\n        # Instantianting backbone and decoder \n        backbone = self._check_model_availability(backbone, backbone_builtin_engine, backbone_engine, **backbone_model_kwargs) \n        decoder = self._check_model_availability(decoder, decoder_builtin_engine, decoder_engine, **decoder_model_kwargs) \n\n        return GenericUnetModelWrapper(\n            backbone, decoder=decoder, relu=task == \"regression\" and regression_relu, squeeze_single_class=task == \"regression\"\n        )\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.generic_unet_model_factory.GenericUnetModelFactory.build_model","title":"<code>build_model(task='segmentation', backbone=None, decoder=None, dilations=(1, 6, 12, 18), in_channels=6, pretrained=True, regression_relu=False, **kwargs)</code>","text":"<p>Factory to create model based on mmseg.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Must be \"segmentation\".</p> <code>'segmentation'</code> <code>model</code> <code>str</code> <p>Decoder architecture. Currently only supports \"unet\".</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> <code>6</code> <code>pretrained(str</code> <code>| bool</code> <p>Which weights to use for the backbone. If true, will use \"imagenet\". If false or None, random weights. Defaults to True.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>UNet model.</p> Source code in <code>terratorch/models/generic_unet_model_factory.py</code> <pre><code>def build_model(\n    self,\n    task: str = \"segmentation\",\n    backbone: str | None = None,\n    decoder: str | None = None,\n    dilations: tuple[int] = (1, 6, 12, 18),\n    in_channels: int = 6,\n    pretrained: str | bool | None = True,\n    regression_relu: bool = False,\n    **kwargs,\n) -&gt; Model:\n    \"\"\"Factory to create model based on mmseg.\n\n    Args:\n        task (str): Must be \"segmentation\".\n        model (str): Decoder architecture. Currently only supports \"unet\".\n        in_channels (int): Number of input channels.\n        pretrained(str | bool): Which weights to use for the backbone. If true, will use \"imagenet\". If false or None, random weights. Defaults to True.\n        regression_relu (bool). Whether to apply a ReLU if task is regression. Defaults to False.\n\n    Returns:\n        Model: UNet model.\n    \"\"\"\n    if task not in [\"segmentation\", \"regression\"]:\n        msg = f\"This model can only perform pixel wise tasks, but got task {task}\"\n        raise Exception(msg)\n\n    builtin_engine_decoders = importlib.import_module(\"terratorch.models.decoders\")\n    builtin_engine_encoders = importlib.import_module(\"terratorch.models.backbones\")\n\n    # Default values\n    backbone_builtin_engine = None\n    decoder_builtin_engine = None\n    backbone_engine = None \n    decoder_engine = None \n    backbone_model_kwargs = {}\n    decoder_model_kwargs = {}\n\n    try:\n        engine_decoders = importlib.import_module(\"mmseg.models.decode_heads\")\n        engine_encoders = importlib.import_module(\"mmseg.models.backbones\")\n        _has_mmseg = True\n    except:\n        engine_decoders = None\n        engine_encoders = None\n        _has_mmseg = False\n        print(\"mmseg is not installed.\")\n\n    if backbone:\n        backbone_kwargs = _extract_prefix_keys(kwargs, \"backbone_\")\n        backbone_model_kwargs = backbone_kwargs\n        backbone_engine = engine_encoders\n        backbone_builtin_engine = builtin_engine_encoders\n    else:\n        backbone=None\n\n    if decoder: \n        decoder_kwargs = _extract_prefix_keys(kwargs, \"decoder_\")\n        decoder_model_kwargs = decoder_kwargs\n        decoder_engine = engine_decoders\n        decoder_builtin_engine = builtin_engine_decoders\n    else:\n        decoder = None \n\n    if not backbone and not decoder:\n        print(\"It is necessary to define a backbone and/or a decoder.\")\n\n    # Instantianting backbone and decoder \n    backbone = self._check_model_availability(backbone, backbone_builtin_engine, backbone_engine, **backbone_model_kwargs) \n    decoder = self._check_model_availability(decoder, decoder_builtin_engine, decoder_engine, **decoder_model_kwargs) \n\n    return GenericUnetModelWrapper(\n        backbone, decoder=decoder, relu=task == \"regression\" and regression_relu, squeeze_single_class=task == \"regression\"\n    )\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.satmae_model_factory.SatMAEModelFactory","title":"<code>terratorch.models.satmae_model_factory.SatMAEModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/satmae_model_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass SatMAEModelFactory(ModelFactory):\n    def build_model(\n        self,\n        task: str,\n        backbone: str | nn.Module,\n        decoder: str | nn.Module,\n        in_channels: int,\n        bands: list[HLSBands | int],\n        num_classes: int | None = None,\n        pretrained: bool = True,  # noqa: FBT001, FBT002\n        num_frames: int = 1,\n        prepare_features_for_image_model: Callable | None = None,\n        aux_decoders: list[AuxiliaryHead] | None = None,\n        rescale: bool = True,  # noqa: FBT002, FBT001\n        checkpoint_path: str = None,\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"Model factory for SatMAE  models.\n\n        Further arguments to be passed to the backbone, decoder or head. They should be prefixed with\n        `backbone_`, `decoder_` and `head_` respectively.\n\n        Args:\n            task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\".\n            backbone (str, nn.Module): Backbone to be used. If string, should be able to be parsed\n                by the specified factory. Defaults to \"prithvi_100\".\n            decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model.\n                    If a string, it will be created from a class exposed in decoder.__init__.py with the same name.\n                    If an nn.Module, we expect it to expose a property `decoder.out_channels`.\n                    Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".\n            in_channels (int, optional): Number of input channels. Defaults to 3.\n            bands (list[terratorch.datasets.HLSBands], optional): Bands the model will be trained on.\n                    Should be a list of terratorch.datasets.HLSBands.\n                    Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE].\n            num_classes (int, optional): Number of classes. None for regression tasks.\n            pretrained (Union[bool, Path], optional): Whether to load pretrained weights for the backbone, if available.\n                Defaults to True.\n            num_frames (int, optional): Number of timesteps for the model to handle. Defaults to 1.\n            prepare_features_for_image_model (Callable | None): Function to be called on encoder features\n                before passing them to the decoder. Defaults to None, which applies the identity function.\n            aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead deciders to be added to the model.\n                These decoders take the input from the encoder as well.\n            rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n                is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.\n\n        Raises:\n            NotImplementedError: _description_\n            DecoderNotFoundException: _description_\n\n        Returns:\n            nn.Module: _description_\n        \"\"\"\n\n        self.possible_modules = None \n\n        if not torch.cuda.is_available():\n            self.CPU_ONLY = True\n        else:\n            self.CPU_ONLY = False\n\n        # Path for accessing the model source code.\n        self.syspath_kwarg = \"model_sys_path\"\n\n        bands = [HLSBands.try_convert_to_hls_bands_enum(b) for b in bands]\n\n        # TODO: support auxiliary heads\n        if not isinstance(backbone, nn.Module):\n            if not 'SatMAE' in kwargs[self.syspath_kwarg]:\n                msg = \"This class only handles models for `SatMAE` encoders\"\n                raise NotImplementedError(msg)\n\n\n            task = task.lower()\n            if task not in SUPPORTED_TASKS:\n                msg = f\"Task {task} not supported. Please choose one of {SUPPORTED_TASKS}\"\n                raise NotImplementedError(msg)\n\n            backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n            backbone_name = backbone\n\n            # Trying to find the model on HuggingFace.\n            try:\n                backbone: nn.Module = timm.create_model(\n                    backbone,\n                    pretrained=pretrained,\n                    in_chans=in_channels,\n                    num_frames=num_frames,\n                    bands=bands,\n                    features_only=True,\n                    **backbone_kwargs,\n                )\n            except Exception:\n\n                # When the model is not on HG, it needs be restored locally.\n                print(\"This model is not available on HuggingFace. Trying to instantiate locally ...\")\n\n                assert checkpoint_path, \"A checkpoint must be provided to restore the model.\"\n\n                # The SatMAE source code must be installed or available via PYTHONPATH.\n                try:  \n                    if self.syspath_kwarg in kwargs:\n                        syspath_value = kwargs.get(self.syspath_kwarg)\n\n                    else:\n\n                        Exception(f\"It is necessary to define the variable {self.syspath_kwarg} on yaml\"\n                                                           \"config for restoring local model.\")\n\n                    sys.path.insert(0, syspath_value)\n\n                    # There are dozens of classes in the SatMAE repo, but it seems to be the right open_generic_torch_model\n                    backbone_template = None\n\n                    self.possible_modules = [importlib.import_module(mod) for mod in [\"models_mae\", \"models_vit\"]]\n\n                    for backbone_module in self.possible_modules:\n\n                        backbone_template_ = getattr(backbone_module, backbone_name, None)\n                        if not backbone_template_ :\n                            pass\n                        else:\n                            backbone_template = backbone_template_\n\n                except ModuleNotFoundError:\n\n                    print(f\"It is better to review the field {self.syspath_kwarg} in the yaml file.\")\n\n                # Is it a ViT or a ViT-MAE ?\n                backbone_kind = check_the_kind_of_vit(name=backbone_name)\n\n                backbone: nn.Module = ModelWrapper(model=backbone_template(**backbone_kwargs), kind=backbone_kind)\n\n                if self.CPU_ONLY:\n                    model_dict = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n                else:\n                    model_dict = torch.load(checkpoint_path, weights_only=True)\n\n\n                # Filtering parameters from the model state_dict (when necessary)\n                model_dict = filter_cefficients_when_necessary(model_state_dict=model_dict, kind=backbone_kind)\n\n                if backbone_kind == \"vit\":\n                    backbone.model.fc_norm = nn.Identity()\n                    backbone.model.head_drop = nn.Identity()\n                    backbone.model.head = nn.Identity()\n                    backbone.model.pos_embed = None # TODO It needs be corrected from source\n\n                # Load saved model when it exists\n                if  pretrained: \n                    backbone.model.load_state_dict(model_dict['model'], strict=False)\n\n                # Print the general architecture\n                backbone.summary()\n\n                print(\"Model SatMAE was successfully restored.\")\n\n        # allow decoder to be a module passed directly\n        decoder_cls = _get_decoder(decoder)\n\n        decoder_kwargs, kwargs = extract_prefix_keys(kwargs, \"decoder_\")\n\n        # If backabone is a ViT-MAE, the attribute \"num_patches\" will be necessary\n        if hasattr(backbone, \"num_patches\"):\n            decoder_kwargs[\"num_patches\"] = backbone.num_patches\n\n        # TODO: remove this\n        if \"SatMAEHead\" in decoder:\n            decoder: nn.Module = decoder_cls(**decoder_kwargs)\n        else:\n            decoder: nn.Module = decoder_cls(backbone.channels(), **decoder_kwargs)\n\n        head_kwargs, kwargs = extract_prefix_keys(kwargs, \"head_\")\n        if num_classes:\n            head_kwargs[\"num_classes\"] = num_classes\n        if aux_decoders is None:\n            return _build_appropriate_model(\n                task, backbone, decoder, head_kwargs, prepare_features_for_image_model, rescale=rescale\n            )\n\n        to_be_aux_decoders: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] = []\n        for aux_decoder in aux_decoders:\n            args = aux_decoder.decoder_args if aux_decoder.decoder_args else {}\n            aux_decoder_cls: nn.Module = _get_decoder(aux_decoder.decoder)\n            aux_decoder_kwargs, kwargs = extract_prefix_keys(args, \"decoder_\")\n            aux_decoder_instance = aux_decoder_cls(backbone.feature_info.channels(), **aux_decoder_kwargs)\n            # aux_decoder_instance = aux_decoder_cls([128, 256, 512, 1024], **decoder_kwargs)\n\n            aux_head_kwargs, kwargs = extract_prefix_keys(args, \"head_\")\n            if num_classes:\n                aux_head_kwargs[\"num_classes\"] = num_classes\n            # aux_head: nn.Module = _get_head(task, aux_decoder_instance, num_classes=num_classes, **head_kwargs)\n            # aux_decoder.decoder = nn.Sequential(aux_decoder_instance, aux_head)\n            to_be_aux_decoders.append(\n                AuxiliaryHeadWithDecoderWithoutInstantiatedHead(aux_decoder.name, aux_decoder_instance, aux_head_kwargs)\n            )\n\n        return _build_appropriate_model(\n            task,\n            backbone,\n            decoder,\n            head_kwargs,\n            prepare_features_for_image_model,\n            rescale=rescale,\n            auxiliary_heads=to_be_aux_decoders,\n        )\n</code></pre>"},{"location":"package/model_factories/#terratorch.models.satmae_model_factory.SatMAEModelFactory.build_model","title":"<code>build_model(task, backbone, decoder, in_channels, bands, num_classes=None, pretrained=True, num_frames=1, prepare_features_for_image_model=None, aux_decoders=None, rescale=True, checkpoint_path=None, **kwargs)</code>","text":"<p>Model factory for SatMAE  models.</p> <p>Further arguments to be passed to the backbone, decoder or head. They should be prefixed with <code>backbone_</code>, <code>decoder_</code> and <code>head_</code> respectively.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Currently supports \"segmentation\" and \"regression\".</p> required <code>backbone</code> <code>(str, Module)</code> <p>Backbone to be used. If string, should be able to be parsed by the specified factory. Defaults to \"prithvi_100\".</p> required <code>decoder</code> <code>Union[str, Module]</code> <p>Decoder to be used for the segmentation model.     If a string, it will be created from a class exposed in decoder.init.py with the same name.     If an nn.Module, we expect it to expose a property <code>decoder.out_channels</code>.     Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> required <code>bands</code> <code>list[HLSBands]</code> <p>Bands the model will be trained on.     Should be a list of terratorch.datasets.HLSBands.     Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE].</p> required <code>num_classes</code> <code>int</code> <p>Number of classes. None for regression tasks.</p> <code>None</code> <code>pretrained</code> <code>Union[bool, Path]</code> <p>Whether to load pretrained weights for the backbone, if available. Defaults to True.</p> <code>True</code> <code>num_frames</code> <code>int</code> <p>Number of timesteps for the model to handle. Defaults to 1.</p> <code>1</code> <code>prepare_features_for_image_model</code> <code>Callable | None</code> <p>Function to be called on encoder features before passing them to the decoder. Defaults to None, which applies the identity function.</p> <code>None</code> <code>aux_decoders</code> <code>list[AuxiliaryHead] | None</code> <p>List of AuxiliaryHead deciders to be added to the model. These decoders take the input from the encoder as well.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> <code>DecoderNotFoundException</code> <p>description</p> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: description</p> Source code in <code>terratorch/models/satmae_model_factory.py</code> <pre><code>def build_model(\n    self,\n    task: str,\n    backbone: str | nn.Module,\n    decoder: str | nn.Module,\n    in_channels: int,\n    bands: list[HLSBands | int],\n    num_classes: int | None = None,\n    pretrained: bool = True,  # noqa: FBT001, FBT002\n    num_frames: int = 1,\n    prepare_features_for_image_model: Callable | None = None,\n    aux_decoders: list[AuxiliaryHead] | None = None,\n    rescale: bool = True,  # noqa: FBT002, FBT001\n    checkpoint_path: str = None,\n    **kwargs,\n) -&gt; Model:\n    \"\"\"Model factory for SatMAE  models.\n\n    Further arguments to be passed to the backbone, decoder or head. They should be prefixed with\n    `backbone_`, `decoder_` and `head_` respectively.\n\n    Args:\n        task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\".\n        backbone (str, nn.Module): Backbone to be used. If string, should be able to be parsed\n            by the specified factory. Defaults to \"prithvi_100\".\n        decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model.\n                If a string, it will be created from a class exposed in decoder.__init__.py with the same name.\n                If an nn.Module, we expect it to expose a property `decoder.out_channels`.\n                Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".\n        in_channels (int, optional): Number of input channels. Defaults to 3.\n        bands (list[terratorch.datasets.HLSBands], optional): Bands the model will be trained on.\n                Should be a list of terratorch.datasets.HLSBands.\n                Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE].\n        num_classes (int, optional): Number of classes. None for regression tasks.\n        pretrained (Union[bool, Path], optional): Whether to load pretrained weights for the backbone, if available.\n            Defaults to True.\n        num_frames (int, optional): Number of timesteps for the model to handle. Defaults to 1.\n        prepare_features_for_image_model (Callable | None): Function to be called on encoder features\n            before passing them to the decoder. Defaults to None, which applies the identity function.\n        aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead deciders to be added to the model.\n            These decoders take the input from the encoder as well.\n        rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n            is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.\n\n    Raises:\n        NotImplementedError: _description_\n        DecoderNotFoundException: _description_\n\n    Returns:\n        nn.Module: _description_\n    \"\"\"\n\n    self.possible_modules = None \n\n    if not torch.cuda.is_available():\n        self.CPU_ONLY = True\n    else:\n        self.CPU_ONLY = False\n\n    # Path for accessing the model source code.\n    self.syspath_kwarg = \"model_sys_path\"\n\n    bands = [HLSBands.try_convert_to_hls_bands_enum(b) for b in bands]\n\n    # TODO: support auxiliary heads\n    if not isinstance(backbone, nn.Module):\n        if not 'SatMAE' in kwargs[self.syspath_kwarg]:\n            msg = \"This class only handles models for `SatMAE` encoders\"\n            raise NotImplementedError(msg)\n\n\n        task = task.lower()\n        if task not in SUPPORTED_TASKS:\n            msg = f\"Task {task} not supported. Please choose one of {SUPPORTED_TASKS}\"\n            raise NotImplementedError(msg)\n\n        backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n        backbone_name = backbone\n\n        # Trying to find the model on HuggingFace.\n        try:\n            backbone: nn.Module = timm.create_model(\n                backbone,\n                pretrained=pretrained,\n                in_chans=in_channels,\n                num_frames=num_frames,\n                bands=bands,\n                features_only=True,\n                **backbone_kwargs,\n            )\n        except Exception:\n\n            # When the model is not on HG, it needs be restored locally.\n            print(\"This model is not available on HuggingFace. Trying to instantiate locally ...\")\n\n            assert checkpoint_path, \"A checkpoint must be provided to restore the model.\"\n\n            # The SatMAE source code must be installed or available via PYTHONPATH.\n            try:  \n                if self.syspath_kwarg in kwargs:\n                    syspath_value = kwargs.get(self.syspath_kwarg)\n\n                else:\n\n                    Exception(f\"It is necessary to define the variable {self.syspath_kwarg} on yaml\"\n                                                       \"config for restoring local model.\")\n\n                sys.path.insert(0, syspath_value)\n\n                # There are dozens of classes in the SatMAE repo, but it seems to be the right open_generic_torch_model\n                backbone_template = None\n\n                self.possible_modules = [importlib.import_module(mod) for mod in [\"models_mae\", \"models_vit\"]]\n\n                for backbone_module in self.possible_modules:\n\n                    backbone_template_ = getattr(backbone_module, backbone_name, None)\n                    if not backbone_template_ :\n                        pass\n                    else:\n                        backbone_template = backbone_template_\n\n            except ModuleNotFoundError:\n\n                print(f\"It is better to review the field {self.syspath_kwarg} in the yaml file.\")\n\n            # Is it a ViT or a ViT-MAE ?\n            backbone_kind = check_the_kind_of_vit(name=backbone_name)\n\n            backbone: nn.Module = ModelWrapper(model=backbone_template(**backbone_kwargs), kind=backbone_kind)\n\n            if self.CPU_ONLY:\n                model_dict = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n            else:\n                model_dict = torch.load(checkpoint_path, weights_only=True)\n\n\n            # Filtering parameters from the model state_dict (when necessary)\n            model_dict = filter_cefficients_when_necessary(model_state_dict=model_dict, kind=backbone_kind)\n\n            if backbone_kind == \"vit\":\n                backbone.model.fc_norm = nn.Identity()\n                backbone.model.head_drop = nn.Identity()\n                backbone.model.head = nn.Identity()\n                backbone.model.pos_embed = None # TODO It needs be corrected from source\n\n            # Load saved model when it exists\n            if  pretrained: \n                backbone.model.load_state_dict(model_dict['model'], strict=False)\n\n            # Print the general architecture\n            backbone.summary()\n\n            print(\"Model SatMAE was successfully restored.\")\n\n    # allow decoder to be a module passed directly\n    decoder_cls = _get_decoder(decoder)\n\n    decoder_kwargs, kwargs = extract_prefix_keys(kwargs, \"decoder_\")\n\n    # If backabone is a ViT-MAE, the attribute \"num_patches\" will be necessary\n    if hasattr(backbone, \"num_patches\"):\n        decoder_kwargs[\"num_patches\"] = backbone.num_patches\n\n    # TODO: remove this\n    if \"SatMAEHead\" in decoder:\n        decoder: nn.Module = decoder_cls(**decoder_kwargs)\n    else:\n        decoder: nn.Module = decoder_cls(backbone.channels(), **decoder_kwargs)\n\n    head_kwargs, kwargs = extract_prefix_keys(kwargs, \"head_\")\n    if num_classes:\n        head_kwargs[\"num_classes\"] = num_classes\n    if aux_decoders is None:\n        return _build_appropriate_model(\n            task, backbone, decoder, head_kwargs, prepare_features_for_image_model, rescale=rescale\n        )\n\n    to_be_aux_decoders: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] = []\n    for aux_decoder in aux_decoders:\n        args = aux_decoder.decoder_args if aux_decoder.decoder_args else {}\n        aux_decoder_cls: nn.Module = _get_decoder(aux_decoder.decoder)\n        aux_decoder_kwargs, kwargs = extract_prefix_keys(args, \"decoder_\")\n        aux_decoder_instance = aux_decoder_cls(backbone.feature_info.channels(), **aux_decoder_kwargs)\n        # aux_decoder_instance = aux_decoder_cls([128, 256, 512, 1024], **decoder_kwargs)\n\n        aux_head_kwargs, kwargs = extract_prefix_keys(args, \"head_\")\n        if num_classes:\n            aux_head_kwargs[\"num_classes\"] = num_classes\n        # aux_head: nn.Module = _get_head(task, aux_decoder_instance, num_classes=num_classes, **head_kwargs)\n        # aux_decoder.decoder = nn.Sequential(aux_decoder_instance, aux_head)\n        to_be_aux_decoders.append(\n            AuxiliaryHeadWithDecoderWithoutInstantiatedHead(aux_decoder.name, aux_decoder_instance, aux_head_kwargs)\n        )\n\n    return _build_appropriate_model(\n        task,\n        backbone,\n        decoder,\n        head_kwargs,\n        prepare_features_for_image_model,\n        rescale=rescale,\n        auxiliary_heads=to_be_aux_decoders,\n    )\n</code></pre>"},{"location":"package/necks/","title":"Necks","text":"<p>Necks reshape the output of an encoder into a format suitable for the decoder. By combining different necks, you can combine any backbone with any decoder.</p>"},{"location":"package/necks/#terratorch.models.necks.Neck","title":"<code>terratorch.models.necks.Neck</code>","text":"<p>               Bases: <code>ABC</code>, <code>Module</code></p> <p>Base class for Neck</p> <p>A neck must must implement <code>self.process_channel_list</code> which returns the new channel list.</p> Source code in <code>terratorch/models/necks.py</code> <pre><code>class Neck(ABC, nn.Module):\n    \"\"\"Base class for Neck\n\n    A neck must must implement `self.process_channel_list` which returns the new channel list.\n    \"\"\"\n\n    def __init__(self, channel_list: list[int]) -&gt; None:\n        super().__init__()\n        self.channel_list = channel_list\n\n    @abstractmethod\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        return channel_list\n\n    @abstractmethod\n    def forward(self, channel_list: list[torch.Tensor]) -&gt; list[torch.Tensor]: ...\n</code></pre>"},{"location":"package/necks/#terratorch.models.necks.SelectIndices","title":"<code>terratorch.models.necks.SelectIndices</code>","text":"<p>               Bases: <code>Neck</code></p> Source code in <code>terratorch/models/necks.py</code> <pre><code>@TERRATORCH_NECK_REGISTRY.register\nclass SelectIndices(Neck):\n    def __init__(self, channel_list: list[int], indices: list[int]):\n        \"\"\"Select indices from the embedding list\n\n        Args:\n            indices (list[int]): list of indices to select.\n        \"\"\"\n        super().__init__(channel_list)\n        self.indices = indices\n\n    def forward(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        features = [features[i] for i in self.indices]\n        return features\n\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        channel_list = [channel_list[i] for i in self.indices]\n        return channel_list\n</code></pre>"},{"location":"package/necks/#terratorch.models.necks.SelectIndices.__init__","title":"<code>__init__(channel_list, indices)</code>","text":"<p>Select indices from the embedding list</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>list[int]</code> <p>list of indices to select.</p> required Source code in <code>terratorch/models/necks.py</code> <pre><code>def __init__(self, channel_list: list[int], indices: list[int]):\n    \"\"\"Select indices from the embedding list\n\n    Args:\n        indices (list[int]): list of indices to select.\n    \"\"\"\n    super().__init__(channel_list)\n    self.indices = indices\n</code></pre>"},{"location":"package/necks/#terratorch.models.necks.ReshapeTokensToImage","title":"<code>terratorch.models.necks.ReshapeTokensToImage</code>","text":"<p>               Bases: <code>Neck</code></p> Source code in <code>terratorch/models/necks.py</code> <pre><code>@TERRATORCH_NECK_REGISTRY.register\nclass ReshapeTokensToImage(Neck):\n    def __init__(self, channel_list: list[int], remove_cls_token=True, effective_time_dim: int = 1):  # noqa: FBT002\n        \"\"\"Reshape output of transformer encoder so it can be passed to a conv net.\n\n        Args:\n            remove_cls_token (bool, optional): Whether to remove the cls token from the first position.\n                Defaults to True.\n            effective_time_dim (int, optional): The effective temporal dimension the transformer processes.\n                For a ViT, his will be given by `num_frames // tubelet size`. This is used to determine\n                the temporal dimension of the embedding, which is concatenated with the embedding dimension.\n                For example:\n                - A model which processes 1 frame with a tubelet size of 1 has an effective_time_dim of 1.\n                    The embedding produced by this model has embedding size embed_dim * 1.\n                - A model which processes 3 frames with a tubelet size of 1 has an effective_time_dim of 3.\n                    The embedding produced by this model has embedding size embed_dim * 3.\n                - A model which processes 12 frames with a tubelet size of 4 has an effective_time_dim of 3.\n                    The embedding produced by this model has an embedding size embed_dim * 3.\n                Defaults to 1.\n        \"\"\"\n        super().__init__(channel_list)\n        self.remove_cls_token = remove_cls_token\n        self.effective_time_dim = effective_time_dim\n\n    def collapse_dims(self, x):\n        \"\"\"\n        When the encoder output has more than 3 dimensions, is necessary to \n        reshape it. \n        \"\"\"\n        shape = x.shape\n        batch = x.shape[0]\n        e = x.shape[-1]\n        collapsed_dim = np.prod(x.shape[1:-1])\n\n        return x.reshape(batch, collapsed_dim, e)\n\n    def forward(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        out = []\n        for x in features:\n            if self.remove_cls_token:\n                x_no_token = x[:, 1:, :]\n            else:\n                x_no_token = x\n            x_no_token = self.collapse_dims(x_no_token)\n            number_of_tokens = x_no_token.shape[1]\n            tokens_per_timestep = number_of_tokens // self.effective_time_dim\n            h = int(math.sqrt(tokens_per_timestep))\n\n            encoded = rearrange(\n                x_no_token,\n                \"batch (t h w) e -&gt; batch (t e) h w\",\n                batch=x_no_token.shape[0],\n                t=self.effective_time_dim,\n                h=h,\n            )\n            out.append(encoded)\n        return out\n\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        return super().process_channel_list(channel_list)\n</code></pre>"},{"location":"package/necks/#terratorch.models.necks.ReshapeTokensToImage.__init__","title":"<code>__init__(channel_list, remove_cls_token=True, effective_time_dim=1)</code>","text":"<p>Reshape output of transformer encoder so it can be passed to a conv net.</p> <p>Parameters:</p> Name Type Description Default <code>remove_cls_token</code> <code>bool</code> <p>Whether to remove the cls token from the first position. Defaults to True.</p> <code>True</code> <code>effective_time_dim</code> <code>int</code> <p>The effective temporal dimension the transformer processes. For a ViT, his will be given by <code>num_frames // tubelet size</code>. This is used to determine the temporal dimension of the embedding, which is concatenated with the embedding dimension. For example: - A model which processes 1 frame with a tubelet size of 1 has an effective_time_dim of 1.     The embedding produced by this model has embedding size embed_dim * 1. - A model which processes 3 frames with a tubelet size of 1 has an effective_time_dim of 3.     The embedding produced by this model has embedding size embed_dim * 3. - A model which processes 12 frames with a tubelet size of 4 has an effective_time_dim of 3.     The embedding produced by this model has an embedding size embed_dim * 3. Defaults to 1.</p> <code>1</code> Source code in <code>terratorch/models/necks.py</code> <pre><code>def __init__(self, channel_list: list[int], remove_cls_token=True, effective_time_dim: int = 1):  # noqa: FBT002\n    \"\"\"Reshape output of transformer encoder so it can be passed to a conv net.\n\n    Args:\n        remove_cls_token (bool, optional): Whether to remove the cls token from the first position.\n            Defaults to True.\n        effective_time_dim (int, optional): The effective temporal dimension the transformer processes.\n            For a ViT, his will be given by `num_frames // tubelet size`. This is used to determine\n            the temporal dimension of the embedding, which is concatenated with the embedding dimension.\n            For example:\n            - A model which processes 1 frame with a tubelet size of 1 has an effective_time_dim of 1.\n                The embedding produced by this model has embedding size embed_dim * 1.\n            - A model which processes 3 frames with a tubelet size of 1 has an effective_time_dim of 3.\n                The embedding produced by this model has embedding size embed_dim * 3.\n            - A model which processes 12 frames with a tubelet size of 4 has an effective_time_dim of 3.\n                The embedding produced by this model has an embedding size embed_dim * 3.\n            Defaults to 1.\n    \"\"\"\n    super().__init__(channel_list)\n    self.remove_cls_token = remove_cls_token\n    self.effective_time_dim = effective_time_dim\n</code></pre>"},{"location":"package/necks/#terratorch.models.necks.ReshapeTokensToImage.collapse_dims","title":"<code>collapse_dims(x)</code>","text":"<p>When the encoder output has more than 3 dimensions, is necessary to  reshape it.</p> Source code in <code>terratorch/models/necks.py</code> <pre><code>def collapse_dims(self, x):\n    \"\"\"\n    When the encoder output has more than 3 dimensions, is necessary to \n    reshape it. \n    \"\"\"\n    shape = x.shape\n    batch = x.shape[0]\n    e = x.shape[-1]\n    collapsed_dim = np.prod(x.shape[1:-1])\n\n    return x.reshape(batch, collapsed_dim, e)\n</code></pre>"},{"location":"package/necks/#terratorch.models.necks.InterpolateToPyramidal","title":"<code>terratorch.models.necks.InterpolateToPyramidal</code>","text":"<p>               Bases: <code>Neck</code></p> Source code in <code>terratorch/models/necks.py</code> <pre><code>@TERRATORCH_NECK_REGISTRY.register\nclass InterpolateToPyramidal(Neck):\n    def __init__(self, channel_list: list[int], scale_factor: int = 2, mode: str = \"nearest\"):\n        \"\"\"Spatially interpolate embeddings so that embedding[i - 1] is scale_factor times larger than embedding[i]\n\n        Useful to make non-pyramidal backbones compatible with hierarachical ones\n        Args:\n            scale_factor (int): Amount to scale embeddings by each layer. Defaults to 2.\n            mode (str): Interpolation mode to be passed to torch.nn.functional.interpolate. Defaults to 'nearest'.\n        \"\"\"\n        super().__init__(channel_list)\n        self.scale_factor = scale_factor\n        self.mode = mode\n\n    def forward(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        out = []\n        scale_exponents = list(range(len(features), 0, -1))\n        for x, exponent in zip(features, scale_exponents, strict=True):\n            out.append(F.interpolate(x, scale_factor=self.scale_factor**exponent, mode=self.mode))\n\n        return out\n\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        return super().process_channel_list(channel_list)\n</code></pre>"},{"location":"package/necks/#terratorch.models.necks.InterpolateToPyramidal.__init__","title":"<code>__init__(channel_list, scale_factor=2, mode='nearest')</code>","text":"<p>Spatially interpolate embeddings so that embedding[i - 1] is scale_factor times larger than embedding[i]</p> <p>Useful to make non-pyramidal backbones compatible with hierarachical ones Args:     scale_factor (int): Amount to scale embeddings by each layer. Defaults to 2.     mode (str): Interpolation mode to be passed to torch.nn.functional.interpolate. Defaults to 'nearest'.</p> Source code in <code>terratorch/models/necks.py</code> <pre><code>def __init__(self, channel_list: list[int], scale_factor: int = 2, mode: str = \"nearest\"):\n    \"\"\"Spatially interpolate embeddings so that embedding[i - 1] is scale_factor times larger than embedding[i]\n\n    Useful to make non-pyramidal backbones compatible with hierarachical ones\n    Args:\n        scale_factor (int): Amount to scale embeddings by each layer. Defaults to 2.\n        mode (str): Interpolation mode to be passed to torch.nn.functional.interpolate. Defaults to 'nearest'.\n    \"\"\"\n    super().__init__(channel_list)\n    self.scale_factor = scale_factor\n    self.mode = mode\n</code></pre>"},{"location":"package/necks/#terratorch.models.necks.LearnedInterpolateToPyramidal","title":"<code>terratorch.models.necks.LearnedInterpolateToPyramidal</code>","text":"<p>               Bases: <code>Neck</code></p> <p>Use learned convolutions to transform the output of a non-pyramidal encoder into pyramidal ones</p> <p>Always requires exactly 4 embeddings</p> Source code in <code>terratorch/models/necks.py</code> <pre><code>@TERRATORCH_NECK_REGISTRY.register\nclass LearnedInterpolateToPyramidal(Neck):\n    \"\"\"Use learned convolutions to transform the output of a non-pyramidal encoder into pyramidal ones\n\n    Always requires exactly 4 embeddings\n    \"\"\"\n\n    def __init__(self, channel_list: list[int]):\n        super().__init__(channel_list)\n        if len(channel_list) != 4:\n            msg = \"This class can only handle exactly 4 input embeddings\"\n            raise Exception(msg)\n        self.fpn1 = nn.Sequential(\n            nn.ConvTranspose2d(channel_list[0], channel_list[0] // 2, 2, 2),\n            nn.BatchNorm2d(channel_list[0] // 2),\n            nn.GELU(),\n            nn.ConvTranspose2d(channel_list[0] // 2, channel_list[0] // 4, 2, 2),\n        )\n        self.fpn2 = nn.Sequential(nn.ConvTranspose2d(channel_list[1], channel_list[1] // 2, 2, 2))\n        self.fpn3 = nn.Sequential(nn.Identity())\n        self.fpn4 = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2))\n        self.embedding_dim = [channel_list[0] // 4, channel_list[1] // 2, channel_list[2], channel_list[3]]\n\n    def forward(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        scaled_inputs = []\n        scaled_inputs.append(self.fpn1(features[0]))\n        scaled_inputs.append(self.fpn2(features[1]))\n        scaled_inputs.append(self.fpn3(features[2]))\n        scaled_inputs.append(self.fpn4(features[3]))\n        return scaled_inputs\n\n    def process_channel_list(self, channel_list: list[int]=None) -&gt; list[int]:\n        return [channel_list[0] // 4, channel_list[1] // 2, channel_list[2], channel_list[3]]\n</code></pre>"},{"location":"package/necks/#terratorch.models.necks.PermuteDims","title":"<code>terratorch.models.necks.PermuteDims</code>","text":"<p>               Bases: <code>Neck</code></p> Source code in <code>terratorch/models/necks.py</code> <pre><code>@TERRATORCH_NECK_REGISTRY.register\nclass PermuteDims(Neck):\n    def __init__(self, channel_list: list[int], new_order: list[int]):\n        \"\"\"Permute dimensions of each element in the embedding list\n\n        Args:\n            new_order (list[int]): list of indices to be passed to tensor.permute()\n        \"\"\"\n        super().__init__(channel_list)\n        self.new_order = new_order\n\n    def forward(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        features = [feat.permute(*self.new_order).contiguous() for feat in features]\n        return features\n\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        return super().process_channel_list(channel_list)\n</code></pre>"},{"location":"package/necks/#terratorch.models.necks.PermuteDims.__init__","title":"<code>__init__(channel_list, new_order)</code>","text":"<p>Permute dimensions of each element in the embedding list</p> <p>Parameters:</p> Name Type Description Default <code>new_order</code> <code>list[int]</code> <p>list of indices to be passed to tensor.permute()</p> required Source code in <code>terratorch/models/necks.py</code> <pre><code>def __init__(self, channel_list: list[int], new_order: list[int]):\n    \"\"\"Permute dimensions of each element in the embedding list\n\n    Args:\n        new_order (list[int]): list of indices to be passed to tensor.permute()\n    \"\"\"\n    super().__init__(channel_list)\n    self.new_order = new_order\n</code></pre>"},{"location":"package/necks/#terratorch.models.necks.MaxpoolToPyramidal","title":"<code>terratorch.models.necks.MaxpoolToPyramidal</code>","text":"<p>               Bases: <code>Neck</code></p> Source code in <code>terratorch/models/necks.py</code> <pre><code>@TERRATORCH_NECK_REGISTRY.register\nclass MaxpoolToPyramidal(Neck):\n    def __init__(self, channel_list: list[int], kernel_size: int = 2):\n        \"\"\"Spatially downsample embeddings so that embedding[i - 1] is scale_factor times smaller than embedding[i]\n\n        Useful to make non-pyramidal backbones compatible with hierarachical ones\n        Args:\n            kernel_size (int). Base kernel size to use for maxpool. Defaults to 2.\n        \"\"\"\n        super().__init__(channel_list)\n        self.kernel_size = kernel_size\n\n    def forward(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        out = []\n        scale_exponents = list(range(len(features)))\n        for x, exponent in zip(features, scale_exponents, strict=True):\n            if exponent == 0:\n                out.append(x.clone())\n            else:\n                out.append(F.max_pool2d(x, kernel_size=self.kernel_size**exponent))\n\n        return out\n\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        return super().process_channel_list(channel_list)\n</code></pre>"},{"location":"package/necks/#terratorch.models.necks.MaxpoolToPyramidal.__init__","title":"<code>__init__(channel_list, kernel_size=2)</code>","text":"<p>Spatially downsample embeddings so that embedding[i - 1] is scale_factor times smaller than embedding[i]</p> <p>Useful to make non-pyramidal backbones compatible with hierarachical ones Args:     kernel_size (int). Base kernel size to use for maxpool. Defaults to 2.</p> Source code in <code>terratorch/models/necks.py</code> <pre><code>def __init__(self, channel_list: list[int], kernel_size: int = 2):\n    \"\"\"Spatially downsample embeddings so that embedding[i - 1] is scale_factor times smaller than embedding[i]\n\n    Useful to make non-pyramidal backbones compatible with hierarachical ones\n    Args:\n        kernel_size (int). Base kernel size to use for maxpool. Defaults to 2.\n    \"\"\"\n    super().__init__(channel_list)\n    self.kernel_size = kernel_size\n</code></pre>"},{"location":"package/necks/#terratorch.models.necks.AddBottleneckLayer","title":"<code>terratorch.models.necks.AddBottleneckLayer</code>","text":"<p>               Bases: <code>Neck</code></p> <p>Add a layer that reduces the channel dimension of the final embedding by half, and concatenates it</p> <p>Useful for compatibility with some smp decoders.</p> Source code in <code>terratorch/models/necks.py</code> <pre><code>@TERRATORCH_NECK_REGISTRY.register\nclass AddBottleneckLayer(Neck):\n    \"\"\"Add a layer that reduces the channel dimension of the final embedding by half, and concatenates it\n\n    Useful for compatibility with some smp decoders.\n    \"\"\"\n\n    def __init__(self, channel_list: list[int]):\n        super().__init__(channel_list)\n        self.bottleneck = nn.Conv2d(channel_list[-1], channel_list[-1]//2, kernel_size=1)\n\n    def forward(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        new_embedding = self.bottleneck(features[-1])\n        features.append(new_embedding)\n        return features\n\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        return [*channel_list, channel_list[-1] // 2]\n</code></pre>"},{"location":"package/tasks/","title":"Tasks","text":"<p>Tasks provide a convenient abstraction over the training of a model for a specific downstream task.  They encapsulate the model, optimizer, metrics, loss as well as training, validation and testing steps. The task expects to be passed a <code>model_factory</code>, to which the <code>model_args</code> arguments are passed to instantiate the model that will be trained. The models produced by this model factory should output <code>ModelOutput</code> instances and conform to the Model ABC. Tasks are best leveraged using config files, where they are specified in the <code>model</code> section under <code>class_path</code>. You can check out some examples of config files here. Below are the details of the tasks currently implemented in TerraTorch (Pixelwise Regression, Semantic Segmentation and Classification). </p>"},{"location":"package/tasks/#terratorch.tasks.SemanticSegmentationTask","title":"<code>terratorch.tasks.SemanticSegmentationTask</code>","text":"<p>               Bases: <code>TerraTorchTask</code></p> <p>Semantic Segmentation Task that accepts models from a range of sources.</p> <p>This class is analog in functionality to class SemanticSegmentationTask defined by torchgeo. However, it has some important differences:     - Accepts the specification of a model factory     - Logs metrics per class     - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)     - Allows the setting of optimizers in the constructor     - Allows to evaluate on multiple test dataloaders</p> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>class SemanticSegmentationTask(TerraTorchTask):\n    \"\"\"Semantic Segmentation Task that accepts models from a range of sources.\n\n    This class is analog in functionality to class SemanticSegmentationTask defined by torchgeo.\n    However, it has some important differences:\n        - Accepts the specification of a model factory\n        - Logs metrics per class\n        - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)\n        - Allows the setting of optimizers in the constructor\n        - Allows to evaluate on multiple test dataloaders\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: dict,\n        model_factory: str | None = None,\n        model: torch.nn.Module | None = None,\n        loss: str = \"ce\",\n        aux_heads: list[AuxiliaryHead] | None = None,\n        aux_loss: dict[str, float] | None = None,\n        class_weights: list[float] | None = None,\n        ignore_index: int | None = None,\n        lr: float = 0.001,\n        # the following are optional so CLI doesnt need to pass them\n        optimizer: str | None = None,\n        optimizer_hparams: dict | None = None,\n        scheduler: str | None = None,\n        scheduler_hparams: dict | None = None,\n        #\n        freeze_backbone: bool = False,  # noqa: FBT001, FBT002\n        freeze_decoder: bool = False,  # noqa: FBT002, FBT001\n        freeze_head: bool = False, \n        plot_on_val: bool | int = 10,\n        class_names: list[str] | None = None,\n        tiled_inference_parameters: TiledInferenceParameters = None,\n        test_dataloaders_names: list[str] | None = None,\n        lr_overrides: dict[str, float] | None = None,\n        output_on_inference: str | list[str] = \"prediction\",\n        output_most_probable: bool = True,\n        path_to_record_metrics: str = None,\n        tiled_inference_on_testing: bool = False,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            Defaults to None.\n            model_args (Dict): Arguments passed to the model factory.\n            model_factory (str, optional): ModelFactory class to be used to instantiate the model.\n                Is ignored when model is provided.\n            model (torch.nn.Module, optional): Custom model.\n            loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss.\n                Defaults to \"ce\".\n            aux_loss (dict[str, float] | None, optional): Auxiliary loss weights.\n                Should be a dictionary where the key is the name given to the loss\n                and the value is the weight to be applied to that loss.\n                The name of the loss should match the key in the dictionary output by the model's forward\n                method containing that output. Defaults to None.\n            class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss.\n            class_weights (list[float] | None, optional): List of class weights to be applied to the loss.\n                Defaults to None.\n            ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None.\n            lr (float, optional): Learning rate to be used. Defaults to 0.001.\n            optimizer (str | None, optional): Name of optimizer class from torch.optim to be used.\n            If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.\n            optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer.\n                Overriden by config / cli specification through LightningCLI.\n            scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler\n                to be used (e.g. ReduceLROnPlateau). Defaults to None.\n                Overriden by config / cli specification through LightningCLI.\n            scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler.\n                Overriden by config / cli specification through LightningCLI.\n            freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False.\n            freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False.\n            freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False.\n            plot_on_val (bool | int, optional): Whether to plot visualizations on validation.\n            If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs.\n            class_names (list[str] | None, optional): List of class names passed to metrics for better naming.\n                Defaults to numeric ordering.\n            tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters\n                used to determine if inference is done on the whole image or through tiling.\n            test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when\n                multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None,\n                which assumes only one test dataloader is used.\n            lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific\n                parameters. The key should be a substring of the parameter names (it will check the substring is\n                contained in the parameter name) and the value should be the new lr. Defaults to None.\n            output_on_inference (str | list[str]): A string or a list defining the kind of output to be saved to file during the inference, for example,\n                it can be \"prediction\", to save just the most probable class, or [\"prediction\", \"probabilities\"] to save both prediction and probabilities.\n            output_most_probable (bool): A boolean to define if the prediction step will output just the most probable logit or all of them.\n                This argument has been deprecated and will be replaced with `output_on_inference`. \n            tiled_inference_on_testing (bool): A boolean to define if tiled inference will be used when full inference \n                fails during the test step. \n            path_to_record_metrics (str): A path to save the file containing the metrics log. \n        \"\"\"\n\n        self.tiled_inference_parameters = tiled_inference_parameters\n        self.aux_loss = aux_loss\n        self.aux_heads = aux_heads\n\n        if model is not None and model_factory is not None:\n            logger.warning(\"A model_factory and a model was provided. The model_factory is ignored.\")\n        if model is None and model_factory is None:\n            raise ValueError(\"A model_factory or a model (torch.nn.Module) must be provided.\")\n\n        if model_factory and model is None:\n            self.model_factory = MODEL_FACTORY_REGISTRY.build(model_factory)\n\n        super().__init__(task=\"segmentation\", tiled_inference_on_testing=tiled_inference_on_testing,\n                         path_to_record_metrics=path_to_record_metrics)\n\n        if model is not None:\n            # Custom model\n            self.model = model\n\n        self.train_loss_handler = LossHandler(self.train_metrics.prefix)\n        self.test_loss_handler: list[LossHandler] = []\n        for metrics in self.test_metrics:\n            self.test_loss_handler.append(LossHandler(metrics.prefix))\n        self.val_loss_handler = LossHandler(self.val_metrics.prefix)\n        self.monitor = f\"{self.val_metrics.prefix}loss\"\n        self.plot_on_val = int(plot_on_val)\n        self.output_on_inference = output_on_inference\n\n        # When the user decides to use `output_most_probable` as `False` in\n        # order to output the probabilities instead of the prediction.\n        if not output_most_probable:\n            warnings.warn(\"The argument `output_most_probable` is deprecated and will be replaced with `output_on_inference='probabilities'`.\", stacklevel=1)\n            output_on_inference = \"probabilities\"\n\n        # Processing the `output_on_inference` argument.\n        self.output_prediction = lambda y: (y.argmax(dim=1), \"pred\")\n        self.output_logits = lambda y: (y, \"logits\")\n        self.output_probabilities = lambda y: (torch.nn.Softmax()(y), \"probabilities\")\n\n        # The possible methods to define outputs.\n        self.operation_map = {\n                              \"prediction\": self.output_prediction, \n                              \"logits\": self.output_logits, \n                              \"probabilities\": self.output_probabilities\n                              }\n\n        # `output_on_inference` can be a list or a string.\n        if isinstance(output_on_inference, list):\n            list_of_selectors = ()\n            for var in output_on_inference:\n                if var in self.operation_map:\n                    list_of_selectors += (self.operation_map[var],)\n                else:\n                    raise ValueError(f\"Option {var} is not supported. It must be in ['prediction', 'logits', 'probabilities']\")\n\n            if not len(list_of_selectors):\n                raise ValueError(\"The list of selectors for the output is empty, please, provide a valid value for `output_on_inference`\")\n\n            self.select_classes = lambda y: [op(y) for op in\n                                                   list_of_selectors]\n        elif isinstance(output_on_inference, str):\n            self.select_classes = self.operation_map[output_on_inference]\n\n        else:\n            raise ValueError(f\"The value {output_on_inference} isn't supported for `output_on_inference`.\")\n\n    def configure_losses(self) -&gt; None:\n        \"\"\"Initialize the loss criterion.\n\n        Raises:\n            ValueError: If *loss* is invalid.\n        \"\"\"\n        loss: str = self.hparams[\"loss\"]\n        ignore_index = self.hparams[\"ignore_index\"]\n\n        class_weights = (\n            torch.Tensor(self.hparams[\"class_weights\"]) if self.hparams[\"class_weights\"] is not None else None\n        )\n        if loss == \"ce\":\n            ignore_value = -100 if ignore_index is None else ignore_index\n            self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_value, weight=class_weights)\n        elif loss == \"jaccard\":\n            if ignore_index is not None:\n                exception_message = (\n                    f\"Jaccard loss does not support ignore_index, but found non-None value of {ignore_index}.\"\n                )\n                raise RuntimeError(exception_message)\n            self.criterion = smp.losses.JaccardLoss(mode=\"multiclass\")\n        elif loss == \"focal\":\n            self.criterion = smp.losses.FocalLoss(\"multiclass\", ignore_index=ignore_index, normalized=True)\n        elif loss == \"dice\":\n            self.criterion = smp.losses.DiceLoss(\"multiclass\", ignore_index=ignore_index)\n        else:\n            exception_message = (\n                f\"Loss type '{loss}' is not valid. Currently, supports 'ce', 'jaccard', 'dice' or 'focal' loss.\"\n            )\n            raise ValueError(exception_message)\n\n    def configure_metrics(self) -&gt; None:\n        \"\"\"Initialize the performance metrics.\"\"\"\n        num_classes: int = self.hparams[\"model_args\"][\"num_classes\"]\n        ignore_index: int = self.hparams[\"ignore_index\"]\n        class_names = self.hparams[\"class_names\"]\n        metrics = MetricCollection(\n            {\n                \"Multiclass_Accuracy\": MulticlassAccuracy(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                    multidim_average=\"global\",\n                    average=\"micro\",\n                ),\n                \"Multiclass_Accuracy_Class\": ClasswiseWrapper(\n                    MulticlassAccuracy(\n                        num_classes=num_classes,\n                        ignore_index=ignore_index,\n                        multidim_average=\"global\",\n                        average=None,\n                    ),\n                    labels=class_names,\n                ),\n                \"Multiclass_Jaccard_Index_Micro\": MulticlassJaccardIndex(\n                    num_classes=num_classes, ignore_index=ignore_index, average=\"micro\"\n                ),\n                \"Multiclass_Jaccard_Index\": MulticlassJaccardIndex(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                ),\n                \"Multiclass_Jaccard_Index_Class\": ClasswiseWrapper(\n                    MulticlassJaccardIndex(num_classes=num_classes, ignore_index=ignore_index, average=None),\n                    labels=class_names,\n                ),\n                \"Multiclass_F1_Score\": MulticlassF1Score(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                    multidim_average=\"global\",\n                    average=\"micro\",\n                ),\n            }\n        )\n        self.train_metrics = metrics.clone(prefix=\"train/\")\n        self.val_metrics = metrics.clone(prefix=\"val/\")\n        if self.hparams[\"test_dataloaders_names\"] is not None:\n            self.test_metrics = nn.ModuleList(\n                [metrics.clone(prefix=f\"test/{dl_name}/\") for dl_name in self.hparams[\"test_dataloaders_names\"]]\n            )\n        else:\n            self.test_metrics = nn.ModuleList([metrics.clone(prefix=\"test/\")])\n\n    def training_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n        \"\"\"Compute the train loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        # Testing because of failures.\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        loss = self.train_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.train_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n        y_hat_hard = to_segmentation_prediction(model_output)\n        self.train_metrics.update(y_hat_hard, y)\n\n        return loss[\"loss\"]\n\n    def test_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        \"\"\"Compute the test loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n\n        rest = {k: batch[k] for k in other_keys}\n\n        model_output = self.handle_full_or_tiled_inference(x, self.hparams[\"model_args\"][\"num_classes\"], **rest)\n\n        if dataloader_idx &gt;= len(self.test_loss_handler):\n            msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\"\n            raise ValueError(msg)\n        loss = self.test_loss_handler[dataloader_idx].compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.test_loss_handler[dataloader_idx].log_loss(\n            partial(self.log, add_dataloader_idx=False),  # We don't need the dataloader idx as prefixes are different\n            loss_dict=loss,\n            batch_size=y.shape[0],\n        )\n        y_hat_hard = to_segmentation_prediction(model_output)\n        self.test_metrics[dataloader_idx].update(y_hat_hard, y)\n\n        self.record_metrics(dataloader_idx, y_hat_hard, y)\n\n    def validation_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        \"\"\"Compute the validation loss and additional metrics.\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n\n        loss = self.val_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.val_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n        y_hat_hard = to_segmentation_prediction(model_output)\n        self.val_metrics.update(y_hat_hard, y)\n\n        if self._do_plot_samples(batch_idx):\n            try:\n                datamodule = self.trainer.datamodule\n                batch[\"prediction\"] = y_hat_hard\n\n                if isinstance(batch[\"image\"], dict):\n                    rgb_modality = getattr(datamodule, 'rgb_modality', None) or list(batch[\"image\"].keys())[0]\n                    batch[\"image\"] = batch[\"image\"][rgb_modality]\n\n                for key in [\"image\", \"mask\", \"prediction\"]:\n                    batch[key] = batch[key].cpu()\n                sample = unbind_samples(batch)[0]\n                fig = datamodule.val_dataset.plot(sample)\n                if fig:\n                    summary_writer = self.logger.experiment\n                    if hasattr(summary_writer, \"add_figure\"):\n                        summary_writer.add_figure(f\"image/{batch_idx}\", fig, global_step=self.global_step)\n                    elif hasattr(summary_writer, \"log_figure\"):\n                        summary_writer.log_figure(\n                            self.logger.run_id, fig, f\"epoch_{self.current_epoch}_{batch_idx}.png\"\n                        )\n            except ValueError:\n                pass\n            finally:\n                plt.close()\n\n    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n        \"\"\"Compute the predicted class probabilities.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n\n        Returns:\n            Output predicted probabilities.\n        \"\"\"\n        x = batch[\"image\"]\n        file_names = batch[\"filename\"] if \"filename\" in batch else None\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n\n        rest = {k: batch[k] for k in other_keys}\n\n        def model_forward(x,  **kwargs):\n            return self(x, **kwargs).output\n\n        if self.tiled_inference_parameters:\n            y_hat: Tensor = tiled_inference(\n                model_forward,\n                x,\n                self.hparams[\"model_args\"][\"num_classes\"],\n                self.tiled_inference_parameters,\n                **rest,\n            )\n        else:\n            y_hat: Tensor = self(x, **rest).output\n\n        y_hat_ = self.select_classes(y_hat)\n\n        return y_hat_, file_names\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.SemanticSegmentationTask.__init__","title":"<code>__init__(model_args, model_factory=None, model=None, loss='ce', aux_heads=None, aux_loss=None, class_weights=None, ignore_index=None, lr=0.001, optimizer=None, optimizer_hparams=None, scheduler=None, scheduler_hparams=None, freeze_backbone=False, freeze_decoder=False, freeze_head=False, plot_on_val=10, class_names=None, tiled_inference_parameters=None, test_dataloaders_names=None, lr_overrides=None, output_on_inference='prediction', output_most_probable=True, path_to_record_metrics=None, tiled_inference_on_testing=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict</code> <p>Arguments passed to the model factory.</p> required <code>model_factory</code> <code>str</code> <p>ModelFactory class to be used to instantiate the model. Is ignored when model is provided.</p> <code>None</code> <code>model</code> <code>Module</code> <p>Custom model.</p> <code>None</code> <code>loss</code> <code>str</code> <p>Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\".</p> <code>'ce'</code> <code>aux_loss</code> <code>dict[str, float] | None</code> <p>Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None.</p> <code>None</code> <code>class_weights</code> <code>Union[list[float], None]</code> <p>List of class weights to be applied to the loss.</p> <code>None</code> <code>class_weights</code> <code>list[float] | None</code> <p>List of class weights to be applied to the loss. Defaults to None.</p> <code>None</code> <code>ignore_index</code> <code>int | None</code> <p>Label to ignore in the loss computation. Defaults to None.</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate to be used. Defaults to 0.001.</p> <code>0.001</code> <code>optimizer</code> <code>str | None</code> <p>Name of optimizer class from torch.optim to be used.</p> <code>None</code> <code>optimizer_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler</code> <code>str</code> <p>Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>freeze_backbone</code> <code>bool</code> <p>Whether to freeze the backbone. Defaults to False.</p> <code>False</code> <code>freeze_decoder</code> <code>bool</code> <p>Whether to freeze the decoder. Defaults to False.</p> <code>False</code> <code>freeze_head</code> <code>bool</code> <p>Whether to freeze the segmentation head. Defaults to False.</p> <code>False</code> <code>plot_on_val</code> <code>bool | int</code> <p>Whether to plot visualizations on validation.</p> <code>10</code> <code>class_names</code> <code>list[str] | None</code> <p>List of class names passed to metrics for better naming. Defaults to numeric ordering.</p> <code>None</code> <code>tiled_inference_parameters</code> <code>TiledInferenceParameters | None</code> <p>Inference parameters used to determine if inference is done on the whole image or through tiling.</p> <code>None</code> <code>test_dataloaders_names</code> <code>list[str] | None</code> <p>Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used.</p> <code>None</code> <code>lr_overrides</code> <code>dict[str, float] | None</code> <p>Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name) and the value should be the new lr. Defaults to None.</p> <code>None</code> <code>output_on_inference</code> <code>str | list[str]</code> <p>A string or a list defining the kind of output to be saved to file during the inference, for example, it can be \"prediction\", to save just the most probable class, or [\"prediction\", \"probabilities\"] to save both prediction and probabilities.</p> <code>'prediction'</code> <code>output_most_probable</code> <code>bool</code> <p>A boolean to define if the prediction step will output just the most probable logit or all of them. This argument has been deprecated and will be replaced with <code>output_on_inference</code>. </p> <code>True</code> <code>tiled_inference_on_testing</code> <code>bool</code> <p>A boolean to define if tiled inference will be used when full inference  fails during the test step. </p> <code>False</code> <code>path_to_record_metrics</code> <code>str</code> <p>A path to save the file containing the metrics log.</p> <code>None</code> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>def __init__(\n    self,\n    model_args: dict,\n    model_factory: str | None = None,\n    model: torch.nn.Module | None = None,\n    loss: str = \"ce\",\n    aux_heads: list[AuxiliaryHead] | None = None,\n    aux_loss: dict[str, float] | None = None,\n    class_weights: list[float] | None = None,\n    ignore_index: int | None = None,\n    lr: float = 0.001,\n    # the following are optional so CLI doesnt need to pass them\n    optimizer: str | None = None,\n    optimizer_hparams: dict | None = None,\n    scheduler: str | None = None,\n    scheduler_hparams: dict | None = None,\n    #\n    freeze_backbone: bool = False,  # noqa: FBT001, FBT002\n    freeze_decoder: bool = False,  # noqa: FBT002, FBT001\n    freeze_head: bool = False, \n    plot_on_val: bool | int = 10,\n    class_names: list[str] | None = None,\n    tiled_inference_parameters: TiledInferenceParameters = None,\n    test_dataloaders_names: list[str] | None = None,\n    lr_overrides: dict[str, float] | None = None,\n    output_on_inference: str | list[str] = \"prediction\",\n    output_most_probable: bool = True,\n    path_to_record_metrics: str = None,\n    tiled_inference_on_testing: bool = False,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        Defaults to None.\n        model_args (Dict): Arguments passed to the model factory.\n        model_factory (str, optional): ModelFactory class to be used to instantiate the model.\n            Is ignored when model is provided.\n        model (torch.nn.Module, optional): Custom model.\n        loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss.\n            Defaults to \"ce\".\n        aux_loss (dict[str, float] | None, optional): Auxiliary loss weights.\n            Should be a dictionary where the key is the name given to the loss\n            and the value is the weight to be applied to that loss.\n            The name of the loss should match the key in the dictionary output by the model's forward\n            method containing that output. Defaults to None.\n        class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss.\n        class_weights (list[float] | None, optional): List of class weights to be applied to the loss.\n            Defaults to None.\n        ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None.\n        lr (float, optional): Learning rate to be used. Defaults to 0.001.\n        optimizer (str | None, optional): Name of optimizer class from torch.optim to be used.\n        If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.\n        optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer.\n            Overriden by config / cli specification through LightningCLI.\n        scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler\n            to be used (e.g. ReduceLROnPlateau). Defaults to None.\n            Overriden by config / cli specification through LightningCLI.\n        scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler.\n            Overriden by config / cli specification through LightningCLI.\n        freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False.\n        freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False.\n        freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False.\n        plot_on_val (bool | int, optional): Whether to plot visualizations on validation.\n        If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs.\n        class_names (list[str] | None, optional): List of class names passed to metrics for better naming.\n            Defaults to numeric ordering.\n        tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters\n            used to determine if inference is done on the whole image or through tiling.\n        test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when\n            multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None,\n            which assumes only one test dataloader is used.\n        lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific\n            parameters. The key should be a substring of the parameter names (it will check the substring is\n            contained in the parameter name) and the value should be the new lr. Defaults to None.\n        output_on_inference (str | list[str]): A string or a list defining the kind of output to be saved to file during the inference, for example,\n            it can be \"prediction\", to save just the most probable class, or [\"prediction\", \"probabilities\"] to save both prediction and probabilities.\n        output_most_probable (bool): A boolean to define if the prediction step will output just the most probable logit or all of them.\n            This argument has been deprecated and will be replaced with `output_on_inference`. \n        tiled_inference_on_testing (bool): A boolean to define if tiled inference will be used when full inference \n            fails during the test step. \n        path_to_record_metrics (str): A path to save the file containing the metrics log. \n    \"\"\"\n\n    self.tiled_inference_parameters = tiled_inference_parameters\n    self.aux_loss = aux_loss\n    self.aux_heads = aux_heads\n\n    if model is not None and model_factory is not None:\n        logger.warning(\"A model_factory and a model was provided. The model_factory is ignored.\")\n    if model is None and model_factory is None:\n        raise ValueError(\"A model_factory or a model (torch.nn.Module) must be provided.\")\n\n    if model_factory and model is None:\n        self.model_factory = MODEL_FACTORY_REGISTRY.build(model_factory)\n\n    super().__init__(task=\"segmentation\", tiled_inference_on_testing=tiled_inference_on_testing,\n                     path_to_record_metrics=path_to_record_metrics)\n\n    if model is not None:\n        # Custom model\n        self.model = model\n\n    self.train_loss_handler = LossHandler(self.train_metrics.prefix)\n    self.test_loss_handler: list[LossHandler] = []\n    for metrics in self.test_metrics:\n        self.test_loss_handler.append(LossHandler(metrics.prefix))\n    self.val_loss_handler = LossHandler(self.val_metrics.prefix)\n    self.monitor = f\"{self.val_metrics.prefix}loss\"\n    self.plot_on_val = int(plot_on_val)\n    self.output_on_inference = output_on_inference\n\n    # When the user decides to use `output_most_probable` as `False` in\n    # order to output the probabilities instead of the prediction.\n    if not output_most_probable:\n        warnings.warn(\"The argument `output_most_probable` is deprecated and will be replaced with `output_on_inference='probabilities'`.\", stacklevel=1)\n        output_on_inference = \"probabilities\"\n\n    # Processing the `output_on_inference` argument.\n    self.output_prediction = lambda y: (y.argmax(dim=1), \"pred\")\n    self.output_logits = lambda y: (y, \"logits\")\n    self.output_probabilities = lambda y: (torch.nn.Softmax()(y), \"probabilities\")\n\n    # The possible methods to define outputs.\n    self.operation_map = {\n                          \"prediction\": self.output_prediction, \n                          \"logits\": self.output_logits, \n                          \"probabilities\": self.output_probabilities\n                          }\n\n    # `output_on_inference` can be a list or a string.\n    if isinstance(output_on_inference, list):\n        list_of_selectors = ()\n        for var in output_on_inference:\n            if var in self.operation_map:\n                list_of_selectors += (self.operation_map[var],)\n            else:\n                raise ValueError(f\"Option {var} is not supported. It must be in ['prediction', 'logits', 'probabilities']\")\n\n        if not len(list_of_selectors):\n            raise ValueError(\"The list of selectors for the output is empty, please, provide a valid value for `output_on_inference`\")\n\n        self.select_classes = lambda y: [op(y) for op in\n                                               list_of_selectors]\n    elif isinstance(output_on_inference, str):\n        self.select_classes = self.operation_map[output_on_inference]\n\n    else:\n        raise ValueError(f\"The value {output_on_inference} isn't supported for `output_on_inference`.\")\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.SemanticSegmentationTask.configure_losses","title":"<code>configure_losses()</code>","text":"<p>Initialize the loss criterion.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If loss is invalid.</p> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>def configure_losses(self) -&gt; None:\n    \"\"\"Initialize the loss criterion.\n\n    Raises:\n        ValueError: If *loss* is invalid.\n    \"\"\"\n    loss: str = self.hparams[\"loss\"]\n    ignore_index = self.hparams[\"ignore_index\"]\n\n    class_weights = (\n        torch.Tensor(self.hparams[\"class_weights\"]) if self.hparams[\"class_weights\"] is not None else None\n    )\n    if loss == \"ce\":\n        ignore_value = -100 if ignore_index is None else ignore_index\n        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_value, weight=class_weights)\n    elif loss == \"jaccard\":\n        if ignore_index is not None:\n            exception_message = (\n                f\"Jaccard loss does not support ignore_index, but found non-None value of {ignore_index}.\"\n            )\n            raise RuntimeError(exception_message)\n        self.criterion = smp.losses.JaccardLoss(mode=\"multiclass\")\n    elif loss == \"focal\":\n        self.criterion = smp.losses.FocalLoss(\"multiclass\", ignore_index=ignore_index, normalized=True)\n    elif loss == \"dice\":\n        self.criterion = smp.losses.DiceLoss(\"multiclass\", ignore_index=ignore_index)\n    else:\n        exception_message = (\n            f\"Loss type '{loss}' is not valid. Currently, supports 'ce', 'jaccard', 'dice' or 'focal' loss.\"\n        )\n        raise ValueError(exception_message)\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.SemanticSegmentationTask.configure_metrics","title":"<code>configure_metrics()</code>","text":"<p>Initialize the performance metrics.</p> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>def configure_metrics(self) -&gt; None:\n    \"\"\"Initialize the performance metrics.\"\"\"\n    num_classes: int = self.hparams[\"model_args\"][\"num_classes\"]\n    ignore_index: int = self.hparams[\"ignore_index\"]\n    class_names = self.hparams[\"class_names\"]\n    metrics = MetricCollection(\n        {\n            \"Multiclass_Accuracy\": MulticlassAccuracy(\n                num_classes=num_classes,\n                ignore_index=ignore_index,\n                multidim_average=\"global\",\n                average=\"micro\",\n            ),\n            \"Multiclass_Accuracy_Class\": ClasswiseWrapper(\n                MulticlassAccuracy(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                    multidim_average=\"global\",\n                    average=None,\n                ),\n                labels=class_names,\n            ),\n            \"Multiclass_Jaccard_Index_Micro\": MulticlassJaccardIndex(\n                num_classes=num_classes, ignore_index=ignore_index, average=\"micro\"\n            ),\n            \"Multiclass_Jaccard_Index\": MulticlassJaccardIndex(\n                num_classes=num_classes,\n                ignore_index=ignore_index,\n            ),\n            \"Multiclass_Jaccard_Index_Class\": ClasswiseWrapper(\n                MulticlassJaccardIndex(num_classes=num_classes, ignore_index=ignore_index, average=None),\n                labels=class_names,\n            ),\n            \"Multiclass_F1_Score\": MulticlassF1Score(\n                num_classes=num_classes,\n                ignore_index=ignore_index,\n                multidim_average=\"global\",\n                average=\"micro\",\n            ),\n        }\n    )\n    self.train_metrics = metrics.clone(prefix=\"train/\")\n    self.val_metrics = metrics.clone(prefix=\"val/\")\n    if self.hparams[\"test_dataloaders_names\"] is not None:\n        self.test_metrics = nn.ModuleList(\n            [metrics.clone(prefix=f\"test/{dl_name}/\") for dl_name in self.hparams[\"test_dataloaders_names\"]]\n        )\n    else:\n        self.test_metrics = nn.ModuleList([metrics.clone(prefix=\"test/\")])\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.SemanticSegmentationTask.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the predicted class probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output predicted probabilities.</p> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n    \"\"\"Compute the predicted class probabilities.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n\n    Returns:\n        Output predicted probabilities.\n    \"\"\"\n    x = batch[\"image\"]\n    file_names = batch[\"filename\"] if \"filename\" in batch else None\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n\n    rest = {k: batch[k] for k in other_keys}\n\n    def model_forward(x,  **kwargs):\n        return self(x, **kwargs).output\n\n    if self.tiled_inference_parameters:\n        y_hat: Tensor = tiled_inference(\n            model_forward,\n            x,\n            self.hparams[\"model_args\"][\"num_classes\"],\n            self.tiled_inference_parameters,\n            **rest,\n        )\n    else:\n        y_hat: Tensor = self(x, **rest).output\n\n    y_hat_ = self.select_classes(y_hat)\n\n    return y_hat_, file_names\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.SemanticSegmentationTask.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the test loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>def test_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n    \"\"\"Compute the test loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n\n    rest = {k: batch[k] for k in other_keys}\n\n    model_output = self.handle_full_or_tiled_inference(x, self.hparams[\"model_args\"][\"num_classes\"], **rest)\n\n    if dataloader_idx &gt;= len(self.test_loss_handler):\n        msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\"\n        raise ValueError(msg)\n    loss = self.test_loss_handler[dataloader_idx].compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.test_loss_handler[dataloader_idx].log_loss(\n        partial(self.log, add_dataloader_idx=False),  # We don't need the dataloader idx as prefixes are different\n        loss_dict=loss,\n        batch_size=y.shape[0],\n    )\n    y_hat_hard = to_segmentation_prediction(model_output)\n    self.test_metrics[dataloader_idx].update(y_hat_hard, y)\n\n    self.record_metrics(dataloader_idx, y_hat_hard, y)\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.SemanticSegmentationTask.training_step","title":"<code>training_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the train loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>def training_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n    \"\"\"Compute the train loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    # Testing because of failures.\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    loss = self.train_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.train_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n    y_hat_hard = to_segmentation_prediction(model_output)\n    self.train_metrics.update(y_hat_hard, y)\n\n    return loss[\"loss\"]\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.SemanticSegmentationTask.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the validation loss and additional metrics. Args:     batch: The output of your DataLoader.     batch_idx: Integer displaying index of this batch.     dataloader_idx: Index of the current dataloader.</p> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>def validation_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n    \"\"\"Compute the validation loss and additional metrics.\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n\n    loss = self.val_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.val_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n    y_hat_hard = to_segmentation_prediction(model_output)\n    self.val_metrics.update(y_hat_hard, y)\n\n    if self._do_plot_samples(batch_idx):\n        try:\n            datamodule = self.trainer.datamodule\n            batch[\"prediction\"] = y_hat_hard\n\n            if isinstance(batch[\"image\"], dict):\n                rgb_modality = getattr(datamodule, 'rgb_modality', None) or list(batch[\"image\"].keys())[0]\n                batch[\"image\"] = batch[\"image\"][rgb_modality]\n\n            for key in [\"image\", \"mask\", \"prediction\"]:\n                batch[key] = batch[key].cpu()\n            sample = unbind_samples(batch)[0]\n            fig = datamodule.val_dataset.plot(sample)\n            if fig:\n                summary_writer = self.logger.experiment\n                if hasattr(summary_writer, \"add_figure\"):\n                    summary_writer.add_figure(f\"image/{batch_idx}\", fig, global_step=self.global_step)\n                elif hasattr(summary_writer, \"log_figure\"):\n                    summary_writer.log_figure(\n                        self.logger.run_id, fig, f\"epoch_{self.current_epoch}_{batch_idx}.png\"\n                    )\n        except ValueError:\n            pass\n        finally:\n            plt.close()\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.PixelwiseRegressionTask","title":"<code>terratorch.tasks.PixelwiseRegressionTask</code>","text":"<p>               Bases: <code>TerraTorchTask</code></p> <p>Pixelwise Regression Task that accepts models from a range of sources.</p> <p>This class is analog in functionality to PixelwiseRegressionTask defined by torchgeo. However, it has some important differences:     - Accepts the specification of a model factory     - Logs metrics per class     - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)     - Allows the setting of optimizers in the constructor     - Allows to evaluate on multiple test dataloaders</p> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>class PixelwiseRegressionTask(TerraTorchTask):\n    \"\"\"Pixelwise Regression Task that accepts models from a range of sources.\n\n    This class is analog in functionality to PixelwiseRegressionTask defined by torchgeo.\n    However, it has some important differences:\n        - Accepts the specification of a model factory\n        - Logs metrics per class\n        - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)\n        - Allows the setting of optimizers in the constructor\n        - Allows to evaluate on multiple test dataloaders\"\"\"\n\n    def __init__(\n        self,\n        model_args: dict,\n        model_factory: str | None = None,\n        model: torch.nn.Module | None = None,\n        loss: str = \"mse\",\n        aux_heads: list[AuxiliaryHead] | None = None,\n        aux_loss: dict[str, float] | None = None,\n        class_weights: list[float] | None = None,\n        ignore_index: int | None = None,\n        lr: float = 0.001,\n        # the following are optional so CLI doesnt need to pass them\n        optimizer: str | None = None,\n        optimizer_hparams: dict | None = None,\n        scheduler: str | None = None,\n        scheduler_hparams: dict | None = None,\n        #\n        freeze_backbone: bool = False,  # noqa: FBT001, FBT002\n        freeze_decoder: bool = False,  # noqa: FBT001, FBT002\n        freeze_head: bool = False,  # noqa: FBT001, FBT002\n        plot_on_val: bool | int = 10,\n        tiled_inference_parameters: TiledInferenceParameters | None = None,\n        test_dataloaders_names: list[str] | None = None,\n        lr_overrides: dict[str, float] | None = None,\n        tiled_inference_on_testing: bool = None,\n        path_to_record_metrics: str = None,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            model_args (Dict): Arguments passed to the model factory.\n            model_factory (str, optional): Name of ModelFactory class to be used to instantiate the model.\n                Is ignored when model is provided.\n            model (torch.nn.Module, optional): Custom model.\n            loss (str, optional): Loss to be used. Currently, supports 'mse', 'rmse', 'mae' or 'huber' loss.\n                Defaults to \"mse\".\n            aux_loss (dict[str, float] | None, optional): Auxiliary loss weights.\n                Should be a dictionary where the key is the name given to the loss\n                and the value is the weight to be applied to that loss.\n                The name of the loss should match the key in the dictionary output by the model's forward\n                method containing that output. Defaults to None.\n            class_weights (list[float] | None, optional): List of class weights to be applied to the loss.\n                Defaults to None.\n            ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None.\n            lr (float, optional): Learning rate to be used. Defaults to 0.001.\n            optimizer (str | None, optional): Name of optimizer class from torch.optim to be used.\n                If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.\n            optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer.\n                Overriden by config / cli specification through LightningCLI.\n            scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler\n                to be used (e.g. ReduceLROnPlateau). Defaults to None.\n                Overriden by config / cli specification through LightningCLI.\n            scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler.\n                Overriden by config / cli specification through LightningCLI.\n            freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False.\n            freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False.\n            freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False.\n            plot_on_val (bool | int, optional): Whether to plot visualizations on validation.\n                If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs.\n            tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters\n                used to determine if inference is done on the whole image or through tiling.\n            test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when\n                multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None,\n                which assumes only one test dataloader is used.\n            lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific\n                parameters. The key should be a substring of the parameter names (it will check the substring is\n                contained in the parameter name)and the value should be the new lr. Defaults to None.\n            tiled_inference_on_testing (bool): A boolean to the fine if tiled inference will be used when full inference \n                fails during the test step. \n            path_to_record_metrics (str): A path to save the file containing the metrics log. \n        \"\"\"\n\n        self.tiled_inference_parameters = tiled_inference_parameters\n        self.aux_loss = aux_loss\n        self.aux_heads = aux_heads\n\n        if model is not None and model_factory is not None:\n            logger.warning(\"A model_factory and a model was provided. The model_factory is ignored.\")\n        if model is None and model_factory is None:\n            raise ValueError(\"A model_factory or a model (torch.nn.Module) must be provided.\")\n\n        if model_factory and model is None:\n            self.model_factory = MODEL_FACTORY_REGISTRY.build(model_factory)\n\n        super().__init__(task=\"regression\", tiled_inference_on_testing=tiled_inference_on_testing,\n                         path_to_record_metrics=path_to_record_metrics)\n\n        if model:\n            # Custom_model\n            self.model = model\n\n        self.train_loss_handler = LossHandler(self.train_metrics.prefix)\n        self.test_loss_handler: list[LossHandler] = []\n        for metrics in self.test_metrics:\n            self.test_loss_handler.append(LossHandler(metrics.prefix))\n        self.val_loss_handler = LossHandler(self.val_metrics.prefix)\n        self.monitor = f\"{self.val_metrics.prefix}loss\"\n        self.plot_on_val = int(plot_on_val)\n\n    def configure_losses(self) -&gt; None:\n        \"\"\"Initialize the loss criterion.\n\n        Raises:\n            ValueError: If *loss* is invalid.\n        \"\"\"\n        loss: str = self.hparams[\"loss\"].lower()\n        if loss == \"mse\":\n            self.criterion: nn.Module = IgnoreIndexLossWrapper(\n                nn.MSELoss(reduction=\"none\"), self.hparams[\"ignore_index\"]\n            )\n        elif loss == \"mae\":\n            self.criterion = IgnoreIndexLossWrapper(nn.L1Loss(reduction=\"none\"), self.hparams[\"ignore_index\"])\n        elif loss == \"rmse\":\n            # IMPORTANT! Root is done only after ignore index! Otherwise the mean taken is incorrect\n            self.criterion = RootLossWrapper(\n                IgnoreIndexLossWrapper(nn.MSELoss(reduction=\"none\"), self.hparams[\"ignore_index\"]), reduction=None\n            )\n        elif loss == \"huber\":\n            self.criterion = IgnoreIndexLossWrapper(nn.HuberLoss(reduction=\"none\"), self.hparams[\"ignore_index\"])\n        else:\n            exception_message = f\"Loss type '{loss}' is not valid. Currently, supports 'mse', 'rmse' or 'mae' loss.\"\n            raise ValueError(exception_message)\n\n    def configure_metrics(self) -&gt; None:\n        \"\"\"Initialize the performance metrics.\"\"\"\n\n        def instantiate_metrics():\n            return {\n                \"RMSE\": MeanSquaredError(squared=False),\n                \"MSE\": MeanSquaredError(squared=True),\n                \"MAE\": MeanAbsoluteError(),\n            }\n\n        def wrap_metrics_with_ignore_index(metrics):\n            return {\n                name: IgnoreIndexMetricWrapper(metric, ignore_index=self.hparams[\"ignore_index\"])\n                for name, metric in metrics.items()\n            }\n\n        self.train_metrics = MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=\"train/\")\n        self.val_metrics = MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=\"val/\")\n        if self.hparams[\"test_dataloaders_names\"] is not None:\n            self.test_metrics = nn.ModuleList(\n                [\n                    MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=f\"test/{dl_name}/\")\n                    for dl_name in self.hparams[\"test_dataloaders_names\"]\n                ]\n            )\n        else:\n            self.test_metrics = nn.ModuleList(\n                [MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=\"test/\")]\n            )\n\n    def training_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n        \"\"\"Compute the train loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        loss = self.train_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.train_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=x.shape[0])\n        y_hat = model_output.output\n        self.train_metrics.update(y_hat, y)\n\n        return loss[\"loss\"]\n\n    def validation_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        \"\"\"Compute the validation loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        loss = self.val_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.val_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n        y_hat = model_output.output\n        self.val_metrics.update(y_hat, y)\n\n        if self._do_plot_samples(batch_idx):\n            try:\n                datamodule = self.trainer.datamodule\n                batch[\"prediction\"] = y_hat\n                if isinstance(batch[\"image\"], dict):\n                    rgb_modality = getattr(datamodule, 'rgb_modality', None) or list(batch[\"image\"].keys())[0]\n                    batch[\"image\"] = batch[\"image\"][rgb_modality]\n                for key in [\"image\", \"mask\", \"prediction\"]:\n                    batch[key] = batch[key].cpu()\n                sample = unbind_samples(batch)[0]\n                fig = datamodule.val_dataset.plot(sample)\n                if fig:\n                    summary_writer = self.logger.experiment\n                    if hasattr(summary_writer, \"add_figure\"):\n                        summary_writer.add_figure(f\"image/{batch_idx}\", fig, global_step=self.global_step)\n                    elif hasattr(summary_writer, \"log_figure\"):\n                        summary_writer.log_figure(\n                            self.logger.run_id, fig, f\"epoch_{self.current_epoch}_{batch_idx}.png\"\n                        )\n            except ValueError:\n                pass\n            finally:\n                plt.close()\n\n    def test_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        \"\"\"Compute the test loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n\n        model_output = self.handle_full_or_tiled_inference(x, 1, **rest)\n\n        if dataloader_idx &gt;= len(self.test_loss_handler):\n            msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\"\n            raise ValueError(msg)\n        loss = self.test_loss_handler[dataloader_idx].compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.test_loss_handler[dataloader_idx].log_loss(\n            partial(self.log, add_dataloader_idx=False),  # We don't need the dataloader idx as prefixes are different\n            loss_dict=loss,\n            batch_size=x.shape[0],\n        )\n        y_hat = model_output.output\n        self.test_metrics[dataloader_idx].update(y_hat, y)\n\n        self.record_metrics(dataloader_idx, y_hat, y)\n\n    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n        \"\"\"Compute the predicted class probabilities.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n\n        Returns:\n            Output predicted probabilities.\n        \"\"\"\n        x = batch[\"image\"]\n        file_names = batch[\"filename\"] if \"filename\" in batch else None\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n\n        def model_forward(x, **kwargs):\n            return self(x).output\n\n        if self.tiled_inference_parameters:\n            # TODO: tiled inference does not work with additional input data (**rest)\n            y_hat: Tensor = tiled_inference(model_forward, x, 1, self.tiled_inference_parameters, **rest)\n        else:\n            y_hat: Tensor = self(x, **rest).output\n        return y_hat, file_names\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.PixelwiseRegressionTask.__init__","title":"<code>__init__(model_args, model_factory=None, model=None, loss='mse', aux_heads=None, aux_loss=None, class_weights=None, ignore_index=None, lr=0.001, optimizer=None, optimizer_hparams=None, scheduler=None, scheduler_hparams=None, freeze_backbone=False, freeze_decoder=False, freeze_head=False, plot_on_val=10, tiled_inference_parameters=None, test_dataloaders_names=None, lr_overrides=None, tiled_inference_on_testing=None, path_to_record_metrics=None)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict</code> <p>Arguments passed to the model factory.</p> required <code>model_factory</code> <code>str</code> <p>Name of ModelFactory class to be used to instantiate the model. Is ignored when model is provided.</p> <code>None</code> <code>model</code> <code>Module</code> <p>Custom model.</p> <code>None</code> <code>loss</code> <code>str</code> <p>Loss to be used. Currently, supports 'mse', 'rmse', 'mae' or 'huber' loss. Defaults to \"mse\".</p> <code>'mse'</code> <code>aux_loss</code> <code>dict[str, float] | None</code> <p>Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None.</p> <code>None</code> <code>class_weights</code> <code>list[float] | None</code> <p>List of class weights to be applied to the loss. Defaults to None.</p> <code>None</code> <code>ignore_index</code> <code>int | None</code> <p>Label to ignore in the loss computation. Defaults to None.</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate to be used. Defaults to 0.001.</p> <code>0.001</code> <code>optimizer</code> <code>str | None</code> <p>Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>optimizer_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler</code> <code>str</code> <p>Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>freeze_backbone</code> <code>bool</code> <p>Whether to freeze the backbone. Defaults to False.</p> <code>False</code> <code>freeze_decoder</code> <code>bool</code> <p>Whether to freeze the decoder. Defaults to False.</p> <code>False</code> <code>freeze_head</code> <code>bool</code> <p>Whether to freeze the segmentation head. Defaults to False.</p> <code>False</code> <code>plot_on_val</code> <code>bool | int</code> <p>Whether to plot visualizations on validation. If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs.</p> <code>10</code> <code>tiled_inference_parameters</code> <code>TiledInferenceParameters | None</code> <p>Inference parameters used to determine if inference is done on the whole image or through tiling.</p> <code>None</code> <code>test_dataloaders_names</code> <code>list[str] | None</code> <p>Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used.</p> <code>None</code> <code>lr_overrides</code> <code>dict[str, float] | None</code> <p>Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None.</p> <code>None</code> <code>tiled_inference_on_testing</code> <code>bool</code> <p>A boolean to the fine if tiled inference will be used when full inference  fails during the test step. </p> <code>None</code> <code>path_to_record_metrics</code> <code>str</code> <p>A path to save the file containing the metrics log.</p> <code>None</code> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>def __init__(\n    self,\n    model_args: dict,\n    model_factory: str | None = None,\n    model: torch.nn.Module | None = None,\n    loss: str = \"mse\",\n    aux_heads: list[AuxiliaryHead] | None = None,\n    aux_loss: dict[str, float] | None = None,\n    class_weights: list[float] | None = None,\n    ignore_index: int | None = None,\n    lr: float = 0.001,\n    # the following are optional so CLI doesnt need to pass them\n    optimizer: str | None = None,\n    optimizer_hparams: dict | None = None,\n    scheduler: str | None = None,\n    scheduler_hparams: dict | None = None,\n    #\n    freeze_backbone: bool = False,  # noqa: FBT001, FBT002\n    freeze_decoder: bool = False,  # noqa: FBT001, FBT002\n    freeze_head: bool = False,  # noqa: FBT001, FBT002\n    plot_on_val: bool | int = 10,\n    tiled_inference_parameters: TiledInferenceParameters | None = None,\n    test_dataloaders_names: list[str] | None = None,\n    lr_overrides: dict[str, float] | None = None,\n    tiled_inference_on_testing: bool = None,\n    path_to_record_metrics: str = None,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        model_args (Dict): Arguments passed to the model factory.\n        model_factory (str, optional): Name of ModelFactory class to be used to instantiate the model.\n            Is ignored when model is provided.\n        model (torch.nn.Module, optional): Custom model.\n        loss (str, optional): Loss to be used. Currently, supports 'mse', 'rmse', 'mae' or 'huber' loss.\n            Defaults to \"mse\".\n        aux_loss (dict[str, float] | None, optional): Auxiliary loss weights.\n            Should be a dictionary where the key is the name given to the loss\n            and the value is the weight to be applied to that loss.\n            The name of the loss should match the key in the dictionary output by the model's forward\n            method containing that output. Defaults to None.\n        class_weights (list[float] | None, optional): List of class weights to be applied to the loss.\n            Defaults to None.\n        ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None.\n        lr (float, optional): Learning rate to be used. Defaults to 0.001.\n        optimizer (str | None, optional): Name of optimizer class from torch.optim to be used.\n            If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.\n        optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer.\n            Overriden by config / cli specification through LightningCLI.\n        scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler\n            to be used (e.g. ReduceLROnPlateau). Defaults to None.\n            Overriden by config / cli specification through LightningCLI.\n        scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler.\n            Overriden by config / cli specification through LightningCLI.\n        freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False.\n        freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False.\n        freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False.\n        plot_on_val (bool | int, optional): Whether to plot visualizations on validation.\n            If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs.\n        tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters\n            used to determine if inference is done on the whole image or through tiling.\n        test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when\n            multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None,\n            which assumes only one test dataloader is used.\n        lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific\n            parameters. The key should be a substring of the parameter names (it will check the substring is\n            contained in the parameter name)and the value should be the new lr. Defaults to None.\n        tiled_inference_on_testing (bool): A boolean to the fine if tiled inference will be used when full inference \n            fails during the test step. \n        path_to_record_metrics (str): A path to save the file containing the metrics log. \n    \"\"\"\n\n    self.tiled_inference_parameters = tiled_inference_parameters\n    self.aux_loss = aux_loss\n    self.aux_heads = aux_heads\n\n    if model is not None and model_factory is not None:\n        logger.warning(\"A model_factory and a model was provided. The model_factory is ignored.\")\n    if model is None and model_factory is None:\n        raise ValueError(\"A model_factory or a model (torch.nn.Module) must be provided.\")\n\n    if model_factory and model is None:\n        self.model_factory = MODEL_FACTORY_REGISTRY.build(model_factory)\n\n    super().__init__(task=\"regression\", tiled_inference_on_testing=tiled_inference_on_testing,\n                     path_to_record_metrics=path_to_record_metrics)\n\n    if model:\n        # Custom_model\n        self.model = model\n\n    self.train_loss_handler = LossHandler(self.train_metrics.prefix)\n    self.test_loss_handler: list[LossHandler] = []\n    for metrics in self.test_metrics:\n        self.test_loss_handler.append(LossHandler(metrics.prefix))\n    self.val_loss_handler = LossHandler(self.val_metrics.prefix)\n    self.monitor = f\"{self.val_metrics.prefix}loss\"\n    self.plot_on_val = int(plot_on_val)\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.PixelwiseRegressionTask.configure_losses","title":"<code>configure_losses()</code>","text":"<p>Initialize the loss criterion.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If loss is invalid.</p> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>def configure_losses(self) -&gt; None:\n    \"\"\"Initialize the loss criterion.\n\n    Raises:\n        ValueError: If *loss* is invalid.\n    \"\"\"\n    loss: str = self.hparams[\"loss\"].lower()\n    if loss == \"mse\":\n        self.criterion: nn.Module = IgnoreIndexLossWrapper(\n            nn.MSELoss(reduction=\"none\"), self.hparams[\"ignore_index\"]\n        )\n    elif loss == \"mae\":\n        self.criterion = IgnoreIndexLossWrapper(nn.L1Loss(reduction=\"none\"), self.hparams[\"ignore_index\"])\n    elif loss == \"rmse\":\n        # IMPORTANT! Root is done only after ignore index! Otherwise the mean taken is incorrect\n        self.criterion = RootLossWrapper(\n            IgnoreIndexLossWrapper(nn.MSELoss(reduction=\"none\"), self.hparams[\"ignore_index\"]), reduction=None\n        )\n    elif loss == \"huber\":\n        self.criterion = IgnoreIndexLossWrapper(nn.HuberLoss(reduction=\"none\"), self.hparams[\"ignore_index\"])\n    else:\n        exception_message = f\"Loss type '{loss}' is not valid. Currently, supports 'mse', 'rmse' or 'mae' loss.\"\n        raise ValueError(exception_message)\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.PixelwiseRegressionTask.configure_metrics","title":"<code>configure_metrics()</code>","text":"<p>Initialize the performance metrics.</p> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>def configure_metrics(self) -&gt; None:\n    \"\"\"Initialize the performance metrics.\"\"\"\n\n    def instantiate_metrics():\n        return {\n            \"RMSE\": MeanSquaredError(squared=False),\n            \"MSE\": MeanSquaredError(squared=True),\n            \"MAE\": MeanAbsoluteError(),\n        }\n\n    def wrap_metrics_with_ignore_index(metrics):\n        return {\n            name: IgnoreIndexMetricWrapper(metric, ignore_index=self.hparams[\"ignore_index\"])\n            for name, metric in metrics.items()\n        }\n\n    self.train_metrics = MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=\"train/\")\n    self.val_metrics = MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=\"val/\")\n    if self.hparams[\"test_dataloaders_names\"] is not None:\n        self.test_metrics = nn.ModuleList(\n            [\n                MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=f\"test/{dl_name}/\")\n                for dl_name in self.hparams[\"test_dataloaders_names\"]\n            ]\n        )\n    else:\n        self.test_metrics = nn.ModuleList(\n            [MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=\"test/\")]\n        )\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.PixelwiseRegressionTask.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the predicted class probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output predicted probabilities.</p> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n    \"\"\"Compute the predicted class probabilities.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n\n    Returns:\n        Output predicted probabilities.\n    \"\"\"\n    x = batch[\"image\"]\n    file_names = batch[\"filename\"] if \"filename\" in batch else None\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n\n    def model_forward(x, **kwargs):\n        return self(x).output\n\n    if self.tiled_inference_parameters:\n        # TODO: tiled inference does not work with additional input data (**rest)\n        y_hat: Tensor = tiled_inference(model_forward, x, 1, self.tiled_inference_parameters, **rest)\n    else:\n        y_hat: Tensor = self(x, **rest).output\n    return y_hat, file_names\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.PixelwiseRegressionTask.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the test loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>def test_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n    \"\"\"Compute the test loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n\n    model_output = self.handle_full_or_tiled_inference(x, 1, **rest)\n\n    if dataloader_idx &gt;= len(self.test_loss_handler):\n        msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\"\n        raise ValueError(msg)\n    loss = self.test_loss_handler[dataloader_idx].compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.test_loss_handler[dataloader_idx].log_loss(\n        partial(self.log, add_dataloader_idx=False),  # We don't need the dataloader idx as prefixes are different\n        loss_dict=loss,\n        batch_size=x.shape[0],\n    )\n    y_hat = model_output.output\n    self.test_metrics[dataloader_idx].update(y_hat, y)\n\n    self.record_metrics(dataloader_idx, y_hat, y)\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.PixelwiseRegressionTask.training_step","title":"<code>training_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the train loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>def training_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n    \"\"\"Compute the train loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    loss = self.train_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.train_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=x.shape[0])\n    y_hat = model_output.output\n    self.train_metrics.update(y_hat, y)\n\n    return loss[\"loss\"]\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.PixelwiseRegressionTask.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the validation loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>def validation_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n    \"\"\"Compute the validation loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    loss = self.val_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.val_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n    y_hat = model_output.output\n    self.val_metrics.update(y_hat, y)\n\n    if self._do_plot_samples(batch_idx):\n        try:\n            datamodule = self.trainer.datamodule\n            batch[\"prediction\"] = y_hat\n            if isinstance(batch[\"image\"], dict):\n                rgb_modality = getattr(datamodule, 'rgb_modality', None) or list(batch[\"image\"].keys())[0]\n                batch[\"image\"] = batch[\"image\"][rgb_modality]\n            for key in [\"image\", \"mask\", \"prediction\"]:\n                batch[key] = batch[key].cpu()\n            sample = unbind_samples(batch)[0]\n            fig = datamodule.val_dataset.plot(sample)\n            if fig:\n                summary_writer = self.logger.experiment\n                if hasattr(summary_writer, \"add_figure\"):\n                    summary_writer.add_figure(f\"image/{batch_idx}\", fig, global_step=self.global_step)\n                elif hasattr(summary_writer, \"log_figure\"):\n                    summary_writer.log_figure(\n                        self.logger.run_id, fig, f\"epoch_{self.current_epoch}_{batch_idx}.png\"\n                    )\n        except ValueError:\n            pass\n        finally:\n            plt.close()\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ClassificationTask","title":"<code>terratorch.tasks.ClassificationTask</code>","text":"<p>               Bases: <code>TerraTorchTask</code></p> <p>Classification Task that accepts models from a range of sources.</p> <p>This class is analog in functionality to the class ClassificationTask defined by torchgeo. However, it has some important differences:     - Accepts the specification of a model factory     - Logs metrics per class     - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)     - Allows the setting of optimizers in the constructor     - It provides mIoU with both Micro and Macro averaging     - Allows to evaluate on multiple test dataloaders</p> <p>.. note::        * 'Micro' averaging suits overall performance evaluation but may not reflect          minority class accuracy.        * 'Macro' averaging gives equal weight to each class, useful          for balanced performance assessment across imbalanced classes.</p> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>class ClassificationTask(TerraTorchTask):\n    \"\"\"Classification Task that accepts models from a range of sources.\n\n    This class is analog in functionality to the class ClassificationTask defined by torchgeo.\n    However, it has some important differences:\n        - Accepts the specification of a model factory\n        - Logs metrics per class\n        - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)\n        - Allows the setting of optimizers in the constructor\n        - It provides mIoU with both Micro and Macro averaging\n        - Allows to evaluate on multiple test dataloaders\n\n    .. note::\n           * 'Micro' averaging suits overall performance evaluation but may not reflect\n             minority class accuracy.\n           * 'Macro' averaging gives equal weight to each class, useful\n             for balanced performance assessment across imbalanced classes.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: dict,\n        model_factory: str | None = None,\n        model: torch.nn.Module | None = None,\n        loss: str = \"ce\",\n        aux_heads: list[AuxiliaryHead] | None = None,\n        aux_loss: dict[str, float] | None = None,\n        class_weights: list[float] | None = None,\n        ignore_index: int | None = None,\n        lr: float = 0.001,\n        # the following are optional so CLI doesnt need to pass them\n        optimizer: str | None = None,\n        optimizer_hparams: dict | None = None,\n        scheduler: str | None = None,\n        scheduler_hparams: dict | None = None,\n        #\n        #\n        freeze_backbone: bool = False,  # noqa: FBT001, FBT002\n        freeze_decoder: bool = False,  # noqa: FBT002, FBT001\n        freeze_head: bool = False,  # noqa: FBT002, FBT001\n        class_names: list[str] | None = None,\n        test_dataloaders_names: list[str] | None = None,\n        lr_overrides: dict[str, float] | None = None,\n        path_to_record_metrics: str = None,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            Defaults to None.\n            model_args (Dict): Arguments passed to the model factory.\n            model_factory (str, optional): ModelFactory class to be used to instantiate the model.\n                Is ignored when model is provided.\n            model (torch.nn.Module, optional): Custom model.\n            loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss.\n                Defaults to \"ce\".\n            aux_loss (dict[str, float] | None, optional): Auxiliary loss weights.\n                Should be a dictionary where the key is the name given to the loss\n                and the value is the weight to be applied to that loss.\n                The name of the loss should match the key in the dictionary output by the model's forward\n                method containing that output. Defaults to None.\n            class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss.\n            class_weights (list[float] | None, optional): List of class weights to be applied to the loss.\n                Defaults to None.\n            ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None.\n            lr (float, optional): Learning rate to be used. Defaults to 0.001.\n            optimizer (str | None, optional): Name of optimizer class from torch.optim to be used.\n                If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.\n            optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer.\n                Overriden by config / cli specification through LightningCLI.\n            scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler\n                to be used (e.g. ReduceLROnPlateau). Defaults to None.\n                Overriden by config / cli specification through LightningCLI.\n            scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler.\n                Overriden by config / cli specification through LightningCLI.\n            freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False.\n            freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False.\n            freeze_head (bool, optional): Whether to freeze the segmentation_head. Defaults to False.\n            class_names (list[str] | None, optional): List of class names passed to metrics for better naming.\n                Defaults to numeric ordering.\n            test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when\n                multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None,\n                which assumes only one test dataloader is used.\n            lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific\n                parameters. The key should be a substring of the parameter names (it will check the substring is\n                contained in the parameter name)and the value should be the new lr. Defaults to None.\n            path_to_record_metrics (str): A path to save the file containing the metrics log. \n        \"\"\"\n\n        self.aux_loss = aux_loss\n        self.aux_heads = aux_heads\n\n        if model is not None and model_factory is not None:\n            logger.warning(\"A model_factory and a model was provided. The model_factory is ignored.\")\n        if model is None and model_factory is None:\n            raise ValueError(\"A model_factory or a model (torch.nn.Module) must be provided.\")\n\n        if model_factory and model is None:\n            self.model_factory = MODEL_FACTORY_REGISTRY.build(model_factory)\n\n        super().__init__(task=\"classification\", path_to_record_metrics=path_to_record_metrics)\n\n        if model:\n            # Custom model\n            self.model = model\n\n        self.train_loss_handler = LossHandler(self.train_metrics.prefix)\n        self.test_loss_handler: list[LossHandler] = []\n        for metrics in self.test_metrics:\n            self.test_loss_handler.append(LossHandler(metrics.prefix))\n        self.val_loss_handler = LossHandler(self.val_metrics.prefix)\n        self.monitor = f\"{self.val_metrics.prefix}loss\"\n\n    def configure_losses(self) -&gt; None:\n        \"\"\"Initialize the loss criterion.\n\n        Raises:\n            ValueError: If *loss* is invalid.\n        \"\"\"\n        loss: str = self.hparams[\"loss\"]\n        ignore_index = self.hparams[\"ignore_index\"]\n\n        class_weights = (\n            torch.Tensor(self.hparams[\"class_weights\"]) if self.hparams[\"class_weights\"] is not None else None\n        )\n        if loss == \"ce\":\n            ignore_value = -100 if ignore_index is None else ignore_index\n            self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_value, weight=class_weights)\n        elif loss == \"bce\":\n            self.criterion = nn.BCEWithLogitsLoss()\n        elif loss == \"jaccard\":\n            self.criterion = JaccardLoss(mode=\"multiclass\")\n        elif loss == \"focal\":\n            self.criterion = FocalLoss(mode=\"multiclass\", normalized=True)\n        else:\n            msg = f\"Loss type '{loss}' is not valid.\"\n            raise ValueError(msg)\n\n    def configure_metrics(self) -&gt; None:\n        \"\"\"Initialize the performance metrics.\"\"\"\n        num_classes: int = self.hparams[\"model_args\"][\"num_classes\"]\n        ignore_index: int = self.hparams[\"ignore_index\"]\n        class_names = self.hparams[\"class_names\"]\n        metrics = MetricCollection(\n            {\n                \"Overall_Accuracy\": MulticlassAccuracy(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                    average=\"micro\",\n                ),\n                \"Average_Accuracy\": MulticlassAccuracy(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                    average=\"macro\",\n                ),\n                \"Multiclass_Accuracy_Class\": ClasswiseWrapper(\n                    MulticlassAccuracy(\n                        num_classes=num_classes,\n                        ignore_index=ignore_index,\n                        average=None,\n                    ),\n                    labels=class_names,\n                ),\n                \"Multiclass_Jaccard_Index\": MulticlassJaccardIndex(num_classes=num_classes, ignore_index=ignore_index),\n                \"Multiclass_Jaccard_Index_Class\": ClasswiseWrapper(\n                    MulticlassJaccardIndex(num_classes=num_classes, ignore_index=ignore_index, average=None),\n                    labels=class_names,\n                ),\n                # why FBetaScore\n                \"Multiclass_F1_Score\": MulticlassFBetaScore(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                    beta=1.0,\n                    average=\"micro\",\n                ),\n            }\n        )\n        self.train_metrics = metrics.clone(prefix=\"train/\")\n        self.val_metrics = metrics.clone(prefix=\"val/\")\n        if self.hparams[\"test_dataloaders_names\"] is not None:\n            self.test_metrics = nn.ModuleList(\n                [metrics.clone(prefix=f\"test/{dl_name}/\") for dl_name in self.hparams[\"test_dataloaders_names\"]]\n            )\n        else:\n            self.test_metrics = nn.ModuleList([metrics.clone(prefix=\"test/\")])\n\n    def training_step(self, batch: object, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n        \"\"\"Compute the train loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"label\"] \n        other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        loss = self.train_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.train_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n        y_hat_hard = to_class_prediction(model_output)\n        self.train_metrics.update(y_hat_hard, y)\n\n        return loss[\"loss\"]\n\n    def validation_step(self, batch: object, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        \"\"\"Compute the validation loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"label\"]\n        other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        loss = self.val_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.val_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n        y_hat_hard = to_class_prediction(model_output)\n        self.val_metrics.update(y_hat_hard, y)\n\n    def test_step(self, batch: object, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        \"\"\"Compute the test loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"label\"]\n        other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        if dataloader_idx &gt;= len(self.test_loss_handler):\n            msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\"\n            raise ValueError(msg)\n        loss = self.test_loss_handler[dataloader_idx].compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.test_loss_handler[dataloader_idx].log_loss(\n            partial(self.log, add_dataloader_idx=False),  # We don't need the dataloader idx as prefixes are different\n            loss_dict=loss,\n            batch_size=y.shape[0],\n        )\n        y_hat_hard = to_class_prediction(model_output)\n        self.test_metrics[dataloader_idx].update(y_hat_hard, y)\n\n        self.record_metrics(dataloader_idx, y_hat_hard, y)\n\n    def predict_step(self, batch: object, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n\n        \"\"\"Compute the predicted class probabilities.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n\n        Returns:\n            Output predicted probabilities.\n        \"\"\"\n        x = batch[\"image\"]\n        file_names = batch[\"filename\"] if \"filename\" in batch else None\n        other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        y_hat = self(x, **rest).output\n        y_hat = y_hat.argmax(dim=1)\n        return y_hat, file_names\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ClassificationTask.__init__","title":"<code>__init__(model_args, model_factory=None, model=None, loss='ce', aux_heads=None, aux_loss=None, class_weights=None, ignore_index=None, lr=0.001, optimizer=None, optimizer_hparams=None, scheduler=None, scheduler_hparams=None, freeze_backbone=False, freeze_decoder=False, freeze_head=False, class_names=None, test_dataloaders_names=None, lr_overrides=None, path_to_record_metrics=None)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict</code> <p>Arguments passed to the model factory.</p> required <code>model_factory</code> <code>str</code> <p>ModelFactory class to be used to instantiate the model. Is ignored when model is provided.</p> <code>None</code> <code>model</code> <code>Module</code> <p>Custom model.</p> <code>None</code> <code>loss</code> <code>str</code> <p>Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\".</p> <code>'ce'</code> <code>aux_loss</code> <code>dict[str, float] | None</code> <p>Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None.</p> <code>None</code> <code>class_weights</code> <code>Union[list[float], None]</code> <p>List of class weights to be applied to the loss.</p> <code>None</code> <code>class_weights</code> <code>list[float] | None</code> <p>List of class weights to be applied to the loss. Defaults to None.</p> <code>None</code> <code>ignore_index</code> <code>int | None</code> <p>Label to ignore in the loss computation. Defaults to None.</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate to be used. Defaults to 0.001.</p> <code>0.001</code> <code>optimizer</code> <code>str | None</code> <p>Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>optimizer_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler</code> <code>str</code> <p>Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>freeze_backbone</code> <code>bool</code> <p>Whether to freeze the backbone. Defaults to False.</p> <code>False</code> <code>freeze_decoder</code> <code>bool</code> <p>Whether to freeze the decoder. Defaults to False.</p> <code>False</code> <code>freeze_head</code> <code>bool</code> <p>Whether to freeze the segmentation_head. Defaults to False.</p> <code>False</code> <code>class_names</code> <code>list[str] | None</code> <p>List of class names passed to metrics for better naming. Defaults to numeric ordering.</p> <code>None</code> <code>test_dataloaders_names</code> <code>list[str] | None</code> <p>Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used.</p> <code>None</code> <code>lr_overrides</code> <code>dict[str, float] | None</code> <p>Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None.</p> <code>None</code> <code>path_to_record_metrics</code> <code>str</code> <p>A path to save the file containing the metrics log.</p> <code>None</code> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>def __init__(\n    self,\n    model_args: dict,\n    model_factory: str | None = None,\n    model: torch.nn.Module | None = None,\n    loss: str = \"ce\",\n    aux_heads: list[AuxiliaryHead] | None = None,\n    aux_loss: dict[str, float] | None = None,\n    class_weights: list[float] | None = None,\n    ignore_index: int | None = None,\n    lr: float = 0.001,\n    # the following are optional so CLI doesnt need to pass them\n    optimizer: str | None = None,\n    optimizer_hparams: dict | None = None,\n    scheduler: str | None = None,\n    scheduler_hparams: dict | None = None,\n    #\n    #\n    freeze_backbone: bool = False,  # noqa: FBT001, FBT002\n    freeze_decoder: bool = False,  # noqa: FBT002, FBT001\n    freeze_head: bool = False,  # noqa: FBT002, FBT001\n    class_names: list[str] | None = None,\n    test_dataloaders_names: list[str] | None = None,\n    lr_overrides: dict[str, float] | None = None,\n    path_to_record_metrics: str = None,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        Defaults to None.\n        model_args (Dict): Arguments passed to the model factory.\n        model_factory (str, optional): ModelFactory class to be used to instantiate the model.\n            Is ignored when model is provided.\n        model (torch.nn.Module, optional): Custom model.\n        loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss.\n            Defaults to \"ce\".\n        aux_loss (dict[str, float] | None, optional): Auxiliary loss weights.\n            Should be a dictionary where the key is the name given to the loss\n            and the value is the weight to be applied to that loss.\n            The name of the loss should match the key in the dictionary output by the model's forward\n            method containing that output. Defaults to None.\n        class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss.\n        class_weights (list[float] | None, optional): List of class weights to be applied to the loss.\n            Defaults to None.\n        ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None.\n        lr (float, optional): Learning rate to be used. Defaults to 0.001.\n        optimizer (str | None, optional): Name of optimizer class from torch.optim to be used.\n            If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.\n        optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer.\n            Overriden by config / cli specification through LightningCLI.\n        scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler\n            to be used (e.g. ReduceLROnPlateau). Defaults to None.\n            Overriden by config / cli specification through LightningCLI.\n        scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler.\n            Overriden by config / cli specification through LightningCLI.\n        freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False.\n        freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False.\n        freeze_head (bool, optional): Whether to freeze the segmentation_head. Defaults to False.\n        class_names (list[str] | None, optional): List of class names passed to metrics for better naming.\n            Defaults to numeric ordering.\n        test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when\n            multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None,\n            which assumes only one test dataloader is used.\n        lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific\n            parameters. The key should be a substring of the parameter names (it will check the substring is\n            contained in the parameter name)and the value should be the new lr. Defaults to None.\n        path_to_record_metrics (str): A path to save the file containing the metrics log. \n    \"\"\"\n\n    self.aux_loss = aux_loss\n    self.aux_heads = aux_heads\n\n    if model is not None and model_factory is not None:\n        logger.warning(\"A model_factory and a model was provided. The model_factory is ignored.\")\n    if model is None and model_factory is None:\n        raise ValueError(\"A model_factory or a model (torch.nn.Module) must be provided.\")\n\n    if model_factory and model is None:\n        self.model_factory = MODEL_FACTORY_REGISTRY.build(model_factory)\n\n    super().__init__(task=\"classification\", path_to_record_metrics=path_to_record_metrics)\n\n    if model:\n        # Custom model\n        self.model = model\n\n    self.train_loss_handler = LossHandler(self.train_metrics.prefix)\n    self.test_loss_handler: list[LossHandler] = []\n    for metrics in self.test_metrics:\n        self.test_loss_handler.append(LossHandler(metrics.prefix))\n    self.val_loss_handler = LossHandler(self.val_metrics.prefix)\n    self.monitor = f\"{self.val_metrics.prefix}loss\"\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ClassificationTask.configure_losses","title":"<code>configure_losses()</code>","text":"<p>Initialize the loss criterion.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If loss is invalid.</p> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>def configure_losses(self) -&gt; None:\n    \"\"\"Initialize the loss criterion.\n\n    Raises:\n        ValueError: If *loss* is invalid.\n    \"\"\"\n    loss: str = self.hparams[\"loss\"]\n    ignore_index = self.hparams[\"ignore_index\"]\n\n    class_weights = (\n        torch.Tensor(self.hparams[\"class_weights\"]) if self.hparams[\"class_weights\"] is not None else None\n    )\n    if loss == \"ce\":\n        ignore_value = -100 if ignore_index is None else ignore_index\n        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_value, weight=class_weights)\n    elif loss == \"bce\":\n        self.criterion = nn.BCEWithLogitsLoss()\n    elif loss == \"jaccard\":\n        self.criterion = JaccardLoss(mode=\"multiclass\")\n    elif loss == \"focal\":\n        self.criterion = FocalLoss(mode=\"multiclass\", normalized=True)\n    else:\n        msg = f\"Loss type '{loss}' is not valid.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ClassificationTask.configure_metrics","title":"<code>configure_metrics()</code>","text":"<p>Initialize the performance metrics.</p> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>def configure_metrics(self) -&gt; None:\n    \"\"\"Initialize the performance metrics.\"\"\"\n    num_classes: int = self.hparams[\"model_args\"][\"num_classes\"]\n    ignore_index: int = self.hparams[\"ignore_index\"]\n    class_names = self.hparams[\"class_names\"]\n    metrics = MetricCollection(\n        {\n            \"Overall_Accuracy\": MulticlassAccuracy(\n                num_classes=num_classes,\n                ignore_index=ignore_index,\n                average=\"micro\",\n            ),\n            \"Average_Accuracy\": MulticlassAccuracy(\n                num_classes=num_classes,\n                ignore_index=ignore_index,\n                average=\"macro\",\n            ),\n            \"Multiclass_Accuracy_Class\": ClasswiseWrapper(\n                MulticlassAccuracy(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                    average=None,\n                ),\n                labels=class_names,\n            ),\n            \"Multiclass_Jaccard_Index\": MulticlassJaccardIndex(num_classes=num_classes, ignore_index=ignore_index),\n            \"Multiclass_Jaccard_Index_Class\": ClasswiseWrapper(\n                MulticlassJaccardIndex(num_classes=num_classes, ignore_index=ignore_index, average=None),\n                labels=class_names,\n            ),\n            # why FBetaScore\n            \"Multiclass_F1_Score\": MulticlassFBetaScore(\n                num_classes=num_classes,\n                ignore_index=ignore_index,\n                beta=1.0,\n                average=\"micro\",\n            ),\n        }\n    )\n    self.train_metrics = metrics.clone(prefix=\"train/\")\n    self.val_metrics = metrics.clone(prefix=\"val/\")\n    if self.hparams[\"test_dataloaders_names\"] is not None:\n        self.test_metrics = nn.ModuleList(\n            [metrics.clone(prefix=f\"test/{dl_name}/\") for dl_name in self.hparams[\"test_dataloaders_names\"]]\n        )\n    else:\n        self.test_metrics = nn.ModuleList([metrics.clone(prefix=\"test/\")])\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ClassificationTask.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the predicted class probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>object</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output predicted probabilities.</p> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>def predict_step(self, batch: object, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n\n    \"\"\"Compute the predicted class probabilities.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n\n    Returns:\n        Output predicted probabilities.\n    \"\"\"\n    x = batch[\"image\"]\n    file_names = batch[\"filename\"] if \"filename\" in batch else None\n    other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    y_hat = self(x, **rest).output\n    y_hat = y_hat.argmax(dim=1)\n    return y_hat, file_names\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ClassificationTask.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the test loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>object</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>def test_step(self, batch: object, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n    \"\"\"Compute the test loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"label\"]\n    other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    if dataloader_idx &gt;= len(self.test_loss_handler):\n        msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\"\n        raise ValueError(msg)\n    loss = self.test_loss_handler[dataloader_idx].compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.test_loss_handler[dataloader_idx].log_loss(\n        partial(self.log, add_dataloader_idx=False),  # We don't need the dataloader idx as prefixes are different\n        loss_dict=loss,\n        batch_size=y.shape[0],\n    )\n    y_hat_hard = to_class_prediction(model_output)\n    self.test_metrics[dataloader_idx].update(y_hat_hard, y)\n\n    self.record_metrics(dataloader_idx, y_hat_hard, y)\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ClassificationTask.training_step","title":"<code>training_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the train loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>object</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>def training_step(self, batch: object, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n    \"\"\"Compute the train loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"label\"] \n    other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    loss = self.train_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.train_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n    y_hat_hard = to_class_prediction(model_output)\n    self.train_metrics.update(y_hat_hard, y)\n\n    return loss[\"loss\"]\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ClassificationTask.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the validation loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>object</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>def validation_step(self, batch: object, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n    \"\"\"Compute the validation loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"label\"]\n    other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    loss = self.val_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.val_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n    y_hat_hard = to_class_prediction(model_output)\n    self.val_metrics.update(y_hat_hard, y)\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.MultiLabelClassificationTask","title":"<code>terratorch.tasks.MultiLabelClassificationTask</code>","text":"<p>               Bases: <code>ClassificationTask</code></p> Source code in <code>terratorch/tasks/multilabel_classification_tasks.py</code> <pre><code>class MultiLabelClassificationTask(ClassificationTask):\n    def configure_losses(self) -&gt; None:\n        if self.hparams[\"loss\"] == \"bce\":\n            self.criterion: nn.Module = nn.BCEWithLogitsLoss()\n        elif self.hparams[\"loss\"] == \"balanced_bce\":\n            self.criterion = _balanced_binary_cross_entropy_with_logits\n        else:\n            super().configure_losses()\n\n    def configure_metrics(self) -&gt; None:\n        metrics = MetricCollection(\n            {\n                \"Overall_Accuracy\": MultilabelAccuracy(\n                    num_labels=self.hparams[\"model_args\"][\"num_classes\"], average=\"micro\"\n                ),\n                \"Average_Accuracy\": MultilabelAccuracy(\n                    num_labels=self.hparams[\"model_args\"][\"num_classes\"], average=\"macro\"\n                ),\n                \"Multilabel_F1_Score\": MultilabelFBetaScore(\n                    num_labels=self.hparams[\"model_args\"][\"num_classes\"], beta=1.0, average=\"micro\"\n                ),\n            }\n        )\n\n        self.train_metrics = metrics.clone(prefix=\"train/\")\n        self.val_metrics = metrics.clone(prefix=\"val/\")\n        if self.hparams[\"test_dataloaders_names\"] is not None:\n            self.test_metrics = nn.ModuleList(\n                [metrics.clone(prefix=f\"test/{dl_name}/\") for dl_name in self.hparams[\"test_dataloaders_names\"]]\n            )\n        else:\n            self.test_metrics = nn.ModuleList([metrics.clone(prefix=\"test/\")])\n\n    @staticmethod\n    def to_multilabel_prediction(y: ModelOutput) -&gt; Tensor:\n        y_hat = y.output\n        return torch.sigmoid(y_hat)\n\n    def training_step(self, batch: object, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n        x = batch[\"image\"]\n        y = batch[\"label\"].to(torch.float32)\n        other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n        rest = {k:batch[k] for k in other_keys}\n\n        model_output: ModelOutput = self(x, **rest)\n        loss = self.train_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.train_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n        y_hat = self.to_multilabel_prediction(model_output)\n        self.train_metrics.update(y_hat, y)\n\n        return loss[\"loss\"]\n\n    def validation_step(self, batch: object, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        x = batch[\"image\"]\n        y = batch[\"label\"].to(torch.float32)\n        other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n        rest = {k:batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        loss = self.val_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.val_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n        y_hat = self.to_multilabel_prediction(model_output)\n        self.val_metrics.update(y_hat, y)\n\n    def test_step(self, batch: object, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        x = batch[\"image\"]\n        y = batch[\"label\"].to(torch.float32)\n        other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n        rest = {k:batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        if dataloader_idx &gt;= len(self.test_loss_handler):\n            msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\"\n            raise ValueError(msg)\n        loss = self.test_loss_handler[dataloader_idx].compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.test_loss_handler[dataloader_idx].log_loss(\n            partial(self.log, add_dataloader_idx=False),  # We don't need the dataloader idx as prefixes are different\n            loss_dict=loss,\n            batch_size=y.shape[0],\n        )\n        y_hat = self.to_multilabel_prediction(model_output)\n        self.test_metrics[dataloader_idx].update(y_hat, y)\n\n    def predict_step(self, batch: object, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n        \"\"\"Compute the predicted class probabilities.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n\n        Returns:\n            Output predicted probabilities.\n        \"\"\"\n        x = batch[\"image\"]\n        file_names = batch[\"filename\"] if \"filename\" in batch else None\n        other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output = self(x, **rest)\n        y_hat = self.to_multilabel_prediction(model_output)\n        return y_hat, file_names\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.MultiLabelClassificationTask.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the predicted class probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>object</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output predicted probabilities.</p> Source code in <code>terratorch/tasks/multilabel_classification_tasks.py</code> <pre><code>def predict_step(self, batch: object, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n    \"\"\"Compute the predicted class probabilities.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n\n    Returns:\n        Output predicted probabilities.\n    \"\"\"\n    x = batch[\"image\"]\n    file_names = batch[\"filename\"] if \"filename\" in batch else None\n    other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output = self(x, **rest)\n    y_hat = self.to_multilabel_prediction(model_output)\n    return y_hat, file_names\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ObjectDetectionTask","title":"<code>terratorch.tasks.ObjectDetectionTask</code>","text":"<p>               Bases: <code>BaseTask</code></p> Source code in <code>terratorch/tasks/object_detection_task.py</code> <pre><code>class ObjectDetectionTask(BaseTask):\n\n    ignore = None\n    monitor = 'val_map'\n    mode = 'max'\n\n    def __init__(\n        self,\n        model_factory: str,\n        model_args: dict,\n\n        lr: float = 0.001,\n\n        optimizer: str | None = None,\n        optimizer_hparams: dict | None = None,\n        scheduler: str | None = None,\n        scheduler_hparams: dict | None = None,\n\n        freeze_backbone: bool = False,\n        freeze_decoder: bool = False,\n        class_names: list[str] | None = None,\n\n        iou_threshold: float = 0.5,\n        score_threshold: float = 0.5,\n\n    ) -&gt; None:\n\n        \"\"\"\n        Initialize a new ObjectDetectionTask instance.\n\n        Args:\n            model_factory (str): Name of the model factory to use.\n            model_args (dict): Arguments for the model factory.\n            lr (float, optional): Learning rate for optimizer. Defaults to 0.001.\n            optimizer (str | None, optional): Name of the optimizer to use. Defaults to None.\n            optimizer_hparams (dict | None, optional): Hyperparameters for the optimizer. Defaults to None.\n            scheduler (str | None, optional): Name of the scheduler to use. Defaults to None.\n            scheduler_hparams (dict | None, optional): Hyperparameters for the scheduler. Defaults to None.\n            freeze_backbone (bool, optional): Freeze the backbone network to fine-tune the detection head. Defaults to False.\n            freeze_decoder (bool, optional): Freeze the decoder network to fine-tune the detection head. Defaults to False.\n            class_names (list[str] | None, optional): List of class names. Defaults to None.\n            iou_threshold (float, optional): Intersection over union threshold for evaluation. Defaults to 0.5.\n            score_threshold (float, optional): Score threshold for evaluation. Defaults to 0.5.\n\n        Returns:\n            None\n        \"\"\"\n        warnings.warn(\"The Object Detection Task has to be considered experimental. This is less mature than the other tasks and being further improved.\")\n\n        self.model_factory = MODEL_FACTORY_REGISTRY.build(model_factory)\n        self.framework = model_args['framework']\n        self.monitor = 'val_segm_map' if self.framework == 'mask-rcnn' else self.monitor\n\n        super().__init__()\n        self.train_loss_handler = LossHandler(self.train_metrics.prefix)\n        self.test_loss_handler = LossHandler(self.test_metrics.prefix)\n        self.val_loss_handler = LossHandler(self.val_metrics.prefix)\n        self.iou_threshold = iou_threshold\n        self.score_threshold = score_threshold\n        self.lr = lr\n        if optimizer_hparams is not None:\n            if \"lr\" in self.hparams[\"optimizer_hparams\"].keys():\n                self.lr = float(self.hparams[\"optimizer_hparams\"][\"lr\"])\n                del self.hparams[\"optimizer_hparams\"][\"lr\"]\n\n\n\n    def configure_models(self) -&gt; None:\n        \"\"\"\n        It instantiates the model and freezes/unfreezes the backbone and decoder networks.\n        \"\"\"\n\n        self.model: Model = self.model_factory.build_model(\n            \"object_detection\", **self.hparams[\"model_args\"]\n        )\n        if self.hparams[\"freeze_backbone\"]:\n            self.model.freeze_encoder()\n        if self.hparams[\"freeze_decoder\"]:\n            self.model.freeze_decoder()\n\n    def configure_metrics(self) -&gt; None:\n        \"\"\"\n        Configure metrics for the task.\n        \"\"\"\n        if self.framework == 'mask-rcnn':\n            metrics = MetricCollection([MeanAveragePrecision(iou_type=('bbox', 'segm'))])\n        else:\n            metrics = MetricCollection([MeanAveragePrecision(average='macro')])\n\n        self.train_metrics = metrics.clone(prefix='train_')\n        self.val_metrics = metrics.clone(prefix='val_')\n        self.test_metrics = metrics.clone(prefix='test_')\n\n    def configure_optimizers(\n        self,\n    ) -&gt; \"lightning.pytorch.utilities.types.OptimizerLRSchedulerConfig\":\n        \"\"\"\n        Configure optimiser for the task.\n        \"\"\"\n        optimizer = self.hparams[\"optimizer\"]\n        if optimizer is None:\n            optimizer = \"Adam\"\n        return optimizer_factory(\n            optimizer,\n            self.lr,\n            self.parameters(),\n            self.hparams[\"optimizer_hparams\"],\n            self.hparams[\"scheduler\"],\n            self.monitor,\n            self.hparams[\"scheduler_hparams\"],\n        )\n\n    def reformat_batch(self, batch: Any, batch_size: int):\n        \"\"\"\n        Reformat batch to calculate loss and metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_size: Size of your batch\n        Returns:\n            Reformated batch\n        \"\"\"\n\n        if 'masks' in batch.keys():\n            y = [\n                {'boxes': batch['boxes'][i], 'labels': batch['labels'][i], 'masks': torch.cat([x[None].to(torch.uint8) for x in batch['masks'][i]])}\n                for i in range(batch_size)\n            ]\n        else:\n\n            y = [\n                {'boxes': batch['boxes'][i], 'labels': batch['labels'][i]}\n                for i in range(batch_size)\n            ]\n\n        return y\n\n    def apply_nms_sample(self, y_hat, iou_threshold=0.5, score_threshold=0.5):\n        \"\"\"\n        It applies nms to a sample predictions of the model.\n\n        Args:\n            y_hat: Predictions dictionary.\n            iou_threshold: IoU threshold for evaluation.\n            score_threshold: Score threshold for evaluation.\n        Returns:\n            fintered predictions for a sample after applying nms batch\n        \"\"\"\n\n        boxes, scores, labels = y_hat['boxes'], y_hat['scores'], y_hat['labels']\n        masks = y_hat['masks'] if \"masks\" in y_hat.keys() else None\n\n        # Filter based on score threshold\n        keep_score = scores &gt; score_threshold\n        boxes, scores, labels = boxes[keep_score], scores[keep_score], labels[keep_score]\n        if masks is not None:\n            masks = masks[keep_score]\n\n        # Apply NMS\n        keep_nms = nms(boxes, scores, iou_threshold)\n\n        y_hat['boxes'], y_hat['scores'], y_hat['labels'] = boxes[keep_nms], scores[keep_nms], labels[keep_nms]\n\n        if masks is not None:\n            y_hat['masks'] = masks[keep_nms]\n\n        return y_hat\n\n    def apply_nms_batch(self, y_hat: Any, batch_size: int):\n        \"\"\"\n        It applies nms to a batch predictions of the model.\n\n        Args:\n            y_hat: List of predictions dictionaries.\n            iou_threshold: IoU threshold for evaluation.\n            score_threshold: Score threshold for evaluation.\n        Returns:\n            fintered predictions for a batch after applying nms batch\n        \"\"\"\n\n        for i in range(batch_size):\n            y_hat[i] = self.apply_nms_sample(y_hat[i], iou_threshold=self.iou_threshold, score_threshold=self.score_threshold)\n\n        return y_hat\n\n    def training_step(\n        self, batch: Any, batch_idx: int, dataloader_idx: int = 0\n    ) -&gt; Tensor:\n        \"\"\"\n        Compute the training loss.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n\n        Returns:\n            The loss dictionary.\n        \"\"\"\n\n        x = batch['image']\n        batch_size = x.shape[0]\n        y = self.reformat_batch(batch, batch_size)\n        loss_dict = self(x, y)\n        if isinstance(loss_dict, dict) is False:\n            loss_dict = loss_dict.output\n        train_loss: Tensor = sum(loss_dict.values())\n        self.log_dict(loss_dict, batch_size=batch_size)\n        self.log(\"train_loss\", train_loss)\n        return train_loss\n\n    def validation_step(\n        self, batch: Any, batch_idx: int, dataloader_idx: int = 0\n    ) -&gt; None:\n        \"\"\"\n        Compute the validation metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n\n        x = batch['image']\n        batch_size = x.shape[0]\n        y = self.reformat_batch(batch, batch_size)\n        y_hat = self(x)\n        if isinstance(y_hat, dict) is False:\n            y_hat = y_hat.output\n\n        y_hat = self.apply_nms_batch(y_hat, batch_size)\n\n        if self.framework == 'mask-rcnn':\n\n            for i in range(len(y_hat)):\n                if y_hat[i]['masks'].shape[0] &gt; 0:\n\n                    y_hat[i]['masks']= (y_hat[i]['masks'] &gt; 0.5).squeeze(1).to(torch.uint8)\n\n        metrics = self.val_metrics(y_hat, y) \n\n        # https://github.com/Lightning-AI/torchmetrics/pull/1832#issuecomment-1623890714\n        metrics.pop('val_classes', None)\n\n\n        self.log_dict(metrics, batch_size=batch_size)\n\n        if (\n            batch_idx &lt; 10\n            and hasattr(self.trainer, 'datamodule')\n            and hasattr(self.trainer.datamodule, 'plot')\n            and self.logger\n            and hasattr(self.logger, 'experiment')\n            and hasattr(self.logger.experiment, 'add_figure')\n        ):\n\n            dataset = self.trainer.datamodule.val_dataset\n            batch['prediction_boxes'] = [b['boxes'].cpu() for b in y_hat]\n            batch['prediction_labels'] = [b['labels'].cpu() for b in y_hat]\n            batch['prediction_scores'] = [b['scores'].cpu() for b in y_hat]\n\n            if \"masks\" in y_hat[0].keys():\n                batch['prediction_masks'] = [b['masks'].cpu() for b in y_hat]\n                if self.framework == 'mask-rcnn':\n                    batch['prediction_masks'] = [b.unsqueeze(1) for b in batch['prediction_masks']]\n\n            batch['image'] = batch['image'].cpu()\n            sample = unbind_samples(batch)[0]\n            fig: Figure | None = None\n            try:\n                fig = dataset.plot(sample)\n            except RGBBandsMissingError:\n                pass\n\n            if fig:\n                summary_writer = self.logger.experiment\n                summary_writer.add_figure(\n                    f'image/{batch_idx}', fig, global_step=self.global_step\n                )\n                plt.close()\n\n    def test_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        \"\"\"\n        Compute the test metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n\n        x = batch['image']\n        batch_size = x.shape[0]\n        y = self.reformat_batch(batch, batch_size)\n        y_hat = self(x)\n        if isinstance(y_hat, dict) is False:\n            y_hat = y_hat.output\n\n        y_hat = self.apply_nms_batch(y_hat, batch_size)\n\n        if self.framework == 'mask-rcnn':\n\n            for i in range(len(y_hat)):\n                if y_hat[i]['masks'].shape[0] &gt; 0:\n                    y_hat[i]['masks']= (y_hat[i]['masks'] &gt; 0.5).squeeze(1).to(torch.uint8)\n\n\n        metrics = self.test_metrics(y_hat, y)\n\n        # https://github.com/Lightning-AI/torchmetrics/pull/1832#issuecomment-1623890714\n        metrics.pop('test_classes', None)\n\n        self.log_dict(metrics, batch_size=batch_size)\n\n    def predict_step(\n        self, batch: Any, batch_idx: int, dataloader_idx: int = 0\n    ) -&gt; list[dict[str, Tensor]]:\n        \"\"\"\n        Output predicted bounding boxes, classes and masks.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n\n        Returns:\n            Output predicted bounding boxes, classes and masks.\n        \"\"\"\n        x = batch['image']\n        batch_size = x.shape[0]\n        y_hat: list[dict[str, Tensor]] = self(x)\n        if isinstance(y_hat, dict) is False:\n            y_hat = y_hat.output\n\n        y_hat = self.apply_nms_batch(y_hat, batch_size)\n\n        return y_hat\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ObjectDetectionTask.__init__","title":"<code>__init__(model_factory, model_args, lr=0.001, optimizer=None, optimizer_hparams=None, scheduler=None, scheduler_hparams=None, freeze_backbone=False, freeze_decoder=False, class_names=None, iou_threshold=0.5, score_threshold=0.5)</code>","text":"<p>Initialize a new ObjectDetectionTask instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_factory</code> <code>str</code> <p>Name of the model factory to use.</p> required <code>model_args</code> <code>dict</code> <p>Arguments for the model factory.</p> required <code>lr</code> <code>float</code> <p>Learning rate for optimizer. Defaults to 0.001.</p> <code>0.001</code> <code>optimizer</code> <code>str | None</code> <p>Name of the optimizer to use. Defaults to None.</p> <code>None</code> <code>optimizer_hparams</code> <code>dict | None</code> <p>Hyperparameters for the optimizer. Defaults to None.</p> <code>None</code> <code>scheduler</code> <code>str | None</code> <p>Name of the scheduler to use. Defaults to None.</p> <code>None</code> <code>scheduler_hparams</code> <code>dict | None</code> <p>Hyperparameters for the scheduler. Defaults to None.</p> <code>None</code> <code>freeze_backbone</code> <code>bool</code> <p>Freeze the backbone network to fine-tune the detection head. Defaults to False.</p> <code>False</code> <code>freeze_decoder</code> <code>bool</code> <p>Freeze the decoder network to fine-tune the detection head. Defaults to False.</p> <code>False</code> <code>class_names</code> <code>list[str] | None</code> <p>List of class names. Defaults to None.</p> <code>None</code> <code>iou_threshold</code> <code>float</code> <p>Intersection over union threshold for evaluation. Defaults to 0.5.</p> <code>0.5</code> <code>score_threshold</code> <code>float</code> <p>Score threshold for evaluation. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>terratorch/tasks/object_detection_task.py</code> <pre><code>def __init__(\n    self,\n    model_factory: str,\n    model_args: dict,\n\n    lr: float = 0.001,\n\n    optimizer: str | None = None,\n    optimizer_hparams: dict | None = None,\n    scheduler: str | None = None,\n    scheduler_hparams: dict | None = None,\n\n    freeze_backbone: bool = False,\n    freeze_decoder: bool = False,\n    class_names: list[str] | None = None,\n\n    iou_threshold: float = 0.5,\n    score_threshold: float = 0.5,\n\n) -&gt; None:\n\n    \"\"\"\n    Initialize a new ObjectDetectionTask instance.\n\n    Args:\n        model_factory (str): Name of the model factory to use.\n        model_args (dict): Arguments for the model factory.\n        lr (float, optional): Learning rate for optimizer. Defaults to 0.001.\n        optimizer (str | None, optional): Name of the optimizer to use. Defaults to None.\n        optimizer_hparams (dict | None, optional): Hyperparameters for the optimizer. Defaults to None.\n        scheduler (str | None, optional): Name of the scheduler to use. Defaults to None.\n        scheduler_hparams (dict | None, optional): Hyperparameters for the scheduler. Defaults to None.\n        freeze_backbone (bool, optional): Freeze the backbone network to fine-tune the detection head. Defaults to False.\n        freeze_decoder (bool, optional): Freeze the decoder network to fine-tune the detection head. Defaults to False.\n        class_names (list[str] | None, optional): List of class names. Defaults to None.\n        iou_threshold (float, optional): Intersection over union threshold for evaluation. Defaults to 0.5.\n        score_threshold (float, optional): Score threshold for evaluation. Defaults to 0.5.\n\n    Returns:\n        None\n    \"\"\"\n    warnings.warn(\"The Object Detection Task has to be considered experimental. This is less mature than the other tasks and being further improved.\")\n\n    self.model_factory = MODEL_FACTORY_REGISTRY.build(model_factory)\n    self.framework = model_args['framework']\n    self.monitor = 'val_segm_map' if self.framework == 'mask-rcnn' else self.monitor\n\n    super().__init__()\n    self.train_loss_handler = LossHandler(self.train_metrics.prefix)\n    self.test_loss_handler = LossHandler(self.test_metrics.prefix)\n    self.val_loss_handler = LossHandler(self.val_metrics.prefix)\n    self.iou_threshold = iou_threshold\n    self.score_threshold = score_threshold\n    self.lr = lr\n    if optimizer_hparams is not None:\n        if \"lr\" in self.hparams[\"optimizer_hparams\"].keys():\n            self.lr = float(self.hparams[\"optimizer_hparams\"][\"lr\"])\n            del self.hparams[\"optimizer_hparams\"][\"lr\"]\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ObjectDetectionTask.apply_nms_batch","title":"<code>apply_nms_batch(y_hat, batch_size)</code>","text":"<p>It applies nms to a batch predictions of the model.</p> <p>Parameters:</p> Name Type Description Default <code>y_hat</code> <code>Any</code> <p>List of predictions dictionaries.</p> required <code>iou_threshold</code> <p>IoU threshold for evaluation.</p> required <code>score_threshold</code> <p>Score threshold for evaluation.</p> required <p>Returns:     fintered predictions for a batch after applying nms batch</p> Source code in <code>terratorch/tasks/object_detection_task.py</code> <pre><code>def apply_nms_batch(self, y_hat: Any, batch_size: int):\n    \"\"\"\n    It applies nms to a batch predictions of the model.\n\n    Args:\n        y_hat: List of predictions dictionaries.\n        iou_threshold: IoU threshold for evaluation.\n        score_threshold: Score threshold for evaluation.\n    Returns:\n        fintered predictions for a batch after applying nms batch\n    \"\"\"\n\n    for i in range(batch_size):\n        y_hat[i] = self.apply_nms_sample(y_hat[i], iou_threshold=self.iou_threshold, score_threshold=self.score_threshold)\n\n    return y_hat\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ObjectDetectionTask.apply_nms_sample","title":"<code>apply_nms_sample(y_hat, iou_threshold=0.5, score_threshold=0.5)</code>","text":"<p>It applies nms to a sample predictions of the model.</p> <p>Parameters:</p> Name Type Description Default <code>y_hat</code> <p>Predictions dictionary.</p> required <code>iou_threshold</code> <p>IoU threshold for evaluation.</p> <code>0.5</code> <code>score_threshold</code> <p>Score threshold for evaluation.</p> <code>0.5</code> <p>Returns:     fintered predictions for a sample after applying nms batch</p> Source code in <code>terratorch/tasks/object_detection_task.py</code> <pre><code>def apply_nms_sample(self, y_hat, iou_threshold=0.5, score_threshold=0.5):\n    \"\"\"\n    It applies nms to a sample predictions of the model.\n\n    Args:\n        y_hat: Predictions dictionary.\n        iou_threshold: IoU threshold for evaluation.\n        score_threshold: Score threshold for evaluation.\n    Returns:\n        fintered predictions for a sample after applying nms batch\n    \"\"\"\n\n    boxes, scores, labels = y_hat['boxes'], y_hat['scores'], y_hat['labels']\n    masks = y_hat['masks'] if \"masks\" in y_hat.keys() else None\n\n    # Filter based on score threshold\n    keep_score = scores &gt; score_threshold\n    boxes, scores, labels = boxes[keep_score], scores[keep_score], labels[keep_score]\n    if masks is not None:\n        masks = masks[keep_score]\n\n    # Apply NMS\n    keep_nms = nms(boxes, scores, iou_threshold)\n\n    y_hat['boxes'], y_hat['scores'], y_hat['labels'] = boxes[keep_nms], scores[keep_nms], labels[keep_nms]\n\n    if masks is not None:\n        y_hat['masks'] = masks[keep_nms]\n\n    return y_hat\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ObjectDetectionTask.configure_metrics","title":"<code>configure_metrics()</code>","text":"<p>Configure metrics for the task.</p> Source code in <code>terratorch/tasks/object_detection_task.py</code> <pre><code>def configure_metrics(self) -&gt; None:\n    \"\"\"\n    Configure metrics for the task.\n    \"\"\"\n    if self.framework == 'mask-rcnn':\n        metrics = MetricCollection([MeanAveragePrecision(iou_type=('bbox', 'segm'))])\n    else:\n        metrics = MetricCollection([MeanAveragePrecision(average='macro')])\n\n    self.train_metrics = metrics.clone(prefix='train_')\n    self.val_metrics = metrics.clone(prefix='val_')\n    self.test_metrics = metrics.clone(prefix='test_')\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ObjectDetectionTask.configure_models","title":"<code>configure_models()</code>","text":"<p>It instantiates the model and freezes/unfreezes the backbone and decoder networks.</p> Source code in <code>terratorch/tasks/object_detection_task.py</code> <pre><code>def configure_models(self) -&gt; None:\n    \"\"\"\n    It instantiates the model and freezes/unfreezes the backbone and decoder networks.\n    \"\"\"\n\n    self.model: Model = self.model_factory.build_model(\n        \"object_detection\", **self.hparams[\"model_args\"]\n    )\n    if self.hparams[\"freeze_backbone\"]:\n        self.model.freeze_encoder()\n    if self.hparams[\"freeze_decoder\"]:\n        self.model.freeze_decoder()\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ObjectDetectionTask.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure optimiser for the task.</p> Source code in <code>terratorch/tasks/object_detection_task.py</code> <pre><code>def configure_optimizers(\n    self,\n) -&gt; \"lightning.pytorch.utilities.types.OptimizerLRSchedulerConfig\":\n    \"\"\"\n    Configure optimiser for the task.\n    \"\"\"\n    optimizer = self.hparams[\"optimizer\"]\n    if optimizer is None:\n        optimizer = \"Adam\"\n    return optimizer_factory(\n        optimizer,\n        self.lr,\n        self.parameters(),\n        self.hparams[\"optimizer_hparams\"],\n        self.hparams[\"scheduler\"],\n        self.monitor,\n        self.hparams[\"scheduler_hparams\"],\n    )\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ObjectDetectionTask.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Output predicted bounding boxes, classes and masks.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[dict[str, Tensor]]</code> <p>Output predicted bounding boxes, classes and masks.</p> Source code in <code>terratorch/tasks/object_detection_task.py</code> <pre><code>def predict_step(\n    self, batch: Any, batch_idx: int, dataloader_idx: int = 0\n) -&gt; list[dict[str, Tensor]]:\n    \"\"\"\n    Output predicted bounding boxes, classes and masks.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n\n    Returns:\n        Output predicted bounding boxes, classes and masks.\n    \"\"\"\n    x = batch['image']\n    batch_size = x.shape[0]\n    y_hat: list[dict[str, Tensor]] = self(x)\n    if isinstance(y_hat, dict) is False:\n        y_hat = y_hat.output\n\n    y_hat = self.apply_nms_batch(y_hat, batch_size)\n\n    return y_hat\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ObjectDetectionTask.reformat_batch","title":"<code>reformat_batch(batch, batch_size)</code>","text":"<p>Reformat batch to calculate loss and metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_size</code> <code>int</code> <p>Size of your batch</p> required <p>Returns:     Reformated batch</p> Source code in <code>terratorch/tasks/object_detection_task.py</code> <pre><code>def reformat_batch(self, batch: Any, batch_size: int):\n    \"\"\"\n    Reformat batch to calculate loss and metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_size: Size of your batch\n    Returns:\n        Reformated batch\n    \"\"\"\n\n    if 'masks' in batch.keys():\n        y = [\n            {'boxes': batch['boxes'][i], 'labels': batch['labels'][i], 'masks': torch.cat([x[None].to(torch.uint8) for x in batch['masks'][i]])}\n            for i in range(batch_size)\n        ]\n    else:\n\n        y = [\n            {'boxes': batch['boxes'][i], 'labels': batch['labels'][i]}\n            for i in range(batch_size)\n        ]\n\n    return y\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ObjectDetectionTask.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the test metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/object_detection_task.py</code> <pre><code>def test_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n    \"\"\"\n    Compute the test metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n\n    x = batch['image']\n    batch_size = x.shape[0]\n    y = self.reformat_batch(batch, batch_size)\n    y_hat = self(x)\n    if isinstance(y_hat, dict) is False:\n        y_hat = y_hat.output\n\n    y_hat = self.apply_nms_batch(y_hat, batch_size)\n\n    if self.framework == 'mask-rcnn':\n\n        for i in range(len(y_hat)):\n            if y_hat[i]['masks'].shape[0] &gt; 0:\n                y_hat[i]['masks']= (y_hat[i]['masks'] &gt; 0.5).squeeze(1).to(torch.uint8)\n\n\n    metrics = self.test_metrics(y_hat, y)\n\n    # https://github.com/Lightning-AI/torchmetrics/pull/1832#issuecomment-1623890714\n    metrics.pop('test_classes', None)\n\n    self.log_dict(metrics, batch_size=batch_size)\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ObjectDetectionTask.training_step","title":"<code>training_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the training loss.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The loss dictionary.</p> Source code in <code>terratorch/tasks/object_detection_task.py</code> <pre><code>def training_step(\n    self, batch: Any, batch_idx: int, dataloader_idx: int = 0\n) -&gt; Tensor:\n    \"\"\"\n    Compute the training loss.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n\n    Returns:\n        The loss dictionary.\n    \"\"\"\n\n    x = batch['image']\n    batch_size = x.shape[0]\n    y = self.reformat_batch(batch, batch_size)\n    loss_dict = self(x, y)\n    if isinstance(loss_dict, dict) is False:\n        loss_dict = loss_dict.output\n    train_loss: Tensor = sum(loss_dict.values())\n    self.log_dict(loss_dict, batch_size=batch_size)\n    self.log(\"train_loss\", train_loss)\n    return train_loss\n</code></pre>"},{"location":"package/tasks/#terratorch.tasks.ObjectDetectionTask.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the validation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/object_detection_task.py</code> <pre><code>def validation_step(\n    self, batch: Any, batch_idx: int, dataloader_idx: int = 0\n) -&gt; None:\n    \"\"\"\n    Compute the validation metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n\n    x = batch['image']\n    batch_size = x.shape[0]\n    y = self.reformat_batch(batch, batch_size)\n    y_hat = self(x)\n    if isinstance(y_hat, dict) is False:\n        y_hat = y_hat.output\n\n    y_hat = self.apply_nms_batch(y_hat, batch_size)\n\n    if self.framework == 'mask-rcnn':\n\n        for i in range(len(y_hat)):\n            if y_hat[i]['masks'].shape[0] &gt; 0:\n\n                y_hat[i]['masks']= (y_hat[i]['masks'] &gt; 0.5).squeeze(1).to(torch.uint8)\n\n    metrics = self.val_metrics(y_hat, y) \n\n    # https://github.com/Lightning-AI/torchmetrics/pull/1832#issuecomment-1623890714\n    metrics.pop('val_classes', None)\n\n\n    self.log_dict(metrics, batch_size=batch_size)\n\n    if (\n        batch_idx &lt; 10\n        and hasattr(self.trainer, 'datamodule')\n        and hasattr(self.trainer.datamodule, 'plot')\n        and self.logger\n        and hasattr(self.logger, 'experiment')\n        and hasattr(self.logger.experiment, 'add_figure')\n    ):\n\n        dataset = self.trainer.datamodule.val_dataset\n        batch['prediction_boxes'] = [b['boxes'].cpu() for b in y_hat]\n        batch['prediction_labels'] = [b['labels'].cpu() for b in y_hat]\n        batch['prediction_scores'] = [b['scores'].cpu() for b in y_hat]\n\n        if \"masks\" in y_hat[0].keys():\n            batch['prediction_masks'] = [b['masks'].cpu() for b in y_hat]\n            if self.framework == 'mask-rcnn':\n                batch['prediction_masks'] = [b.unsqueeze(1) for b in batch['prediction_masks']]\n\n        batch['image'] = batch['image'].cpu()\n        sample = unbind_samples(batch)[0]\n        fig: Figure | None = None\n        try:\n            fig = dataset.plot(sample)\n        except RGBBandsMissingError:\n            pass\n\n        if fig:\n            summary_writer = self.logger.experiment\n            summary_writer.add_figure(\n                f'image/{batch_idx}', fig, global_step=self.global_step\n            )\n            plt.close()\n</code></pre>"},{"location":"package/transforms/","title":"Transforms","text":""},{"location":"package/transforms/#terratorch.datasets.transforms.FlattenSamplesIntoChannels","title":"<code>FlattenSamplesIntoChannels</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>FlattenSamplesIntoChannels is an image transformation that merges the sample (and optionally temporal) dimensions into the channel dimension.</p> <p>This transform rearranges an input tensor by flattening the sample dimension, and if specified, also the temporal dimension, thereby concatenating these dimensions into a single channel dimension.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class FlattenSamplesIntoChannels(ImageOnlyTransform):\n    \"\"\"\n    FlattenSamplesIntoChannels is an image transformation that merges the sample (and optionally temporal) dimensions into the channel dimension.\n\n    This transform rearranges an input tensor by flattening the sample dimension, and if specified, also the temporal dimension,\n    thereby concatenating these dimensions into a single channel dimension.\n    \"\"\"\n    def __init__(self, time_dim: bool = True):\n        \"\"\"\n        Initialize the FlattenSamplesIntoChannels transform.\n\n        Args:\n            time_dim (bool): If True, the temporal dimension is included in the flattening process. Default is True.\n        \"\"\"\n        super().__init__(True, 1)\n        self.time_dim = time_dim\n\n    def apply(self, img, **params):\n        if self.time_dim:\n            rearranged = rearrange(img,\n                                   \"samples time height width channels -&gt; height width (samples time channels)\")\n        else:\n            rearranged = rearrange(img, \"samples height width channels -&gt; height width (samples channels)\")\n        return rearranged\n\n    def get_transform_init_args_names(self):\n        return ()\n</code></pre>"},{"location":"package/transforms/#terratorch.datasets.transforms.FlattenSamplesIntoChannels.__init__","title":"<code>__init__(time_dim=True)</code>","text":"<p>Initialize the FlattenSamplesIntoChannels transform.</p> <p>Parameters:</p> Name Type Description Default <code>time_dim</code> <code>bool</code> <p>If True, the temporal dimension is included in the flattening process. Default is True.</p> <code>True</code> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>def __init__(self, time_dim: bool = True):\n    \"\"\"\n    Initialize the FlattenSamplesIntoChannels transform.\n\n    Args:\n        time_dim (bool): If True, the temporal dimension is included in the flattening process. Default is True.\n    \"\"\"\n    super().__init__(True, 1)\n    self.time_dim = time_dim\n</code></pre>"},{"location":"package/transforms/#terratorch.datasets.transforms.FlattenTemporalIntoChannels","title":"<code>FlattenTemporalIntoChannels</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>FlattenTemporalIntoChannels is an image transformation that flattens the temporal dimension into the channel dimension.</p> <p>This transform rearranges an input tensor with a temporal dimension into one where the time and channel dimensions are merged. It expects the input to have a fixed number of dimensions defined by N_DIMS_FOR_TEMPORAL.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class FlattenTemporalIntoChannels(ImageOnlyTransform):\n    \"\"\"\n    FlattenTemporalIntoChannels is an image transformation that flattens the temporal dimension into the channel dimension.\n\n    This transform rearranges an input tensor with a temporal dimension into one where the time and channel dimensions\n    are merged. It expects the input to have a fixed number of dimensions defined by N_DIMS_FOR_TEMPORAL.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the FlattenTemporalIntoChannels transform.\n        \"\"\"\n        super().__init__(True, 1)\n\n    def apply(self, img, **params):\n        if len(img.shape) != N_DIMS_FOR_TEMPORAL:\n            msg = f\"Expected input temporal sequence to have {N_DIMS_FOR_TEMPORAL} dimensions, but got {len(img.shape)}\"\n            raise Exception(msg)\n        rearranged = rearrange(img, \"time height width channels -&gt; height width (time channels)\")\n        return rearranged\n\n    def get_transform_init_args_names(self):\n        return ()\n</code></pre>"},{"location":"package/transforms/#terratorch.datasets.transforms.FlattenTemporalIntoChannels.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the FlattenTemporalIntoChannels transform.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the FlattenTemporalIntoChannels transform.\n    \"\"\"\n    super().__init__(True, 1)\n</code></pre>"},{"location":"package/transforms/#terratorch.datasets.transforms.MultimodalTransforms","title":"<code>MultimodalTransforms</code>","text":"<p>MultimodalTransforms applies albumentations transforms to multiple image modalities.</p> <p>This class supports both shared transformations across modalities and separate transformations for each modality. It also handles non-image modalities by applying a specified non-image transform.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class MultimodalTransforms:\n    \"\"\"\n    MultimodalTransforms applies albumentations transforms to multiple image modalities.\n\n    This class supports both shared transformations across modalities and separate transformations for each modality.\n    It also handles non-image modalities by applying a specified non-image transform.\n    \"\"\"\n    def __init__(\n            self,\n            transforms: dict | A.Compose,\n            shared : bool = True,\n            non_image_modalities: list[str] | None = None,\n            non_image_transform: object | None = None,\n    ):\n        \"\"\"\n        Initialize the MultimodalTransforms.\n\n        Args:\n            transforms (dict or A.Compose): The transformation(s) to apply to the data.\n            shared (bool): If True, the same transform is applied to all modalities; if False, separate transforms are used.\n            non_image_modalities (list[str] | None): List of keys corresponding to non-image modalities.\n            non_image_transform (object | None): A transform to apply to non-image modalities. If None, a default transform is used.\n        \"\"\"\n        self.transforms = transforms\n        self.shared = shared\n        self.non_image_modalities = non_image_modalities\n        self.non_image_transform = non_image_transform or default_non_image_transform\n\n    def __call__(self, data: dict):\n        if self.shared:\n            # albumentations requires a key 'image' and treats all other keys as additional targets\n            # (+ don't select 'mask' as 'image')\n            image_modality = [k for k in data.keys() if k not in self.non_image_modalities + ['mask']][0]\n            data['image'] = data.pop(image_modality)\n            data = self.transforms(**data)\n            data[image_modality] = data.pop('image')\n\n            # Process sequence data which is ignored by albumentations as 'global_label'\n            for modality in self.non_image_modalities:\n                data[modality] = self.non_image_transform(data[modality])\n        else:\n            # Applies transformations for each modality separate\n            for key, value in data.items():\n                data[key] = self.transforms[key](image=value)['image']  # Only works with image modalities\n\n        return data\n</code></pre>"},{"location":"package/transforms/#terratorch.datasets.transforms.MultimodalTransforms.__init__","title":"<code>__init__(transforms, shared=True, non_image_modalities=None, non_image_transform=None)</code>","text":"<p>Initialize the MultimodalTransforms.</p> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>dict or Compose</code> <p>The transformation(s) to apply to the data.</p> required <code>shared</code> <code>bool</code> <p>If True, the same transform is applied to all modalities; if False, separate transforms are used.</p> <code>True</code> <code>non_image_modalities</code> <code>list[str] | None</code> <p>List of keys corresponding to non-image modalities.</p> <code>None</code> <code>non_image_transform</code> <code>object | None</code> <p>A transform to apply to non-image modalities. If None, a default transform is used.</p> <code>None</code> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>def __init__(\n        self,\n        transforms: dict | A.Compose,\n        shared : bool = True,\n        non_image_modalities: list[str] | None = None,\n        non_image_transform: object | None = None,\n):\n    \"\"\"\n    Initialize the MultimodalTransforms.\n\n    Args:\n        transforms (dict or A.Compose): The transformation(s) to apply to the data.\n        shared (bool): If True, the same transform is applied to all modalities; if False, separate transforms are used.\n        non_image_modalities (list[str] | None): List of keys corresponding to non-image modalities.\n        non_image_transform (object | None): A transform to apply to non-image modalities. If None, a default transform is used.\n    \"\"\"\n    self.transforms = transforms\n    self.shared = shared\n    self.non_image_modalities = non_image_modalities\n    self.non_image_transform = non_image_transform or default_non_image_transform\n</code></pre>"},{"location":"package/transforms/#terratorch.datasets.transforms.Padding","title":"<code>Padding</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>Padding to adjust (slight) discrepancies between input images</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class Padding(ImageOnlyTransform):\n    \"\"\"Padding to adjust (slight) discrepancies between input images\"\"\"\n\n    def __init__(self, input_shape: list=None):\n        super().__init__(True, 1)\n        self.input_shape = input_shape\n\n    def apply(self, img, **params):\n\n        shape = img.shape[-2:]\n        pad_values_ = [j - i for i,j in zip(shape, self.input_shape)]\n\n        if all([i%2==0 for i in pad_values_]):\n            pad_values = sum([[int(j/2), int(j/2)] for j in  pad_values_], [])\n        else:\n            pad_values = sum([[0, j] for j in  pad_values_], [])\n\n        return F.pad(img, pad_values)\n\n    def get_transform_init_args_names(self):\n        return ()\n</code></pre>"},{"location":"package/transforms/#terratorch.datasets.transforms.Rearrange","title":"<code>Rearrange</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>Rearrange is a generic image transformation that reshapes an input tensor using a custom einops pattern.</p> <p>This transform allows flexible reordering of tensor dimensions based on the provided pattern and arguments.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class Rearrange(ImageOnlyTransform):\n    \"\"\"\n    Rearrange is a generic image transformation that reshapes an input tensor using a custom einops pattern.\n\n    This transform allows flexible reordering of tensor dimensions based on the provided pattern and arguments.\n    \"\"\"\n\n    def __init__(\n        self, rearrange: str, rearrange_args: dict[str, int] | None = None, always_apply: bool = True, p: float = 1\n    ):\n        \"\"\"\n        Initialize the Rearrange transform.\n\n        Args:\n            rearrange (str): The einops rearrangement pattern to apply.\n            rearrange_args (dict[str, int] | None): Additional arguments for the rearrangement pattern.\n            always_apply (bool): Whether to always apply this transform. Default is True.\n            p (float): The probability of applying the transform. Default is 1.\n        \"\"\"\n        super().__init__(always_apply, p)\n        self.rearrange = rearrange\n        self.vars = rearrange_args if rearrange_args else {}\n\n    def apply(self, img, **params):\n        return rearrange(img, self.rearrange, **self.vars)\n\n    def get_transform_init_args_names(self):\n        return \"rearrange\"\n</code></pre>"},{"location":"package/transforms/#terratorch.datasets.transforms.Rearrange.__init__","title":"<code>__init__(rearrange, rearrange_args=None, always_apply=True, p=1)</code>","text":"<p>Initialize the Rearrange transform.</p> <p>Parameters:</p> Name Type Description Default <code>rearrange</code> <code>str</code> <p>The einops rearrangement pattern to apply.</p> required <code>rearrange_args</code> <code>dict[str, int] | None</code> <p>Additional arguments for the rearrangement pattern.</p> <code>None</code> <code>always_apply</code> <code>bool</code> <p>Whether to always apply this transform. Default is True.</p> <code>True</code> <code>p</code> <code>float</code> <p>The probability of applying the transform. Default is 1.</p> <code>1</code> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>def __init__(\n    self, rearrange: str, rearrange_args: dict[str, int] | None = None, always_apply: bool = True, p: float = 1\n):\n    \"\"\"\n    Initialize the Rearrange transform.\n\n    Args:\n        rearrange (str): The einops rearrangement pattern to apply.\n        rearrange_args (dict[str, int] | None): Additional arguments for the rearrangement pattern.\n        always_apply (bool): Whether to always apply this transform. Default is True.\n        p (float): The probability of applying the transform. Default is 1.\n    \"\"\"\n    super().__init__(always_apply, p)\n    self.rearrange = rearrange\n    self.vars = rearrange_args if rearrange_args else {}\n</code></pre>"},{"location":"package/transforms/#terratorch.datasets.transforms.SelectBands","title":"<code>SelectBands</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>SelectBands is an image transformation that selects a subset of bands (channels) from an input image.</p> <p>This transform uses specified band indices to filter and output only the desired channels from the image tensor.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class SelectBands(ImageOnlyTransform):\n    \"\"\"\n    SelectBands is an image transformation that selects a subset of bands (channels) from an input image.\n\n    This transform uses specified band indices to filter and output only the desired channels from the image tensor.\n    \"\"\"\n\n    def __init__(self, band_indices: list[int]):\n        \"\"\"\n        Initialize the SelectBands transform.\n\n        Args:\n            band_indices (list[int]): A list of indices specifying which bands to select.\n        \"\"\"\n        super().__init__(True, 1)\n        self.band_indices = band_indices\n\n    def apply(self, img, **params):\n        return img[..., self.band_indices]\n\n    def get_transform_init_args_names(self):\n        return \"band_indices\"\n</code></pre>"},{"location":"package/transforms/#terratorch.datasets.transforms.SelectBands.__init__","title":"<code>__init__(band_indices)</code>","text":"<p>Initialize the SelectBands transform.</p> <p>Parameters:</p> Name Type Description Default <code>band_indices</code> <code>list[int]</code> <p>A list of indices specifying which bands to select.</p> required Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>def __init__(self, band_indices: list[int]):\n    \"\"\"\n    Initialize the SelectBands transform.\n\n    Args:\n        band_indices (list[int]): A list of indices specifying which bands to select.\n    \"\"\"\n    super().__init__(True, 1)\n    self.band_indices = band_indices\n</code></pre>"},{"location":"package/transforms/#terratorch.datasets.transforms.UnflattenSamplesFromChannels","title":"<code>UnflattenSamplesFromChannels</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>UnflattenSamplesFromChannels is an image transformation that restores the sample (and optionally temporal) dimensions from the channel dimension.</p> <p>This transform is designed to reverse the flattening performed by FlattenSamplesIntoChannels and is typically applied after converting images to a channels-first format.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class UnflattenSamplesFromChannels(ImageOnlyTransform):\n    \"\"\"\n    UnflattenSamplesFromChannels is an image transformation that restores the sample (and optionally temporal) dimensions from the channel dimension.\n\n    This transform is designed to reverse the flattening performed by FlattenSamplesIntoChannels and is typically applied\n    after converting images to a channels-first format.\n    \"\"\"\n    def __init__(\n            self,\n            time_dim: bool = True,\n            n_samples: int | None = None,\n            n_timesteps: int | None = None,\n            n_channels: int | None = None\n    ):\n        \"\"\"\n        Initialize the UnflattenSamplesFromChannels transform.\n\n        Args:\n            time_dim (bool): If True, the temporal dimension is considered during unflattening.\n            n_samples (int | None): The number of samples.\n            n_timesteps (int | None): The number of time steps.\n            n_channels (int | None): The number of channels per time step.\n\n        Raises:\n            Exception: If time_dim is True and fewer than two of n_channels, n_timesteps, and n_samples are provided.\n            Exception: If time_dim is False and neither n_channels nor n_samples is provided.\n        \"\"\"\n        super().__init__(True, 1)\n\n        self.time_dim = time_dim\n        if self.time_dim:\n            if bool(n_channels) + bool(n_timesteps) + bool(n_samples) &lt; 2:\n                msg = \"Two of n_channels, n_timesteps, and n_channels must be provided\"\n                raise Exception(msg)\n            if n_timesteps and n_channels:\n                self.additional_info = {\"channels\": n_channels, \"time\": n_timesteps}\n            elif n_timesteps and n_samples:\n                self.additional_info = {\"time\": n_timesteps, \"samples\": n_samples}\n            else:\n                self.additional_info = {\"channels\": n_channels, \"samples\": n_samples}\n        else:\n            if n_channels is None and n_samples is None:\n                msg = \"One of n_channels or n_samples must be provided\"\n                raise Exception(msg)\n            self.additional_info = {\"channels\": n_channels} if n_channels else {\"samples\": n_samples}\n\n    def apply(self, img, **params):\n        if self.time_dim:\n            rearranged = rearrange(\n                img, \"(samples time channels) height width -&gt; samples channels time height width\",\n                **self.additional_info\n            )\n        else:\n            rearranged = rearrange(\n                img, \"(samples channels) height width -&gt; samples channels height width\", **self.additional_info\n            )\n        return rearranged\n\n    def get_transform_init_args_names(self):\n        return (\"n_timesteps\", \"n_channels\")\n</code></pre>"},{"location":"package/transforms/#terratorch.datasets.transforms.UnflattenSamplesFromChannels.__init__","title":"<code>__init__(time_dim=True, n_samples=None, n_timesteps=None, n_channels=None)</code>","text":"<p>Initialize the UnflattenSamplesFromChannels transform.</p> <p>Parameters:</p> Name Type Description Default <code>time_dim</code> <code>bool</code> <p>If True, the temporal dimension is considered during unflattening.</p> <code>True</code> <code>n_samples</code> <code>int | None</code> <p>The number of samples.</p> <code>None</code> <code>n_timesteps</code> <code>int | None</code> <p>The number of time steps.</p> <code>None</code> <code>n_channels</code> <code>int | None</code> <p>The number of channels per time step.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If time_dim is True and fewer than two of n_channels, n_timesteps, and n_samples are provided.</p> <code>Exception</code> <p>If time_dim is False and neither n_channels nor n_samples is provided.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>def __init__(\n        self,\n        time_dim: bool = True,\n        n_samples: int | None = None,\n        n_timesteps: int | None = None,\n        n_channels: int | None = None\n):\n    \"\"\"\n    Initialize the UnflattenSamplesFromChannels transform.\n\n    Args:\n        time_dim (bool): If True, the temporal dimension is considered during unflattening.\n        n_samples (int | None): The number of samples.\n        n_timesteps (int | None): The number of time steps.\n        n_channels (int | None): The number of channels per time step.\n\n    Raises:\n        Exception: If time_dim is True and fewer than two of n_channels, n_timesteps, and n_samples are provided.\n        Exception: If time_dim is False and neither n_channels nor n_samples is provided.\n    \"\"\"\n    super().__init__(True, 1)\n\n    self.time_dim = time_dim\n    if self.time_dim:\n        if bool(n_channels) + bool(n_timesteps) + bool(n_samples) &lt; 2:\n            msg = \"Two of n_channels, n_timesteps, and n_channels must be provided\"\n            raise Exception(msg)\n        if n_timesteps and n_channels:\n            self.additional_info = {\"channels\": n_channels, \"time\": n_timesteps}\n        elif n_timesteps and n_samples:\n            self.additional_info = {\"time\": n_timesteps, \"samples\": n_samples}\n        else:\n            self.additional_info = {\"channels\": n_channels, \"samples\": n_samples}\n    else:\n        if n_channels is None and n_samples is None:\n            msg = \"One of n_channels or n_samples must be provided\"\n            raise Exception(msg)\n        self.additional_info = {\"channels\": n_channels} if n_channels else {\"samples\": n_samples}\n</code></pre>"},{"location":"package/transforms/#terratorch.datasets.transforms.UnflattenTemporalFromChannels","title":"<code>UnflattenTemporalFromChannels</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>UnflattenTemporalFromChannels is an image transformation that restores the temporal dimension from the channel dimension.</p> <p>This transform is typically applied after converting images to a channels-first format (e.g., after ToTensorV2) and rearranges the flattened temporal information back into separate time and channel dimensions.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class UnflattenTemporalFromChannels(ImageOnlyTransform):\n    \"\"\"\n    UnflattenTemporalFromChannels is an image transformation that restores the temporal dimension from the channel dimension.\n\n    This transform is typically applied after converting images to a channels-first format (e.g., after ToTensorV2)\n    and rearranges the flattened temporal information back into separate time and channel dimensions.\n    \"\"\"\n\n    def __init__(self, n_timesteps: int | None = None, n_channels: int | None = None):\n        super().__init__(True, 1)\n        \"\"\"\n        Initialize the UnflattenTemporalFromChannels transform.\n\n        Args:\n            n_timesteps (int | None): The number of time steps. Must be provided if n_channels is not provided.\n            n_channels (int | None): The number of channels per time step. Must be provided if n_timesteps is not provided.\n\n        Raises:\n            Exception: If neither n_timesteps nor n_channels is provided.\n        \"\"\"\n        if n_timesteps is None and n_channels is None:\n            msg = \"One of n_timesteps or n_channels must be provided\"\n            raise Exception(msg)\n        self.additional_info = {\"channels\": n_channels} if n_channels else {\"time\": n_timesteps}\n\n    def apply(self, img, **params):\n        if len(img.shape) != N_DIMS_FLATTENED_TEMPORAL:\n            msg = f\"Expected input temporal sequence to have {N_DIMS_FLATTENED_TEMPORAL} dimensions\\\n                , but got {len(img.shape)}\"\n            raise Exception(msg)\n\n        rearranged = rearrange(\n            img, \"(time channels) height width -&gt; channels time height width\", **self.additional_info\n        )\n        return rearranged\n\n    def get_transform_init_args_names(self):\n        return (\"n_timesteps\", \"n_channels\")\n</code></pre>"},{"location":"tutorials/adding_custom_modules/","title":"How to Add Custom Modules to TerraTorch","text":"<p>TerraTorch is designed to be extensible, allowing you to integrate your own custom components, such as models (backbones, decoders), tasks, datamodules, callbacks, or augmentation transforms, into the fine-tuning pipeline. This is primarily achieved by making your custom Python code discoverable by TerraTorch and then referencing your custom classes within the YAML configuration file.</p> <p>This tutorial outlines the steps required to add and use a custom module. We'll use a simple custom model component as an example.</p>"},{"location":"tutorials/adding_custom_modules/#prerequisites","title":"Prerequisites","text":"<ul> <li>A working installation of TerraTorch.</li> <li>Your custom Python code (e.g., a new model architecture implemented as a <code>torch.nn.Module</code>).</li> </ul>"},{"location":"tutorials/adding_custom_modules/#step-1-create-your-custom-module-files","title":"Step 1: Create Your Custom Module File(s)","text":"<p>First, organize your custom code into Python files (<code>.py</code>). It's recommended to place these files within a dedicated directory. For example, let's create a directory named <code>custom_modules</code> in your project's working directory and place our custom model definition inside a file named <code>my_custom_model.py</code>:</p> <pre><code>my_project_root/\n\u251c\u2500\u2500 custom_modules/\n\u2502   \u251c\u2500\u2500 __init__.py       &lt;-- **Required** to make the directory a Python package\n\u2502   \u2514\u2500\u2500 my_custom_model.py\n\u251c\u2500\u2500 my_config.yaml\n\u2514\u2500\u2500 ... (other project files)\n</code></pre> <p>Inside <code>custom_modules/my_custom_model.py</code>, define your custom class. If you intend for TerraTorch's factories to discover this module (e.g., to use it as a backbone or decoder selected by name), you must register it using the appropriate registry decorator.</p> <p>For instance, to register a simple custom CNN as a backbone:</p> <pre><code># custom_modules/my_custom_model.py\nimport torch\nimport torch.nn as nn\n# Import the relevant registry\nfrom terratorch.registry import TERRATORCH_BACKBONE_REGISTRY\n\n# Register the class with the backbone registry\n@TERRATORCH_BACKBONE_REGISTRY.register\nclass MySimpleCNN(nn.Module):\n    # Note: Backbones typically don't take num_classes directly in __init__\n    # They output features which are then processed by a decoder/head.\n    # This example is simplified for demonstration.\n    def __init__(self, in_channels: int, out_features: int = 512): # Example: output feature size\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n        self.pool = nn.AdaptiveAvgPool2d((1, 1)) # Example pooling\n        self.fc = nn.Linear(32, out_features) # Example final layer\n        print(f\"Initialized MySimpleCNN backbone with in_channels={in_channels}, out_features={out_features}\")\n\n    def forward(self, x: torch.Tensor) -&gt; list[torch.Tensor]: # Backbones often return a list of features\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        # Return as a list, mimicking multi-stage feature outputs\n        return [x]\n\n# You could also define custom tasks, datamodules, decoders (registering with TERRATORCH_DECODER_REGISTRY), etc. here\n# from terratorch.tasks import SemanticSegmentationTask\n# class MyCustomTask(SemanticSegmentationTask):\n#     # ... override methods ...\n#     pass\n</code></pre> <p>The <code>@TERRATORCH_BACKBONE_REGISTRY.register</code> line makes <code>MySimpleCNN</code> available to be selected by name (i.e., <code>\"MySimpleCNN\"</code>) in the <code>backbone</code> field of your model configuration. Similar registries exist for decoders (<code>TERRATORCH_DECODER_REGISTRY</code>), necks, and full models.</p> <p>Make sure your custom classes inherit from appropriate base classes if needed (e.g., <code>torch.nn.Module</code> for models, <code>lightning.pytorch.LightningModule</code> or <code>terratorch.tasks.BaseTask</code> for tasks, <code>lightning.pytorch.LightningDataModule</code> for datamodules).</p>"},{"location":"tutorials/adding_custom_modules/#step-2-inform-terratorch-about-your-custom-module-directory","title":"Step 2: Inform TerraTorch About Your Custom Module Directory","text":"<p>TerraTorch needs to know where to find your custom code. You can specify the path to your custom modules directory using the <code>custom_modules_path</code> argument either in your YAML configuration file or directly via the command line.</p> <p>Option A: In YAML Configuration (<code>my_config.yaml</code>)</p> <p>Add the <code>custom_modules_path</code> key at the top level of your configuration:</p> <pre><code># my_config.yaml\ncustom_modules_path: ./custom_modules  # Path relative to where you run terratorch\n\n# ... rest of your configuration ...\n\nmodel:\n  # ... other model config ...\ndata:\n  # ... data config ...\ntrainer:\n  # ... trainer config ...\n</code></pre> <p>Option B: Via Command Line</p> <pre><code>terratorch fit --config my_config.yaml --custom_modules_path ./custom_modules\n</code></pre> <p>When provided, TerraTorch will add the specified directory (<code>./custom_modules</code> in this case) to Python's <code>sys.path</code>, making the modules within it importable.</p>"},{"location":"tutorials/adding_custom_modules/#step-3-use-your-custom-component-in-the-configuration","title":"Step 3: Use Your Custom Component in the Configuration","text":"<p>Now that TerraTorch can find and import your custom code, you can reference your custom classes in the YAML configuration.</p> <ul> <li> <p>For registered components (Recommended for backbones, decoders, etc.): If you registered your class (like <code>MySimpleCNN</code> above), you can simply use its class name as the identifier:</p> <pre><code># my_config.yaml\ncustom_modules_path: ./custom_modules\n\nmodel:\n  class_path: terratorch.models.EncoderDecoder # Example using a standard factory\n  init_args:\n    backbone: \"MySimpleCNN\" # Reference the registered custom backbone by name\n    decoder: \"UNetDecoder\" # Example using a standard decoder\n    model_args:\n      backbone_in_channels: 3 # Passed to MySimpleCNN.__init__\n      # backbone_out_features: 512 # Default or specify if needed\n      decoder_num_classes: 5 # Example number of output classes\n      # ... other args for backbone/decoder if needed\n# ... rest of config\n</code></pre> </li> <li> <p>For components not using the registry or for full class path reference: You can always reference a class using its full Python path: <code>&lt;directory_name&gt;.&lt;filename&gt;.&lt;ClassName&gt;</code>. This is useful for custom tasks, callbacks, or if you choose not to register a model component and instead specify its full path.</p> <pre><code># my_config.yaml\ncustom_modules_path: ./custom_modules\n\n# Example: Using the custom model directly via class_path (less common for backbones/decoders)\n# model:\n#   class_path: custom_modules.my_custom_model.MySimpleCNN # Using full path\n#   init_args:\n#     in_channels: 3\n#     out_features: 512\n\n# Example: Using a custom task\ntask:\n  class_path: custom_modules.my_custom_task_module.MyCustomTask # Assuming it's defined elsewhere\n  init_args:\n    # ... task args ...\n\n# Example: Using a custom callback\ntrainer:\n  callbacks:\n    - class_path: custom_modules.my_custom_callbacks.MyCallback\n      init_args:\n        # ... callback args ...\n# ... rest of config\n</code></pre> </li> </ul>"},{"location":"tutorials/adding_custom_modules/#summary","title":"Summary","text":"<p>By following these steps: 1.  Creating your custom Python code in a dedicated directory. 2.  Specifying the path to this directory using <code>custom_modules_path</code>. 3.  Referencing your custom classes via their full Python path in the YAML configuration.</p> <p>You can seamlessly integrate your own modules into the TerraTorch framework. </p>"},{"location":"tutorials/basic_inference_burn_scars/","title":"Performing an inference task with TerraTorch CLI: the Burn Scars test case","text":""},{"location":"tutorials/basic_inference_burn_scars/#step-1-download-the-test-case-from-huggingface","title":"Step 1: Download the test case from HuggingFace","text":"<p>We will use the burn scars identification test case, in which we are interested in estimating the area affected by wildfires using a finetuned model (Prithvi-EO backbone + CNN decoder). To download the complete example, do: <pre><code>git clone https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-300M-BurnScars/\n</code></pre></p>"},{"location":"tutorials/basic_inference_burn_scars/#step-2-run-the-default-inference-case","title":"Step 2: Run the default inference case","text":"<p>The example you downloaded already contains some sample images to be used as input, so you just need to go to the local repository and create a directory to save the outputs: <pre><code>cd Prithvi-EO-2.0-300M-BurnScars\nmkdir outputs\n</code></pre> and to execute a command line like: <pre><code>terratorch predict -c burn_scars_config.yaml --predict_output_dir outputs/ --data.init_args.predict_data_root examples/ --ckpt_path Prithvi_EO_V2_300M_BurnScars.pt\n</code></pre> You will see the outputs being saved in the <code>outputs</code> directory. </p>"},{"location":"tutorials/basic_inference_burn_scars/#input-image-rgb-components","title":"Input image (RGB components)","text":""},{"location":"tutorials/basic_inference_burn_scars/#predicted-mask","title":"Predicted mask","text":"<p>}</p>"},{"location":"tutorials/basic_inference_burn_scars/#more-examples","title":"More examples","text":"<p>For some examples of training using the existing tasks, check out the following pages on our github repo:</p>"},{"location":"tutorials/basic_inference_burn_scars/#from-config-files","title":"From config files","text":"<p>Under <code>examples/confs</code></p> <ul> <li> <p>Flood Segmentation with ViT: <code>sen1floods11_vit.yaml</code></p> </li> <li> <p>Flood Segmentation with ViT and an SMP head: <code>sen1floods11_vit_smp.yaml</code></p> </li> <li> <p>Flood Segmentation with ViT and an MMSeg head: <code>sen1floods11_vit_mmseg.yaml</code></p> </li> <li> <p>Multitemporal Crop Segmentation: <code>multitemporal_crop.yaml</code></p> </li> <li> <p>Burn Scar Segmentation: <code>burn_scars.yaml</code></p> </li> <li> <p>Scene Classification: <code>eurosat.yaml</code></p> </li> </ul> <p>External examples available in Prithvi-EO-2.0</p> <ul> <li>Carbon Flux</li> <li>Landslide</li> <li>Multitemporal Crop</li> </ul>"},{"location":"tutorials/burn_scars_finetuning/","title":"Creating a finetuning workload with the script interface","text":"<p>This tutorial does not intend to create an accurate finetuned example (we are running for a single epoch!), but to describe step-by-step how to instantiate and run this kind of task. </p> <pre><code>from lightning.pytorch import Trainer\nimport terratorch\nimport albumentations\nfrom albumentations.pytorch import ToTensorV2\nfrom terratorch.models import EncoderDecoderFactory\nfrom terratorch.models.necks import SelectIndices, LearnedInterpolateToPyramidal, ReshapeTokensToImage\nfrom terratorch.models.decoders import UNetDecoder\nfrom terratorch.datasets import HLSBands\nfrom terratorch.datamodules import GenericNonGeoSegmentationDataModule\nfrom terratorch.tasks import SemanticSegmentationTask\n</code></pre>"},{"location":"tutorials/burn_scars_finetuning/#defining-fundamental-parameters","title":"Defining fundamental parameters:","text":"<ul> <li><code>lr</code> - learning rate.</li> <li><code>accelerator</code> - The kind of device in which the model will be executed. It is usually <code>gpu</code> or <code>cpu</code>. If we set it as <code>auto</code>, Lightning will select the most appropiate available device.</li> <li><code>max_epochs</code> - The maximum number of epochs used to train the model. </li> </ul> <pre><code>lr = 1e-4\naccelerator = \"auto\"\nmax_epochs = 1\n</code></pre>"},{"location":"tutorials/burn_scars_finetuning/#next-we-will-instantiate-the-datamodule-the-object-we-will-use-to-load-the-files-from-disk-to-memory","title":"Next, we will instantiate the datamodule, the object we will use to load the files from disk to memory.","text":"<pre><code>datamodule = GenericNonGeoSegmentationDataModule(\n    batch_size = 2,\n    num_workers = 8,\n    dataset_bands = [HLSBands.BLUE, HLSBands.GREEN, HLSBands.RED, HLSBands.NIR_NARROW, HLSBands.SWIR_1, HLSBands.SWIR_2],\n    output_bands = [HLSBands.BLUE, HLSBands.GREEN, HLSBands.RED, HLSBands.NIR_NARROW, HLSBands.SWIR_1, HLSBands.SWIR_2],\n    rgb_indices = [2, 1, 0],\n    means = [\n          0.033349706741586264,\n          0.05701185520536176,\n          0.05889748132001316,\n          0.2323245113436119,\n          0.1972854853760658,\n          0.11944914225186566,\n    ],\n    stds = [\n          0.02269135568823774,\n          0.026807560223070237,\n          0.04004109844362779,\n          0.07791732423672691,\n          0.08708738838140137,\n          0.07241979477437814,\n    ],\n    train_data_root = \"../burn_scars/hls_burn_scars/training\",\n    val_data_root = \"../burn_scars/hls_burn_scars/validation\",\n    test_data_root = \"../burn_scars/hls_burn_scars/validation\",\n    img_grep = \"*_merged.tif\",\n    label_grep = \"*.mask.tif\",\n    num_classes = 2,\n    train_transform = [albumentations.D4(), ToTensorV2()],\n    test_transform = [ToTensorV2()],\n    no_data_replace = 0,\n    no_label_replace =  -1,\n)\n</code></pre>"},{"location":"tutorials/burn_scars_finetuning/#a-dictionary-containing-all-the-arguments-necessary-to-instantiate-a-complete-backbone-neck-decoder-head-which-will-be-passed-to-the-task-object","title":"A dictionary containing all the arguments necessary to instantiate a complete <code>backbone-neck-decoder-head</code>, which will be passed to the task object.","text":"<pre><code>model_args = dict(\n  backbone=\"prithvi_eo_v2_300\",\n  backbone_pretrained=True,\n  backbone_num_frames=1,\n  num_classes = 2,\n  backbone_bands=[\n      \"BLUE\",\n      \"GREEN\",\n      \"RED\",\n      \"NIR_NARROW\",\n      \"SWIR_1\",\n      \"WIR_2\",\n  ],\n  decoder = \"UNetDecoder\",\n  decoder_channels = [512, 256, 128, 64],\n  necks=[{\"name\": \"SelectIndices\", \"indices\": [5, 11, 17, 23]},\n         {\"name\": \"ReshapeTokensToImage\"},\n         {\"name\": \"LearnedInterpolateToPyramidal\"}],\n  head_dropout=0.1\n)\n</code></pre>"},{"location":"tutorials/burn_scars_finetuning/#creating-the-task-object-which-will-be-used-to-properly-define-how-the-model-will-be-trained-and-used-after-it","title":"Creating the <code>task</code> object, which will be used to properly define how the model will be trained and used after it.","text":"<pre><code>task = SemanticSegmentationTask(\n    model_args,\n    \"EncoderDecoderFactory\",\n    loss=\"ce\",\n    lr=lr,\n    ignore_index=-1,\n    optimizer=\"AdamW\",\n    optimizer_hparams={\"weight_decay\": 0.05},\n    freeze_backbone = False,\n    plot_on_val = False,\n    class_names = [\"Not burned\", \"Burn scar\"],\n)\n</code></pre>"},{"location":"tutorials/burn_scars_finetuning/#the-object-trainer-manages-all-the-training-process-it-can-be-interpreted-as-an-improved-optimization-loop-in-which-parallelism-and-checkpointing-are-transparently-managed-by-the-system","title":"The object <code>Trainer</code> manages all the training process. It can be interpreted as an improved optimization loop, in which parallelism and checkpointing are transparently managed by the system.","text":"<pre><code>trainer = Trainer(\n    accelerator=accelerator,\n    max_epochs=max_epochs,\n)\n</code></pre>"},{"location":"tutorials/burn_scars_finetuning/#executing-the-training","title":"Executing the training.","text":"<pre><code>trainer.fit(model=task, datamodule=datamodule)\n</code></pre>"},{"location":"tutorials/burn_scars_finetuning/#testing-the-trained-model-extracting-metrics","title":"Testing the trained model (extracting metrics).","text":"<pre><code>trainer.test(dataloaders=datamodule)\n</code></pre> <p>The metrics output:</p> <pre><code>    [{'test/loss': 0.2669268250465393,\n      'test/Multiclass_Accuracy': 0.9274423718452454,\n      'test/multiclassaccuracy_Not burned': 0.9267654418945312,\n      'test/multiclassaccuracy_Burn scar': 0.9340785145759583,\n      'test/Multiclass_F1_Score': 0.9274423718452454,\n      'test/Multiclass_Jaccard_Index': 0.7321492433547974,\n      'test/multiclassjaccardindex_Not burned': 0.9205750226974487,\n      'test/multiclassjaccardindex_Burn scar': 0.5437235236167908,\n      'test/Multiclass_Jaccard_Index_Micro': 0.8647016882896423}]\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/","title":"Performing a simple inference task using the TerraTorch's script interface.","text":""},{"location":"tutorials/burn_scars_inference_simplified/#direct-inference-or-full-image-inference","title":"Direct inference (or full image inference).","text":"<pre><code>import argparse\nimport os\nfrom typing import List, Union\nimport re\nimport datetime\nimport numpy as np\nimport rasterio\nimport torch\nimport rioxarray\nimport yaml\nfrom einops import rearrange\nfrom terratorch.cli_tools import LightningInferenceModel\nfrom terratorch.utils import view_api\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#the-directory-in-which-we-will-save-the-model-output","title":"The directory in which we will save the model output.","text":"<pre><code>output_dir = \"inference_output\"\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#the-path-to-the-configuration-yaml-file","title":"The path to the configuration (YAML) file.","text":"<pre><code>config_file = \"burn_scars_config.yaml\"\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#the-path-to-the-local-checkpoint-a-file-storing-the-model-weights","title":"The path to the local checkpoint (a file storing the model weights).","text":"<pre><code>checkpoint = \"checkpoints/Prithvi_EO_V2_300M_BurnScars.pt\"\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#the-path-for-the-directory-containing-the-input-images","title":"The path for the directory containing the input images.","text":"<pre><code>input_dir = \"data/examples/\"\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#an-image-chosen-to-be-used-in-the-single-file-inference","title":"An image chosen to be used in the single-file inference.","text":"<pre><code>example_file = \"data/examples/subsetted_512x512_HLS.S30.T10SEH.2018190.v1.4_merged.tif\"\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#a-list-indicating-the-bands-contained-in-the-input-files","title":"A list indicating the bands contained in the input files.","text":"<pre><code>predict_dataset_bands=[\n      \"BLUE\",\n      \"GREEN\",\n      \"RED\",\n      \"NIR_NARROW\",\n      \"SWIR_1\",\n      \"SWIR_2\",\n  ]\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#a-subset-of-the-dataset-bands-to-be-used-as-input-for-the-model","title":"A subset of the dataset bands to be used as input for the model.","text":"<pre><code>predict_output_bands = predict_dataset_bands\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#creating-a-directory-to-store-the-output-when-it-does-not-exist","title":"Creating a directory to store the output (when it does not exist).","text":"<pre><code>os.makedirs(output_dir, exist_ok=True)\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#instantiating-the-model-from-the-config-file-and-the-others-arguments-defined-previously","title":"Instantiating the model from the config file and the others arguments defined previously.","text":"<pre><code>lightning_model = LightningInferenceModel.from_config(config_file, checkpoint, predict_dataset_bands, predict_output_bands)\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#performing-the-inference-for-a-single-file-the-output-is-a-tensor-torchtensor","title":"Performing the inference for a single file. The output is a tensor (<code>torch.Tensor</code>).","text":"<pre><code>prediction = lightning_model.inference(example_file)\nprediction\n</code></pre> <pre><code>    tensor([[1, 1, 1,  ..., 0, 0, 0],\n            [1, 1, 1,  ..., 0, 0, 0],\n            [1, 1, 1,  ..., 0, 0, 0],\n            ...,\n            [0, 0, 0,  ..., 1, 1, 1],\n            [0, 0, 0,  ..., 1, 1, 1],\n            [0, 0, 0,  ..., 1, 1, 1]])\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#visualizing-the-input-image","title":"Visualizing the input image.","text":"<p>From the file object <code>fp</code> we select just the bands corresponding to RGB (indexes que correspondem aos \u00edndices 2, 1 and 0 of the TIFF file) for sake of visualization. Notice that we added a shift to white (<code>fp[[2,1,0]]+0.20</code>) in order to lighten the image. </p> <pre><code>import rioxarray\nfp = rioxarray.open_rasterio(example_file)\n(fp[[2,1,0]]+0.20).plot.imshow(rgb=\"band\")\n</code></pre> <p></p> <p>Visualizing the output image. </p> <pre><code>import matplotlib.pyplot as plt\n(fp[[2,1,0]] + 0.10 + 0.5*np.stack(3*[prediction], axis=0)).plot.imshow(rgb=\"band\")\n</code></pre> <p></p> <p>We also can perform inference for an entire directory of images by using the </p> <pre><code>predictions, file_names = lightning_model.inference_on_dir(input_dir)\n</code></pre> <p>This operation will return two lists, one containing predictions and another with the names of the corresponding input files. </p> <pre><code>for pred, input_file in zip(predictions, file_names):\n    fp = rioxarray.open_rasterio(input_file)              \n    f, ax = plt.subplots(1,2, figsize=(14,6))\n    (fp[[2,1,0]]+0.10).plot.imshow(rgb=\"band\", ax=ax[0])\n    (fp[[2,1,0]] + 0.10 + 0.5*np.stack(3*[pred], axis=0)).plot.imshow(rgb=\"band\", ax=ax[1])\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"tutorials/burn_scars_inference_simplified/#tiled-inference","title":"Tiled Inference","text":"<p>Now let's try an alternative form of inference - tiled inference. This type of inference is useful when the GPU (or the RAM associated with the CPU, if applicable) is insufficient to allocate all the information needed to run the model (basic libraries, model and data), because instead of applying the model to the whole image, it divides it into small rectangles, the dimensions of which are defined by the user, applies the model separately and then reconstructs the output figure. To perform this type of inference, we will use the file below. </p> <pre><code>config_file_tiled = \"burn_scars_config_tiled.yaml\"\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#notice-that-the-content-is-identical-to-the-other-yaml-file-but-the-addition-of-the-subfield","title":"Notice that the content is identical to the other YAML file, but the addition of the subfield:","text":"<p><pre><code>    tiled_inference_parameters:\n      h_crop: 128\n      h_stride: 64\n      w_crop: 128\n      w_stride: 64\n      average_patches: true\n</code></pre> to the variables sent to the field <code>model</code>. The variables containing the suffix <code>_crop</code> refer to the dimensions of the tiles while those ones with the suffix <code>_stride</code> control the distance between them (the tiles can overlap).  </p> <pre><code>lightning_model = LightningInferenceModel.from_config(config_file_tiled, checkpoint, predict_dataset_bands, predict_output_bands)\n</code></pre> <pre><code>predictions, file_names = lightning_model.inference_on_dir(input_dir)\n</code></pre> <pre><code>for pred, input_file in zip(predictions, file_names):\n    fp = rioxarray.open_rasterio(input_file)\n    f, ax = plt.subplots(1,2, figsize=(14,6))\n    (fp[[2,1,0]]+0.10).plot.imshow(rgb=\"band\", ax=ax[0])\n    (fp[[2,1,0]] + 0.10 + 0.5*np.stack(3*[pred], axis=0)).plot.imshow(rgb=\"band\", ax=ax[1])\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"tutorials/the_yaml_config/","title":"The YAML configuration file: an overview","text":"<p>If you are using the command-line interface (CLI) to run jobs using TerraTorch, so you must became familiar with YAML, the format used to configure all the workflow within the toolkit. Writing a YAML file is very similar to coding, because even if you are not direclty handling the classes and others structures defined inside a codebase, you need to know how they work, their input argments and their position in the pipeline. In this way, we could call it a \"low-code\" task. The YAML file used for TerraTorch has an almost closed format, since there are a few fixed fields that must be filled with limited sets of classes, which makes easier for new users to get a pre-existing YAML file and adapt it to their own purposes. </p> <p>In the next sections, we describe each field of a YAML file used for Earth Observation Foundation Models (EOFM) and try to make it clearer for a new user. However, we will not go into detail, since the complementary documentation (Lightning, PyTorch, ...) must fill this gap. The example can be downloaded here. </p>"},{"location":"tutorials/the_yaml_config/#trainer","title":"Trainer","text":"<p>In the section called <code>trainer</code> are defined all the arguments that must be directly sent to the Lightning Trainer object. If you need a deeper explantion about this object, check the Lightning's documentation. In the first lines we have:</p> <p><pre><code>trainer:\n  accelerator: cpu\n  strategy: auto\n  devices: auto\n  num_nodes: 1\n  precision: 16-mixed\n</code></pre> In which:</p> <ul> <li><code>accelerator</code> refers to the kind of device is being used to run the experiment. We are usually more interested in <code>cpu</code> and <code>gpu</code>, but if you set <code>auto</code>, it will automaticaly select allocate the GPU is that is availble or otherwise run on CPU.</li> <li><code>strategy</code> is related to the kind of parallelism is available. As we have usually ran the experiments using a single device for finetuning or inference, we do not care about it and choose the option <code>auto</code> by default. </li> <li><code>devices</code> indicates the list of available devices to use for the experiment. Leave it as <code>auto</code> if you are running with a single device. </li> <li><code>num_nodes</code> is self-explanatory. We have mostly tested TerraTorch for a single-node jobs, so, it is better to set it as <code>1</code> for now. </li> <li><code>precision</code> is the kind of precision used for your model. <code>16-mixed</code> have been an usual choice. </li> </ul> <p>Just below this initial stage, we have <code>logger</code>: <pre><code>  logger:\n    class_path: TensorBoardLogger\n    init_args:\n      save_dir: tests/\n      name: all_ecos_random\n</code></pre> In this field we define the configuration for logging the model state. In this example we are using Tensorboard, and saving all the logs in a directory <code>tests/all_ecos_random</code>.  Others frameworks, as MLFlow are also supported. Check the Lightning documentation about logging for a more complete description. </p> <p>The <code>callbacks</code> field: <pre><code>  callbacks:\n    - class_path: RichProgressBar\n    - class_path: LearningRateMonitor\n      init_args:\n        logging_interval: epoch\n    - class_path: EarlyStopping\n      init_args:\n        monitor: val/loss\n        patience: 100\n</code></pre> Represents a list of operations that can be invoked with determined frequency. The user is free to add others operations from Lightning or custom ones. In the current config we are basically defining: a progress bar to be printed during the model training/validation and a learning rate monitor, determined to call early-stopping when the model shows signals of overfitting.  The rest of the arguments are: <pre><code>  max_epochs: 1\n  check_val_every_n_epoch: 1\n  log_every_n_steps: 20\n  enable_checkpointing: true\n  default_root_dir: tests/\n</code></pre></p> <ul> <li><code>max_epochs</code>: the maximum number of epochs to train the model. Notice that, if you are using early-stopping,     maybe the training will finish before achieving this number. </li> <li><code>check_val_every_n_epoch</code>: the frequency to evaluate the model using the validation dataset. The validation     is important to verify if the model is tending to overfit and  can be used, for example, to define when update the learning rate, or to invoke the early-stopping. </li> <li><code>enable_checkpointing</code>: it enables the checkpointing, the action of periodically saving the state of the     model to a file. </li> <li><code>default_root_dir</code>: the directory used to save the model checkpoints. </li> </ul>"},{"location":"tutorials/the_yaml_config/#datamodule","title":"Datamodule","text":"<p>In this section, we start direclty handling TerraTorch's built-in structures. The field <code>data</code> is expected to receive a generic datamodule or any other datamodule compatible with Lightning Datamodules, as those defined in our collection of datamodules. </p> <p>In the beginning of the field we have: <pre><code>data:\n  class_path: GenericNonGeoPixelwiseRegressionDataModule\n  init_args:\n</code></pre> It means that we have chosen the generic regression datamodule and we will pass all its required arguments below <code>init_args</code> and with one new level of identation. The best practice here is to check the documentation of the datamodule class you are using (in our case, [here][terratorch.datamodules.generic_pixel_wise_data_module]) and verify all the arguments it expects to receive ant then to fill the lines with <code>&lt;argument_name&gt;: &lt;argument_value&gt;</code>.  As the TerraTorch and Lightning modules were already imported in the CLI script (<code>terratorch/cli_tools.py</code>), you do not need to provide the complete paths for them. Otherwise, if you are using a datamodule defined in an external package, indicate the path to import the model, as <code>package.datamodules.SomeDatamodule</code>. </p>"},{"location":"tutorials/the_yaml_config/#model","title":"Model","text":"<p>The field <code>model</code> is, in fact, the configuration for <code>task + model</code>:  <pre><code>model:\n  class_path: terratorch.tasks.PixelwiseRegressionTask\n  init_args:\n    model_args:\n      decoder: UperNetDecoder\n      pretrained: false\n      backbone: prithvi_eo_v2_600\n      backbone_drop_path_rate: 0.3\n      backbone_window_size: 8\n      decoder_channels: 64\n      num_frames: 1\n      in_channels: 6\n      bands:\n        - BLUE\n        - GREEN\n        - RED\n        - NIR_NARROW\n        - SWIR_1\n        - SWIR_2\n      head_dropout: 0.5708022831486758\n      head_final_act: torch.nn.ReLU\n      head_learned_upscale_layers: 2\n    loss: rmse\n    ignore_index: -1\n    freeze_backbone: true\n    freeze_decoder: false\n    model_factory: PrithviModelFactory\n    tiled_inference_parameters:\n       h_crop: 224\n       h_stride: 192\n       w_crop: 224\n       w_stride: 192\n       average_patches: true\n</code></pre> Notice that there is a field <code>model_args</code>, which it is intended to receive all the necessary configuration to instantiate the model itself, that means, the structure <code>backbone + decoder + head</code>. Inside <code>model_args</code>, it is possible do define which arguments will be sent to each component by including a prefix to the argument names, as <code>backbone_&lt;argument&gt;</code> or <code>decoder_&lt;other_argument&gt;</code>. Alternatively, it is possible to pass the arguments using dictionaries <code>backbone_kwargs</code>, <code>decoder_kwargs</code> and <code>head_kwargs</code>. The same recommendation made for the <code>data</code> field is repeated here, check the documentation of the task and model classes (backbones, decoders and heads) you are using in order to define which arguments to write for each subfield of <code>model</code>. </p>"},{"location":"tutorials/the_yaml_config/#optimizer-and-learning-rate-scheduler","title":"Optimizer and Learning Rate Scheduler","text":"<p>The last two fields of out example are the configuration of the optimizer and the lr scheduler. Those fields are mostly self-explanatory for users already familiar with machine learning:</p> <p><pre><code>optimizer:\n  class_path: torch.optim.AdamW\n  init_args:\n    lr: 0.00013524680528283027\n    weight_decay: 0.047782217873995426\nlr_scheduler:\n  class_path: ReduceLROnPlateau\n  init_args:\n    monitor: val/loss\n</code></pre> Check the PyTorch documentation about optimization to understand them more deeply. </p>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/","title":"Using a dedicated datamodule to perform inference: the crop classification example.","text":""},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#to-run-this-example-the-following-packages-are-necessary","title":"To run this example, the following packages are necessary.","text":"<pre><code>!pip install terratorch gdown tensorboard &gt;&amp; install.log\n</code></pre> <pre><code>import os\nimport sys\nimport torch\nimport gdown\nimport terratorch\nimport albumentations\nimport lightning.pytorch as pl\nimport matplotlib.pyplot as plt\nfrom terratorch.datamodules import MultiTemporalCropClassificationDataModule\nimport warnings\n</code></pre>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#downloading-a-random-subset-of-the-required-dataset-1-gb","title":"Downloading a random subset of the required dataset (~1 GB).","text":"<pre><code>if not os.path.isfile('multi-temporal-crop-classification-subset.tar.gz'):\n    !gdown 1LL6thkuKA0kVyMI39PxgsrJ1FJJDV7-u\n\nif not os.path.isdir('multi-temporal-crop-classification-subset/'):\n    !tar -xzvf multi-temporal-crop-classification-subset.tar.gz\n\ndataset_path = \"multi-temporal-crop-classification-subset\"\n</code></pre>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#instantiating-the-corresponding-datamodule","title":"Instantiating the corresponding datamodule.","text":"<pre><code>datamodule = MultiTemporalCropClassificationDataModule(\n    batch_size=8,\n    num_workers=2,\n    data_root=dataset_path,\n    train_transform=[\n        terratorch.datasets.transforms.FlattenTemporalIntoChannels(),  # Required for temporal data\n        albumentations.D4(), # Random flips and rotation\n        albumentations.pytorch.transforms.ToTensorV2(),\n        terratorch.datasets.transforms.UnflattenTemporalFromChannels(n_timesteps=3),\n    ],\n    val_transform=None,  # Using ToTensor() by default\n    test_transform=None,\n    expand_temporal_dimension=True,\n    use_metadata=False, # The crop dataset has metadata for location and time\n    reduce_zero_label=True,\n)\n\n# Setup train and val datasets\ndatamodule.setup(\"predict\")\n</code></pre>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#visualizing-a-few-samples","title":"Visualizing a few samples.","text":"<pre><code>for i in range(5):\n    datamodule.predict_dataset.plot(datamodule.predict_dataset[i])\n</code></pre>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#downloading-the-best-pretrained-checkpoint","title":"Downloading the best pretrained checkpoint.","text":"<pre><code>best_ckpt_100_epoch_path = \"multicrop_best-epoch=76.ckpt\"\n\nif not os.path.isfile(best_ckpt_100_epoch_path):\n    gdown.download(\"https://drive.google.com/uc?id=1o1Hzd4yyiKyYdzfotQlEOeGTjsM8cHSw\")\n</code></pre>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#instantiating-the-lightning-trainer","title":"Instantiating the Lightning Trainer.","text":"<pre><code>checkpoint_callback = pl.callbacks.ModelCheckpoint(\n    dirpath=\"output/multicrop/checkpoints/\",\n    mode=\"max\",\n    monitor=\"val/Multiclass_Jaccard_Index\", # Variable to monitor\n    filename=\"best-{epoch:02d}\",\n)\ntrainer = pl.Trainer(\n    accelerator=\"auto\",\n    strategy=\"auto\",\n    devices=1, # Lightning multi-gpu often fails in notebooks\n    precision='bf16-mixed',  # Speed up training\n    num_nodes=1,\n    logger=True, # Uses TensorBoard by default\n    max_epochs=1, # For demos\n    log_every_n_steps=5,\n    enable_checkpointing=True,\n    callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n    default_root_dir=\"output/multicrop\",\n)\n</code></pre>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#instantiating-the-task-to-handle-the-model","title":"Instantiating the task to handle the model.","text":"<pre><code>model = terratorch.tasks.SemanticSegmentationTask(\n    model_factory=\"EncoderDecoderFactory\",\n    model_args={\n        # Backbone\n        \"backbone\": \"prithvi_eo_v2_300\", \n        \"backbone_pretrained\": True,\n        \"backbone_num_frames\": 3,\n        \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_NARROW\", \"SWIR_1\", \"SWIR_2\"],\n        \"backbone_coords_encoding\": [], # use [\"time\", \"location\"] for time and location metadata\n\n        # Necks \n        \"necks\": [\n            {\n                \"name\": \"SelectIndices\",\n                \"indices\": [5, 11, 17, 23] \n            },\n            {\n                \"name\": \"ReshapeTokensToImage\",\n                \"effective_time_dim\": 3\n            },\n            {\"name\": \"LearnedInterpolateToPyramidal\"},            \n        ],\n\n        # Decoder\n        \"decoder\": \"UNetDecoder\",\n        \"decoder_channels\": [512, 256, 128, 64],\n\n        # Head\n        \"head_dropout\": 0.1,\n        \"num_classes\": 13,\n    },\n\n    loss=\"ce\",\n    lr=1e-4,\n    optimizer=\"AdamW\",\n    ignore_index=-1,\n    freeze_backbone=True,  \n    freeze_decoder=False,\n    plot_on_val=True,\n\n)\n</code></pre>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#predicting-for-some-samples-in-the-prediction-dataset","title":"Predicting for some samples in the prediction dataset.","text":"<pre><code>preds = trainer.predict(model, datamodule=datamodule, ckpt_path=best_ckpt_100_epoch_path)\n# get data \ndata_loader = trainer.predict_dataloaders\nbatch = next(iter(data_loader))\n\nBATCH_SIZE = 8\nfor i in range(BATCH_SIZE):\n\n    sample = {key: batch[key][i] for key in batch}\n    sample[\"prediction\"] = preds[0][0][i].cpu().numpy()\n\n    datamodule.predict_dataset.plot(sample)\n</code></pre>"}]}