{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to TerraTorch","text":""},{"location":"#overview","title":"Overview","text":"<p>The purpose of this package is to build a flexible fine-tuning framework for Geospatial Foundation Models (GFMs) based on TorchGeo and Lightning which can be employed at different abstraction levels. It currently supports models from the Prithvi and Granite series, and also have been tested with others models available on HuggingFace. </p> <p>This library provides:</p> <ul> <li>All the functionality in TorchGeo.</li> <li>Easy access to Prithvi, timm and smp backbones.</li> <li>Flexible trainers for Image Segmentation, Pixel Wise Regression and Classification (more in progress).</li> <li>Launching of fine-tuning tasks through powerful configuration files.</li> </ul> <p>A good starting place is familiarization with PyTorch Lightning, which this project is built on.  TorchGeo is also an important complementary reference. </p> <p>Check out the architecture overview for a general description about how TerraTorch is organized. </p>"},{"location":"#quick-start","title":"Quick start","text":"<p>To get started, check out the quick start guide</p>"},{"location":"#license","title":"License","text":"<p>TerraTorch is distributed under the terms of License Apache 2.0, see here for more details. </p>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>The main goal of the design is to extend TorchGeo's existing tasks to be able to handle Prithvi backbones with appropriate decoders and heads. At the same time, we wish to keep the existing TorchGeo functionality intact so it can be leveraged with pretrained models that are already included.</p> <p>We achieve this by making new tasks that accept model factory classes, containing a <code>build_model</code> method. This strategy in principle allows arbitrary models to be trained for these tasks, given they respect some reasonable minimal interface. Together with this, we provide the EncoderDecoderFactory, which should enable users to plug together different Encoders and Decoders, with the aid of Necks for intermediate operations.</p> <p>Additionally, we extend TorchGeo with generic datasets and datamodules which can be defined at runtime, rather than requiring classes to be defined beforehand.</p> <p>The glue that holds everything together is LightningCLI, allowing the model, datamodule and Lightning Trainer to be instantiated from a config file or from the CLI. We make extensive use of for training and inference.</p> <p>Initial reading for a full understanding of the platform includes:</p> <ul> <li>Familiarity with PyTorch Lightning</li> <li>Familiarity with TorchGeo</li> <li>Familiarity with LightningCLI</li> </ul> <p>The scheme below illustrates the general TerraTorch's workflow for a CLI job.   </p>"},{"location":"architecture/#tasks","title":"Tasks","text":"<p>Tasks are the main coordinators for training and inference for specific tasks. They are LightningModules that contain a model and abstract away all the logic for training steps, metric computation and inference.</p> <p>One of the most important design decisions was delegating the model construction to a model factory. This has a few advantages:</p> <ul> <li>Avoids code repetition among tasks - different tasks can use the same factory</li> <li>Prefers composition over inheritance</li> <li>Allows new models to be easily added by introducing new factories</li> </ul> <p>Models are expected to be <code>torch.nn.Module</code> and implement the Model interface, providing:</p> <ul> <li><code>freeze_encoder()</code></li> <li><code>freeze_decoder()</code></li> <li><code>forward()</code></li> </ul> <p>Additionally, the <code>forward()</code> method is expected to return an object of type ModelOutput, containing the main head's output, as well as any additional auxiliary outputs. The names of these auxiliary heads are matched with the names of the provided auxiliary losses. The tasks currently deployed in TerraTorch are described here.  </p>"},{"location":"architecture/#models","title":"Models","text":"<p>Models constructed by the EncoderDecoderFactory have an internal structure explicitly divided into backbones, necks, decoders and heads. This structure is provided by the PixelWiseModel and ScalarOutputModel classes.</p> <p>However, as long as models implement the Model interface, and return ModelOutput in their forward method, they can take on any structure.</p> <p>See the models documentation for more details about the core models ScalarOutputModel and PixelWiseModel. For details about backbones (encoders) see the backbones documentation, the same for necks, decoders and heads.  </p>"},{"location":"architecture/#model-factories","title":"Model Factories","text":"<p>A model factory is a class desgined to search a model in the register and properly instantiate it. TerraTorch has a few types of model factories for different situations, as models which require specific wrappers and processing.</p> <p>See the models factories documentation for a better explanation about it. </p>"},{"location":"architecture/#encoderdecoderfactory","title":"EncoderDecoderFactory","text":"<p>However, as we have tried as much as possible to avoid the limitless replication of model factories dedicate to very specific models by concentrating efforts on the EncoderDecoderFactory, which intends to be more general-purpose. With that in mind, we dive deeper into it here.</p>"},{"location":"architecture/#loss","title":"Loss","text":"<p>For convenience, we provide a loss handler that can be used to compute the full loss (from the main head and auxiliary heads as well).</p>"},{"location":"architecture/#generic-datasets-datamodules","title":"Generic datasets / datamodules","text":"<p>Refer to the section on data</p>"},{"location":"architecture/#exporting-models","title":"Exporting models","text":"<p>Models are saved using the PyTorch format, which basically serializes the model weights using pickle and store them into a binary file. </p> <p>"},{"location":"backbones/","title":"Backbones","text":""},{"location":"backbones/#built-in-backbones","title":"Built-in Backbones","text":""},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder","title":"<code>terratorch.models.backbones.swin_encoder_decoder</code>","text":"<p>Swin transformer implementation. Mix of MMSegmentation implementation and timm implementation.</p> <p>We use this implementation instead of the original implementation or timm's. This is because it offers a few advantages, namely being able to handle a dynamic input size through padding.</p> <p>Please note the original timm implementation can still be used as a backbone via <code>timm.create_model(\"swin_...\")</code>. You can see the available models with <code>timm.list_models(\"swin*\")</code>.</p>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.AdaptivePadding","title":"<code>AdaptivePadding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Applies padding to input (if needed) so that input can get fully covered by filter you specified. It support two modes \"same\" and \"corner\". The \"same\" mode is same with \"SAME\" padding mode in TensorFlow, pad zero around input. The \"corner\"  mode would pad zero to bottom right.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_size</code> <code>int | tuple</code> <p>Size of the kernel:</p> <code>1</code> <code>stride</code> <code>int | tuple</code> <p>Stride of the filter. Default: 1:</p> <code>1</code> <code>dilation</code> <code>int | tuple</code> <p>Spacing between kernel elements. Default: 1.</p> <code>1</code> <code>padding</code> <code>str</code> <p>Support \"same\" and \"corner\", \"corner\" mode would pad zero to bottom right, and \"same\" mode would pad zero around input. Default: \"corner\".</p> <code>'corner'</code> <p>Example: <pre><code>    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; kernel_size = 16\n    &gt;&gt;&gt; stride = 16\n    &gt;&gt;&gt; dilation = 1\n    &gt;&gt;&gt; input = torch.rand(1, 1, 15, 17)\n    &gt;&gt;&gt; adap_pad = AdaptivePadding(\n    &gt;&gt;&gt;     kernel_size=kernel_size,\n    &gt;&gt;&gt;     stride=stride,\n    &gt;&gt;&gt;     dilation=dilation,\n    &gt;&gt;&gt;     padding=\"corner\")\n    &gt;&gt;&gt; out = adap_pad(input)\n    &gt;&gt;&gt; assert (out.shape[2], out.shape[3]) == (16, 32)\n    &gt;&gt;&gt; input = torch.rand(1, 1, 16, 17)\n    &gt;&gt;&gt; out = adap_pad(input)\n    &gt;&gt;&gt; assert (out.shape[2], out.shape[3]) == (16, 32)\n</code></pre></p>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.FFN","title":"<code>FFN</code>","text":"<p>               Bases: <code>Module</code></p> <p>Implements feed-forward networks (FFNs) with identity connection.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dims</code> <code>int</code> <p>The feature dimension. Same as <code>MultiheadAttention</code>. Defaults: 256.</p> <code>256</code> <code>feedforward_channels</code> <code>int</code> <p>The hidden dimension of FFNs. Defaults: 1024.</p> <code>1024</code> <code>num_fcs</code> <code>int</code> <p>The number of fully-connected layers in FFNs. Default: 2.</p> <code>2</code> <code>act_cfg</code> <code>dict</code> <p>The activation config for FFNs. Default: dict(type='ReLU')</p> required <code>ffn_drop</code> <code>float</code> <p>Probability of an element to be zeroed in FFN. Default 0.0.</p> <code>0.0</code> <code>add_identity</code> <code>bool</code> <p>Whether to add the identity connection. Default: <code>True</code>.</p> <code>True</code> <code>dropout_layer</code> <code>obj</code> <p><code>ConfigDict</code>): The dropout_layer used when adding the shortcut.</p> <code>None</code> <code>init_cfg</code> <code>obj</code> <p><code>mmcv.ConfigDict</code>): The Config for initialization. Default: None.</p> required"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.FFN.forward","title":"<code>forward(x, identity=None)</code>","text":"<p>Forward function for <code>FFN</code>.</p> <p>The function would add x to the output tensor if residue is None.</p>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.MMSegSwinTransformer","title":"<code>MMSegSwinTransformer</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.MMSegSwinTransformer.__init__","title":"<code>__init__(pretrain_img_size=224, in_chans=3, embed_dim=96, patch_size=4, window_size=7, mlp_ratio=4, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24), strides=(4, 2, 2, 2), num_classes=1000, global_pool='avg', out_indices=(0, 1, 2, 3), qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.1, act_layer=nn.GELU, norm_layer=nn.LayerNorm, with_cp=False, frozen_stages=-1)</code>","text":"<p>MMSeg Swin Transformer backbone.</p> <p>This backbone is the implementation of <code>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows &lt;https://arxiv.org/abs/2103.14030&gt;</code>_. Inspiration from https://github.com/microsoft/Swin-Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>pretrain_img_size</code> <code>int | tuple[int]</code> <p>The size of input image when pretrain. Defaults: 224.</p> <code>224</code> <code>in_chans</code> <code>int</code> <p>The num of input channels. Defaults: 3.</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>The feature dimension. Default: 96.</p> <code>96</code> <code>patch_size</code> <code>int | tuple[int]</code> <p>Patch size. Default: 4.</p> <code>4</code> <code>window_size</code> <code>int</code> <p>Window size. Default: 7.</p> <code>7</code> <code>mlp_ratio</code> <code>int | float</code> <p>Ratio of mlp hidden dim to embedding dim. Default: 4.</p> <code>4</code> <code>depths</code> <code>tuple[int]</code> <p>Depths of each Swin Transformer stage. Default: (2, 2, 6, 2).</p> <code>(2, 2, 6, 2)</code> <code>num_heads</code> <code>tuple[int]</code> <p>Parallel attention heads of each Swin Transformer stage. Default: (3, 6, 12, 24).</p> <code>(3, 6, 12, 24)</code> <code>strides</code> <code>tuple[int]</code> <p>The patch merging or patch embedding stride of each Swin Transformer stage. (In swin, we set kernel size equal to stride.) Default: (4, 2, 2, 2).</p> <code>(4, 2, 2, 2)</code> <code>out_indices</code> <code>tuple[int]</code> <p>Output from which stages. Default: (0, 1, 2, 3).</p> <code>(0, 1, 2, 3)</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to query, key, value. Default: True</p> <code>True</code> <code>qk_scale</code> <code>float | None</code> <p>Override default qk scale of head_dim ** -0.5 if set. Default: None.</p> <code>None</code> <code>patch_norm</code> <code>bool</code> <p>If add a norm layer for patch embed and patch merging. Default: True.</p> required <code>drop_rate</code> <code>float</code> <p>Dropout rate. Defaults: 0.</p> <code>0.0</code> <code>attn_drop_rate</code> <code>float</code> <p>Attention dropout rate. Default: 0.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float</code> <p>Stochastic depth rate. Defaults: 0.1.</p> <code>0.1</code> <code>act_layer</code> <code>dict</code> <p>activation layer. Default: nn.GELU.</p> <code>GELU</code> <code>norm_layer</code> <code>dict</code> <p>normalization layer at output of backone. Defaults: nn.LayerNorm.</p> <code>LayerNorm</code> <code>with_cp</code> <code>bool</code> <p>Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False.</p> <code>False</code> <code>frozen_stages</code> <code>int</code> <p>Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters.</p> <code>-1</code>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.MMSegSwinTransformer.train","title":"<code>train(mode=True)</code>","text":"<p>Convert the model into training mode while keep layers freezed.</p>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.PatchEmbed","title":"<code>PatchEmbed</code>","text":"<p>               Bases: <code>Module</code></p> <p>Image to Patch Embedding.</p> <p>We use a conv layer to implement PatchEmbed.</p> <p>Parameters:</p> Name Type Description Default <code>in_chans</code> <code>int</code> <p>The num of input channels. Default: 3</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>The dimensions of embedding. Default: 768</p> <code>768</code> <code>kernel_size</code> <code>int</code> <p>The kernel_size of embedding conv. Default: 16.</p> <code>16</code> <code>stride</code> <code>int</code> <p>The slide stride of embedding conv. Default: None (Would be set as <code>kernel_size</code>).</p> <code>None</code> <code>padding</code> <code>int | tuple | string</code> <p>The padding length of embedding conv. When it is a string, it means the mode of adaptive padding, support \"same\" and \"corner\" now. Default: \"corner\".</p> <code>'corner'</code> <code>padding_mode</code> <code>string</code> <p>The padding mode to use. Default \"constant\".</p> <code>'constant'</code> <code>dilation</code> <code>int</code> <p>The dilation rate of embedding conv. Default: 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Bias of embed conv. Default: True.</p> <code>True</code> <code>norm_cfg</code> <code>dict</code> <p>Config dict for normalization layer. Default: None.</p> required <code>input_size</code> <code>int | tuple | None</code> <p>The size of input, which will be used to calculate the out size. Only work when <code>dynamic_size</code> is False. Default: None.</p> <code>None</code>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.PatchEmbed.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Has shape (B, C, H, W). In most case, C is 3.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>Contains merged results and its spatial shape.</p> <ul> <li>x (Tensor): Has shape (B, out_h * out_w, embed_dim)</li> <li>out_size (tuple[int]): Spatial shape of x, arrange as     (out_h, out_w).</li> </ul>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.PatchMerging","title":"<code>PatchMerging</code>","text":"<p>               Bases: <code>Module</code></p> <p>Merge patch feature map.</p> <p>This layer groups feature map by kernel_size, and applies norm and linear layers to the grouped feature map. Our implementation uses <code>nn.Unfold</code> to merge patch, which is about 25% faster than original implementation. Instead, we need to modify pretrained models for compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>in_chans</code> <code>int</code> <p>The num of input channels.</p> required <code>out_channels</code> <code>int</code> <p>The num of output channels.</p> required <code>kernel_size</code> <code>int | tuple</code> <p>the kernel size in the unfold layer. Defaults to 2.</p> <code>2</code> <code>stride</code> <code>int | tuple</code> <p>the stride of the sliding blocks in the unfold layer. Default: None. (Would be set as <code>kernel_size</code>)</p> <code>None</code> <code>padding</code> <code>int | tuple | string</code> <p>The padding length of embedding conv. When it is a string, it means the mode of adaptive padding, support \"same\" and \"corner\" now. Default: \"corner\".</p> <code>'corner'</code> <code>dilation</code> <code>int | tuple</code> <p>dilation parameter in the unfold layer. Default: 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to add bias in linear layer or not. Defaults: False.</p> <code>False</code> <code>norm_cfg</code> <code>dict</code> <p>Config dict for normalization layer. Default: dict(type='LN').</p> required"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.PatchMerging.forward","title":"<code>forward(x, input_size)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Has shape (B, H*W, C_in).</p> required <code>input_size</code> <code>tuple[int]</code> <p>The spatial shape of x, arrange as (H, W). Default: None.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>Contains merged results and its spatial shape.</p> <ul> <li>x (Tensor): Has shape (B, Merged_H * Merged_W, C_out)</li> <li>out_size (tuple[int]): Spatial shape of x, arrange as     (Merged_H, Merged_W).</li> </ul>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.ShiftWindowMSA","title":"<code>ShiftWindowMSA</code>","text":"<p>               Bases: <code>Module</code></p> <p>Shifted Window Multihead Self-Attention Module.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Number of input channels.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>window_size</code> <code>int</code> <p>The height and width of the window.</p> required <code>shift_size</code> <code>int</code> <p>The shift step of each window towards right-bottom. If zero, act as regular window-msa. Defaults to 0.</p> <code>0</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to q, k, v. Default: True</p> <code>True</code> <code>qk_scale</code> <code>float | None</code> <p>Override default qk scale of head_dim ** -0.5 if set. Defaults: None.</p> <code>None</code> <code>attn_drop_rate</code> <code>float</code> <p>Dropout ratio of attention weight. Defaults: 0.</p> <code>0</code> <code>proj_drop_rate</code> <code>float</code> <p>Dropout ratio of output. Defaults: 0.</p> <code>0</code> <code>drop_path_rate</code> <code>float</code> <p>Dropout ratio of layer used before output. Defaults: 0.</p> <code>0</code>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.ShiftWindowMSA.window_partition","title":"<code>window_partition(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>(B, H, W, C)</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(num_windows*B, window_size, window_size, C)</p>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.ShiftWindowMSA.window_reverse","title":"<code>window_reverse(windows, H, W)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>windows</code> <p>(num_windows*B, window_size, window_size, C)</p> required <code>H</code> <code>int</code> <p>Height of image</p> required <code>W</code> <code>int</code> <p>Width of image</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(B, H, W, C)</p>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.SwinBlock","title":"<code>SwinBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The feature dimension.</p> required <code>num_heads</code> <code>int</code> <p>Parallel attention heads.</p> required <code>feedforward_channels</code> <code>int</code> <p>The hidden dimension for Mlps.</p> required <code>window_size</code> <code>int</code> <p>The local window scale. Default: 7.</p> <code>7</code> <code>shift</code> <code>bool</code> <p>whether to shift window or not. Default False.</p> <code>False</code> <code>qkv_bias</code> <code>bool</code> <p>enable bias for qkv if True. Default: True.</p> <code>True</code> <code>qk_scale</code> <code>float | None</code> <p>Override default qk scale of head_dim ** -0.5 if set. Default: None.</p> <code>None</code> <code>drop_rate</code> <code>float</code> <p>Dropout rate. Default: 0.</p> <code>0.0</code> <code>attn_drop_rate</code> <code>float</code> <p>Attention dropout rate. Default: 0.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float</code> <p>Stochastic depth rate. Default: 0.</p> <code>0.0</code> <code>act_cfg</code> <code>dict</code> <p>The config dict of activation function. Default: dict(type='GELU').</p> required <code>norm_cfg</code> <code>dict</code> <p>The config dict of normalization. Default: dict(type='LN').</p> required <code>with_cp</code> <code>bool</code> <p>Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False.</p> <code>False</code>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.SwinBlockSequence","title":"<code>SwinBlockSequence</code>","text":"<p>               Bases: <code>Module</code></p> <p>Implements one stage in Swin Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The feature dimension.</p> required <code>num_heads</code> <code>int</code> <p>Parallel attention heads.</p> required <code>feedforward_channels</code> <code>int</code> <p>The hidden dimension for Mlps.</p> required <code>depth</code> <code>int</code> <p>The number of blocks in this stage.</p> required <code>window_size</code> <code>int</code> <p>The local window scale. Default: 7.</p> <code>7</code> <code>qkv_bias</code> <code>bool</code> <p>enable bias for qkv if True. Default: True.</p> <code>True</code> <code>qk_scale</code> <code>float | None</code> <p>Override default qk scale of head_dim ** -0.5 if set. Default: None.</p> <code>None</code> <code>drop_rate</code> <code>float</code> <p>Dropout rate. Default: 0.</p> <code>0.0</code> <code>attn_drop_rate</code> <code>float</code> <p>Attention dropout rate. Default: 0.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float | list[float]</code> <p>Stochastic depth rate. Default: 0.</p> <code>0.0</code> <code>downsample</code> <code>BaseModule | None</code> <p>The downsample operation module. Default: None.</p> <code>None</code> <code>act_cfg</code> <code>dict</code> <p>The config dict of activation function. Default: dict(type='GELU').</p> required <code>norm_cfg</code> <code>dict</code> <p>The config dict of normalization. Default: dict(type='LN').</p> required <code>with_cp</code> <code>bool</code> <p>Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False.</p> <code>False</code>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.WindowMSA","title":"<code>WindowMSA</code>","text":"<p>               Bases: <code>Module</code></p> <p>Window based multi-head self-attention (W-MSA) module with relative position bias.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Number of input channels.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>window_size</code> <code>tuple[int]</code> <p>The height and width of the window.</p> required <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to q, k, v. Default: True.</p> <code>True</code> <code>qk_scale</code> <code>float | None</code> <p>Override default qk scale of head_dim ** -0.5 if set. Default: None.</p> <code>None</code> <code>attn_drop_rate</code> <code>float</code> <p>Dropout ratio of attention weight. Default: 0.0</p> <code>0.0</code> <code>proj_drop_rate</code> <code>float</code> <p>Dropout ratio of output. Default: 0.</p> <code>0.0</code>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.WindowMSA.forward","title":"<code>forward(x, mask=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>tensor</code> <p>input features with shape of (num_windows*B, N, C)</p> required <code>mask</code> <code>(tensor | None, Optional)</code> <p>mask with shape of (num_windows, WhWw, WhWw), value should be between (-inf, 0].</p> <code>None</code>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae","title":"<code>terratorch.models.backbones.prithvi_mae</code>","text":""},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.LocationEncoder","title":"<code>LocationEncoder</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.LocationEncoder.forward","title":"<code>forward(location_coords)</code>","text":"<p>location_coords: lat and lon info with shape (B, 2).</p>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.MAEDecoder","title":"<code>MAEDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Decoder used in the Prithvi MAE</p>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.PatchEmbed","title":"<code>PatchEmbed</code>","text":"<p>               Bases: <code>Module</code></p> <p>3D version of timm.models.vision_transformer.PatchEmbed</p>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.PrithviMAE","title":"<code>PrithviMAE</code>","text":"<p>               Bases: <code>Module</code></p> <p>Prithvi Masked Autoencoder</p>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.PrithviMAE.forward_loss","title":"<code>forward_loss(pixel_values, pred, mask)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>`torch.FloatTensor` of shape `(batch_size, num_channels, time, height, width)`</code> <p>Pixel values.</p> required <code>mask</code> <code>`torch.FloatTensor` of shape `(batch_size, sequence_length)`</code> <p>Tensor indicating which patches are masked (1) and which are not (0).</p> required <p>Returns:</p> Type Description <p><code>torch.FloatTensor</code>: Pixel reconstruction loss.</p>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.PrithviMAE.patchify","title":"<code>patchify(pixel_values)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>torch.FloatTensor of shape `(batch_size, num_channels, time, height, width)`</code> <p>Pixel values.</p> required <p>Returns:</p> Type Description <p>torch.FloatTensor of shape <code>(batch_size, num_patches, patch_size[0]*patch_size[1]*patch_size[2] * num_channels)</code>: Patchified pixel values.</p>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.PrithviMAE.unpatchify","title":"<code>unpatchify(patchified_pixel_values, image_size=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>`tuple[int, int]`, *optional*</code> <p>Original image size.</p> <code>None</code> <p>Returns:</p> Type Description <p><code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>: Pixel values.</p>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.PrithviViT","title":"<code>PrithviViT</code>","text":"<p>               Bases: <code>Module</code></p> <p>Prithvi ViT Encoder</p>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.PrithviViT.random_masking","title":"<code>random_masking(sequence, mask_ratio, noise=None)</code>","text":"<p>Perform per-sample random masking by per-sample shuffling. Per-sample shuffling is done by argsort random noise.</p> <p>Parameters:</p> Name Type Description Default <code>mask_ratio</code> <code>float</code> <p>mask ratio to use.</p> required"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.TemporalEncoder","title":"<code>TemporalEncoder</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.TemporalEncoder.forward","title":"<code>forward(temporal_coords, tokens_per_frame=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>temporal_coords</code> <code>Tensor</code> <p>year and day-of-year info with shape (B, T, 2).</p> required <code>tokens_per_frame</code> <code>int | None</code> <p>number of tokens for each frame in the sample. If provided, embeddings will be repeated over T dimension, and final shape is (B, T*tokens_per_frame, embed_dim).</p> <code>None</code>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.get_1d_sincos_pos_embed_from_grid","title":"<code>get_1d_sincos_pos_embed_from_grid(embed_dim, pos)</code>","text":"<p>embed_dim: output dimension for each position pos: a list of positions to be encoded: size (M,) out: (M, D)</p>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.get_3d_sincos_pos_embed","title":"<code>get_3d_sincos_pos_embed(embed_dim, grid_size, add_cls_token=False)</code>","text":"<p>Create 3D sin/cos positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Embedding dimension.</p> required <code>grid_size</code> <code>tuple[int, int, int] | list[int]</code> <p>The grid depth, height and width.</p> required <code>add_cls_token</code> <code>bool, *optional*, defaults to False</code> <p>Whether or not to add a classification (CLS) token.</p> <code>False</code> <p>Returns:</p> Type Description <p>(<code>torch.FloatTensor</code> of shape (grid_size[0]grid_size[1]grid_size[2], embed_dim) or</p> <code>(1 + grid_size[0] * grid_size[1] * grid_size[2], embed_dim)</code> <p>the position embeddings (with or without cls token)</p>"},{"location":"backbones/#terratorch.models.backbones.unet","title":"<code>terratorch.models.backbones.unet</code>","text":""},{"location":"backbones/#terratorch.models.backbones.unet.UNet","title":"<code>UNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>UNet backbone.</p> <p>This backbone is the implementation of <code>U-Net: Convolutional Networks for Biomedical Image Segmentation &lt;https://arxiv.org/abs/1505.04597&gt;</code>_.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input image channels. Default\" 3.</p> <code>3</code> <code>out_channels</code> <code>int</code> <p>Number of base channels of each stage. The output channels of the first stage. Default: 64.</p> <code>64</code> <code>num_stages</code> <code>int</code> <p>Number of stages in encoder, normally 5. Default: 5.</p> <code>5</code> <code>strides</code> <code>Sequence[int 1 | 2]</code> <p>Strides of each stage in encoder. len(strides) is equal to num_stages. Normally the stride of the first stage in encoder is 1. If strides[i]=2, it uses stride convolution to downsample in the correspondence encoder stage. Default: (1, 1, 1, 1, 1).</p> <code>(1, 1, 1, 1, 1)</code> <code>enc_num_convs</code> <code>Sequence[int]</code> <p>Number of convolutional layers in the convolution block of the correspondence encoder stage. Default: (2, 2, 2, 2, 2).</p> <code>(2, 2, 2, 2, 2)</code> <code>dec_num_convs</code> <code>Sequence[int]</code> <p>Number of convolutional layers in the convolution block of the correspondence decoder stage. Default: (2, 2, 2, 2).</p> <code>(2, 2, 2, 2)</code> <code>downsamples</code> <code>Sequence[int]</code> <p>Whether use MaxPool to downsample the feature map after the first stage of encoder (stages: [1, num_stages)). If the correspondence encoder stage use stride convolution (strides[i]=2), it will never use MaxPool to downsample, even downsamples[i-1]=True. Default: (True, True, True, True).</p> <code>(True, True, True, True)</code> <code>enc_dilations</code> <code>Sequence[int]</code> <p>Dilation rate of each stage in encoder. Default: (1, 1, 1, 1, 1).</p> <code>(1, 1, 1, 1, 1)</code> <code>dec_dilations</code> <code>Sequence[int]</code> <p>Dilation rate of each stage in decoder. Default: (1, 1, 1, 1).</p> <code>(1, 1, 1, 1)</code> <code>with_cp</code> <code>bool</code> <p>Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False.</p> <code>False</code> <code>conv_cfg</code> <code>dict | None</code> <p>Config dict for convolution layer. Default: None.</p> <code>None</code> <code>norm_cfg</code> <code>dict | None</code> <p>Config dict for normalization layer. Default: dict(type='BN').</p> <code>dict(type='BN')</code> <code>act_cfg</code> <code>dict | None</code> <p>Config dict for activation layer in ConvModule. Default: dict(type='ReLU').</p> <code>dict(type='ReLU')</code> <code>upsample_cfg</code> <code>dict</code> <p>The upsample config of the upsample module in decoder. Default: dict(type='InterpConv').</p> <code>None</code> <code>norm_eval</code> <code>bool</code> <p>Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. Default: False.</p> <code>False</code> <code>dcn</code> <code>bool</code> <p>Use deformable convolution in convolutional layer or not. Default: None.</p> <code>None</code> <code>plugins</code> <code>dict</code> <p>plugins for convolutional layers. Default: None.</p> <code>None</code> <code>pretrained</code> <code>str</code> <p>model pretrained path. Default: None</p> <code>None</code> <code>init_cfg</code> <code>dict or list[dict]</code> <p>Initialization config dict. Default: None</p> <code>None</code> Notice <p>The input image size should be divisible by the whole downsample rate of the encoder. More detail of the whole downsample rate can be found in <code>UNet._check_input_divisible</code>.</p>"},{"location":"backbones/#terratorch.models.backbones.unet.UNet.train","title":"<code>train(mode=True)</code>","text":"<p>Convert the model into training mode while keep normalization layer freezed.</p>"},{"location":"backbones/#apis-for-external-models","title":"APIs for External Models","text":""},{"location":"backbones/#terratorch.models.backbones.torchgeo_vit","title":"<code>terratorch.models.backbones.torchgeo_vit</code>","text":""},{"location":"backbones/#terratorch.models.backbones.torchgeo_vit.ViTEncoderWrapper","title":"<code>ViTEncoderWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>A wrapper for ViT models from torchgeo to return only the forward pass of the encoder  Attributes:     satlas_model (VisionTransformer): The instantiated dofa model     weights Methods:     forward(x: List[torch.Tensor], wavelengths: list[float]) -&gt; torch.Tensor:         Forward pass for embeddings with specified indices.</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_vit.ViTEncoderWrapper.__init__","title":"<code>__init__(vit_model, vit_meta, weights=None, out_indices=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dofa_model</code> <code>DOFA</code> <p>The decoder module to be wrapped.</p> required"},{"location":"backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_etm_sr_moco","title":"<code>ssl4eol_vit_small_patch16_224_landsat_etm_sr_moco(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_ETM_SR_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_etm_sr_simclr","title":"<code>ssl4eol_vit_small_patch16_224_landsat_etm_sr_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_ETM_SR_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_etm_toa_moco","title":"<code>ssl4eol_vit_small_patch16_224_landsat_etm_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_ETM_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_etm_toa_simclr","title":"<code>ssl4eol_vit_small_patch16_224_landsat_etm_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_ETM_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_oli_sr_moco","title":"<code>ssl4eol_vit_small_patch16_224_landsat_oli_sr_moco(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_OLI_SR_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_oli_sr_simclr","title":"<code>ssl4eol_vit_small_patch16_224_landsat_oli_sr_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_OLI_SR_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_oli_tirs_toa_simclr","title":"<code>ssl4eol_vit_small_patch16_224_landsat_oli_tirs_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_OLI_TIRS_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_tm_toa_moco","title":"<code>ssl4eol_vit_small_patch16_224_landsat_tm_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_TM_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eol_vit_small_patch16_224_landsat_tm_toa_simclr","title":"<code>ssl4eol_vit_small_patch16_224_landsat_tm_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.LANDSAT_TM_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eos12_vit_small_patch16_224_sentinel2_all_dino","title":"<code>ssl4eos12_vit_small_patch16_224_sentinel2_all_dino(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.SENTINEL2_ALL_DINO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_vit.ssl4eos12_vit_small_patch16_224_sentinel2_all_moco","title":"<code>ssl4eos12_vit_small_patch16_224_sentinel2_all_moco(model_bands, pretrained=False, ckpt_data=None, weights=ViTSmall16_Weights.SENTINEL2_ALL_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet","title":"<code>terratorch.models.backbones.torchgeo_resnet</code>","text":""},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ResNetEncoderWrapper","title":"<code>ResNetEncoderWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>A wrapper for ViT models from torchgeo to return only the forward pass of the encoder  Attributes:     satlas_model (VisionTransformer): The instantiated dofa model     weights Methods:     forward(x: List[torch.Tensor], wavelengths: list[float]) -&gt; torch.Tensor:         Forward pass for embeddings with specified indices.</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ResNetEncoderWrapper.__init__","title":"<code>__init__(resnet_model, resnet_meta, weights=None, out_indices=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dofa_model</code> <code>DOFA</code> <p>The decoder module to be wrapped.</p> required"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.fmow_resnet50_fmow_rgb_gassl","title":"<code>fmow_resnet50_fmow_rgb_gassl(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.FMOW_RGB_GASSL, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet152_sentinel2_mi_ms","title":"<code>satlas_resnet152_sentinel2_mi_ms(model_bands, pretrained=False, ckpt_data=None, weights=ResNet152_Weights.SENTINEL2_MI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet152_sentinel2_mi_rgb","title":"<code>satlas_resnet152_sentinel2_mi_rgb(model_bands, pretrained=False, ckpt_data=None, weights=ResNet152_Weights.SENTINEL2_MI_RGB_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet152_sentinel2_si_ms_satlas","title":"<code>satlas_resnet152_sentinel2_si_ms_satlas(model_bands, pretrained=False, ckpt_data=None, weights=ResNet152_Weights.SENTINEL2_SI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet152_sentinel2_si_rgb_satlas","title":"<code>satlas_resnet152_sentinel2_si_rgb_satlas(model_bands, pretrained=False, ckpt_data=None, weights=ResNet152_Weights.SENTINEL2_SI_RGB_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet50_sentinel2_mi_ms_satlas","title":"<code>satlas_resnet50_sentinel2_mi_ms_satlas(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_MI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet50_sentinel2_mi_rgb_satlas","title":"<code>satlas_resnet50_sentinel2_mi_rgb_satlas(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_MI_RGB_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet50_sentinel2_si_ms_satlas","title":"<code>satlas_resnet50_sentinel2_si_ms_satlas(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_SI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.satlas_resnet50_sentinel2_si_rgb_satlas","title":"<code>satlas_resnet50_sentinel2_si_rgb_satlas(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_SI_RGB_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.seco_resnet18_sentinel2_rgb_seco","title":"<code>seco_resnet18_sentinel2_rgb_seco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.SENTINEL2_RGB_SECO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.seco_resnet50_sentinel2_rgb_seco","title":"<code>seco_resnet50_sentinel2_rgb_seco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_RGB_SECO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_etm_sr_moco","title":"<code>ssl4eol_resnet18_landsat_etm_sr_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_ETM_SR_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_etm_sr_simclr","title":"<code>ssl4eol_resnet18_landsat_etm_sr_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_ETM_SR_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_etm_toa_moco","title":"<code>ssl4eol_resnet18_landsat_etm_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_ETM_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_etm_toa_simclr","title":"<code>ssl4eol_resnet18_landsat_etm_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_ETM_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_oli_sr_moco","title":"<code>ssl4eol_resnet18_landsat_oli_sr_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_OLI_SR_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_oli_sr_simclr","title":"<code>ssl4eol_resnet18_landsat_oli_sr_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_OLI_SR_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_oli_tirs_toa_moco","title":"<code>ssl4eol_resnet18_landsat_oli_tirs_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_OLI_TIRS_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_oli_tirs_toa_simclr","title":"<code>ssl4eol_resnet18_landsat_oli_tirs_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_OLI_TIRS_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_tm_toa_moco","title":"<code>ssl4eol_resnet18_landsat_tm_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_TM_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet18_landsat_tm_toa_simclr","title":"<code>ssl4eol_resnet18_landsat_tm_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.LANDSAT_TM_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_etm_sr_moco","title":"<code>ssl4eol_resnet50_landsat_etm_sr_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_ETM_SR_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_etm_sr_simclr","title":"<code>ssl4eol_resnet50_landsat_etm_sr_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_ETM_SR_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_etm_toa_moco","title":"<code>ssl4eol_resnet50_landsat_etm_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_ETM_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_etm_toa_simclr","title":"<code>ssl4eol_resnet50_landsat_etm_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_ETM_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_oli_sr_moco","title":"<code>ssl4eol_resnet50_landsat_oli_sr_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_OLI_SR_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_oli_sr_simclr","title":"<code>ssl4eol_resnet50_landsat_oli_sr_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_OLI_SR_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_oli_tirs_toa_moco","title":"<code>ssl4eol_resnet50_landsat_oli_tirs_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_OLI_TIRS_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_oli_tirs_toa_simclr","title":"<code>ssl4eol_resnet50_landsat_oli_tirs_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_OLI_TIRS_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_tm_toa_moco","title":"<code>ssl4eol_resnet50_landsat_tm_toa_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_TM_TOA_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eol_resnet50_landsat_tm_toa_simclr","title":"<code>ssl4eol_resnet50_landsat_tm_toa_simclr(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.LANDSAT_TM_TOA_SIMCLR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet18_sentinel2_all_moco","title":"<code>ssl4eos12_resnet18_sentinel2_all_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.SENTINEL2_ALL_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet18_sentinel2_rgb_moco","title":"<code>ssl4eos12_resnet18_sentinel2_rgb_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet18_Weights.SENTINEL2_RGB_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet50_sentinel1_all_decur","title":"<code>ssl4eos12_resnet50_sentinel1_all_decur(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL1_ALL_DECUR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet50_sentinel1_all_moco","title":"<code>ssl4eos12_resnet50_sentinel1_all_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL1_ALL_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet50_sentinel2_all_decur","title":"<code>ssl4eos12_resnet50_sentinel2_all_decur(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_ALL_DECUR, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet50_sentinel2_all_dino","title":"<code>ssl4eos12_resnet50_sentinel2_all_dino(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_ALL_DINO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet50_sentinel2_all_moco","title":"<code>ssl4eos12_resnet50_sentinel2_all_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_ALL_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_resnet.ssl4eos12_resnet50_sentinel2_rgb_moco","title":"<code>ssl4eos12_resnet50_sentinel2_rgb_moco(model_bands, pretrained=False, ckpt_data=None, weights=ResNet50_Weights.SENTINEL2_RGB_MOCO, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     ViTEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas","title":"<code>terratorch.models.backbones.torchgeo_swin_satlas</code>","text":""},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.SwinEncoderWrapper","title":"<code>SwinEncoderWrapper</code>","text":"<p>               Bases: <code>Module</code></p> <p>A wrapper for Satlas models from torchgeo to return only the forward pass of the encoder  Attributes:     swin_model (SwinTransformer): The instantiated dofa model     weights Methods:     forward(x: List[torch.Tensor], wavelengths: list[float]) -&gt; torch.Tensor:         Forward pass for embeddings with specified indices.</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.SwinEncoderWrapper.__init__","title":"<code>__init__(swin_model, swin_meta, weights=None, out_indices=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>swin_model</code> <code>SwinTransformer</code> <p>The backbone module to be wrapped.</p> required <code>swin_meta</code> <code>dict</code> <p>dict containing the metadata for swin.</p> required <code>weights</code> <code>Weights</code> <p>Weights class for the swin model to be wrapped.</p> <code>None</code> <code>out_indices</code> <code>list</code> <p>List containing the feature indices to be returned.</p> <code>None</code>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_landsat_mi_ms","title":"<code>satlas_swin_b_landsat_mi_ms(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.LANDSAT_MI_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_landsat_mi_rgb","title":"<code>satlas_swin_b_landsat_mi_rgb(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.LANDSAT_SI_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_naip_mi_rgb","title":"<code>satlas_swin_b_naip_mi_rgb(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.NAIP_RGB_MI_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_naip_si_rgb","title":"<code>satlas_swin_b_naip_si_rgb(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.NAIP_RGB_SI_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_sentinel1_mi","title":"<code>satlas_swin_b_sentinel1_mi(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.SENTINEL1_MI_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_sentinel1_si","title":"<code>satlas_swin_b_sentinel1_si(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.SENTINEL1_SI_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_sentinel2_mi_ms","title":"<code>satlas_swin_b_sentinel2_mi_ms(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.SENTINEL2_MI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_sentinel2_si_ms","title":"<code>satlas_swin_b_sentinel2_si_ms(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.SENTINEL2_SI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_b_sentinel2_si_rgb","title":"<code>satlas_swin_b_sentinel2_si_rgb(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_B_Weights.SENTINEL2_SI_RGB_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_t_sentinel2_mi_ms","title":"<code>satlas_swin_t_sentinel2_mi_ms(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_T_Weights.SENTINEL2_MI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_t_sentinel2_mi_rgb","title":"<code>satlas_swin_t_sentinel2_mi_rgb(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_T_Weights.SENTINEL2_MI_RGB_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_t_sentinel2_si_ms","title":"<code>satlas_swin_t_sentinel2_si_ms(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_T_Weights.SENTINEL2_SI_MS_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p>"},{"location":"backbones/#terratorch.models.backbones.torchgeo_swin_satlas.satlas_swin_t_sentinel2_si_rgb","title":"<code>satlas_swin_t_sentinel2_si_rgb(model_bands, pretrained=False, ckpt_data=None, weights=Swin_V2_T_Weights.SENTINEL2_SI_RGB_SATLAS, out_indices=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model_bands</code> <code>list[str]</code> <p>A list containing the names for the bands expected by the model.</p> required <code>pretrained</code> <code>bool</code> <p>The model is already pretrained (weights are available and can be restored) or not.</p> <code>False</code> <code>ckpt_data</code> <code>str | None</code> <p>Path for a checkpoint containing the model weights.</p> <code>None</code> <p>Returns:     SwinEncoderWrapper</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#101","title":"1.0.1","text":"<p>https://github.com/IBM/terratorch/releases/tag/1.0.1</p>"},{"location":"changelog/#10","title":"1.0","text":"<p>https://github.com/IBM/terratorch/releases/tag/1.0</p>"},{"location":"changelog/#0999post1","title":"0.99.9post1","text":"<p>https://github.com/IBM/terratorch/releases/tag/0.99.9post1</p>"},{"location":"changelog/#0999","title":"0.99.9","text":"<p>https://github.com/IBM/terratorch/releases/tag/0.99.9</p>"},{"location":"changelog/#0998","title":"0.99.8","text":"<p>https://github.com/IBM/terratorch/releases/tag/0.99.8</p>"},{"location":"changelog/#0997","title":"0.99.7","text":"<p>https://github.com/IBM/terratorch/releases/tag/0.99.8</p>"},{"location":"contributing/","title":"Contributing to TerraTorch","text":"<p>Contributions to an open source project can came in different ways, but we could summarize them in three main components: adding code (as new models, tasks and auxiliary algorithms or even addressing the solution of a bug), examples using the software (scripts, yaml files and notebooks showcasing the package) and documentation. All these ways are valid for TerraTorch and the users are welcome to contribute in any of these fronts. However, some recommendations and rules are necessary in order to facilitate and organize his process. And this is the matter of the next paragraphs. </p>"},{"location":"contributing/#contributing-with-code","title":"Contributing with code","text":"<p>It is not a trivial task to determine how a modification in the source code will impact already implemented and established features, in this way, for any modification in the core source code (<code>terratorch/</code>) we automatically execute a pipeline with hundreds of unit and integration tests to verify that the package have not broken after the modification be merged to <code>main</code>. In this way, when an user wants to modify <code>terratorch</code> for adding new features or bufixes, this are the best practices. </p> <ul> <li>If you are an user outside the IBM org, create a fork to add your modifications. If you are inside the IBM     org or have received writing provileges, create a branch for it. </li> <li>If you are adding new features, we ask you to also add tests for it. These tests are defined in the     directory <code>tests/</code> and are fundamental to check if your feature is working as expected and not breaking     anything. If your feature is something more complex, as a new model or auxiliary algorithm, you can also     (optionally) to add a complete example, as a notebook, demonstrating how the feature works.</li> <li>After finishing your modifications, we recommend you to test locally using <code>pytest</code>, for example:     <pre><code>pytest -s -v tests/\n</code></pre></li> <li>If all the tests are passing, you can open a PR to <code>terratorch:main</code> describing what you are adding and why     that is important to be merged. You     do not need to choose a reviewer, since the maintainers will check the new open PR and request review for it by themselves.  </li> <li>The PR will pass through the tests in GitHub Actions and if the reviewer approve it, it will soon be merged. </li> <li>It is recommended to add a label to your PR. For example <code>bug</code>, when it solves some issue or <code>enhancement</code>     when it adds new features. </li> </ul> <p>NOTICE: The PR will not be merged if the automatic tests are failing and the user which has sent the PR is     responsible for fixing it. </p>"},{"location":"contributing/#contributing-with-documentation","title":"Contributing with documentation","text":"<p>Documentation is core for any project, however, most part of the time, the developers do not have the time (or patience) to carefully document all the codebase, in this way, contributions from interested users are always welcome.  To add documentation to TerraTorch, you need to be familiar with Markdown, a clean markup language, and MkDocs, a framework which relies on Markdown in order to create webpages as this you are reading. </p> <ul> <li>Install the MkDocs dependencies. Install as a developer <code>pip install terratorch[dev]</code> to include them or manually using this list.</li> <li>Clone the branch dedicated to documentation to a local branch: <pre><code>    git fetch origin improve/docs\n</code></pre></li> <li>Add your modifications and open a PR to <code>improve/docs</code>. It is recommended to add the label <code>documentation</code> to your PR. </li> <li>The PR will be reviewed and approved if it is considered relevant by the maintainers. </li> </ul>"},{"location":"contributing/#contributing-by-reporting-issues","title":"Contributing by reporting issues","text":"<p>The users also can contribute by reporting issues they observed during their experiments. When reporting an issue, provide the more details as possible about the problem, as the configuration of your system, the terminal output and even the files required to reproduce it. To illustrate it, take a look on an example of a good issue. </p>"},{"location":"data/","title":"Data Processing","text":"<p>In our workflow, we leverage TorchGeo to implement datasets and data modules, ensuring robust and flexible data handling. For a deeper dive into working with datasets using TorchGeo, please refer to the TorchGeo tutorials on datasets.</p> <p>In most cases, it\u2019s best to create a custom TorchGeo dataset tailored to your specific data. Doing so gives you complete control over: - Data Loading: Customize how your data is read and organized. - Transforms: Decide which preprocessing or augmentation steps to apply. - Visualization: Define custom plotting methods (for example, when logging with TensorBoard).</p> <p>TorchGeo offers two primary classes to suit different data formats: - <code>NonGeoDataset</code>:   Use this if your dataset is already split into neatly tiled pieces ready for neural network consumption. Essentially, <code>NonGeoDataset</code> is a wrapper around a standard PyTorch dataset, making it straightforward to integrate into your pipeline. - <code>GeoDataset</code>:   Opt for this class if your data comes in the form of large GeoTiff files from which you need to sample during training. <code>GeoDataset</code> automatically aligns your input data with corresponding labels and supports a range of geo-aware sampling techniques.</p> <p>In addition to these specialized TorchGeo datasets, TerraTorch offers generic datasets and data modules designed to work with directory-based data structures, similar to those used in MMLab libraries. These generic tools simplify data loading when your data is organized in conventional file directories: - The Generic Pixel-wise Dataset is ideal for tasks where each pixel represents a sample (e.g., segmentation or dense prediction problems). - The Generic Scalar Label Dataset is best suited for classification tasks where each sample is associated with a single label.</p> <p>TerraTorch also provides corresponding generic data modules that bundle the dataset with training, validation, and testing splits, integrating seamlessly with PyTorch Lightning. This arrangement makes it easy to manage data loading, batching, and preprocessing with minimal configuration.</p> <p>While generic datasets offer a quick start for common data structures, many projects require more tailored solutions. Custom datasets and data modules give you complete control over the entire data handling process\u2014from fine-tuned data loading and specific transformations to enhanced visualization. By developing your own dataset and data module classes, you ensure that every step\u2014from data ingestion to final model input\u2014is optimized for your particular use case. TerraTorch\u2019s examples provide an excellent starting point to build these custom components and integrate them seamlessly into your training pipeline.</p> <p>For additional examples on fine-tuning a TerraTorch model using these components, please refer to the Prithvi EO Examples repository.</p>"},{"location":"data/#data-curation","title":"Data curation","text":"<p>Generally speaking, all the datamodules work by collecting sets of files and concatenating them into batches with a size determined by the user. TerraTorch automatically checks the dimensionality of the files in order to guarantee that they are stackable, otherwise a stackability error will be raised. If you are sure that your data files are in the proper format and do not want to check for stackability, define <code>check_stackability: false</code> in the field <code>data</code> of your yaml file. If you are using the script interface, you just need to pass it as argument to your dataloader class. Alternatively, if you want to fix discrepancies related to dimensionality in your input files at the data loading stage, you can add a pad correction pipeline, as seen in the example <code>tests/resources/configs/manufactured-finetune_prithvi_eo_v2_300_pad_transform.yaml</code>. </p>"},{"location":"data/#using-datasets-already-implemented-in-torchgeo","title":"Using Datasets already implemented in TorchGeo","text":"<p>Using existing TorchGeo DataModules is very easy! Just plug them in! For instance, to use the <code>EuroSATDataModule</code>, in your config file, set the data as: <pre><code>data:\n  class_path: torchgeo.datamodules.EuroSATDataModule\n  init_args:\n    batch_size: 32\n    num_workers: 8\n  dict_kwargs:\n    root: /dccstor/geofm-pre/EuroSat\n    download: True\n    bands:\n      - B02\n      - B03\n      - B04\n      - B08A\n      - B09\n      - B10\n</code></pre> Modifying each parameter as you see fit.</p> <p>You can also do this outside of config files! Simply instantiate the data module as normal and plug it in.</p> <p>Warning</p> <p>To define <code>transforms</code> to be passed to DataModules from TorchGeo from config files, you must use the following format: <pre><code>data:\nclass_path: terratorch.datamodules.TorchNonGeoDataModule\ninit_args:\n  cls: torchgeo.datamodules.EuroSATDataModule\n  transforms:\n    - class_path: albumentations.augmentations.geometric.resize.Resize\n      init_args:\n        height: 224\n        width: 224\n    - class_path: ToTensorV2\n</code></pre> Note the class_path is <code>TorchNonGeoDataModule</code> and the class to be used is passed through <code>cls</code> (there is also a <code>TorchGeoDataModule</code> for geo modules). This has to be done as the <code>transforms</code> argument is passed through <code>**kwargs</code> in TorchGeo, making it difficult to instantiate with LightningCLI. See more details below.</p>"},{"location":"data/#generic-datasets-and-data-modules","title":"Generic datasets and data modules","text":"<p>For the <code>NonGeoDataset</code> case, we also provide \"generic\" datasets and datamodules. These can be used when you would like to load data from given directories, in a style similar to the MMLab libraries.</p>"},{"location":"data/#custom-datasets-and-data-modules","title":"Custom datasets and data modules","text":"<p>Our custom datasets and data modules are crafted to handle specific data, offering enhanced control and flexibility throughout the workflow.  In case you want to use TerraTorch on your specific data, we invite you to develop your own dataset and data module classes by following the examples below. </p>"},{"location":"data/#transforms","title":"Transforms","text":"<p>The transforms module provides a set of specialized image transformations designed to manipulate spatial, temporal, and multimodal data efficiently.  These transformations allow for greater flexibility when working with multi-temporal, multi-channel, and multi-modal datasets, ensuring that data can be formatted appropriately for different model architectures.</p>"},{"location":"datamodules/","title":"Datamodules","text":""},{"location":"datamodules/#terratorch.datamodules.biomassters","title":"<code>terratorch.datamodules.biomassters</code>","text":""},{"location":"datamodules/#terratorch.datamodules.biomassters.BioMasstersNonGeoDataModule","title":"<code>BioMasstersNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for BioMassters datamodule.</p>"},{"location":"datamodules/#terratorch.datamodules.biomassters.BioMasstersNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=BioMasstersNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, aug=None, drop_last=True, sensors=['S1', 'S2'], as_time_series=False, metadata_filename=default_metadata_filename, max_cloud_percentage=None, max_red_mean=None, include_corrupt=True, subset=1, seed=42, use_four_frames=False, **kwargs)</code>","text":"<p>Initializes the DataModule for the non-geospatial BioMassters datamodule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory containing the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>dict[str, Sequence[str]] | Sequence[str]</code> <p>Band configuration; either a dict mapping sensors to bands or a list for the first sensor. Defaults to BioMasstersNonGeo.all_band_names</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation or normalization to apply. Defaults to normalization if not provided.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch. Defaults to True.</p> <code>True</code> <code>sensors</code> <code>Sequence[str]</code> <p>List of sensors to use (e.g., [\"S1\", \"S2\"]). Defaults to [\"S1\", \"S2\"].</p> <code>['S1', 'S2']</code> <code>as_time_series</code> <code>bool</code> <p>Whether to treat data as a time series. Defaults to False.</p> <code>False</code> <code>metadata_filename</code> <code>str</code> <p>Metadata filename. Defaults to \"The_BioMassters_-_features_metadata.csv.csv\".</p> <code>default_metadata_filename</code> <code>max_cloud_percentage</code> <code>float | None</code> <p>Maximum allowed cloud percentage. Defaults to None.</p> <code>None</code> <code>max_red_mean</code> <code>float | None</code> <p>Maximum allowed red band mean. Defaults to None.</p> <code>None</code> <code>include_corrupt</code> <code>bool</code> <p>Whether to include corrupt data. Defaults to True.</p> <code>True</code> <code>subset</code> <code>float</code> <p>Fraction of the dataset to use. Defaults to 1.</p> <code>1</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>use_four_frames</code> <code>bool</code> <p>Whether to use a four frames configuration. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p>"},{"location":"datamodules/#terratorch.datamodules.biomassters.BioMasstersNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required"},{"location":"datamodules/#terratorch.datamodules.burn_intensity","title":"<code>terratorch.datamodules.burn_intensity</code>","text":""},{"location":"datamodules/#terratorch.datamodules.burn_intensity.BurnIntensityNonGeoDataModule","title":"<code>BurnIntensityNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for BurnIntensity datamodule.</p>"},{"location":"datamodules/#terratorch.datamodules.burn_intensity.BurnIntensityNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=BurnIntensityNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, use_full_data=True, no_data_replace=0.0001, no_label_replace=-1, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the DataModule for the BurnIntensity non-geospatial datamodule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of bands to use. Defaults to BurnIntensityNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction.</p> <code>None</code> <code>use_full_data</code> <code>bool</code> <p>Whether to use the full dataset or data with less than 25 percent zeros. Defaults to True.</p> <code>True</code> <code>no_data_replace</code> <code>float | None</code> <p>Value to replace missing data. Defaults to 0.0001.</p> <code>0.0001</code> <code>no_label_replace</code> <code>int | None</code> <p>Value to replace missing labels. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.burn_intensity.BurnIntensityNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required"},{"location":"datamodules/#terratorch.datamodules.carbonflux","title":"<code>terratorch.datamodules.carbonflux</code>","text":""},{"location":"datamodules/#terratorch.datamodules.carbonflux.CarbonFluxNonGeoDataModule","title":"<code>CarbonFluxNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Carbon FLux dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.carbonflux.CarbonFluxNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=CarbonFluxNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, aug=None, no_data_replace=0.0001, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the CarbonFluxNonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of bands to use. Defaults to CarbonFluxNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation sequence; if None, applies multimodal normalization.</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Value to replace missing data. Defaults to 0.0001.</p> <code>0.0001</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.carbonflux.CarbonFluxNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required"},{"location":"datamodules/#terratorch.datamodules.forestnet","title":"<code>terratorch.datamodules.forestnet</code>","text":""},{"location":"datamodules/#terratorch.datamodules.forestnet.ForestNetNonGeoDataModule","title":"<code>ForestNetNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Landslide4Sense dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.forestnet.ForestNetNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, label_map=ForestNetNonGeo.default_label_map, bands=ForestNetNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, fraction=1.0, aug=None, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the ForestNetNonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Directory containing the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for data loaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>label_map</code> <code>dict[str, int]</code> <p>Mapping of labels to integers. Defaults to ForestNetNonGeo.default_label_map.</p> <code>default_label_map</code> <code>bands</code> <code>Sequence[str]</code> <p>List of band names to use. Defaults to ForestNetNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction.</p> <code>None</code> <code>fraction</code> <code>float</code> <p>Fraction of data to use. Defaults to 1.0.</p> <code>1.0</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline; if None, uses Normalize.</p> <code>None</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.forestnet.ForestNetNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required"},{"location":"datamodules/#terratorch.datamodules.fire_scars","title":"<code>terratorch.datamodules.fire_scars</code>","text":""},{"location":"datamodules/#terratorch.datamodules.fire_scars.FireScarsDataModule","title":"<code>FireScarsDataModule</code>","text":"<p>               Bases: <code>GeoDataModule</code></p> <p>Geo Fire Scars data module implementation that merges input data with ground truth segmentation masks.</p>"},{"location":"datamodules/#terratorch.datamodules.fire_scars.FireScarsNonGeoDataModule","title":"<code>FireScarsNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Fire Scars dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.fire_scars.FireScarsNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=FireScarsNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, drop_last=True, no_data_replace=0, no_label_replace=-1, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the FireScarsNonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of band names. Defaults to FireScarsNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch. Defaults to True.</p> <code>True</code> <code>no_data_replace</code> <code>float | None</code> <p>Replacement value for missing data. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replacement value for missing labels. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.fire_scars.FireScarsNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required"},{"location":"datamodules/#terratorch.datamodules.landslide4sense","title":"<code>terratorch.datamodules.landslide4sense</code>","text":""},{"location":"datamodules/#terratorch.datamodules.landslide4sense.Landslide4SenseNonGeoDataModule","title":"<code>Landslide4SenseNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Landslide4Sense dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.landslide4sense.Landslide4SenseNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=Landslide4SenseNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, aug=None, **kwargs)</code>","text":"<p>Initializes the Landslide4SenseNonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for data loaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of band names to use. Defaults to Landslide4SenseNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation pipeline; if None, applies normalization using computed means and stds.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.landslide4sense.Landslide4SenseNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required"},{"location":"datamodules/#terratorch.datamodules.m_eurosat","title":"<code>terratorch.datamodules.m_eurosat</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_eurosat.MEuroSATNonGeoDataModule","title":"<code>MEuroSATNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-EuroSAT dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.m_eurosat.MEuroSATNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MEuroSATNonGeoDataModule for the MEuroSATNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.m_bigearthnet","title":"<code>terratorch.datamodules.m_bigearthnet</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_bigearthnet.MBigEarthNonGeoDataModule","title":"<code>MBigEarthNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-BigEarthNet dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.m_bigearthnet.MBigEarthNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MBigEarthNonGeoDataModule for the M-BigEarthNet dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.m_brick_kiln","title":"<code>terratorch.datamodules.m_brick_kiln</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_brick_kiln.MBrickKilnNonGeoDataModule","title":"<code>MBrickKilnNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-BrickKiln dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.m_brick_kiln.MBrickKilnNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MBrickKilnNonGeoDataModule for the M-BrickKilnNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.m_forestnet","title":"<code>terratorch.datamodules.m_forestnet</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_forestnet.MForestNetNonGeoDataModule","title":"<code>MForestNetNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-ForestNet dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.m_forestnet.MForestNetNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MForestNetNonGeoDataModule for the MForestNetNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.m_so2sat","title":"<code>terratorch.datamodules.m_so2sat</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_so2sat.MSo2SatNonGeoDataModule","title":"<code>MSo2SatNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-So2Sat dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.m_so2sat.MSo2SatNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MSo2SatNonGeoDataModule for the MSo2SatNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.m_pv4ger","title":"<code>terratorch.datamodules.m_pv4ger</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_pv4ger.MPv4gerNonGeoDataModule","title":"<code>MPv4gerNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-Pv4ger dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.m_pv4ger.MPv4gerNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MPv4gerNonGeoDataModule for the MPv4gerNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.m_cashew_plantation","title":"<code>terratorch.datamodules.m_cashew_plantation</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_cashew_plantation.MBeninSmallHolderCashewsNonGeoDataModule","title":"<code>MBeninSmallHolderCashewsNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-Cashew Plantation dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.m_cashew_plantation.MBeninSmallHolderCashewsNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MBeninSmallHolderCashewsNonGeoDataModule for the M-BeninSmallHolderCashewsNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.m_nz_cattle","title":"<code>terratorch.datamodules.m_nz_cattle</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_nz_cattle.MNzCattleNonGeoDataModule","title":"<code>MNzCattleNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-NZCattle dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.m_nz_cattle.MNzCattleNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MNzCattleNonGeoDataModule for the MNzCattleNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.m_chesapeake_landcover","title":"<code>terratorch.datamodules.m_chesapeake_landcover</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_chesapeake_landcover.MChesapeakeLandcoverNonGeoDataModule","title":"<code>MChesapeakeLandcoverNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-ChesapeakeLandcover dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.m_chesapeake_landcover.MChesapeakeLandcoverNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MChesapeakeLandcoverNonGeoDataModule for the M-BigEarthNet dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.m_pv4ger_seg","title":"<code>terratorch.datamodules.m_pv4ger_seg</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_pv4ger_seg.MPv4gerSegNonGeoDataModule","title":"<code>MPv4gerSegNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-Pv4gerSeg dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.m_pv4ger_seg.MPv4gerSegNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MPv4gerNonGeoDataModule for the MPv4gerSegNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.m_SA_crop_type","title":"<code>terratorch.datamodules.m_SA_crop_type</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_SA_crop_type.MSACropTypeNonGeoDataModule","title":"<code>MSACropTypeNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-SA-CropType dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.m_SA_crop_type.MSACropTypeNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MSACropTypeNonGeoDataModule for the MSACropTypeNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.m_neontree","title":"<code>terratorch.datamodules.m_neontree</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_neontree.MNeonTreeNonGeoDataModule","title":"<code>MNeonTreeNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-NeonTree dataset.</p>"},{"location":"datamodules/#terratorch.datamodules.m_neontree.MNeonTreeNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MNeonTreeNonGeoDataModule for the MNeonTreeNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.multi_temporal_crop_classification","title":"<code>terratorch.datamodules.multi_temporal_crop_classification</code>","text":""},{"location":"datamodules/#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule","title":"<code>MultiTemporalCropClassificationDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for multi-temporal crop classification.</p>"},{"location":"datamodules/#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=MultiTemporalCropClassification.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, drop_last=True, no_data_replace=0, no_label_replace=-1, expand_temporal_dimension=True, reduce_zero_label=True, use_metadata=False, metadata_file_name='chips_df.csv', **kwargs)</code>","text":"<p>Initializes the MultiTemporalCropClassificationDataModule for multi-temporal crop classification.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Directory containing the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of bands to use. Defaults to MultiTemporalCropClassification.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch during training. Defaults to True.</p> <code>True</code> <code>no_data_replace</code> <code>float | None</code> <p>Replacement value for missing data. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replacement value for missing labels. Defaults to -1.</p> <code>-1</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True.</p> <code>True</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True.</p> <code>True</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required"},{"location":"datamodules/#terratorch.datamodules.open_sentinel_map","title":"<code>terratorch.datamodules.open_sentinel_map</code>","text":""},{"location":"datamodules/#terratorch.datamodules.open_sentinel_map.OpenSentinelMapDataModule","title":"<code>OpenSentinelMapDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Open Sentinel Map.</p>"},{"location":"datamodules/#terratorch.datamodules.open_sentinel_map.OpenSentinelMapDataModule.__init__","title":"<code>__init__(bands=None, batch_size=8, num_workers=0, data_root='./', train_transform=None, val_transform=None, test_transform=None, predict_transform=None, spatial_interpolate_and_stack_temporally=True, pad_image=None, truncate_image=None, **kwargs)</code>","text":"<p>Initializes the OpenSentinelMapDataModule for the Open Sentinel Map dataset.</p> <p>Parameters:</p> Name Type Description Default <code>bands</code> <code>list[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>spatial_interpolate_and_stack_temporally</code> <code>bool</code> <p>If True, the bands are interpolated and concatenated over time. Default is True.</p> <code>True</code> <code>pad_image</code> <code>int | None</code> <p>Number of timesteps to pad the time dimension of the image. If None, no padding is applied.</p> <code>None</code> <code>truncate_image</code> <code>int | None</code> <p>Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.open_sentinel_map.OpenSentinelMapDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required"},{"location":"datamodules/#terratorch.datamodules.openearthmap","title":"<code>terratorch.datamodules.openearthmap</code>","text":""},{"location":"datamodules/#terratorch.datamodules.openearthmap.OpenEarthMapNonGeoDataModule","title":"<code>OpenEarthMapNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Open Earth Map.</p>"},{"location":"datamodules/#terratorch.datamodules.openearthmap.OpenEarthMapNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', train_transform=None, val_transform=None, test_transform=None, predict_transform=None, aug=None, **kwargs)</code>","text":"<p>Initializes the OpenEarthMapNonGeoDataModule for the Open Earth Map dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for test data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation pipeline; if None, defaults to normalization using computed means and stds.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Can include 'bands' (list[str]) to specify the bands; defaults to OpenEarthMapNonGeo.all_band_names if not provided.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.openearthmap.OpenEarthMapNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required"},{"location":"datamodules/#terratorch.datamodules.pastis","title":"<code>terratorch.datamodules.pastis</code>","text":""},{"location":"datamodules/#terratorch.datamodules.pastis.PASTISDataModule","title":"<code>PASTISDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for PASTIS.</p>"},{"location":"datamodules/#terratorch.datamodules.pastis.PASTISDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', truncate_image=None, pad_image=None, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, **kwargs)</code>","text":"<p>Initializes the PASTISDataModule for the PASTIS dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Directory containing the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>truncate_image</code> <code>int</code> <p>Truncate the time dimension of the image to  a specified number of timesteps. If None, no truncation is performed.</p> <code>None</code> <code>pad_image</code> <code>int</code> <p>Pad the time dimension of the image to a specified  number of timesteps. If None, no padding is applied.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.pastis.PASTISDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required"},{"location":"datamodules/#terratorch.datamodules.sen1floods11","title":"<code>terratorch.datamodules.sen1floods11</code>","text":""},{"location":"datamodules/#terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule","title":"<code>Sen1Floods11NonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Fire Scars.</p>"},{"location":"datamodules/#terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=Sen1Floods11NonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, drop_last=True, constant_scale=0.0001, no_data_replace=0, no_label_replace=-1, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the Sen1Floods11NonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of bands to use. Defaults to Sen1Floods11NonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for test data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch. Defaults to True.</p> <code>True</code> <code>constant_scale</code> <code>float</code> <p>Scale constant applied to the dataset. Defaults to 0.0001.</p> <code>0.0001</code> <code>no_data_replace</code> <code>float | None</code> <p>Replacement value for missing data. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replacement value for missing labels. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required"},{"location":"datamodules/#terratorch.datamodules.sen4agrinet","title":"<code>terratorch.datamodules.sen4agrinet</code>","text":""},{"location":"datamodules/#terratorch.datamodules.sen4agrinet.Sen4AgriNetDataModule","title":"<code>Sen4AgriNetDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Sen4AgriNet.</p>"},{"location":"datamodules/#terratorch.datamodules.sen4agrinet.Sen4AgriNetDataModule.__init__","title":"<code>__init__(bands=None, batch_size=8, num_workers=0, data_root='./', train_transform=None, val_transform=None, test_transform=None, predict_transform=None, seed=42, scenario='random', requires_norm=True, binary_labels=False, linear_encoder=None, **kwargs)</code>","text":"<p>Initializes the Sen4AgriNetDataModule for the Sen4AgriNet dataset.</p> <p>Parameters:</p> Name Type Description Default <code>bands</code> <code>list[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for test data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>scenario</code> <code>str</code> <p>Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020).</p> <code>'random'</code> <code>requires_norm</code> <code>bool</code> <p>Whether normalization is required. Defaults to True.</p> <code>True</code> <code>binary_labels</code> <code>bool</code> <p>Whether to use binary labels. Defaults to False.</p> <code>False</code> <code>linear_encoder</code> <code>dict</code> <p>Mapping for label encoding. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.sen4agrinet.Sen4AgriNetDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required"},{"location":"datamodules/#terratorch.datamodules.sen4map","title":"<code>terratorch.datamodules.sen4map</code>","text":""},{"location":"datamodules/#terratorch.datamodules.sen4map.Sen4MapLucasDataModule","title":"<code>Sen4MapLucasDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>NonGeo LightningDataModule implementation for Sen4map.</p>"},{"location":"datamodules/#terratorch.datamodules.sen4map.Sen4MapLucasDataModule.__init__","title":"<code>__init__(batch_size, num_workers, prefetch_factor=0, train_hdf5_path=None, train_hdf5_keys_path=None, test_hdf5_path=None, test_hdf5_keys_path=None, val_hdf5_path=None, val_hdf5_keys_path=None, **kwargs)</code>","text":"<p>Initializes the Sen4MapLucasDataModule for handling Sen4Map monthly composites.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders.</p> required <code>num_workers</code> <code>int</code> <p>Number of worker processes for data loading.</p> required <code>prefetch_factor</code> <code>int</code> <p>Number of samples to prefetch per worker. Defaults to 0.</p> <code>0</code> <code>train_hdf5_path</code> <code>str</code> <p>Path to the training HDF5 file.</p> <code>None</code> <code>train_hdf5_keys_path</code> <code>str</code> <p>Path to the training HDF5 keys file.</p> <code>None</code> <code>test_hdf5_path</code> <code>str</code> <p>Path to the testing HDF5 file.</p> <code>None</code> <code>test_hdf5_keys_path</code> <code>str</code> <p>Path to the testing HDF5 keys file.</p> <code>None</code> <code>val_hdf5_path</code> <code>str</code> <p>Path to the validation HDF5 file.</p> <code>None</code> <code>val_hdf5_keys_path</code> <code>str</code> <p>Path to the validation HDF5 keys file.</p> <code>None</code> <code>train_hdf5_keys_save_path</code> <code>str</code> <p>(from kwargs) Path to save generated train keys.</p> required <code>test_hdf5_keys_save_path</code> <code>str</code> <p>(from kwargs) Path to save generated test keys.</p> required <code>val_hdf5_keys_save_path</code> <code>str</code> <p>(from kwargs) Path to save generated validation keys.</p> required <code>shuffle</code> <code>bool</code> <p>Global shuffle flag.</p> required <code>train_shuffle</code> <code>bool</code> <p>Shuffle flag for training data; defaults to global shuffle if unset.</p> required <code>val_shuffle</code> <code>bool</code> <p>Shuffle flag for validation data.</p> required <code>test_shuffle</code> <code>bool</code> <p>Shuffle flag for test data.</p> required <code>train_data_fraction</code> <code>float</code> <p>Fraction of training data to use. Defaults to 1.0.</p> required <code>val_data_fraction</code> <code>float</code> <p>Fraction of validation data to use. Defaults to 1.0.</p> required <code>test_data_fraction</code> <code>float</code> <p>Fraction of test data to use. Defaults to 1.0.</p> required <code>all_hdf5_data_path</code> <code>str</code> <p>General HDF5 data path for all splits. If provided, overrides specific paths.</p> required <code>resize</code> <code>bool</code> <p>Whether to resize images. Defaults to False.</p> required <code>resize_to</code> <code>int or tuple</code> <p>Target size for resizing images.</p> required <code>resize_interpolation</code> <code>str</code> <p>Interpolation mode for resizing ('bilinear', 'bicubic', etc.).</p> required <code>resize_antialiasing</code> <code>bool</code> <p>Whether to apply antialiasing during resizing. Defaults to True.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.sen4map.Sen4MapLucasDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, test.</p> required"},{"location":"datamodules/#terratorch.datamodules.torchgeo_data_module","title":"<code>terratorch.datamodules.torchgeo_data_module</code>","text":"<p>Ugly proxy objects so parsing config file works with transforms.</p> <p>These are necessary since, for LightningCLI to instantiate arguments as objects from the config, they must have type annotations</p> <p>In TorchGeo, <code>transforms</code> is passed in **kwargs, so it has no type annotations! To get around that, we create these wrappers that have transforms type annotated. They create the transforms and forward all method and attribute calls to the original TorchGeo datamodule.</p> <p>Additionally, TorchGeo datasets pass the data to the transforms callable as a dict, and as a tensor.</p> <p>Albumentations expects this data not as a dict but as different key-value arguments, and as numpy. We handle that conversion here.</p>"},{"location":"datamodules/#terratorch.datamodules.torchgeo_data_module.TorchGeoDataModule","title":"<code>TorchGeoDataModule</code>","text":"<p>               Bases: <code>GeoDataModule</code></p> <p>Proxy object for using Geo data modules defined by TorchGeo.</p> <p>Allows for transforms to be defined and passed using config files. The only reason this class exists is so that we can annotate the transforms argument with a type. This is required for lightningcli and config files. As such, all getattr and setattr will be redirected to the underlying class.</p>"},{"location":"datamodules/#terratorch.datamodules.torchgeo_data_module.TorchGeoDataModule.__init__","title":"<code>__init__(cls, batch_size=None, num_workers=0, transforms=None, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[GeoDataModule]</code> <p>TorchGeo DataModule class to be instantiated</p> required <code>batch_size</code> <code>int | None</code> <p>batch_size. Defaults to None.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>num_workers. Defaults to 0.</p> <code>0</code> <code>transforms</code> <code>None | list[BasicTransform]</code> <p>List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Arguments passed to instantiate <code>cls</code>.</p> <code>{}</code>"},{"location":"datamodules/#terratorch.datamodules.torchgeo_data_module.TorchNonGeoDataModule","title":"<code>TorchNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>Proxy object for using NonGeo data modules defined by TorchGeo.</p> <p>Allows for transforms to be defined and passed using config files. The only reason this class exists is so that we can annotate the transforms argument with a type. This is required for lightningcli and config files. As such, all getattr and setattr will be redirected to the underlying class.</p>"},{"location":"datamodules/#terratorch.datamodules.torchgeo_data_module.TorchNonGeoDataModule.__init__","title":"<code>__init__(cls, batch_size=None, num_workers=0, transforms=None, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[NonGeoDataModule]</code> <p>TorchGeo DataModule class to be instantiated</p> required <code>batch_size</code> <code>int | None</code> <p>batch_size. Defaults to None.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>num_workers. Defaults to 0.</p> <code>0</code> <code>transforms</code> <code>None | list[BasicTransform]</code> <p>List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Arguments passed to instantiate <code>cls</code>.</p> <code>{}</code>"},{"location":"datasets/","title":"Datasets","text":""},{"location":"datasets/#terratorch.datasets.biomassters","title":"<code>terratorch.datasets.biomassters</code>","text":""},{"location":"datasets/#terratorch.datasets.biomassters.BioMasstersNonGeo","title":"<code>BioMasstersNonGeo</code>","text":"<p>               Bases: <code>BioMassters</code></p> <p>BioMassters Dataset for Aboveground Biomass prediction.</p> <p>Dataset intended for Aboveground Biomass (AGB) prediction over Finnish forests based on Sentinel 1 and 2 data with corresponding target AGB mask values generated by Light Detection and Ranging (LiDAR).</p> <p>Dataset Format:</p> <ul> <li>.tif files for Sentinel 1 and 2 data</li> <li>.tif file for pixel wise AGB target mask</li> <li>.csv files for metadata regarding features and targets</li> </ul> <p>Dataset Features:</p> <ul> <li>13,000 target AGB masks of size (256x256px)</li> <li>12 months of data per target mask</li> <li>Sentinel 1 and Sentinel 2 data for each location</li> <li>Sentinel 1 available for every month</li> <li>Sentinel 2 available for almost every month   (not available for every month due to ESA acquisition halt over the region   during particular periods)</li> </ul> <p>If you use this dataset in your research, please cite the following paper:</p> <ul> <li>https://nascetti-a.github.io/BioMasster/</li> </ul> <p>.. versionadded:: 0.5</p>"},{"location":"datasets/#terratorch.datasets.biomassters.BioMasstersNonGeo.__init__","title":"<code>__init__(root='data', split='train', bands=BAND_SETS['all'], transform=None, mask_mean=63.4584, mask_std=72.21242, sensors=['S1', 'S2'], as_time_series=False, metadata_filename=default_metadata_filename, max_cloud_percentage=None, max_red_mean=None, include_corrupt=True, subset=1, seed=42, use_four_frames=False)</code>","text":"<p>Initialize a new instance of BioMassters dataset.</p> <p>If <code>as_time_series=False</code> (the default), each time step becomes its own sample with the target being shared across multiple samples.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <p>root directory where dataset can be found</p> <code>'data'</code> <code>split</code> <code>str</code> <p>train or test split</p> <code>'train'</code> <code>sensors</code> <code>Sequence[str]</code> <p>which sensors to consider for the sample, Sentinel 1 and/or Sentinel 2 ('S1', 'S2')</p> <code>['S1', 'S2']</code> <code>as_time_series</code> <code>bool</code> <p>whether or not to return all available time-steps or just a single one for a given target location</p> <code>False</code> <code>metadata_filename</code> <code>str</code> <p>metadata file to be used</p> <code>default_metadata_filename</code> <code>max_cloud_percentage</code> <code>float | None</code> <p>maximum allowed cloud percentage for images</p> <code>None</code> <code>max_red_mean</code> <code>float | None</code> <p>maximum allowed red_mean value for images</p> <code>None</code> <code>include_corrupt</code> <code>bool</code> <p>whether to include images marked as corrupted</p> <code>True</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if <code>split</code> or <code>sensors</code> is invalid</p> <code>DatasetNotFoundError</code> <p>If dataset is not found.</p>"},{"location":"datasets/#terratorch.datasets.biomassters.BioMasstersNonGeo.plot","title":"<code>plot(sample, show_titles=True, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>show_titles</code> <code>bool</code> <p>flag indicating whether to show titles above each panel</p> <code>True</code> <code>suptitle</code> <code>str | None</code> <p>optional suptitle to use for figure</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p>"},{"location":"datasets/#terratorch.datasets.burn_intensity","title":"<code>terratorch.datasets.burn_intensity</code>","text":""},{"location":"datasets/#terratorch.datasets.burn_intensity.BurnIntensityNonGeo","title":"<code>BurnIntensityNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>Dataset implementation for Burn Intensity classification.</p>"},{"location":"datasets/#terratorch.datasets.burn_intensity.BurnIntensityNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, use_full_data=True, no_data_replace=0.0001, no_label_replace=-1, use_metadata=False)</code>","text":"<p>Initialize the BurnIntensity dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train' or 'val'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to output. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Optional[Compose]</code> <p>Albumentations transform to be applied.</p> <code>None</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (location).</p> <code>False</code> <code>use_full_data</code> <code>bool</code> <p>Wheter to use full data or data with less than 25 percent zeros.</p> <code>True</code> <code>no_data_replace</code> <code>Optional[float]</code> <p>Value to replace NaNs in images.</p> <code>0.0001</code> <code>no_label_replace</code> <code>Optional[int]</code> <p>Value to replace NaNs in labels.</p> <code>-1</code>"},{"location":"datasets/#terratorch.datasets.burn_intensity.BurnIntensityNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by <code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A matplotlib Figure with the rendered sample.</p>"},{"location":"datasets/#terratorch.datasets.carbonflux","title":"<code>terratorch.datasets.carbonflux</code>","text":""},{"location":"datasets/#terratorch.datasets.carbonflux.CarbonFluxNonGeo","title":"<code>CarbonFluxNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>Dataset for Carbon Flux regression from HLS images and MERRA data.</p>"},{"location":"datasets/#terratorch.datasets.carbonflux.CarbonFluxNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, gpp_mean=None, gpp_std=None, no_data_replace=0.0001, use_metadata=False, modalities=('image', 'merra_vars'))</code>","text":"<p>Initialize the CarbonFluxNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>'train' or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to use. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Optional[Compose]</code> <p>Albumentations transform to be applied.</p> <code>None</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata (coordinates and date).</p> <code>False</code> <code>merra_means</code> <code>Sequence[float]</code> <p>Means for MERRA data normalization.</p> required <code>merra_stds</code> <code>Sequence[float]</code> <p>Standard deviations for MERRA data normalization.</p> required <code>gpp_mean</code> <code>float</code> <p>Mean for GPP normalization.</p> <code>None</code> <code>gpp_std</code> <code>float</code> <p>Standard deviation for GPP normalization.</p> <code>None</code> <code>no_data_replace</code> <code>Optional[float]</code> <p>Value to replace NO_DATA values in images.</p> <code>0.0001</code>"},{"location":"datasets/#terratorch.datasets.carbonflux.CarbonFluxNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Any]</code> <p>A sample returned by <code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional title for the figure.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A matplotlib figure with the rendered sample.</p>"},{"location":"datasets/#terratorch.datasets.forestnet","title":"<code>terratorch.datasets.forestnet</code>","text":""},{"location":"datasets/#terratorch.datasets.forestnet.ForestNetNonGeo","title":"<code>ForestNetNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for ForestNet.</p>"},{"location":"datasets/#terratorch.datasets.forestnet.ForestNetNonGeo.__init__","title":"<code>__init__(data_root, split='train', label_map=default_label_map, transform=None, fraction=1.0, bands=BAND_SETS['all'], use_metadata=False)</code>","text":"<p>Initialize the ForestNetNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>label_map</code> <code>Dict[str, int]</code> <p>Mapping from label names to integer labels.</p> <code>default_label_map</code> <code>transform</code> <code>Compose | None</code> <p>Transformations to be applied to the images.</p> <code>None</code> <code>fraction</code> <code>float</code> <p>Fraction of the dataset to use. Defaults to 1.0 (use all data).</p> <code>1.0</code>"},{"location":"datasets/#terratorch.datasets.forestnet.ForestNetNonGeo.map_label","title":"<code>map_label(index)</code>","text":"<p>Map the label name to an integer label.</p>"},{"location":"datasets/#terratorch.datasets.fire_scars","title":"<code>terratorch.datasets.fire_scars</code>","text":""},{"location":"datasets/#terratorch.datasets.fire_scars.FireScarsHLS","title":"<code>FireScarsHLS</code>","text":"<p>               Bases: <code>RasterDataset</code></p> <p>RasterDataset implementation for fire scars input images.</p>"},{"location":"datasets/#terratorch.datasets.fire_scars.FireScarsNonGeo","title":"<code>FireScarsNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for fire scars.</p>"},{"location":"datasets/#terratorch.datasets.fire_scars.FireScarsNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, no_data_replace=0, no_label_replace=-1, use_metadata=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>bands</code> <code>list[str]</code> <p>Bands that should be output by the dataset. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If None, does no replacement. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>whether to return metadata info (time and location).</p> <code>False</code>"},{"location":"datasets/#terratorch.datasets.fire_scars.FireScarsNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p>"},{"location":"datasets/#terratorch.datasets.fire_scars.FireScarsSegmentationMask","title":"<code>FireScarsSegmentationMask</code>","text":"<p>               Bases: <code>RasterDataset</code></p> <p>RasterDataset implementation for fire scars segmentation mask. Can be easily merged with input images using the &amp; operator.</p>"},{"location":"datasets/#terratorch.datasets.landslide4sense","title":"<code>terratorch.datasets.landslide4sense</code>","text":""},{"location":"datasets/#terratorch.datasets.landslide4sense.Landslide4SenseNonGeo","title":"<code>Landslide4SenseNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for Landslide4Sense.</p>"},{"location":"datasets/#terratorch.datasets.landslide4sense.Landslide4SenseNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None)</code>","text":"<p>Initialize the Landslide4Sense dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'validation', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code>"},{"location":"datasets/#terratorch.datasets.m_eurosat","title":"<code>terratorch.datasets.m_eurosat</code>","text":""},{"location":"datasets/#terratorch.datasets.m_eurosat.MEuroSATNonGeo","title":"<code>MEuroSATNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-EuroSAT.</p>"},{"location":"datasets/#terratorch.datasets.m_eurosat.MEuroSATNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code>"},{"location":"datasets/#terratorch.datasets.m_eurosat.MEuroSATNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p>"},{"location":"datasets/#terratorch.datasets.m_bigearthnet","title":"<code>terratorch.datasets.m_bigearthnet</code>","text":""},{"location":"datasets/#terratorch.datasets.m_bigearthnet.MBigEarthNonGeo","title":"<code>MBigEarthNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-BigEarthNet.</p>"},{"location":"datasets/#terratorch.datasets.m_bigearthnet.MBigEarthNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code>"},{"location":"datasets/#terratorch.datasets.m_bigearthnet.MBigEarthNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p>"},{"location":"datasets/#terratorch.datasets.m_brick_kiln","title":"<code>terratorch.datasets.m_brick_kiln</code>","text":""},{"location":"datasets/#terratorch.datasets.m_brick_kiln.MBrickKilnNonGeo","title":"<code>MBrickKilnNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-BrickKiln.</p>"},{"location":"datasets/#terratorch.datasets.m_brick_kiln.MBrickKilnNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code>"},{"location":"datasets/#terratorch.datasets.m_brick_kiln.MBrickKilnNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p>"},{"location":"datasets/#terratorch.datasets.m_forestnet","title":"<code>terratorch.datasets.m_forestnet</code>","text":""},{"location":"datasets/#terratorch.datasets.m_forestnet.MForestNetNonGeo","title":"<code>MForestNetNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-ForestNet.</p>"},{"location":"datasets/#terratorch.datasets.m_forestnet.MForestNetNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code>"},{"location":"datasets/#terratorch.datasets.m_forestnet.MForestNetNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p>"},{"location":"datasets/#terratorch.datasets.m_so2sat","title":"<code>terratorch.datasets.m_so2sat</code>","text":""},{"location":"datasets/#terratorch.datasets.m_so2sat.MSo2SatNonGeo","title":"<code>MSo2SatNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-So2Sat.</p>"},{"location":"datasets/#terratorch.datasets.m_so2sat.MSo2SatNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code>"},{"location":"datasets/#terratorch.datasets.m_so2sat.MSo2SatNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p>"},{"location":"datasets/#terratorch.datasets.m_pv4ger","title":"<code>terratorch.datasets.m_pv4ger</code>","text":""},{"location":"datasets/#terratorch.datasets.m_pv4ger.MPv4gerNonGeo","title":"<code>MPv4gerNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-PV4GER.</p>"},{"location":"datasets/#terratorch.datasets.m_pv4ger.MPv4gerNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (location coordinates).</p> <code>False</code>"},{"location":"datasets/#terratorch.datasets.m_pv4ger.MPv4gerNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p>"},{"location":"datasets/#terratorch.datasets.m_cashew_plantation","title":"<code>terratorch.datasets.m_cashew_plantation</code>","text":""},{"location":"datasets/#terratorch.datasets.m_cashew_plantation.MBeninSmallHolderCashewsNonGeo","title":"<code>MBeninSmallHolderCashewsNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-BeninSmallHolderCashews.</p>"},{"location":"datasets/#terratorch.datasets.m_cashew_plantation.MBeninSmallHolderCashewsNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time).</p> <code>False</code>"},{"location":"datasets/#terratorch.datasets.m_cashew_plantation.MBeninSmallHolderCashewsNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p>"},{"location":"datasets/#terratorch.datasets.m_nz_cattle","title":"<code>terratorch.datasets.m_nz_cattle</code>","text":""},{"location":"datasets/#terratorch.datasets.m_nz_cattle.MNzCattleNonGeo","title":"<code>MNzCattleNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-NZ-Cattle.</p>"},{"location":"datasets/#terratorch.datasets.m_nz_cattle.MNzCattleNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code>"},{"location":"datasets/#terratorch.datasets.m_nz_cattle.MNzCattleNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p>"},{"location":"datasets/#terratorch.datasets.m_chesapeake_landcover","title":"<code>terratorch.datasets.m_chesapeake_landcover</code>","text":""},{"location":"datasets/#terratorch.datasets.m_chesapeake_landcover.MChesapeakeLandcoverNonGeo","title":"<code>MChesapeakeLandcoverNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-ChesapeakeLandcover.</p>"},{"location":"datasets/#terratorch.datasets.m_chesapeake_landcover.MChesapeakeLandcoverNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code>"},{"location":"datasets/#terratorch.datasets.m_chesapeake_landcover.MChesapeakeLandcoverNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p>"},{"location":"datasets/#terratorch.datasets.m_pv4ger_seg","title":"<code>terratorch.datasets.m_pv4ger_seg</code>","text":""},{"location":"datasets/#terratorch.datasets.m_pv4ger_seg.MPv4gerSegNonGeo","title":"<code>MPv4gerSegNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-PV4GER-SEG.</p>"},{"location":"datasets/#terratorch.datasets.m_pv4ger_seg.MPv4gerSegNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (location coordinates).</p> <code>False</code>"},{"location":"datasets/#terratorch.datasets.m_pv4ger_seg.MPv4gerSegNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p>"},{"location":"datasets/#terratorch.datasets.m_SA_crop_type","title":"<code>terratorch.datasets.m_SA_crop_type</code>","text":""},{"location":"datasets/#terratorch.datasets.m_SA_crop_type.MSACropTypeNonGeo","title":"<code>MSACropTypeNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-SA-Crop-Type.</p>"},{"location":"datasets/#terratorch.datasets.m_SA_crop_type.MSACropTypeNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code>"},{"location":"datasets/#terratorch.datasets.m_SA_crop_type.MSACropTypeNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p>"},{"location":"datasets/#terratorch.datasets.m_neontree","title":"<code>terratorch.datasets.m_neontree</code>","text":""},{"location":"datasets/#terratorch.datasets.m_neontree.MNeonTreeNonGeo","title":"<code>MNeonTreeNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-NeonTree.</p>"},{"location":"datasets/#terratorch.datasets.m_neontree.MNeonTreeNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=rgb_bands, transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to RGB bands.</p> <code>rgb_bands</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code>"},{"location":"datasets/#terratorch.datasets.m_neontree.MNeonTreeNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p>"},{"location":"datasets/#terratorch.datasets.multi_temporal_crop_classification","title":"<code>terratorch.datasets.multi_temporal_crop_classification</code>","text":""},{"location":"datasets/#terratorch.datasets.multi_temporal_crop_classification.MultiTemporalCropClassification","title":"<code>MultiTemporalCropClassification</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for multi-temporal crop classification.</p>"},{"location":"datasets/#terratorch.datasets.multi_temporal_crop_classification.MultiTemporalCropClassification.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=True, reduce_zero_label=True, use_metadata=False, metadata_file_name='chips_df.csv')</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>one of 'train' or 'val'.</p> <code>'train'</code> <code>bands</code> <code>list[str]</code> <p>Bands that should be output by the dataset. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If None, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True.</p> <code>True</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True.</p> <code>True</code> <code>use_metadata</code> <code>bool</code> <p>whether to return metadata info (time and location).</p> <code>False</code>"},{"location":"datasets/#terratorch.datasets.multi_temporal_crop_classification.MultiTemporalCropClassification.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p>"},{"location":"datasets/#terratorch.datasets.open_sentinel_map","title":"<code>terratorch.datasets.open_sentinel_map</code>","text":""},{"location":"datasets/#terratorch.datasets.open_sentinel_map.OpenSentinelMap","title":"<code>OpenSentinelMap</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>Pytorch Dataset class to load samples from the OpenSentinelMap dataset, supporting multiple bands and temporal sampling strategies.</p>"},{"location":"datasets/#terratorch.datasets.open_sentinel_map.OpenSentinelMap.__init__","title":"<code>__init__(data_root, split='train', bands=None, transform=None, spatial_interpolate_and_stack_temporally=True, pad_image=None, truncate_image=None, target=0, pick_random_pair=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>str</code> <p>Dataset split to load. Options are 'train', 'val', or 'test'. Defaults to 'train'.</p> <code>'train'</code> <code>bands</code> <code>list of str</code> <p>List of band names to load. Defaults to ['gsd_10', 'gsd_20', 'gsd_60'].</p> <code>None</code> <code>transform</code> <code>Compose</code> <p>Albumentations transformations to apply to the data.</p> <code>None</code> <code>spatial_interpolate_and_stack_temporally</code> <code>bool</code> <p>If True, the bands are interpolated and concatenated over time. Default is True.</p> <code>True</code> <code>pad_image</code> <code>int</code> <p>Number of timesteps to pad the time dimension of the image. If None, no padding is applied.</p> <code>None</code> <code>truncate_image</code> <code>int</code> <p>Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed.</p> <code>None</code> <code>target</code> <code>int</code> <p>Specifies which target class to use from the mask. Default is 0.</p> <code>0</code> <code>pick_random_pair</code> <code>bool</code> <p>If True, selects two random images from the temporal sequence. Default is True.</p> <code>True</code>"},{"location":"datasets/#terratorch.datasets.openearthmap","title":"<code>terratorch.datasets.openearthmap</code>","text":""},{"location":"datasets/#terratorch.datasets.openearthmap.OpenEarthMapNonGeo","title":"<code>OpenEarthMapNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>OpenEarthMapNonGeo Dataset for non-georeferenced imagery.</p> <p>This dataset class handles non-georeferenced image data from the OpenEarthMap dataset. It supports configurable band sets and transformations, and performs cropping operations to ensure that the images conform to the required input dimensions. The dataset is split into \"train\", \"test\", and \"val\" subsets based on the provided split parameter.</p>"},{"location":"datasets/#terratorch.datasets.openearthmap.OpenEarthMapNonGeo.__init__","title":"<code>__init__(data_root, bands=BAND_SETS['all'], transform=None, split='train', crop_size=256, random_crop=True)</code>","text":"<p>Initialize a new instance of the OpenEarthMapNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>The root directory containing the dataset files.</p> required <code>bands</code> <code>Sequence[str]</code> <p>A list of band names to be used. Default is BAND_SETS[\"all\"].</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose or None</code> <p>A transformation pipeline to be applied to the data. If None, a default transform converting the data to a tensor is applied.</p> <code>None</code> <code>split</code> <code>str</code> <p>The dataset split to use (\"train\", \"test\", or \"val\"). Default is \"train\".</p> <code>'train'</code> <code>crop_size</code> <code>int</code> <p>The size (in pixels) of the crop to apply to images. Must be greater than 0. Default is 256.</p> <code>256</code> <code>random_crop</code> <code>bool</code> <p>If True, performs a random crop; otherwise, performs a center crop. Default is True.</p> <code>True</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the provided split is not one of \"train\", \"test\", or \"val\".</p> <code>AssertionError</code> <p>If crop_size is not greater than 0.</p>"},{"location":"datasets/#terratorch.datasets.pastis","title":"<code>terratorch.datasets.pastis</code>","text":""},{"location":"datasets/#terratorch.datasets.pastis.PASTIS","title":"<code>PASTIS</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>\" Pytorch Dataset class to load samples from the PASTIS dataset, for semantic and panoptic segmentation.</p>"},{"location":"datasets/#terratorch.datasets.pastis.PASTIS.__init__","title":"<code>__init__(data_root, norm=True, target='semantic', folds=None, reference_date='2018-09-01', date_interval=(-200, 600), class_mapping=None, transform=None, truncate_image=None, pad_image=None, satellites=['S2'])</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the dataset.</p> required <code>norm</code> <code>bool</code> <p>If true, images are standardised using pre-computed channel-wise means and standard deviations.</p> <code>True</code> <code>reference_date</code> <code>(str, Format)</code> <p>'YYYY-MM-DD'): Defines the reference date based on which all observation dates are expressed. Along with the image time series and the target tensor, this dataloader yields the sequence of observation dates (in terms of number of days since the reference date). This sequence of dates is used for instance for the positional encoding in attention based approaches.</p> <code>'2018-09-01'</code> <code>target</code> <code>str</code> <p>'semantic' or 'instance'. Defines which type of target is returned by the dataloader. * If 'semantic' the target tensor is a tensor containing the class of each pixel. * If 'instance' the target tensor is the concatenation of several signals, necessary to train the Parcel-as-Points module:     - the centerness heatmap,     - the instance ids,     - the voronoi partitioning of the patch with regards to the parcels'     centers,     - the (height, width) size of each parcel,     - the semantic label of each parcel,     - the semantic label of each pixel.</p> <code>'semantic'</code> <code>folds</code> <code>list</code> <p>List of ints specifying which of the 5 official folds to load. By default (when None is specified), all folds are loaded.</p> <code>None</code> <code>class_mapping</code> <code>dict</code> <p>A dictionary to define a mapping between the default 18 class nomenclature and another class grouping. If not provided, the default class mapping is used.</p> <code>None</code> <code>transform</code> <code>callable</code> <p>A transform to apply to the loaded data (images, dates, and masks). By default, no transformation is applied.</p> <code>None</code> <code>truncate_image</code> <code>int</code> <p>Truncate the time dimension of the image to a specified number of timesteps. If None, no truncation is performed.</p> <code>None</code> <code>pad_image</code> <code>int</code> <p>Pad the time dimension of the image to a specified number of timesteps. If None, no padding is applied.</p> <code>None</code> <code>satellites</code> <code>list</code> <p>Defines the satellites to use. If you are using PASTIS-R, you have access to Sentinel-2 imagery and Sentinel-1 observations in Ascending and Descending orbits, respectively S2, S1A, and S1D. For example, use satellites=['S2', 'S1A'] for Sentinel-2 + Sentinel-1 ascending time series, or satellites=['S2', 'S1A', 'S1D'] to retrieve all time series. If you are using PASTIS, only S2 observations are available.</p> <code>['S2']</code>"},{"location":"datasets/#terratorch.datasets.sen1floods11","title":"<code>terratorch.datasets.sen1floods11</code>","text":""},{"location":"datasets/#terratorch.datasets.sen1floods11.Sen1Floods11NonGeo","title":"<code>Sen1Floods11NonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for sen1floods11.</p>"},{"location":"datasets/#terratorch.datasets.sen1floods11.Sen1Floods11NonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, constant_scale=0.0001, no_data_replace=0, no_label_replace=-1, use_metadata=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>one of 'train', 'val' or 'test'.</p> <code>'train'</code> <code>bands</code> <code>list[str]</code> <p>Bands that should be output by the dataset. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). Defaults to None, which applies ToTensorV2().</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 0.0001.</p> <code>0.0001</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If None, does no replacement. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>whether to return metadata info (time and location).</p> <code>False</code>"},{"location":"datasets/#terratorch.datasets.sen1floods11.Sen1Floods11NonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p>"},{"location":"datasets/#terratorch.datasets.sen4agrinet","title":"<code>terratorch.datasets.sen4agrinet</code>","text":""},{"location":"datasets/#terratorch.datasets.sen4agrinet.Sen4AgriNet","title":"<code>Sen4AgriNet</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p>"},{"location":"datasets/#terratorch.datasets.sen4agrinet.Sen4AgriNet.__init__","title":"<code>__init__(data_root, bands=None, scenario='random', split='train', transform=None, truncate_image=4, pad_image=4, spatial_interpolate_and_stack_temporally=True, seed=42)</code>","text":"<p>Pytorch Dataset class to load samples from the Sen4AgriNet dataset, supporting multiple scenarios for splitting the data.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>bands</code> <code>list of str</code> <p>List of band names to load. Defaults to all available bands.</p> <code>None</code> <code>scenario</code> <code>str</code> <p>Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020).</p> <code>'random'</code> <code>split</code> <code>str</code> <p>Specifies the dataset split. Options are 'train', 'val', or 'test'.</p> <code>'train'</code> <code>transform</code> <code>Compose</code> <p>Albumentations transformations to apply to the data.</p> <code>None</code> <code>truncate_image</code> <code>int</code> <p>Number of timesteps to truncate the time dimension of the image. If None, no truncation is applied. Default is 4.</p> <code>4</code> <code>pad_image</code> <code>int</code> <p>Number of timesteps to pad the time dimension of the image. If None, no padding is applied. Default is 4.</p> <code>4</code> <code>spatial_interpolate_and_stack_temporally</code> <code>bool</code> <p>Whether to interpolate bands and concatenate them over time</p> <code>True</code> <code>seed</code> <code>int</code> <p>Random seed used for data splitting.</p> <code>42</code>"},{"location":"datasets/#terratorch.datasets.sen4map","title":"<code>terratorch.datasets.sen4map</code>","text":""},{"location":"datasets/#terratorch.datasets.sen4map.Sen4MapDatasetMonthlyComposites","title":"<code>Sen4MapDatasetMonthlyComposites</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Sen4Map Dataset for Monthly Composites.</p> <p>Dataset intended for land-cover and crop classification tasks based on monthly composites derived from multi-temporal satellite data stored in HDF5 files.</p> <p>Dataset Format:</p> <ul> <li>HDF5 files containing multi-temporal acquisitions with spectral bands (e.g., B2, B3, \u2026, B12)</li> <li>Composite images computed as the median across available acquisitions for each month.</li> <li>Classification labels provided via HDF5 attributes (e.g., 'lc1') with mappings defined for:<ul> <li>Land-cover: using <code>land_cover_classification_map</code></li> <li>Crops: using <code>crop_classification_map</code></li> </ul> </li> </ul> <p>Dataset Features:</p> <ul> <li>Supports two classification tasks: \"land-cover\" (default) and \"crops\".</li> <li>Pre-processing options include center cropping, reverse tiling, and resizing.</li> <li>Option to save the keys HDF5 for later filtering.</li> <li>Input channel selection via a mapping between available bands and input bands.</li> </ul>"},{"location":"datasets/#terratorch.datasets.sen4map.Sen4MapDatasetMonthlyComposites.__init__","title":"<code>__init__(h5py_file_object, h5data_keys=None, crop_size=None, dataset_bands=None, input_bands=None, resize=False, resize_to=[224, 224], resize_interpolation=InterpolationMode.BILINEAR, resize_antialiasing=True, reverse_tile=False, reverse_tile_size=3, save_keys_path=None, classification_map='land-cover')</code>","text":"<p>Initialize a new instance of Sen4MapDatasetMonthlyComposites.</p> <p>This dataset loads data from an HDF5 file object containing multi-temporal satellite data and computes monthly composite images by aggregating acquisitions (via median).</p> <p>Parameters:</p> Name Type Description Default <code>h5py_file_object</code> <code>File</code> <p>An open h5py.File object containing the dataset.</p> required <code>h5data_keys</code> <p>Optional list of keys to select a subset of data samples from the HDF5 file. If None, all keys are used.</p> <code>None</code> <code>crop_size</code> <code>None | int</code> <p>Optional integer specifying the square crop size for the output image.</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Optional list of bands available in the dataset.</p> <code>None</code> <code>input_bands</code> <code>list[HLSBands | int] | None</code> <p>Optional list of bands to be used as input channels. Must be provided along with <code>dataset_bands</code>.</p> <code>None</code> <code>resize</code> <p>Boolean flag indicating whether the image should be resized. Default is False.</p> <code>False</code> <code>resize_to</code> <p>Target dimensions [height, width] for resizing. Default is [224, 224].</p> <code>[224, 224]</code> <code>resize_interpolation</code> <p>Interpolation mode used for resizing. Default is InterpolationMode.BILINEAR.</p> <code>BILINEAR</code> <code>resize_antialiasing</code> <p>Boolean flag to apply antialiasing during resizing. Default is True.</p> <code>True</code> <code>reverse_tile</code> <p>Boolean flag indicating whether to apply reverse tiling to the image. Default is False.</p> <code>False</code> <code>reverse_tile_size</code> <p>Kernel size for the reverse tiling operation. Must be an odd number &gt;= 3. Default is 3.</p> <code>3</code> <code>save_keys_path</code> <p>Optional file path to save the list of dataset keys.</p> <code>None</code> <code>classification_map</code> <p>String specifying the classification mapping to use (\"land-cover\" or \"crops\"). Default is \"land-cover\".</p> <code>'land-cover'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>input_bands</code> is provided without specifying <code>dataset_bands</code>.</p> <code>ValueError</code> <p>If an invalid <code>classification_map</code> is provided.</p>"},{"location":"datasets/#terratorch.datasets.sen4map.Sen4MapDatasetMonthlyComposites.reverse_tiling_pytorch","title":"<code>reverse_tiling_pytorch(img_tensor, kernel_size=3)</code>","text":"<p>Upscales an image where every pixel is expanded into <code>kernel_size</code>*<code>kernel_size</code> pixels. Used to test whether the benefit of resizing images to the pre-trained size comes from the bilnearly interpolated pixels, or if the same would be realized with no interpolated pixels.</p>"},{"location":"decoders/","title":"Decoders","text":""},{"location":"decoders/#terratorch.models.decoders.fcn_decoder","title":"<code>terratorch.models.decoders.fcn_decoder</code>","text":""},{"location":"decoders/#terratorch.models.decoders.fcn_decoder.FCNDecoder","title":"<code>FCNDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Fully Convolutional Decoder</p>"},{"location":"decoders/#terratorch.models.decoders.fcn_decoder.FCNDecoder.__init__","title":"<code>__init__(embed_dim, channels=256, num_convs=4, in_index=-1)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>_type_</code> <p>Input embedding dimension</p> required <code>channels</code> <code>int</code> <p>Number of channels for each conv. Defaults to 256.</p> <code>256</code> <code>num_convs</code> <code>int</code> <p>Number of convs. Defaults to 4.</p> <code>4</code> <code>in_index</code> <code>int</code> <p>Index of the input list to take. Defaults to -1.</p> <code>-1</code>"},{"location":"decoders/#terratorch.models.decoders.identity_decoder","title":"<code>terratorch.models.decoders.identity_decoder</code>","text":"<p>Pass the features straight through</p>"},{"location":"decoders/#terratorch.models.decoders.identity_decoder.IdentityDecoder","title":"<code>IdentityDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Identity decoder. Useful to pass the feature straight to the head.</p>"},{"location":"decoders/#terratorch.models.decoders.identity_decoder.IdentityDecoder.__init__","title":"<code>__init__(embed_dim, out_index=-1)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Input embedding dimension</p> required <code>out_index</code> <code>int</code> <p>Index of the input list to take.. Defaults to -1.</p> <code>-1</code>"},{"location":"decoders/#terratorch.models.decoders.upernet_decoder","title":"<code>terratorch.models.decoders.upernet_decoder</code>","text":""},{"location":"decoders/#terratorch.models.decoders.upernet_decoder.PPM","title":"<code>PPM</code>","text":"<p>               Bases: <code>ModuleList</code></p> <p>Pooling Pyramid Module used in PSPNet.</p>"},{"location":"decoders/#terratorch.models.decoders.upernet_decoder.PPM.__init__","title":"<code>__init__(pool_scales, in_channels, channels, align_corners)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>pool_scales</code> <code>tuple[int]</code> <p>Pooling scales used in Pooling Pyramid Module.</p> required <code>in_channels</code> <code>int</code> <p>Input channels.</p> required <code>channels</code> <code>int</code> <p>Channels after modules, before conv_seg.</p> required <code>align_corners</code> <code>bool</code> <p>align_corners argument of F.interpolate.</p> required"},{"location":"decoders/#terratorch.models.decoders.upernet_decoder.PPM.forward","title":"<code>forward(x)</code>","text":"<p>Forward function.</p>"},{"location":"decoders/#terratorch.models.decoders.upernet_decoder.UperNetDecoder","title":"<code>UperNetDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>UperNetDecoder. Adapted from MMSegmentation.</p>"},{"location":"decoders/#terratorch.models.decoders.upernet_decoder.UperNetDecoder.__init__","title":"<code>__init__(embed_dim, pool_scales=(1, 2, 3, 6), channels=256, align_corners=True, scale_modules=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>list[int]</code> <p>Input embedding dimension for each input.</p> required <code>pool_scales</code> <code>tuple[int]</code> <p>Pooling scales used in Pooling Pyramid Module applied on the last feature. Default: (1, 2, 3, 6).</p> <code>(1, 2, 3, 6)</code> <code>channels</code> <code>int</code> <p>Channels used in the decoder. Defaults to 256.</p> <code>256</code> <code>align_corners</code> <code>bool</code> <p>Wheter to align corners in rescaling. Defaults to True.</p> <code>True</code> <code>scale_modules</code> <code>bool</code> <p>Whether to apply scale modules to the inputs. Needed for plain ViT. Defaults to False.</p> <code>False</code>"},{"location":"decoders/#terratorch.models.decoders.upernet_decoder.UperNetDecoder.forward","title":"<code>forward(inputs)</code>","text":"<p>Forward function for feature maps before classifying each pixel with Args:     inputs (list[Tensor]): List of multi-level img features.</p> <p>Returns:</p> Name Type Description <code>feats</code> <code>Tensor</code> <p>A tensor of shape (batch_size, self.channels, H, W) which is feature map for last layer of decoder head.</p>"},{"location":"decoders/#terratorch.models.decoders.upernet_decoder.UperNetDecoder.psp_forward","title":"<code>psp_forward(inputs)</code>","text":"<p>Forward function of PSP module.</p>"},{"location":"encoder_decoder_factory/","title":"EncoderDecoderFactory","text":"<p>Check the Glossary for more information about the terms used in this page.</p> <p>The EncoderDecoderFactory is the main class used to instantiate and compose models for general tasks. </p> <p>This factory leverages the <code>BACKBONE_REGISTRY</code>, <code>DECODER_REGISTRY</code> and <code>NECK_REGISTRY</code> to compose models formed as encoder + decoder, with some optional glue in between provided by the necks. As most current models work this way, this is a particularly important factory, allowing for great flexibility in combining encoders and decoders from different sources.</p> <p>The factory allows arguments to be passed to the encoder, decoder and head. Arguments with the prefix <code>backbone_</code> will be routed to the backbone constructor, with <code>decoder_</code> and <code>head_</code> working the same way. These are accepted dynamically and not checked. Any unused arguments will raise a <code>ValueError</code>.</p> <p>Both encoder and decoder may be passed as strings, in which case they will be looked in the respective registry, or as <code>nn.Modules</code>, in which case they will be used as is. In the second case, the factory assumes in good faith that the encoder or decoder which is passed conforms to the expected contract.</p> <p>Not all decoders will readily accept the raw output of the given encoder. This is where necks come in.  Necks are a sequence of operations which are applied to the output of the encoder before it is passed to the decoder. They must be instances of Neck, which is a subclass of <code>nn.Module</code>, meaning they can even define new trainable parameters.</p> <p>The EncoderDecoderFactory returns a PixelWiseModel or a ScalarOutputModel depending on the task.</p>"},{"location":"encoder_decoder_factory/#terratorch.models.encoder_decoder_factory.EncoderDecoderFactory","title":"<code>terratorch.models.encoder_decoder_factory.EncoderDecoderFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p>"},{"location":"encoder_decoder_factory/#terratorch.models.encoder_decoder_factory.EncoderDecoderFactory.build_model","title":"<code>build_model(task, backbone, decoder, backbone_kwargs=None, decoder_kwargs=None, head_kwargs=None, num_classes=None, necks=None, aux_decoders=None, rescale=True, peft_config=None, **kwargs)</code>","text":"<p>Generic model factory that combines an encoder and decoder, together with a head, for a specific task.</p> <p>Further arguments to be passed to the backbone, decoder or head. They should be prefixed with <code>backbone_</code>, <code>decoder_</code> and <code>head_</code> respectively.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Currently supports \"segmentation\", \"regression\" and \"classification\".</p> required <code>backbone</code> <code>(str, Module)</code> <p>Backbone to be used. If a string, will look for such models in the different registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it directly. The backbone should have and <code>out_channels</code> attribute and its <code>forward</code> should return a list[Tensor].</p> required <code>decoder</code> <code>Union[str, Module]</code> <p>Decoder to be used for the segmentation model.     If a string, will look for such decoders in the different     registries supported (internal terratorch registry, smp, ...).     If an nn.Module, we expect it to expose a property <code>decoder.out_channels</code>.     Pixel wise tasks will be concatenated with a Conv2d for the final convolution.     Defaults to \"FCNDecoder\".</p> required <code>backbone_kwargs</code> <code>dict, optional) </code> <p>Arguments to be passed to instantiate the backbone.</p> <code>None</code> <code>decoder_kwargs</code> <code>dict, optional) </code> <p>Arguments to be passed to instantiate the decoder.</p> <code>None</code> <code>head_kwargs</code> <code>dict, optional) </code> <p>Arguments to be passed to the head network. </p> <code>None</code> <code>num_classes</code> <code>int</code> <p>Number of classes. None for regression tasks.</p> <code>None</code> <code>necks</code> <code>list[dict]</code> <p>nn.Modules to be called in succession on encoder features before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry. Expects each one to have a key \"name\" and subsequent keys for arguments, if any. Defaults to None, which applies the identity function.</p> <code>None</code> <code>aux_decoders</code> <code>list[AuxiliaryHead] | None</code> <p>List of AuxiliaryHead decoders to be added to the model. These decoders take the input from the encoder as well.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.</p> <code>True</code> <code>peft_config</code> <code>dict</code> <p>Configuration options for using PEFT. The dictionary should have the following keys:</p> <ul> <li>\"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available here.</li> <li>\"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep.   This should be used when the qkv matrices are merged together in a single linear layer and the PEFT   method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in   Q and V matrices). e.g. If using Prithvi this should be \"qkv\"</li> <li>\"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to PeftConfig</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: Full model with encoder, decoder and head.</p>"},{"location":"encoder_decoder_factory/#encoders","title":"Encoders","text":"<p>To be a valid encoder, an object must be an <code>nn.Module</code> and contain an attribute <code>out_channels</code>, basically a list of the channel dimensions corresponding to the features it returns. The forward method of any encoder should return a list of <code>torch.Tensor</code>.</p> <pre><code>In [19]: backbone = BACKBONE_REGISTRY.build(\"prithvi_eo_v2_300\", pretrained=True)\n\nIn [20]: import numpy as np\n\nIn [21]: import torch\n\nIn [22]: input_image = torch.tensor(np.random.rand(1,6,224,224).astype(\"float32\"))\n\nIn [23]: output = backbone.forward(input_image)\n\nIn [24]: [item.shape for item in output]\n\nOut[24]: \n\n[torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024])]\n</code></pre>"},{"location":"encoder_decoder_factory/#necks","title":"Necks","text":"<p>Necks are the connectors between encoder and decoder. They can perform operations such as selecting elements from the output of the encoder (SelectIndices), reshaping the outputs of ViTs so they are compatible with CNNs (ReshapeTokensToImage), amongst others. Necks are <code>nn.Modules</code>, with an additional method <code>process_channel_list</code> which informs the EncoderDecoderFactory about how it will alter the channel list provided by <code>encoder.out_channels</code>. See a better description about necks here.</p>"},{"location":"encoder_decoder_factory/#decoders","title":"Decoders","text":"<p>To be a valid decoder, an object must be an <code>nn.Module</code> with an attribute <code>out_channels</code>, an <code>int</code> representing the channel dimension of the output. The first argument to its constructor will be a list of channel dimensions it should expect as input. It's forward method should accept a list of embeddings. To see a list of built-in decoders check the related documentation. </p>"},{"location":"encoder_decoder_factory/#heads","title":"Heads","text":"<p>Most decoders require a final head to be added for a specific task (e.g. semantic segmentation vs pixel wise regression). Those registries producing decoders that dont require a head must expose the attribute <code>includes_head=True</code> so that a head is not added. Decoders passed as <code>nn.Modules</code> which do not require a head must expose the same attribute themselves. More about heads can be seen in its documentation. </p>"},{"location":"encoder_decoder_factory/#decoder-compatibilities","title":"Decoder compatibilities","text":"<p>Not all encoders and decoders are compatible. Below we include some caveats. Some decoders expect pyramidal outputs, but some encoders do not produce such outputs (e.g. vanilla ViT models). In this case, the InterpolateToPyramidal, MaxpoolToPyramidal and LearnedInterpolateToPyramidal necks may be particularly useful.</p>"},{"location":"encoder_decoder_factory/#smp-decoders","title":"SMP decoders","text":"<p>Not all decoders are guaranteed to work with all encoders without additional necks. Please check smp documentation to understand the embedding spatial dimensions expected by each decoder.</p> <p>In particular, smp seems to assume the first feature in the passed feature list has the same spatial resolution as the input, which may not always be true, and may break some decoders.</p> <p>In addition, for some decoders, the final 2 features have the same spatial resolution. Adding the AddBottleneckLayer neck will make this compatible.</p> <p>Some smp decoders require additional parameters, such as <code>decoder_channels</code>. These must be passed through the factory. In the case of <code>decoder_channels</code>, it would be passed as <code>decoder_decoder_channels</code> (the first <code>decoder_</code> routes the parameter to the decoder, where it is passed as <code>decoder_channels</code>).</p>"},{"location":"encoder_decoder_factory/#mmsegmentation-decoders","title":"MMSegmentation decoders","text":"<p>MMSegmentation decoders are available through the BACKBONE_REGISTRY. </p> <p>Warning</p> <p>MMSegmentation currently requires <code>mmcv==2.1.0</code>. Pre-built wheels for this only exist for <code>torch==2.1.0</code>. In order to use mmseg without building from source, you must downgrade your <code>torch</code> to this version. Install mmseg with: <pre><code>pip install -U openmim\nmim install mmengine\nmim install mmcv==2.1.0\npip install regex ftfy mmsegmentation\n</code></pre></p> <p>We provide access to mmseg decoders as an external source of decoders, but are not directly responsible for the maintainence of that library.</p> <p>Some mmseg decoders require the parameter <code>in_index</code>, which performs the same function as the <code>SelectIndices</code> neck. For use for pixel wise regression, mmseg decoders should take <code>num_classes=1</code>.</p>"},{"location":"extra_model_structures/","title":"Extra model structures","text":""},{"location":"extra_model_structures/#terratorch.models.model.Model","title":"<code>terratorch.models.model.Model</code>","text":"<p>               Bases: <code>ABC</code>, <code>Module</code></p> Source code in <code>terratorch/models/model.py</code> <pre><code>class Model(ABC, nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n    @abstractmethod\n    def freeze_encoder(self):\n        pass\n\n    @abstractmethod\n    def freeze_decoder(self):\n        pass\n\n    @abstractmethod\n    def forward(self, *args, **kwargs) -&gt; ModelOutput:\n        pass\n</code></pre>"},{"location":"extra_model_structures/#terratorch.models.model.AuxiliaryHead","title":"<code>terratorch.models.model.AuxiliaryHead</code>  <code>dataclass</code>","text":"<p>Class containing all information to create auxiliary heads.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the head. Should match the name given to the auxiliary loss.</p> required <code>decoder</code> <code>str</code> <p>Name of the decoder class to be used.</p> required <code>decoder_args</code> <code>dict | None</code> <p>parameters to be passed to the decoder constructor. Parameters for the decoder should be prefixed with <code>decoder_</code>. Parameters for the head should be prefixed with <code>head_</code>.</p> required Source code in <code>terratorch/models/model.py</code> <pre><code>@dataclass\nclass AuxiliaryHead:\n    \"\"\"Class containing all information to create auxiliary heads.\n\n    Args:\n        name (str): Name of the head. Should match the name given to the auxiliary loss.\n        decoder (str): Name of the decoder class to be used.\n        decoder_args (dict | None): parameters to be passed to the decoder constructor.\n            Parameters for the decoder should be prefixed with `decoder_`.\n            Parameters for the head should be prefixed with `head_`.\n    \"\"\"\n\n    name: str\n    decoder: str\n    decoder_args: dict | None\n</code></pre>"},{"location":"extra_model_structures/#terratorch.models.model.ModelOutput","title":"<code>terratorch.models.model.ModelOutput</code>  <code>dataclass</code>","text":"Source code in <code>terratorch/models/model.py</code> <pre><code>@dataclass\nclass ModelOutput:\n    output: Tensor\n    auxiliary_heads: dict[str, Tensor] = None\n</code></pre>"},{"location":"generic_datamodules/","title":"Generic Datamodules","text":""},{"location":"generic_datamodules/#terratorch.datamodules.generic_pixel_wise_data_module","title":"<code>terratorch.datamodules.generic_pixel_wise_data_module</code>","text":"<p>This module contains generic data modules for instantiation at runtime.</p>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoPixelwiseRegressionDataModule","title":"<code>GenericNonGeoPixelwiseRegressionDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoPixelwiseRegressionDataset</p>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoPixelwiseRegressionDataModule.__init__","title":"<code>__init__(batch_size, num_workers, train_data_root, val_data_root, test_data_root, means, stds, predict_data_root=None, img_grep='*', label_grep='*', train_label_data_root=None, val_label_data_root=None, test_label_data_root=None, train_split=None, val_split=None, test_split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, dataset_bands=None, output_bands=None, predict_dataset_bands=None, predict_output_bands=None, constant_scale=1, rgb_indices=None, train_transform=None, val_transform=None, test_transform=None, expand_temporal_dimension=False, reduce_zero_label=False, no_data_replace=None, no_label_replace=None, drop_last=True, pin_memory=False, check_stackability=True, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>description</p> required <code>num_workers</code> <code>int</code> <p>description</p> required <code>train_data_root</code> <code>Path</code> <p>description</p> required <code>val_data_root</code> <code>Path</code> <p>description</p> required <code>test_data_root</code> <code>Path</code> <p>description</p> required <code>predict_data_root</code> <code>Path</code> <p>description</p> <code>None</code> <code>img_grep</code> <code>str</code> <p>description</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>description</p> <code>'*'</code> <code>means</code> <code>list[float]</code> <p>description</p> required <code>stds</code> <code>list[float]</code> <p>description</p> required <code>train_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None.</p> <code>None</code> <code>predict_dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>predict_output_bands</code> <code>list[HLSBands | int] | None</code> <p>Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>description. Defaults to 1.</p> <code>1</code> <code>rgb_indices</code> <code>list[int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>val_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>test_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Drop the last batch if it is not complete. Defaults to True.</p> <code>True</code> <code>pin_memory</code> <code>bool</code> <p>If <code>True</code>, the data loader will copy Tensors</p> <code>False</code> <code>check_stackability</code> <code>bool</code> <p>Check if all the files in the dataset has the same size and can be stacked.</p> <code>True</code>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoSegmentationDataModule","title":"<code>GenericNonGeoSegmentationDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoSegmentationDatasets</p>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoSegmentationDataModule.__init__","title":"<code>__init__(batch_size, num_workers, train_data_root, val_data_root, test_data_root, img_grep, label_grep, means, stds, num_classes, predict_data_root=None, train_label_data_root=None, val_label_data_root=None, test_label_data_root=None, train_split=None, val_split=None, test_split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, dataset_bands=None, output_bands=None, predict_dataset_bands=None, predict_output_bands=None, constant_scale=1, rgb_indices=None, train_transform=None, val_transform=None, test_transform=None, expand_temporal_dimension=False, reduce_zero_label=False, no_data_replace=None, no_label_replace=None, drop_last=True, pin_memory=False, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>description</p> required <code>num_workers</code> <code>int</code> <p>description</p> required <code>train_data_root</code> <code>Path</code> <p>description</p> required <code>val_data_root</code> <code>Path</code> <p>description</p> required <code>test_data_root</code> <code>Path</code> <p>description</p> required <code>predict_data_root</code> <code>Path</code> <p>description</p> <code>None</code> <code>img_grep</code> <code>str</code> <p>description</p> required <code>label_grep</code> <code>str</code> <p>description</p> required <code>means</code> <code>list[float]</code> <p>description</p> required <code>stds</code> <code>list[float]</code> <p>description</p> required <code>num_classes</code> <code>int</code> <p>description</p> required <code>train_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None.</p> <code>None</code> <code>predict_dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>predict_output_bands</code> <code>list[HLSBands | int] | None</code> <p>Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>description. Defaults to 1.</p> <code>1</code> <code>rgb_indices</code> <code>list[int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>val_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>test_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Drop the last batch if it is not complete. Defaults to True.</p> <code>True</code> <code>pin_memory</code> <code>bool</code> <p>If <code>True</code>, the data loader will copy Tensors</p> <code>False</code>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_scalar_label_data_module","title":"<code>terratorch.datamodules.generic_scalar_label_data_module</code>","text":"<p>This module contains generic data modules for instantiation at runtime.</p>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_scalar_label_data_module.GenericNonGeoClassificationDataModule","title":"<code>GenericNonGeoClassificationDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoClassificationDatasets</p>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_scalar_label_data_module.GenericNonGeoClassificationDataModule.__init__","title":"<code>__init__(batch_size, num_workers, train_data_root, val_data_root, test_data_root, means, stds, num_classes, predict_data_root=None, train_split=None, val_split=None, test_split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, dataset_bands=None, predict_dataset_bands=None, output_bands=None, constant_scale=1, rgb_indices=None, train_transform=None, val_transform=None, test_transform=None, expand_temporal_dimension=False, no_data_replace=0, drop_last=True, check_stackability=True, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>description</p> required <code>num_workers</code> <code>int</code> <p>description</p> required <code>train_data_root</code> <code>Path</code> <p>description</p> required <code>val_data_root</code> <code>Path</code> <p>description</p> required <code>test_data_root</code> <code>Path</code> <p>description</p> required <code>means</code> <code>list[float]</code> <p>description</p> required <code>stds</code> <code>list[float]</code> <p>description</p> required <code>num_classes</code> <code>int</code> <p>description</p> required <code>predict_data_root</code> <code>Path</code> <p>description</p> <code>None</code> <code>train_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\".</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>predict_dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>description. Defaults to 1.</p> <code>1</code> <code>rgb_indices</code> <code>list[int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>val_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>test_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float</code> <p>Replace nan values in input images with this value. Defaults to 0.</p> <code>0</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Drop the last batch if it is not complete. Defaults to True.</p> <code>True</code> <code>check_stackability</code> <code>bool</code> <p>Check if all the files in the dataset has the same size and can be stacked.</p> <code>True</code>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_multimodal_data_module","title":"<code>terratorch.datamodules.generic_multimodal_data_module</code>","text":"<p>This module contains generic data modules for instantiation at runtime.</p>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_multimodal_data_module.GenericMultiModalDataModule","title":"<code>GenericMultiModalDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoSegmentationDatasets</p> Source code in <code>terratorch/datamodules/generic_multimodal_data_module.py</code> <pre><code>class GenericMultiModalDataModule(NonGeoDataModule):\n    \"\"\"\n    This is a generic datamodule class for instantiating data modules at runtime.\n    Composes several [GenericNonGeoSegmentationDatasets][terratorch.datasets.GenericNonGeoSegmentationDataset]\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        modalities: list[str],\n        train_data_root: dict[str, Path],\n        val_data_root: dict[str, Path],\n        test_data_root: dict[str, Path],\n        means: dict[str, list],\n        stds: dict[str, list],\n        task: str | None = None,\n        num_classes: int | None = None,\n        image_grep: str | dict[str, str] | None = None,\n        label_grep: str | None = None,\n        train_label_data_root: Path | str | None = None,\n        val_label_data_root: Path | str | None = None,\n        test_label_data_root: Path | str | None = None,\n        predict_data_root: dict[str, Path] | str | None = None,\n        train_split: Path | str | None = None,\n        val_split: Path | str | None = None,\n        test_split: Path| str | None = None,\n        dataset_bands: dict[str, list] | None = None,\n        output_bands: dict[str, list] | None = None,\n        predict_dataset_bands: dict[str, list] | None = None,\n        predict_output_bands: dict[str, list] | None = None,\n        image_modalities: list[str] | None = None,\n        rgb_modality: str | None = None,\n        rgb_indices: list[int] | None = None,\n        allow_substring_file_names: bool = True,\n        class_names: list[str] | None = None,\n        constant_scale: dict[float] = None,\n        train_transform: dict | A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: dict | A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: dict | A.Compose | None | list[A.BasicTransform] = None,\n        shared_transforms: list | bool = True,\n        expand_temporal_dimension: bool = False,\n        no_data_replace: float | None = None,\n        no_label_replace: float | None = -1,\n        reduce_zero_label: bool = False,\n        drop_last: bool = True,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n        data_with_sample_dim: bool = False,\n        sample_num_modalities: int | None = None,\n        sample_replace: bool = False,\n        channel_position: int = -3,\n        concat_bands: bool = False,\n        check_stackability: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            batch_size (int): Number of samples in per batch.\n            modalities (list[str]): List of modalities.\n            train_data_root (dict[Path]): Dictionary of paths to training data root directory or csv/parquet files with \n                image-level data, with modalities as keys.\n            val_data_root (dict[Path]): Dictionary of paths to validation data root directory or csv/parquet files with \n                image-level data, with modalities as keys.\n            test_data_root (dict[Path]): Dictionary of paths to test data root directory or csv/parquet files with \n                image-level data, with modalities as keys.\n            means (dict[list]): Dictionary of mean values as lists with modalities as keys.\n            stds (dict[list]): Dictionary of std values as lists with modalities as keys.\n            task (str, optional): Selected task form segmentation, regression (pixel-wise), classification,\n                multilabel_classification, scalar_regression, scalar (custom image-level task), or None (no targets).\n                Defaults to None.\n            num_classes (int, optional): Number of classes in classification or segmentation tasks.\n            predict_data_root (dict[Path], optional): Dictionary of paths to data root directory or csv/parquet files\n                with image-level data, with modalities as keys.\n            image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n                images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            label_grep (str, optional): Regular expression appended to label_data_root to find labels or mask files.\n                Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            train_label_data_root (Path | None, optional): Path to data root directory with training labels or\n                csv/parquet files with labels. Required for supervised tasks.\n            val_label_data_root (Path | None, optional): Path to data root directory with validation labels or\n                csv/parquet files with labels. Required for supervised tasks.\n            test_label_data_root (Path | None, optional): Path to data root directory with test labels or\n                csv/parquet files with labels. Required for supervised tasks.\n            train_split (Path, optional): Path to file containing training samples prefixes to be used for this split.\n                The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n                sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n                files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n                If not specified, search samples based on files in data_root. Defaults to None.\n            val_split (Path, optional): Path to file containing validation samples prefixes to be used for this split.\n                The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n                sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n                files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n                If not specified, search samples based on files in data_root. Defaults to None.\n            test_split (Path, optional): Path to file containing test samples prefixes to be used for this split.\n                The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n                sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n                files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n                If not specified, search samples based on files in data_root. Defaults to None.\n            dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n                as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n                that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n                of all modalities. Defaults to None.\n            output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n                provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n            predict_dataset_bands (list[dict], optional): Overwrites dataset_bands with this value at predict time.\n                Defaults to None, which does not overwrite.\n            predict_output_bands (list[dict], optional): Overwrites output_bands with this value at predict time.\n                Defaults to None, which does not overwrite.\n            image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n                The difference between all modalities and image_modalities are non-image modalities which are treated\n                differently during the transforms and are not modified but only converted into a tensor if possible.\n            rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n            rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n            allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n                image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n                If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n                Defaults to True.\n            class_names (list[str], optional): Names of the classes. Defaults to None.\n            constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n                keys. Can be subset of all modalities. Defaults to None.\n            train_transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n                modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to \n                non-image data, which is only converted to tensors if possible. If dict, can include separate transforms \n                per modality (no shared parameters between modalities). \n                Defaults to None, which simply applies ToTensorV2().\n            val_transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n                modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to \n                non-image data, which is only converted to tensors if possible. If dict, can include separate transforms \n                per modality (no shared parameters between modalities). \n                Defaults to None, which simply applies ToTensorV2().\n            test_transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n                modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to \n                non-image data, which is only converted to tensors if possible. If dict, can include separate transforms \n                per modality (no shared parameters between modalities). \n                Defaults to None, which simply applies ToTensorV2().\n            shared_transforms (bool): transforms are shared between all image modalities (e.g., similar crop). \n                This setting is ignored if transforms are defined per modality. Defaults to True.  \n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n                Defaults to False.\n            no_data_replace (float | None): Replace nan values in input images with this value. If none, does no \n                replacement. Defaults to None.\n            no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. \n                Defaults to None.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n            drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n            num_workers (int): Number of parallel workers. Defaults to 0 for single threaded process.\n            pin_memory (bool): If ``True``, the data loader will copy Tensors into device/CUDA pinned memory before \n                returning them. Defaults to False.\n            data_with_sample_dim (bool): Use a specific collate function to concatenate samples along a existing sample\n                dimension instead of stacking the samples. Defaults to False.\n            sample_num_modalities (int, optional): Load only a subset of modalities per batch. Defaults to None.\n            sample_replace (bool): If sample_num_modalities is set, sample modalities with replacement.\n                Defaults to False.\n            channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n            concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n                that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n                Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n            check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked.\n        \"\"\"\n\n        if task == \"segmentation\":\n            dataset_class = GenericMultimodalSegmentationDataset\n        elif task == \"regression\":\n            dataset_class = GenericMultimodalPixelwiseRegressionDataset\n        elif task in [\"classification\", \"multilabel_classification\", \"scalar_regression\", \"scalar\"]:\n            dataset_class = GenericMultimodalScalarDataset\n            task = \"scalar\"\n        elif task is None:\n            dataset_class = GenericMultimodalDataset\n        else:\n            raise ValueError(f\"Unknown task {task}, only segmentation and regression are supported.\")\n\n        super().__init__(dataset_class, batch_size, num_workers, **kwargs)\n        self.num_classes = num_classes\n        self.class_names = class_names\n        self.modalities = modalities\n        self.image_modalities = image_modalities or modalities\n        self.non_image_modalities = list(set(self.modalities) - set(self.image_modalities))\n        if task == \"scalar\":\n            self.non_image_modalities += [\"label\"]\n        if isinstance(image_grep, dict):\n            self.image_grep = {m: image_grep[m] if m in image_grep else \"*\" for m in modalities}\n        else:\n            self.image_grep = {m: image_grep or \"*\" for m in modalities}\n        self.label_grep = label_grep or \"*\"\n        self.train_root = train_data_root\n        self.val_root = val_data_root\n        self.test_root = test_data_root\n        self.train_label_data_root = train_label_data_root\n        self.val_label_data_root = val_label_data_root\n        self.test_label_data_root = test_label_data_root\n        self.predict_root = predict_data_root\n        self.train_split = train_split\n        self.val_split = val_split\n        self.test_split = test_split\n        self.allow_substring_file_names = allow_substring_file_names\n        self.constant_scale = constant_scale\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.drop_last = drop_last\n        self.pin_memory = pin_memory\n        self.sample_num_modalities = sample_num_modalities\n        self.sample_replace = sample_replace        \n\n        self.dataset_bands = dataset_bands\n        self.output_bands = output_bands\n        self.predict_dataset_bands = predict_dataset_bands\n        self.predict_output_bands = predict_output_bands\n\n        self.rgb_modality = rgb_modality or modalities[0]\n        self.rgb_indices = rgb_indices\n        self.expand_temporal_dimension = expand_temporal_dimension\n        self.reduce_zero_label = reduce_zero_label\n        self.channel_position = channel_position\n        self.concat_bands = concat_bands\n        if not concat_bands and check_stackability:\n            logger.debug(f\"Cannot check stackability if bands are not concatenated.\")\n        self.check_stackability = check_stackability\n\n        if isinstance(train_transform, dict):\n            self.train_transform = {m: wrap_in_compose_is_list(train_transform[m]) if m in train_transform else None\n                                    for m in modalities}\n        elif shared_transforms:\n            self.train_transform = wrap_in_compose_is_list(train_transform,\n                                                           image_modalities=self.image_modalities,\n                                                           non_image_modalities=self.non_image_modalities)\n        else:\n            self.train_transform = {m: wrap_in_compose_is_list(train_transform)\n                                    for m in modalities}\n\n        if isinstance(val_transform, dict):\n            self.val_transform = {m: wrap_in_compose_is_list(val_transform[m]) if m in val_transform else None\n                                    for m in modalities}\n        elif shared_transforms:\n            self.val_transform = wrap_in_compose_is_list(val_transform,\n                                                         image_modalities=self.image_modalities,\n                                                         non_image_modalities=self.non_image_modalities)\n        else:\n            self.val_transform = {m: wrap_in_compose_is_list(val_transform)\n                                    for m in modalities}\n\n        if isinstance(test_transform, dict):\n            self.test_transform = {m: wrap_in_compose_is_list(test_transform[m]) if m in test_transform else None\n                                    for m in modalities}\n        elif shared_transforms:\n            self.test_transform = wrap_in_compose_is_list(test_transform,\n                                                          image_modalities=self.image_modalities,\n                                                          non_image_modalities=self.non_image_modalities,                                                          \n                                                          )\n        else:\n            self.test_transform = {m: wrap_in_compose_is_list(test_transform)\n                                    for m in modalities}\n\n        if self.concat_bands:\n            # Concatenate mean and std values\n            means = load_from_file_or_attribute(np.concatenate([means[m] for m in self.image_modalities]).tolist())\n            stds = load_from_file_or_attribute(np.concatenate([stds[m] for m in self.image_modalities]).tolist())\n\n            self.aug = Normalize(means, stds)\n        else:\n            # Apply standardization per modality\n            means = {m: load_from_file_or_attribute(means[m]) for m in means.keys()}\n            stds = {m: load_from_file_or_attribute(stds[m]) for m in stds.keys()}\n\n            self.aug = MultimodalNormalize(means, stds)\n\n        self.data_with_sample_dim = data_with_sample_dim\n\n        self.collate_fn = collate_chunk_dicts if data_with_sample_dim else collate_samples\n\n\n    def setup(self, stage: str) -&gt; None:\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                data_root=self.train_root,\n                num_classes=self.num_classes,\n                image_grep=self.image_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.train_label_data_root,\n                split=self.train_split,\n                allow_substring_file_names=self.allow_substring_file_names,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                image_modalities=self.image_modalities,\n                rgb_modality=self.rgb_modality,\n                rgb_indices=self.rgb_indices,\n                transform=self.train_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n                channel_position=self.channel_position,\n                data_with_sample_dim = self.data_with_sample_dim,\n                concat_bands=self.concat_bands,\n            )\n            logger.info(f\"Train dataset: {len(self.train_dataset)}\")\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                data_root=self.val_root,\n                num_classes=self.num_classes,\n                image_grep=self.image_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.val_label_data_root,\n                split=self.val_split,\n                allow_substring_file_names=self.allow_substring_file_names,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                image_modalities=self.image_modalities,\n                rgb_modality=self.rgb_modality,\n                rgb_indices=self.rgb_indices,\n                transform=self.val_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n                channel_position=self.channel_position,\n                data_with_sample_dim = self.data_with_sample_dim,\n                concat_bands=self.concat_bands,\n            )\n            logger.info(f\"Val dataset: {len(self.val_dataset)}\")\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                data_root=self.test_root,\n                num_classes=self.num_classes,\n                image_grep=self.image_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.test_label_data_root,\n                split=self.test_split,\n                allow_substring_file_names=self.allow_substring_file_names,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                image_modalities=self.image_modalities,\n                rgb_modality=self.rgb_modality,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n                channel_position=self.channel_position,\n                data_with_sample_dim = self.data_with_sample_dim,\n                concat_bands=self.concat_bands,\n            )\n            logger.info(f\"Test dataset: {len(self.test_dataset)}\")\n        if stage in [\"predict\"] and self.predict_root:\n            self.predict_dataset = self.dataset_class(\n                data_root=self.predict_root,\n                num_classes=self.num_classes,\n                allow_substring_file_names=self.allow_substring_file_names,\n                dataset_bands=self.predict_dataset_bands,\n                output_bands=self.predict_output_bands,\n                constant_scale=self.constant_scale,\n                image_modalities=self.image_modalities,\n                rgb_modality=self.rgb_modality,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n                channel_position=self.channel_position,\n                data_with_sample_dim=self.data_with_sample_dim,\n                concat_bands=self.concat_bands,\n            )\n            logger.info(f\"Predict dataset: {len(self.predict_dataset)}\")\n\n    def _dataloader_factory(self, split: str) -&gt; DataLoader[dict[str, Tensor]]:\n        \"\"\"Implement one or more PyTorch DataLoaders.\n\n        Args:\n            split: Either \"train\", \"val\", \"test\", or \"predict\".\n\n        Returns:\n            A collection of data loaders specifying samples.\n\n        Raises:\n            MisconfigurationException: If :meth:`setup` does not define a\n                dataset or sampler, or if the dataset or sampler has length 0.\n        \"\"\"\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n\n        if self.check_stackability:\n            logger.info(f'Checking dataset stackability for {split} split')\n            if self.concat_bands:\n                batch_size = check_dataset_stackability(dataset, batch_size)\n            else:\n                batch_size = check_dataset_stackability_dict(dataset, batch_size)\n\n        if self.sample_num_modalities:\n            # Custom batch sampler for sampling modalities per batch\n            batch_sampler = MultiModalBatchSampler(\n                self.modalities, self.sample_num_modalities, self.sample_replace,\n                RandomSampler(dataset) if split == \"train\" else SequentialSampler(dataset),\n                batch_size=batch_size,\n                drop_last=split == \"train\" and self.drop_last\n            )\n        else:\n            batch_sampler = BatchSampler(\n                RandomSampler(dataset) if split == \"train\" else SequentialSampler(dataset),\n                batch_size=batch_size,\n                drop_last=split == \"train\" and self.drop_last\n            )\n\n        return DataLoader(\n            dataset=dataset,\n            batch_sampler=batch_sampler,\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            pin_memory=self.pin_memory,\n        )\n</code></pre>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_multimodal_data_module.GenericMultiModalDataModule.__init__","title":"<code>__init__(batch_size, modalities, train_data_root, val_data_root, test_data_root, means, stds, task=None, num_classes=None, image_grep=None, label_grep=None, train_label_data_root=None, val_label_data_root=None, test_label_data_root=None, predict_data_root=None, train_split=None, val_split=None, test_split=None, dataset_bands=None, output_bands=None, predict_dataset_bands=None, predict_output_bands=None, image_modalities=None, rgb_modality=None, rgb_indices=None, allow_substring_file_names=True, class_names=None, constant_scale=None, train_transform=None, val_transform=None, test_transform=None, shared_transforms=True, expand_temporal_dimension=False, no_data_replace=None, no_label_replace=-1, reduce_zero_label=False, drop_last=True, num_workers=0, pin_memory=False, data_with_sample_dim=False, sample_num_modalities=None, sample_replace=False, channel_position=-3, concat_bands=False, check_stackability=True, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of samples in per batch.</p> required <code>modalities</code> <code>list[str]</code> <p>List of modalities.</p> required <code>train_data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to training data root directory or csv/parquet files with  image-level data, with modalities as keys.</p> required <code>val_data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to validation data root directory or csv/parquet files with  image-level data, with modalities as keys.</p> required <code>test_data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to test data root directory or csv/parquet files with  image-level data, with modalities as keys.</p> required <code>means</code> <code>dict[list]</code> <p>Dictionary of mean values as lists with modalities as keys.</p> required <code>stds</code> <code>dict[list]</code> <p>Dictionary of std values as lists with modalities as keys.</p> required <code>task</code> <code>str</code> <p>Selected task form segmentation, regression (pixel-wise), classification, multilabel_classification, scalar_regression, scalar (custom image-level task), or None (no targets). Defaults to None.</p> <code>None</code> <code>num_classes</code> <code>int</code> <p>Number of classes in classification or segmentation tasks.</p> <code>None</code> <code>predict_data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to data root directory or csv/parquet files with image-level data, with modalities as keys.</p> <code>None</code> <code>image_grep</code> <code>dict[str]</code> <p>Dictionary with regular expression appended to data_root to find input images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>None</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to label_data_root to find labels or mask files. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>None</code> <code>train_label_data_root</code> <code>Path | None</code> <p>Path to data root directory with training labels or csv/parquet files with labels. Required for supervised tasks.</p> <code>None</code> <code>val_label_data_root</code> <code>Path | None</code> <p>Path to data root directory with validation labels or csv/parquet files with labels. Required for supervised tasks.</p> <code>None</code> <code>test_label_data_root</code> <code>Path | None</code> <p>Path to data root directory with test labels or csv/parquet files with labels. Required for supervised tasks.</p> <code>None</code> <code>train_split</code> <code>Path</code> <p>Path to file containing training samples prefixes to be used for this split. The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise, files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]). If not specified, search samples based on files in data_root. Defaults to None.</p> <code>None</code> <code>val_split</code> <code>Path</code> <p>Path to file containing validation samples prefixes to be used for this split. The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise, files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]). If not specified, search samples based on files in data_root. Defaults to None.</p> <code>None</code> <code>test_split</code> <code>Path</code> <p>Path to file containing test samples prefixes to be used for this split. The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise, files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]). If not specified, search samples based on files in data_root. Defaults to None.</p> <code>None</code> <code>dataset_bands</code> <code>dict[list]</code> <p>Bands present in the dataset, provided in a dictionary with modalities as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset of all modalities. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>dict[list]</code> <p>Bands that should be output by the dataset as named by dataset_bands, provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.</p> <code>None</code> <code>predict_dataset_bands</code> <code>list[dict]</code> <p>Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>predict_output_bands</code> <code>list[dict]</code> <p>Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>image_modalities(list[str],</code> <code>optional</code> <p>List of pixel-level raster modalities. Defaults to data_root.keys(). The difference between all modalities and image_modalities are non-image modalities which are treated differently during the transforms and are not modified but only converted into a tensor if possible.</p> required <code>rgb_modality</code> <code>str</code> <p>Modality used for RGB plots. Defaults to first modality in data_root.keys().</p> <code>None</code> <code>rgb_indices</code> <code>list[int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>allow_substring_file_names</code> <code>bool</code> <p>Allow substrings during sample identification by adding image or label grep to the sample prefixes. If False, treats sample prefixes as full file names. If True and no split file is provided, considers the file stem as prefix, otherwise the full file name. Defaults to True.</p> <code>True</code> <code>class_names</code> <code>list[str]</code> <p>Names of the classes. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>dict[float]</code> <p>Factor to multiply data values by, provided as a dictionary with modalities as keys. Can be subset of all modalities. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | dict | None</code> <p>Albumentations transform to be applied to all image modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to  non-image data, which is only converted to tensors if possible. If dict, can include separate transforms  per modality (no shared parameters between modalities).  Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>val_transform</code> <code>Compose | dict | None</code> <p>Albumentations transform to be applied to all image modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to  non-image data, which is only converted to tensors if possible. If dict, can include separate transforms  per modality (no shared parameters between modalities).  Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>test_transform</code> <code>Compose | dict | None</code> <p>Albumentations transform to be applied to all image modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to  non-image data, which is only converted to tensors if possible. If dict, can include separate transforms  per modality (no shared parameters between modalities).  Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>shared_transforms</code> <code>bool</code> <p>transforms are shared between all image modalities (e.g., similar crop).  This setting is ignored if transforms are defined per modality. Defaults to True.  </p> <code>True</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Only works with image modalities. Is only applied to modalities with defined dataset_bands. Defaults to False.</p> <code>False</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no  replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement.  Defaults to None.</p> <code>-1</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Drop the last batch if it is not complete. Defaults to True.</p> <code>True</code> <code>num_workers</code> <code>int</code> <p>Number of parallel workers. Defaults to 0 for single threaded process.</p> <code>0</code> <code>pin_memory</code> <code>bool</code> <p>If <code>True</code>, the data loader will copy Tensors into device/CUDA pinned memory before  returning them. Defaults to False.</p> <code>False</code> <code>data_with_sample_dim</code> <code>bool</code> <p>Use a specific collate function to concatenate samples along a existing sample dimension instead of stacking the samples. Defaults to False.</p> <code>False</code> <code>sample_num_modalities</code> <code>int</code> <p>Load only a subset of modalities per batch. Defaults to None.</p> <code>None</code> <code>sample_replace</code> <code>bool</code> <p>If sample_num_modalities is set, sample modalities with replacement. Defaults to False.</p> <code>False</code> <code>channel_position</code> <code>int</code> <p>Position of the channel dimension in the image modalities. Defaults to -3.</p> <code>-3</code> <code>concat_bands</code> <code>bool</code> <p>Concatenate all image modalities along the band dimension into a single \"image\", so that it can be processed by single-modal models. Concatenate in the order of provided modalities. Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.</p> <code>False</code> <code>check_stackability</code> <code>bool</code> <p>Check if all the files in the dataset has the same size and can be stacked.</p> <code>True</code> Source code in <code>terratorch/datamodules/generic_multimodal_data_module.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    modalities: list[str],\n    train_data_root: dict[str, Path],\n    val_data_root: dict[str, Path],\n    test_data_root: dict[str, Path],\n    means: dict[str, list],\n    stds: dict[str, list],\n    task: str | None = None,\n    num_classes: int | None = None,\n    image_grep: str | dict[str, str] | None = None,\n    label_grep: str | None = None,\n    train_label_data_root: Path | str | None = None,\n    val_label_data_root: Path | str | None = None,\n    test_label_data_root: Path | str | None = None,\n    predict_data_root: dict[str, Path] | str | None = None,\n    train_split: Path | str | None = None,\n    val_split: Path | str | None = None,\n    test_split: Path| str | None = None,\n    dataset_bands: dict[str, list] | None = None,\n    output_bands: dict[str, list] | None = None,\n    predict_dataset_bands: dict[str, list] | None = None,\n    predict_output_bands: dict[str, list] | None = None,\n    image_modalities: list[str] | None = None,\n    rgb_modality: str | None = None,\n    rgb_indices: list[int] | None = None,\n    allow_substring_file_names: bool = True,\n    class_names: list[str] | None = None,\n    constant_scale: dict[float] = None,\n    train_transform: dict | A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: dict | A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: dict | A.Compose | None | list[A.BasicTransform] = None,\n    shared_transforms: list | bool = True,\n    expand_temporal_dimension: bool = False,\n    no_data_replace: float | None = None,\n    no_label_replace: float | None = -1,\n    reduce_zero_label: bool = False,\n    drop_last: bool = True,\n    num_workers: int = 0,\n    pin_memory: bool = False,\n    data_with_sample_dim: bool = False,\n    sample_num_modalities: int | None = None,\n    sample_replace: bool = False,\n    channel_position: int = -3,\n    concat_bands: bool = False,\n    check_stackability: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        batch_size (int): Number of samples in per batch.\n        modalities (list[str]): List of modalities.\n        train_data_root (dict[Path]): Dictionary of paths to training data root directory or csv/parquet files with \n            image-level data, with modalities as keys.\n        val_data_root (dict[Path]): Dictionary of paths to validation data root directory or csv/parquet files with \n            image-level data, with modalities as keys.\n        test_data_root (dict[Path]): Dictionary of paths to test data root directory or csv/parquet files with \n            image-level data, with modalities as keys.\n        means (dict[list]): Dictionary of mean values as lists with modalities as keys.\n        stds (dict[list]): Dictionary of std values as lists with modalities as keys.\n        task (str, optional): Selected task form segmentation, regression (pixel-wise), classification,\n            multilabel_classification, scalar_regression, scalar (custom image-level task), or None (no targets).\n            Defaults to None.\n        num_classes (int, optional): Number of classes in classification or segmentation tasks.\n        predict_data_root (dict[Path], optional): Dictionary of paths to data root directory or csv/parquet files\n            with image-level data, with modalities as keys.\n        image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n            images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        label_grep (str, optional): Regular expression appended to label_data_root to find labels or mask files.\n            Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        train_label_data_root (Path | None, optional): Path to data root directory with training labels or\n            csv/parquet files with labels. Required for supervised tasks.\n        val_label_data_root (Path | None, optional): Path to data root directory with validation labels or\n            csv/parquet files with labels. Required for supervised tasks.\n        test_label_data_root (Path | None, optional): Path to data root directory with test labels or\n            csv/parquet files with labels. Required for supervised tasks.\n        train_split (Path, optional): Path to file containing training samples prefixes to be used for this split.\n            The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n            sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n            files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n            If not specified, search samples based on files in data_root. Defaults to None.\n        val_split (Path, optional): Path to file containing validation samples prefixes to be used for this split.\n            The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n            sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n            files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n            If not specified, search samples based on files in data_root. Defaults to None.\n        test_split (Path, optional): Path to file containing test samples prefixes to be used for this split.\n            The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n            sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n            files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n            If not specified, search samples based on files in data_root. Defaults to None.\n        dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n            as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n            that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n            of all modalities. Defaults to None.\n        output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n            provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n        predict_dataset_bands (list[dict], optional): Overwrites dataset_bands with this value at predict time.\n            Defaults to None, which does not overwrite.\n        predict_output_bands (list[dict], optional): Overwrites output_bands with this value at predict time.\n            Defaults to None, which does not overwrite.\n        image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n            The difference between all modalities and image_modalities are non-image modalities which are treated\n            differently during the transforms and are not modified but only converted into a tensor if possible.\n        rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n        rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n        allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n            image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n            If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n            Defaults to True.\n        class_names (list[str], optional): Names of the classes. Defaults to None.\n        constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n            keys. Can be subset of all modalities. Defaults to None.\n        train_transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n            modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to \n            non-image data, which is only converted to tensors if possible. If dict, can include separate transforms \n            per modality (no shared parameters between modalities). \n            Defaults to None, which simply applies ToTensorV2().\n        val_transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n            modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to \n            non-image data, which is only converted to tensors if possible. If dict, can include separate transforms \n            per modality (no shared parameters between modalities). \n            Defaults to None, which simply applies ToTensorV2().\n        test_transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n            modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to \n            non-image data, which is only converted to tensors if possible. If dict, can include separate transforms \n            per modality (no shared parameters between modalities). \n            Defaults to None, which simply applies ToTensorV2().\n        shared_transforms (bool): transforms are shared between all image modalities (e.g., similar crop). \n            This setting is ignored if transforms are defined per modality. Defaults to True.  \n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n            Defaults to False.\n        no_data_replace (float | None): Replace nan values in input images with this value. If none, does no \n            replacement. Defaults to None.\n        no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. \n            Defaults to None.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n        drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n        num_workers (int): Number of parallel workers. Defaults to 0 for single threaded process.\n        pin_memory (bool): If ``True``, the data loader will copy Tensors into device/CUDA pinned memory before \n            returning them. Defaults to False.\n        data_with_sample_dim (bool): Use a specific collate function to concatenate samples along a existing sample\n            dimension instead of stacking the samples. Defaults to False.\n        sample_num_modalities (int, optional): Load only a subset of modalities per batch. Defaults to None.\n        sample_replace (bool): If sample_num_modalities is set, sample modalities with replacement.\n            Defaults to False.\n        channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n        concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n            that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n            Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n        check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked.\n    \"\"\"\n\n    if task == \"segmentation\":\n        dataset_class = GenericMultimodalSegmentationDataset\n    elif task == \"regression\":\n        dataset_class = GenericMultimodalPixelwiseRegressionDataset\n    elif task in [\"classification\", \"multilabel_classification\", \"scalar_regression\", \"scalar\"]:\n        dataset_class = GenericMultimodalScalarDataset\n        task = \"scalar\"\n    elif task is None:\n        dataset_class = GenericMultimodalDataset\n    else:\n        raise ValueError(f\"Unknown task {task}, only segmentation and regression are supported.\")\n\n    super().__init__(dataset_class, batch_size, num_workers, **kwargs)\n    self.num_classes = num_classes\n    self.class_names = class_names\n    self.modalities = modalities\n    self.image_modalities = image_modalities or modalities\n    self.non_image_modalities = list(set(self.modalities) - set(self.image_modalities))\n    if task == \"scalar\":\n        self.non_image_modalities += [\"label\"]\n    if isinstance(image_grep, dict):\n        self.image_grep = {m: image_grep[m] if m in image_grep else \"*\" for m in modalities}\n    else:\n        self.image_grep = {m: image_grep or \"*\" for m in modalities}\n    self.label_grep = label_grep or \"*\"\n    self.train_root = train_data_root\n    self.val_root = val_data_root\n    self.test_root = test_data_root\n    self.train_label_data_root = train_label_data_root\n    self.val_label_data_root = val_label_data_root\n    self.test_label_data_root = test_label_data_root\n    self.predict_root = predict_data_root\n    self.train_split = train_split\n    self.val_split = val_split\n    self.test_split = test_split\n    self.allow_substring_file_names = allow_substring_file_names\n    self.constant_scale = constant_scale\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.drop_last = drop_last\n    self.pin_memory = pin_memory\n    self.sample_num_modalities = sample_num_modalities\n    self.sample_replace = sample_replace        \n\n    self.dataset_bands = dataset_bands\n    self.output_bands = output_bands\n    self.predict_dataset_bands = predict_dataset_bands\n    self.predict_output_bands = predict_output_bands\n\n    self.rgb_modality = rgb_modality or modalities[0]\n    self.rgb_indices = rgb_indices\n    self.expand_temporal_dimension = expand_temporal_dimension\n    self.reduce_zero_label = reduce_zero_label\n    self.channel_position = channel_position\n    self.concat_bands = concat_bands\n    if not concat_bands and check_stackability:\n        logger.debug(f\"Cannot check stackability if bands are not concatenated.\")\n    self.check_stackability = check_stackability\n\n    if isinstance(train_transform, dict):\n        self.train_transform = {m: wrap_in_compose_is_list(train_transform[m]) if m in train_transform else None\n                                for m in modalities}\n    elif shared_transforms:\n        self.train_transform = wrap_in_compose_is_list(train_transform,\n                                                       image_modalities=self.image_modalities,\n                                                       non_image_modalities=self.non_image_modalities)\n    else:\n        self.train_transform = {m: wrap_in_compose_is_list(train_transform)\n                                for m in modalities}\n\n    if isinstance(val_transform, dict):\n        self.val_transform = {m: wrap_in_compose_is_list(val_transform[m]) if m in val_transform else None\n                                for m in modalities}\n    elif shared_transforms:\n        self.val_transform = wrap_in_compose_is_list(val_transform,\n                                                     image_modalities=self.image_modalities,\n                                                     non_image_modalities=self.non_image_modalities)\n    else:\n        self.val_transform = {m: wrap_in_compose_is_list(val_transform)\n                                for m in modalities}\n\n    if isinstance(test_transform, dict):\n        self.test_transform = {m: wrap_in_compose_is_list(test_transform[m]) if m in test_transform else None\n                                for m in modalities}\n    elif shared_transforms:\n        self.test_transform = wrap_in_compose_is_list(test_transform,\n                                                      image_modalities=self.image_modalities,\n                                                      non_image_modalities=self.non_image_modalities,                                                          \n                                                      )\n    else:\n        self.test_transform = {m: wrap_in_compose_is_list(test_transform)\n                                for m in modalities}\n\n    if self.concat_bands:\n        # Concatenate mean and std values\n        means = load_from_file_or_attribute(np.concatenate([means[m] for m in self.image_modalities]).tolist())\n        stds = load_from_file_or_attribute(np.concatenate([stds[m] for m in self.image_modalities]).tolist())\n\n        self.aug = Normalize(means, stds)\n    else:\n        # Apply standardization per modality\n        means = {m: load_from_file_or_attribute(means[m]) for m in means.keys()}\n        stds = {m: load_from_file_or_attribute(stds[m]) for m in stds.keys()}\n\n        self.aug = MultimodalNormalize(means, stds)\n\n    self.data_with_sample_dim = data_with_sample_dim\n\n    self.collate_fn = collate_chunk_dicts if data_with_sample_dim else collate_samples\n</code></pre>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_multimodal_data_module.MultiModalBatchSampler","title":"<code>MultiModalBatchSampler</code>","text":"<p>               Bases: <code>BatchSampler</code></p> <p>Sample a defined number of modalities per batch (see sample_num_modalities and sample_replace)</p> Source code in <code>terratorch/datamodules/generic_multimodal_data_module.py</code> <pre><code>class MultiModalBatchSampler(BatchSampler):\n    \"\"\"\n    Sample a defined number of modalities per batch (see sample_num_modalities and sample_replace)\n    \"\"\"\n    def __init__(self, modalities, sample_num_modalities, sample_replace, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.modalities = modalities\n        self.sample_num_modalities = sample_num_modalities\n        self.sample_replace = sample_replace\n\n    def __iter__(self) -&gt; Iterator[list[int]]:\n        \"\"\"\n        Code similar to BatchSampler but samples tuples in the format (idx, [\"m1\", \"m2\", ...])\n        \"\"\"\n        # Select sampled modalities per batch\n        sampled_modalities = np.random.choice(self.modalities, self.sample_num_modalities, replace=self.sample_replace)\n\n        if self.drop_last:\n            sampler_iter = iter(self.sampler)\n            while True:\n                try:\n                    batch = [(next(sampler_iter), sampled_modalities) for _ in range(self.batch_size)]\n                    yield batch\n                except StopIteration:\n                    break\n        else:\n            batch = [0] * self.batch_size\n            idx_in_batch = 0\n            for idx in self.sampler:\n                batch[idx_in_batch] = (idx, sampled_modalities)\n                idx_in_batch += 1\n                if idx_in_batch == self.batch_size:\n                    yield batch\n                    idx_in_batch = 0\n                    batch = [0] * self.batch_size\n            if idx_in_batch &gt; 0:\n                yield batch[:idx_in_batch]\n</code></pre>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_multimodal_data_module.MultiModalBatchSampler.__iter__","title":"<code>__iter__()</code>","text":"<p>Code similar to BatchSampler but samples tuples in the format (idx, [\"m1\", \"m2\", ...])</p> Source code in <code>terratorch/datamodules/generic_multimodal_data_module.py</code> <pre><code>def __iter__(self) -&gt; Iterator[list[int]]:\n    \"\"\"\n    Code similar to BatchSampler but samples tuples in the format (idx, [\"m1\", \"m2\", ...])\n    \"\"\"\n    # Select sampled modalities per batch\n    sampled_modalities = np.random.choice(self.modalities, self.sample_num_modalities, replace=self.sample_replace)\n\n    if self.drop_last:\n        sampler_iter = iter(self.sampler)\n        while True:\n            try:\n                batch = [(next(sampler_iter), sampled_modalities) for _ in range(self.batch_size)]\n                yield batch\n            except StopIteration:\n                break\n    else:\n        batch = [0] * self.batch_size\n        idx_in_batch = 0\n        for idx in self.sampler:\n            batch[idx_in_batch] = (idx, sampled_modalities)\n            idx_in_batch += 1\n            if idx_in_batch == self.batch_size:\n                yield batch\n                idx_in_batch = 0\n                batch = [0] * self.batch_size\n        if idx_in_batch &gt; 0:\n            yield batch[:idx_in_batch]\n</code></pre>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_multimodal_data_module.collate_samples","title":"<code>collate_samples(batch_list)</code>","text":"<p>Wrapper for default_collate as it cannot handle some datatypes such as np.datetime64.</p> Source code in <code>terratorch/datamodules/generic_multimodal_data_module.py</code> <pre><code>def collate_samples(batch_list):\n    \"\"\"\n    Wrapper for default_collate as it cannot handle some datatypes such as np.datetime64.\n    \"\"\"\n    batch = {}\n    for key, value in batch_list[0].items():  # TODO: Handle missing modalities when allow_missing_modalities is set.\n        if isinstance(value, dict):\n            batch[key] = collate_samples([chunk[key] for chunk in batch_list])\n        else:\n            try:\n                batch[key] = default_collate([chunk[key] for chunk in batch_list])\n            except TypeError:\n                # Fallback to numpy or simple list\n                if isinstance(value, np.ndarray):\n                    batch[key] = np.stack([chunk[key] for chunk in batch_list])\n                else:\n                    batch[key] = [chunk[key] for chunk in batch_list]\n    return batch\n</code></pre>"},{"location":"generic_datasets/","title":"Generic Datasets","text":""},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset","title":"<code>terratorch.datasets.generic_pixel_wise_dataset</code>","text":"<p>Module containing generic dataset classes</p>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoPixelwiseRegressionDataset","title":"<code>GenericNonGeoPixelwiseRegressionDataset</code>","text":"<p>               Bases: <code>GenericPixelWiseDataset</code></p> <p>GenericNonGeoPixelwiseRegressionDataset</p>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoPixelwiseRegressionDataset.__init__","title":"<code>__init__(data_root, label_data_root=None, image_grep='*', label_grep='*', split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, constant_scale=1, transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=False, reduce_zero_label=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with labels. If not specified, will use the same as for images.</p> <code>None</code> <code>image_grep</code> <code>str</code> <p>Regular expression appended to data_root to find input images. Defaults to \"*\".</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to data_root to find ground truth masks. Defaults to \"*\".</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoPixelwiseRegressionDataset.plot","title":"<code>plot(sample, suptitle=None, show_axes=False)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <code>show_axes</code> <code>bool | None</code> <p>whether to show axes or not</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> <p>.. versionadded:: 0.2</p>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoSegmentationDataset","title":"<code>GenericNonGeoSegmentationDataset</code>","text":"<p>               Bases: <code>GenericPixelWiseDataset</code></p> <p>GenericNonGeoSegmentationDataset</p>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoSegmentationDataset.__init__","title":"<code>__init__(data_root, num_classes, label_data_root=None, image_grep='*', label_grep='*', split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, class_names=None, constant_scale=1, transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=False, reduce_zero_label=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>num_classes</code> <code>int</code> <p>Number of classes in the dataset</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with labels. If not specified, will use the same as for images.</p> <code>None</code> <code>image_grep</code> <code>str</code> <p>Regular expression appended to data_root to find input images. Defaults to \"*\".</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to data_root to find ground truth masks. Defaults to \"*\".</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset.</p> <code>None</code> <code>class_names</code> <code>list[str]</code> <p>Class names. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoSegmentationDataset.plot","title":"<code>plot(sample, suptitle=None, show_axes=False)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <code>show_axes</code> <code>bool | None</code> <p>whether to show axes or not</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> <p>.. versionadded:: 0.2</p>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericPixelWiseDataset","title":"<code>GenericPixelWiseDataset</code>","text":"<p>               Bases: <code>NonGeoDataset</code>, <code>ABC</code></p> <p>This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset.</p>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericPixelWiseDataset.__init__","title":"<code>__init__(data_root, label_data_root=None, image_grep='*', label_grep='*', split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, constant_scale=1, transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=False, reduce_zero_label=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with labels. If not specified, will use the same as for images.</p> <code>None</code> <code>image_grep</code> <code>str</code> <p>Regular expression appended to data_root to find input images. Defaults to \"*\".</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to data_root to find ground truth masks. Defaults to \"*\".</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int | tuple[int, int] | str] | None</code> <p>Bands present in the dataset. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be refered to by output_bands. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int | tuple[int, int] | str] | None</code> <p>Bands that should be output by the dataset as named by dataset_bands.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code>"},{"location":"generic_datasets/#terratorch.datasets.generic_scalar_label_dataset","title":"<code>terratorch.datasets.generic_scalar_label_dataset</code>","text":"<p>Module containing generic dataset classes</p>"},{"location":"generic_datasets/#terratorch.datasets.generic_scalar_label_dataset.GenericNonGeoClassificationDataset","title":"<code>GenericNonGeoClassificationDataset</code>","text":"<p>               Bases: <code>GenericScalarLabelDataset</code></p> <p>GenericNonGeoClassificationDataset</p>"},{"location":"generic_datasets/#terratorch.datasets.generic_scalar_label_dataset.GenericNonGeoClassificationDataset.__init__","title":"<code>__init__(data_root, num_classes, split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, class_names=None, constant_scale=1, transform=None, no_data_replace=0, expand_temporal_dimension=False)</code>","text":"<p>A generic Non-Geo dataset for classification.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>num_classes</code> <code>int</code> <p>Number of classes in the dataset</p> required <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset.</p> <code>None</code> <code>class_names</code> <code>list[str]</code> <p>Class names. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float</code> <p>Replace nan values in input images with this value. Defaults to 0.</p> <code>0</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code>"},{"location":"generic_datasets/#terratorch.datasets.generic_scalar_label_dataset.GenericScalarLabelDataset","title":"<code>GenericScalarLabelDataset</code>","text":"<p>               Bases: <code>NonGeoDataset</code>, <code>ImageFolder</code>, <code>ABC</code></p> <p>This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset.</p>"},{"location":"generic_datasets/#terratorch.datasets.generic_scalar_label_dataset.GenericScalarLabelDataset.__base_getitem__","title":"<code>__base_getitem__(index)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Any, Any]</code> <p>(sample, target) where target is class_index of the target class.</p>"},{"location":"generic_datasets/#terratorch.datasets.generic_scalar_label_dataset.GenericScalarLabelDataset.__init__","title":"<code>__init__(data_root, split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, constant_scale=1, transform=None, no_data_replace=0, expand_temporal_dimension=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int | tuple[int, int] | str] | None</code> <p>Bands present in the dataset. This parameter gives identifiers to input channels (bands) so that they can then be refered to by output_bands. Can use the HLSBands enum, ints, int ranges, or strings. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int | tuple[int, int] | str] | None</code> <p>Bands that should be output by the dataset as named by dataset_bands.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float</code> <p>Replace nan values in input images with this value. Defaults to 0.</p> <code>0</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code>"},{"location":"generic_datasets/#terratorch.datasets.generic_multimodal_dataset","title":"<code>terratorch.datasets.generic_multimodal_dataset</code>","text":"<p>Module containing generic dataset classes</p>"},{"location":"generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalDataset","title":"<code>GenericMultimodalDataset</code>","text":"<p>               Bases: <code>NonGeoDataset</code>, <code>ABC</code></p> <p>This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset.</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>class GenericMultimodalDataset(NonGeoDataset, ABC):\n    \"\"\"\n    This is a generic dataset class to be used for instantiating datasets from arguments.\n    Ideally, one would create a dataset class specific to a dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_root: dict[str, Path | str],\n        label_data_root: Path | str | list[Path | str] | None = None,\n        image_grep: dict[str, str] | None = \"*\",\n        label_grep: str | None = \"*\",\n        split: Path | None = None,\n        image_modalities: list[str] | None = None,\n        rgb_modality: str | None = None,\n        rgb_indices: list[int] | None = None,\n        allow_missing_modalities: bool = False,\n        allow_substring_file_names: bool = True,\n        dataset_bands: dict[str, list] | None = None,\n        output_bands: dict[str, list] | None = None,\n        constant_scale: dict[str, float] = None,\n        transform: A.Compose | dict | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: float | None = -1,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n        channel_position: int = -3,\n        scalar_label: bool = False,\n        data_with_sample_dim: bool = False,\n        concat_bands: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n                data, with modalities as keys.\n            label_data_root (Path, optional): Path to data root directory with labels or csv/parquet files with\n                image-level labels. Needs to be specified for supervised tasks.\n            image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n                images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            label_grep (str, optional): Regular expression appended to label_data_root to find labels or mask files.\n                Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            split (Path, optional): Path to file containing samples prefixes to be used for this split.\n                The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n                sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n                files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n                If not specified, search samples based on files in data_root. Defaults to None.\n            image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n                The difference between all modalities and image_modalities are non-image modalities which are treated\n                differently during the transforms and are not modified but only converted into a tensor if possible.\n            rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n                TODO: Currently not implemented on a data module level!\n            allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n                image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n                If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n                Defaults to True.\n            dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n                as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n                that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n                of all modalities. Defaults to None.\n            output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n                provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n            constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n                keys. Can be subset of all modalities. Defaults to None.\n            transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n                modalities (transformation are shared between image modalities, e.g., similar crop or rotation).\n                Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization.\n                Not supported for multi-temporal data. The transform is not applied to non-image data, which is only\n                converted to tensors if possible. If dict, can include multiple transforms per modality which are\n                applied separately (no shared parameters between modalities).\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input data with this value.\n                If None, does no replacement. Defaults to None.\n            no_label_replace (float | None): Replace nan values in label with this value.\n                If none, does no replacement. Defaults to -1.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n            channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n            scalar_label (bool): Returns a image mask if False or otherwise the raw labels. Defaults to False.\n            concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n                that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n                Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n        \"\"\"\n        super().__init__()\n\n        self.split_file = split\n\n        self.modalities = list(data_root.keys())\n        assert \"mask\" not in self.modalities, \"Modality cannot be called 'mask'.\"\n        self.image_modalities = image_modalities or self.modalities\n        self.non_image_modalities = list(set(self.modalities) - set(image_modalities))\n        self.modalities = self.image_modalities + self.non_image_modalities  # Ensure image modalities to be first\n\n        if scalar_label:\n            self.non_image_modalities += [\"label\"]\n\n        # Order by modalities and convert path strings to lists as the code expects a list of paths per modality\n        data_root = {m: data_root[m] for m in self.modalities}\n\n        self.constant_scale = constant_scale or {}\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.reduce_zero_label = reduce_zero_label\n        self.expand_temporal_dimension = expand_temporal_dimension\n        self.channel_position = channel_position\n        self.scalar_label = scalar_label\n        self.data_with_sample_dim = data_with_sample_dim\n        self.concat_bands = concat_bands\n        assert not self.concat_bands or len(self.non_image_modalities) == 0, (\n            f\"concat_bands can only be used with image modalities, \"\n            f\"but non-image modalities are given: {self.non_image_modalities}\"\n        )\n        assert (\n            not self.concat_bands or not allow_missing_modalities\n        ), \"concat_bands cannot be used with allow_missing_modalities.\"\n\n        if self.expand_temporal_dimension and dataset_bands is None:\n            msg = \"Please provide dataset_bands when expand_temporal_dimension is True\"\n            raise Exception(msg)\n\n        # Load samples based on split file\n        if self.split_file is not None:\n            if str(self.split_file).endswith(\".txt\"):\n                with open(self.split_file) as f:\n                    split = f.readlines()\n                valid_files = [rf\"{substring.strip()}\" for substring in split]\n            else:\n                valid_files = list(load_table_data(self.split_file).index)\n\n        else:\n            image_files = {}\n            for m, m_paths in data_root.items():\n                image_files[m] = sorted(glob.glob(os.path.join(m_paths, image_grep[m])))\n\n            if label_data_root is not None:\n                image_files[\"mask\"] = sorted(glob.glob(os.path.join(label_data_root, label_grep)))\n\n            if allow_substring_file_names:\n                # Remove file extensions\n                get_file_id = lambda s: os.path.basename(s).split('.')[0]\n            else:\n                # Get exact match of filenames\n                get_file_id = lambda s: os.path.basename(s)\n\n            if allow_missing_modalities:\n                valid_files = list(set([get_file_id(file) for file in np.concatenate(list(image_files.values()))]))\n            else:\n                valid_files = [get_file_id(file) for file in image_files[self.modalities[0]]]\n\n        self.samples = []\n        num_modalities = len(self.modalities) + int(label_data_root is not None)\n\n        # Check for parquet and csv files with modality data and read the file\n\n        for m, m_path in data_root.items():\n            if os.path.isfile(m_path):\n                data_root[m] = load_table_data(m_path)\n                # Check for some sample keys\n                if not any(f in data_root[m].index for f in valid_files[:100]):\n                    warnings.warn(f\"Sample key expected in table index (first column) for {m} (file: {m_path}). \"\n                                  f\"{valid_files[:3]+['...']} are not in index {list(data_root[m].index[:3])+['...']}.\")\n\n        if label_data_root is not None:\n            if os.path.isfile(label_data_root):\n                label_data_root = load_table_data(label_data_root)\n                # Check for some sample keys\n                if not any(f in label_data_root.index for f in valid_files[:100]):\n                    warnings.warn(f\"Keys expected in table index (first column) for labels (file: {label_data_root}). \"\n                                  f\"The keys {valid_files[:3] + ['...']} are not in the index.\")\n\n        # Iterate over all files in split\n        for file in valid_files:\n            sample = {}\n            # Iterate over all modalities\n            for m, m_path in data_root.items():\n                if isinstance(m_path, pd.DataFrame):\n                    # Add tabular data to sample\n                    sample[m] = m_path.loc[file].values\n                elif allow_substring_file_names:\n                    # Substring match with image_grep\n                    m_files = glob.glob(os.path.join(m_path, file + image_grep[m]))\n                    if m_files:\n                        sample[m] = m_files[0]\n                else:\n                    # Exact match\n                    file_path = os.path.join(m_path, file)\n                    if os.path.exists(file_path):\n                        sample[m] = file_path\n\n            if label_data_root is not None:\n                if isinstance(label_data_root, pd.DataFrame):\n                    # Add tabular data to sample\n                    sample[\"mask\"] = label_data_root.loc[file].values\n                elif allow_substring_file_names:\n                    # Substring match with label_grep\n                    l_files = glob.glob(os.path.join(label_data_root, file + label_grep))\n                    if l_files:\n                        sample[\"mask\"] = l_files[0]\n                else:\n                    # Exact match\n                    file_path = os.path.join(label_data_root, file)\n                    if os.path.exists(file_path):\n                        sample[\"mask\"] = file_path\n                if \"mask\" not in sample:\n                    # Only add sample if mask is present\n                    break\n\n            if len(sample) == num_modalities or allow_missing_modalities:\n                self.samples.append(sample)\n\n        self.rgb_modality = rgb_modality or self.modalities[0]\n        self.rgb_indices = rgb_indices or [0, 1, 2]\n\n        if dataset_bands is not None:\n            self.dataset_bands = {m: generate_bands_intervals(m_bands) for m, m_bands in dataset_bands.items()}\n        else:\n            self.dataset_bands = None\n        if output_bands is not None:\n            self.output_bands = {m: generate_bands_intervals(m_bands) for m, m_bands in output_bands.items()}\n            for modality in self.modalities:\n                if modality in self.output_bands and modality not in self.dataset_bands:\n                    msg = f\"If output bands are provided, dataset_bands must also be provided (modality: {modality})\"\n                    raise Exception(msg)  # noqa: PLE0101\n        else:\n            self.output_bands = {}\n\n        self.filter_indices = {}\n        if self.output_bands:\n            for m in self.output_bands.keys():\n                if m not in self.output_bands or self.output_bands[m] == self.dataset_bands[m]:\n                    continue\n                if len(set(self.output_bands[m]) &amp; set(self.dataset_bands[m])) != len(self.output_bands[m]):\n                    msg = f\"Output bands must be a subset of dataset bands (Modality: {m})\"\n                    raise Exception(msg)\n\n                self.filter_indices[m] = [self.dataset_bands[m].index(band) for band in self.output_bands[m]]\n\n            if not self.channel_position:\n                logger.warning(\n                    \"output_bands is defined but no channel_position is provided. \"\n                    \"Channels must be in the last dimension, otherwise provide channel_position.\"\n                )\n\n        # If no transform is given, apply only to transform to torch tensor\n        if isinstance(transform, A.Compose):\n            self.transform = MultimodalTransforms(transform,\n                                                  non_image_modalities=self.non_image_modalities + ['label']\n                                                  if scalar_label else self.non_image_modalities)\n        elif transform is None:\n            self.transform = MultimodalToTensor(self.modalities)\n        else:\n            # Modality-specific transforms\n            transform = {m: transform[m] if m in transform else default_transform for m in self.modalities}\n            self.transform = MultimodalTransforms(transform, shared=False)\n\n        # Ignore rasterio warning for not geo-referenced files\n        import rasterio\n\n        warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n\n    def __len__(self) -&gt; int:\n        return len(self.samples)\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        output = {}\n        if isinstance(index, tuple):\n            # Load only sampled modalities instead of all modalities\n            # (see sample_num_modalities in GenericMultiModalDataModule for details)\n            index, modalities = index\n            sample = {m: self.samples[index][m] for m in modalities}\n        else:\n            sample = self.samples[index]\n\n        for modality, file in sample.items():\n            data = self._load_file(\n                file,\n                nan_replace=self.no_label_replace if modality == \"mask\" else self.no_data_replace,\n                modality=modality,\n            )\n\n            # Expand temporal dim\n            if modality in self.filter_indices and self.expand_temporal_dimension:\n                data = rearrange(\n                    data, \"(channels time) h w -&gt; channels time h w\", channels=len(self.dataset_bands[modality])\n                )\n\n            if modality == \"mask\" and len(data.shape) == 3 and len(data) == 1:\n                # tasks expect image masks without channel dim\n                data = data[0]\n\n            if modality in self.image_modalities and len(data.shape) &gt;= 3 and self.channel_position:\n                # to channels last (required by albumentations)\n                data = np.moveaxis(data, self.channel_position, -1)\n\n            if modality in self.filter_indices:\n                data = data[..., self.filter_indices[modality]]\n\n            if modality in self.constant_scale:\n                data = data.astype(np.float32) * self.constant_scale[modality]\n\n            output[modality] = data\n\n        if self.reduce_zero_label:\n            output[\"mask\"] -= 1\n\n        if self.scalar_label:\n            output[\"label\"] = output.pop(\"mask\")\n\n        if self.transform:\n            output = self.transform(output)\n\n        if self.concat_bands:\n            # Concatenate bands of all image modalities\n            data = [output.pop(m) for m in self.image_modalities if m in output]\n            output[\"image\"] = torch.cat(data, dim=1 if self.data_with_sample_dim else 0)\n        else:\n            # Tasks expect data to be stored in \"image\", moving modalities to image dict\n            output[\"image\"] = {m: output.pop(m) for m in self.modalities if m in output}\n\n        output[\"filename\"] = self.samples[index]\n\n        return output\n\n    def _load_file(self, path, nan_replace: int | float | None = None, modality: str | None = None) -&gt; xr.DataArray:\n        if isinstance(path, np.ndarray):\n            # data was loaded from table and is saved in memory\n            data = path\n        elif path.endswith(\".zarr\") or path.endswith(\".zarr.zip\"):\n            data = xr.open_zarr(path, mask_and_scale=True)\n            data_var = modality if modality in data.data_vars else list(data.data_vars)[0]\n            data = data[data_var].to_numpy()\n        elif path.endswith(\".npy\"):\n            data = np.load(path)\n        else:\n            data = rioxarray.open_rasterio(path, masked=True).to_numpy()\n\n        if nan_replace is not None:\n            data = np.nan_to_num(data, nan=nan_replace)\n        return data\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            suptitle: optional string to use as a suptitle\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n\n        .. versionadded:: 0.2\n        \"\"\"\n        image = sample[\"image\"]\n        if isinstance(image, dict):\n            image = image[self.rgb_modality]\n        if isinstance(image, torch.Tensor):\n            image = image.numpy()\n        image = image.take(self.rgb_indices, axis=0)\n        image = np.transpose(image, (1, 2, 0))\n        image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n        image = np.clip(image, 0, 1)\n\n        if \"mask\" in sample:\n            mask = sample[\"mask\"]\n            if isinstance(mask, torch.Tensor):\n                mask = mask.numpy()\n            if mask.ndim == 2:\n                mask = np.expand_dims(mask, axis=-1)\n            # Convert masked regions to 0.\n            mask = mask * -1 + 1\n        else:\n            mask = None\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"]\n            if isinstance(image, dict):\n                prediction = prediction[self.rgb_modality]\n            if isinstance(prediction, torch.Tensor):\n                prediction = prediction.numpy()\n            # Assuming reconstructed image\n            prediction = prediction.take(self.rgb_indices, axis=0)\n            prediction = np.transpose(prediction, (1, 2, 0))\n            prediction = (prediction - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n            prediction = np.clip(prediction, 0, 1)\n        else:\n            prediction = None\n\n        return self._plot_sample(\n            image,\n            mask=mask,\n            prediction=prediction,\n            suptitle=suptitle,\n        )\n\n    @staticmethod\n    def _plot_sample(image, mask=None, prediction=None, suptitle=None):\n        num_images = 1 + int(mask is not None) + int(prediction is not None)\n        fig, ax = plt.subplots(1, num_images, figsize=(5*num_images, 5), layout=\"compressed\")\n\n        ax[0].axis(\"off\")\n        ax[0].imshow(image)\n\n        if mask is not None:\n            ax[1].axis(\"off\")\n            ax[1].imshow(image * mask)\n\n        if prediction is not None:\n            ax[num_images-1].axis(\"off\")\n            ax[num_images-1].imshow(prediction)\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n        return fig\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalDataset.__init__","title":"<code>__init__(data_root, label_data_root=None, image_grep='*', label_grep='*', split=None, image_modalities=None, rgb_modality=None, rgb_indices=None, allow_missing_modalities=False, allow_substring_file_names=True, dataset_bands=None, output_bands=None, constant_scale=None, transform=None, no_data_replace=None, no_label_replace=-1, expand_temporal_dimension=False, reduce_zero_label=False, channel_position=-3, scalar_label=False, data_with_sample_dim=False, concat_bands=False, *args, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to data root directory or csv/parquet files with image-level data, with modalities as keys.</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with labels or csv/parquet files with image-level labels. Needs to be specified for supervised tasks.</p> <code>None</code> <code>image_grep</code> <code>dict[str]</code> <p>Dictionary with regular expression appended to data_root to find input images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to label_data_root to find labels or mask files. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing samples prefixes to be used for this split. The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise, files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]). If not specified, search samples based on files in data_root. Defaults to None.</p> <code>None</code> <code>image_modalities(list[str],</code> <code>optional</code> <p>List of pixel-level raster modalities. Defaults to data_root.keys(). The difference between all modalities and image_modalities are non-image modalities which are treated differently during the transforms and are not modified but only converted into a tensor if possible.</p> required <code>rgb_modality</code> <code>str</code> <p>Modality used for RGB plots. Defaults to first modality in data_root.keys().</p> <code>None</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>allow_missing_modalities</code> <code>bool</code> <p>Allow missing modalities during data loading. Defaults to False. TODO: Currently not implemented on a data module level!</p> <code>False</code> <code>allow_substring_file_names</code> <code>bool</code> <p>Allow substrings during sample identification by adding image or label grep to the sample prefixes. If False, treats sample prefixes as full file names. If True and no split file is provided, considers the file stem as prefix, otherwise the full file name. Defaults to True.</p> <code>True</code> <code>dataset_bands</code> <code>dict[list]</code> <p>Bands present in the dataset, provided in a dictionary with modalities as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset of all modalities. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>dict[list]</code> <p>Bands that should be output by the dataset as named by dataset_bands, provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>dict[float]</code> <p>Factor to multiply data values by, provided as a dictionary with modalities as keys. Can be subset of all modalities. Defaults to None.</p> <code>None</code> <code>transform</code> <code>Compose | dict | None</code> <p>Albumentations transform to be applied to all image modalities (transformation are shared between image modalities, e.g., similar crop or rotation). Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. The transform is not applied to non-image data, which is only converted to tensors if possible. If dict, can include multiple transforms per modality which are applied separately (no shared parameters between modalities). Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input data with this value. If None, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>float | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>-1</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Only works with image modalities. Is only applied to modalities with defined dataset_bands. Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>channel_position</code> <code>int</code> <p>Position of the channel dimension in the image modalities. Defaults to -3.</p> <code>-3</code> <code>scalar_label</code> <code>bool</code> <p>Returns a image mask if False or otherwise the raw labels. Defaults to False.</p> <code>False</code> <code>concat_bands</code> <code>bool</code> <p>Concatenate all image modalities along the band dimension into a single \"image\", so that it can be processed by single-modal models. Concatenate in the order of provided modalities. Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: dict[str, Path | str],\n    label_data_root: Path | str | list[Path | str] | None = None,\n    image_grep: dict[str, str] | None = \"*\",\n    label_grep: str | None = \"*\",\n    split: Path | None = None,\n    image_modalities: list[str] | None = None,\n    rgb_modality: str | None = None,\n    rgb_indices: list[int] | None = None,\n    allow_missing_modalities: bool = False,\n    allow_substring_file_names: bool = True,\n    dataset_bands: dict[str, list] | None = None,\n    output_bands: dict[str, list] | None = None,\n    constant_scale: dict[str, float] = None,\n    transform: A.Compose | dict | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: float | None = -1,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n    channel_position: int = -3,\n    scalar_label: bool = False,\n    data_with_sample_dim: bool = False,\n    concat_bands: bool = False,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n            data, with modalities as keys.\n        label_data_root (Path, optional): Path to data root directory with labels or csv/parquet files with\n            image-level labels. Needs to be specified for supervised tasks.\n        image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n            images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        label_grep (str, optional): Regular expression appended to label_data_root to find labels or mask files.\n            Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        split (Path, optional): Path to file containing samples prefixes to be used for this split.\n            The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n            sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n            files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n            If not specified, search samples based on files in data_root. Defaults to None.\n        image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n            The difference between all modalities and image_modalities are non-image modalities which are treated\n            differently during the transforms and are not modified but only converted into a tensor if possible.\n        rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n            TODO: Currently not implemented on a data module level!\n        allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n            image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n            If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n            Defaults to True.\n        dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n            as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n            that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n            of all modalities. Defaults to None.\n        output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n            provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n        constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n            keys. Can be subset of all modalities. Defaults to None.\n        transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n            modalities (transformation are shared between image modalities, e.g., similar crop or rotation).\n            Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization.\n            Not supported for multi-temporal data. The transform is not applied to non-image data, which is only\n            converted to tensors if possible. If dict, can include multiple transforms per modality which are\n            applied separately (no shared parameters between modalities).\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input data with this value.\n            If None, does no replacement. Defaults to None.\n        no_label_replace (float | None): Replace nan values in label with this value.\n            If none, does no replacement. Defaults to -1.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n        channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n        scalar_label (bool): Returns a image mask if False or otherwise the raw labels. Defaults to False.\n        concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n            that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n            Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n    \"\"\"\n    super().__init__()\n\n    self.split_file = split\n\n    self.modalities = list(data_root.keys())\n    assert \"mask\" not in self.modalities, \"Modality cannot be called 'mask'.\"\n    self.image_modalities = image_modalities or self.modalities\n    self.non_image_modalities = list(set(self.modalities) - set(image_modalities))\n    self.modalities = self.image_modalities + self.non_image_modalities  # Ensure image modalities to be first\n\n    if scalar_label:\n        self.non_image_modalities += [\"label\"]\n\n    # Order by modalities and convert path strings to lists as the code expects a list of paths per modality\n    data_root = {m: data_root[m] for m in self.modalities}\n\n    self.constant_scale = constant_scale or {}\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.reduce_zero_label = reduce_zero_label\n    self.expand_temporal_dimension = expand_temporal_dimension\n    self.channel_position = channel_position\n    self.scalar_label = scalar_label\n    self.data_with_sample_dim = data_with_sample_dim\n    self.concat_bands = concat_bands\n    assert not self.concat_bands or len(self.non_image_modalities) == 0, (\n        f\"concat_bands can only be used with image modalities, \"\n        f\"but non-image modalities are given: {self.non_image_modalities}\"\n    )\n    assert (\n        not self.concat_bands or not allow_missing_modalities\n    ), \"concat_bands cannot be used with allow_missing_modalities.\"\n\n    if self.expand_temporal_dimension and dataset_bands is None:\n        msg = \"Please provide dataset_bands when expand_temporal_dimension is True\"\n        raise Exception(msg)\n\n    # Load samples based on split file\n    if self.split_file is not None:\n        if str(self.split_file).endswith(\".txt\"):\n            with open(self.split_file) as f:\n                split = f.readlines()\n            valid_files = [rf\"{substring.strip()}\" for substring in split]\n        else:\n            valid_files = list(load_table_data(self.split_file).index)\n\n    else:\n        image_files = {}\n        for m, m_paths in data_root.items():\n            image_files[m] = sorted(glob.glob(os.path.join(m_paths, image_grep[m])))\n\n        if label_data_root is not None:\n            image_files[\"mask\"] = sorted(glob.glob(os.path.join(label_data_root, label_grep)))\n\n        if allow_substring_file_names:\n            # Remove file extensions\n            get_file_id = lambda s: os.path.basename(s).split('.')[0]\n        else:\n            # Get exact match of filenames\n            get_file_id = lambda s: os.path.basename(s)\n\n        if allow_missing_modalities:\n            valid_files = list(set([get_file_id(file) for file in np.concatenate(list(image_files.values()))]))\n        else:\n            valid_files = [get_file_id(file) for file in image_files[self.modalities[0]]]\n\n    self.samples = []\n    num_modalities = len(self.modalities) + int(label_data_root is not None)\n\n    # Check for parquet and csv files with modality data and read the file\n\n    for m, m_path in data_root.items():\n        if os.path.isfile(m_path):\n            data_root[m] = load_table_data(m_path)\n            # Check for some sample keys\n            if not any(f in data_root[m].index for f in valid_files[:100]):\n                warnings.warn(f\"Sample key expected in table index (first column) for {m} (file: {m_path}). \"\n                              f\"{valid_files[:3]+['...']} are not in index {list(data_root[m].index[:3])+['...']}.\")\n\n    if label_data_root is not None:\n        if os.path.isfile(label_data_root):\n            label_data_root = load_table_data(label_data_root)\n            # Check for some sample keys\n            if not any(f in label_data_root.index for f in valid_files[:100]):\n                warnings.warn(f\"Keys expected in table index (first column) for labels (file: {label_data_root}). \"\n                              f\"The keys {valid_files[:3] + ['...']} are not in the index.\")\n\n    # Iterate over all files in split\n    for file in valid_files:\n        sample = {}\n        # Iterate over all modalities\n        for m, m_path in data_root.items():\n            if isinstance(m_path, pd.DataFrame):\n                # Add tabular data to sample\n                sample[m] = m_path.loc[file].values\n            elif allow_substring_file_names:\n                # Substring match with image_grep\n                m_files = glob.glob(os.path.join(m_path, file + image_grep[m]))\n                if m_files:\n                    sample[m] = m_files[0]\n            else:\n                # Exact match\n                file_path = os.path.join(m_path, file)\n                if os.path.exists(file_path):\n                    sample[m] = file_path\n\n        if label_data_root is not None:\n            if isinstance(label_data_root, pd.DataFrame):\n                # Add tabular data to sample\n                sample[\"mask\"] = label_data_root.loc[file].values\n            elif allow_substring_file_names:\n                # Substring match with label_grep\n                l_files = glob.glob(os.path.join(label_data_root, file + label_grep))\n                if l_files:\n                    sample[\"mask\"] = l_files[0]\n            else:\n                # Exact match\n                file_path = os.path.join(label_data_root, file)\n                if os.path.exists(file_path):\n                    sample[\"mask\"] = file_path\n            if \"mask\" not in sample:\n                # Only add sample if mask is present\n                break\n\n        if len(sample) == num_modalities or allow_missing_modalities:\n            self.samples.append(sample)\n\n    self.rgb_modality = rgb_modality or self.modalities[0]\n    self.rgb_indices = rgb_indices or [0, 1, 2]\n\n    if dataset_bands is not None:\n        self.dataset_bands = {m: generate_bands_intervals(m_bands) for m, m_bands in dataset_bands.items()}\n    else:\n        self.dataset_bands = None\n    if output_bands is not None:\n        self.output_bands = {m: generate_bands_intervals(m_bands) for m, m_bands in output_bands.items()}\n        for modality in self.modalities:\n            if modality in self.output_bands and modality not in self.dataset_bands:\n                msg = f\"If output bands are provided, dataset_bands must also be provided (modality: {modality})\"\n                raise Exception(msg)  # noqa: PLE0101\n    else:\n        self.output_bands = {}\n\n    self.filter_indices = {}\n    if self.output_bands:\n        for m in self.output_bands.keys():\n            if m not in self.output_bands or self.output_bands[m] == self.dataset_bands[m]:\n                continue\n            if len(set(self.output_bands[m]) &amp; set(self.dataset_bands[m])) != len(self.output_bands[m]):\n                msg = f\"Output bands must be a subset of dataset bands (Modality: {m})\"\n                raise Exception(msg)\n\n            self.filter_indices[m] = [self.dataset_bands[m].index(band) for band in self.output_bands[m]]\n\n        if not self.channel_position:\n            logger.warning(\n                \"output_bands is defined but no channel_position is provided. \"\n                \"Channels must be in the last dimension, otherwise provide channel_position.\"\n            )\n\n    # If no transform is given, apply only to transform to torch tensor\n    if isinstance(transform, A.Compose):\n        self.transform = MultimodalTransforms(transform,\n                                              non_image_modalities=self.non_image_modalities + ['label']\n                                              if scalar_label else self.non_image_modalities)\n    elif transform is None:\n        self.transform = MultimodalToTensor(self.modalities)\n    else:\n        # Modality-specific transforms\n        transform = {m: transform[m] if m in transform else default_transform for m in self.modalities}\n        self.transform = MultimodalTransforms(transform, shared=False)\n\n    # Ignore rasterio warning for not geo-referenced files\n    import rasterio\n\n    warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalDataset.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> <p>.. versionadded:: 0.2</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        suptitle: optional string to use as a suptitle\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n\n    .. versionadded:: 0.2\n    \"\"\"\n    image = sample[\"image\"]\n    if isinstance(image, dict):\n        image = image[self.rgb_modality]\n    if isinstance(image, torch.Tensor):\n        image = image.numpy()\n    image = image.take(self.rgb_indices, axis=0)\n    image = np.transpose(image, (1, 2, 0))\n    image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n    image = np.clip(image, 0, 1)\n\n    if \"mask\" in sample:\n        mask = sample[\"mask\"]\n        if isinstance(mask, torch.Tensor):\n            mask = mask.numpy()\n        if mask.ndim == 2:\n            mask = np.expand_dims(mask, axis=-1)\n        # Convert masked regions to 0.\n        mask = mask * -1 + 1\n    else:\n        mask = None\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"]\n        if isinstance(image, dict):\n            prediction = prediction[self.rgb_modality]\n        if isinstance(prediction, torch.Tensor):\n            prediction = prediction.numpy()\n        # Assuming reconstructed image\n        prediction = prediction.take(self.rgb_indices, axis=0)\n        prediction = np.transpose(prediction, (1, 2, 0))\n        prediction = (prediction - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n        prediction = np.clip(prediction, 0, 1)\n    else:\n        prediction = None\n\n    return self._plot_sample(\n        image,\n        mask=mask,\n        prediction=prediction,\n        suptitle=suptitle,\n    )\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalPixelwiseRegressionDataset","title":"<code>GenericMultimodalPixelwiseRegressionDataset</code>","text":"<p>               Bases: <code>GenericMultimodalDataset</code></p> <p>GenericNonGeoPixelwiseRegressionDataset</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>class GenericMultimodalPixelwiseRegressionDataset(GenericMultimodalDataset):\n    \"\"\"GenericNonGeoPixelwiseRegressionDataset\"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        label_data_root: Path,\n        image_grep: str | None = \"*\",\n        label_grep: str | None = \"*\",\n        split: Path | None = None,\n        image_modalities: list[str] | None = None,\n        rgb_modality: str | None = None,\n        rgb_indices: list[int] | None = None,\n        allow_missing_modalities: bool = False,\n        allow_substring_file_names: bool = False,\n        dataset_bands: dict[list] | None = None,\n        output_bands: dict[list] | None = None,\n        constant_scale: dict[float] = 1.0,\n        transform: A.Compose | dict | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: float | None = None,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n        channel_position: int = -3,\n        concat_bands: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n                data, with modalities as keys.\n            label_data_root (Path): Path to data root directory with ground truth files.\n            image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n                images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            label_grep (str, optional): Regular expression appended to label_data_root to find ground truth files.\n                Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            split (Path, optional): Path to file containing samples prefixes to be used for this split.\n                The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n                sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n                files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n                If not specified, search samples based on files in data_root. Defaults to None.\n            image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n                The difference between all modalities and image_modalities are non-image modalities which are treated\n                differently during the transforms and are not modified but only converted into a tensor if possible.\n            rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n                TODO: Currently not implemented on a data module level!\n            allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n                image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n                If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n                Defaults to True.\n            dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n                as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n                that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n                of all modalities. Defaults to None.\n            output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n                provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n            constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n                keys. Can be subset of all modalities. Defaults to None.\n            transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n                modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to\n                non-image data, which is only converted to tensors if possible. If dict, can include separate transforms\n                per modality (no shared parameters between modalities).\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input data with this value.\n                If None, does no replacement. Defaults to None.\n            no_label_replace (float | None): Replace nan values in label with this value.\n                If none, does no replacement. Defaults to None.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n            channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n            concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n                that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n                Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n        \"\"\"\n        assert label_data_root is not None, \"label_data_root must be specified for regression tasks.\"\n\n        super().__init__(\n            data_root,\n            label_data_root=label_data_root,\n            image_grep=image_grep,\n            label_grep=label_grep,\n            split=split,\n            image_modalities=image_modalities,\n            rgb_modality=rgb_modality,\n            rgb_indices=rgb_indices,\n            allow_missing_modalities=allow_missing_modalities,\n            allow_substring_file_names=allow_substring_file_names,\n            dataset_bands=dataset_bands,\n            output_bands=output_bands,\n            constant_scale=constant_scale,\n            transform=transform,\n            no_data_replace=no_data_replace,\n            no_label_replace=no_label_replace,\n            expand_temporal_dimension=expand_temporal_dimension,\n            reduce_zero_label=reduce_zero_label,\n            channel_position=channel_position,\n            concat_bands=concat_bands,\n            *args,\n            **kwargs,\n        )\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        item = super().__getitem__(index)\n        item[\"mask\"] = item[\"mask\"].float()\n        return item\n\n    def plot(\n        self, sample: dict[str, torch.Tensor], suptitle: str | None = None, show_axes: bool | None = False\n    ) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__`\n            suptitle (str|None): optional string to use as a suptitle\n            show_axes (bool|None): whether to show axes or not\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n\n        .. versionadded:: 0.2\n        \"\"\"\n\n        image = sample[\"image\"]\n        if isinstance(image, dict):\n            image = image[self.rgb_modality]\n        if isinstance(image, torch.Tensor):\n            image = image.numpy()\n        image = image.take(self.rgb_indices, axis=0)\n        image = np.transpose(image, (1, 2, 0))\n        image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n        image = np.clip(image, 0, 1)\n\n        label_mask = sample[\"mask\"]\n        if isinstance(label_mask, torch.Tensor):\n            label_mask = label_mask.numpy()\n\n        showing_predictions = \"prediction\" in sample\n        if showing_predictions:\n            prediction_mask = sample[\"prediction\"]\n            if isinstance(prediction_mask, torch.Tensor):\n                prediction_mask = prediction_mask.numpy()\n\n        return self._plot_sample(\n            image,\n            label_mask,\n            prediction=prediction_mask if showing_predictions else None,\n            suptitle=suptitle,\n            show_axes=show_axes,\n        )\n\n    @staticmethod\n    def _plot_sample(image, label, prediction=None, suptitle=None, show_axes=False):\n        num_images = 4 if prediction is not None else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 10), layout=\"compressed\")\n        axes_visibility = \"on\" if show_axes else \"off\"\n\n        norm = mpl.colors.Normalize(vmin=label.min(), vmax=label.max())\n        ax[0].axis(axes_visibility)\n        ax[0].title.set_text(\"Image\")\n        ax[0].imshow(image)\n\n        ax[1].axis(axes_visibility)\n        ax[1].title.set_text(\"Ground Truth Mask\")\n        ax[1].imshow(label, cmap=\"Greens\", norm=norm)\n\n        ax[2].axis(axes_visibility)\n        ax[2].title.set_text(\"GT Mask on Image\")\n        ax[2].imshow(image)\n        ax[2].imshow(label, cmap=\"Greens\", alpha=0.3, norm=norm)\n        # ax[2].legend()\n\n        if prediction is not None:\n            ax[3].axis(axes_visibility)\n            ax[3].title.set_text(\"Predicted Mask\")\n            ax[3].imshow(prediction, cmap=\"Greens\", norm=norm)\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n        return fig\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalPixelwiseRegressionDataset.__init__","title":"<code>__init__(data_root, label_data_root, image_grep='*', label_grep='*', split=None, image_modalities=None, rgb_modality=None, rgb_indices=None, allow_missing_modalities=False, allow_substring_file_names=False, dataset_bands=None, output_bands=None, constant_scale=1.0, transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=False, reduce_zero_label=False, channel_position=-3, concat_bands=False, *args, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to data root directory or csv/parquet files with image-level data, with modalities as keys.</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with ground truth files.</p> required <code>image_grep</code> <code>dict[str]</code> <p>Dictionary with regular expression appended to data_root to find input images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to label_data_root to find ground truth files. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing samples prefixes to be used for this split. The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise, files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]). If not specified, search samples based on files in data_root. Defaults to None.</p> <code>None</code> <code>image_modalities(list[str],</code> <code>optional</code> <p>List of pixel-level raster modalities. Defaults to data_root.keys(). The difference between all modalities and image_modalities are non-image modalities which are treated differently during the transforms and are not modified but only converted into a tensor if possible.</p> required <code>rgb_modality</code> <code>str</code> <p>Modality used for RGB plots. Defaults to first modality in data_root.keys().</p> <code>None</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>allow_missing_modalities</code> <code>bool</code> <p>Allow missing modalities during data loading. Defaults to False. TODO: Currently not implemented on a data module level!</p> <code>False</code> <code>allow_substring_file_names</code> <code>bool</code> <p>Allow substrings during sample identification by adding image or label grep to the sample prefixes. If False, treats sample prefixes as full file names. If True and no split file is provided, considers the file stem as prefix, otherwise the full file name. Defaults to True.</p> <code>False</code> <code>dataset_bands</code> <code>dict[list]</code> <p>Bands present in the dataset, provided in a dictionary with modalities as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset of all modalities. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>dict[list]</code> <p>Bands that should be output by the dataset as named by dataset_bands, provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>dict[float]</code> <p>Factor to multiply data values by, provided as a dictionary with modalities as keys. Can be subset of all modalities. Defaults to None.</p> <code>1.0</code> <code>transform</code> <code>Compose | dict | None</code> <p>Albumentations transform to be applied to all image modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to non-image data, which is only converted to tensors if possible. If dict, can include separate transforms per modality (no shared parameters between modalities). Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input data with this value. If None, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>float | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Only works with image modalities. Is only applied to modalities with defined dataset_bands. Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>channel_position</code> <code>int</code> <p>Position of the channel dimension in the image modalities. Defaults to -3.</p> <code>-3</code> <code>concat_bands</code> <code>bool</code> <p>Concatenate all image modalities along the band dimension into a single \"image\", so that it can be processed by single-modal models. Concatenate in the order of provided modalities. Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    label_data_root: Path,\n    image_grep: str | None = \"*\",\n    label_grep: str | None = \"*\",\n    split: Path | None = None,\n    image_modalities: list[str] | None = None,\n    rgb_modality: str | None = None,\n    rgb_indices: list[int] | None = None,\n    allow_missing_modalities: bool = False,\n    allow_substring_file_names: bool = False,\n    dataset_bands: dict[list] | None = None,\n    output_bands: dict[list] | None = None,\n    constant_scale: dict[float] = 1.0,\n    transform: A.Compose | dict | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: float | None = None,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n    channel_position: int = -3,\n    concat_bands: bool = False,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n            data, with modalities as keys.\n        label_data_root (Path): Path to data root directory with ground truth files.\n        image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n            images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        label_grep (str, optional): Regular expression appended to label_data_root to find ground truth files.\n            Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        split (Path, optional): Path to file containing samples prefixes to be used for this split.\n            The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n            sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n            files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n            If not specified, search samples based on files in data_root. Defaults to None.\n        image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n            The difference between all modalities and image_modalities are non-image modalities which are treated\n            differently during the transforms and are not modified but only converted into a tensor if possible.\n        rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n            TODO: Currently not implemented on a data module level!\n        allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n            image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n            If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n            Defaults to True.\n        dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n            as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n            that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n            of all modalities. Defaults to None.\n        output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n            provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n        constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n            keys. Can be subset of all modalities. Defaults to None.\n        transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n            modalities. Should end with ToTensorV2() and not include normalization. The transform is not applied to\n            non-image data, which is only converted to tensors if possible. If dict, can include separate transforms\n            per modality (no shared parameters between modalities).\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input data with this value.\n            If None, does no replacement. Defaults to None.\n        no_label_replace (float | None): Replace nan values in label with this value.\n            If none, does no replacement. Defaults to None.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n        channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n        concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n            that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n            Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n    \"\"\"\n    assert label_data_root is not None, \"label_data_root must be specified for regression tasks.\"\n\n    super().__init__(\n        data_root,\n        label_data_root=label_data_root,\n        image_grep=image_grep,\n        label_grep=label_grep,\n        split=split,\n        image_modalities=image_modalities,\n        rgb_modality=rgb_modality,\n        rgb_indices=rgb_indices,\n        allow_missing_modalities=allow_missing_modalities,\n        allow_substring_file_names=allow_substring_file_names,\n        dataset_bands=dataset_bands,\n        output_bands=output_bands,\n        constant_scale=constant_scale,\n        transform=transform,\n        no_data_replace=no_data_replace,\n        no_label_replace=no_label_replace,\n        expand_temporal_dimension=expand_temporal_dimension,\n        reduce_zero_label=reduce_zero_label,\n        channel_position=channel_position,\n        concat_bands=concat_bands,\n        *args,\n        **kwargs,\n    )\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalPixelwiseRegressionDataset.plot","title":"<code>plot(sample, suptitle=None, show_axes=False)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <code>show_axes</code> <code>bool | None</code> <p>whether to show axes or not</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> <p>.. versionadded:: 0.2</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def plot(\n    self, sample: dict[str, torch.Tensor], suptitle: str | None = None, show_axes: bool | None = False\n) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__`\n        suptitle (str|None): optional string to use as a suptitle\n        show_axes (bool|None): whether to show axes or not\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n\n    .. versionadded:: 0.2\n    \"\"\"\n\n    image = sample[\"image\"]\n    if isinstance(image, dict):\n        image = image[self.rgb_modality]\n    if isinstance(image, torch.Tensor):\n        image = image.numpy()\n    image = image.take(self.rgb_indices, axis=0)\n    image = np.transpose(image, (1, 2, 0))\n    image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n    image = np.clip(image, 0, 1)\n\n    label_mask = sample[\"mask\"]\n    if isinstance(label_mask, torch.Tensor):\n        label_mask = label_mask.numpy()\n\n    showing_predictions = \"prediction\" in sample\n    if showing_predictions:\n        prediction_mask = sample[\"prediction\"]\n        if isinstance(prediction_mask, torch.Tensor):\n            prediction_mask = prediction_mask.numpy()\n\n    return self._plot_sample(\n        image,\n        label_mask,\n        prediction=prediction_mask if showing_predictions else None,\n        suptitle=suptitle,\n        show_axes=show_axes,\n    )\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalScalarDataset","title":"<code>GenericMultimodalScalarDataset</code>","text":"<p>               Bases: <code>GenericMultimodalDataset</code></p> <p>GenericMultimodalClassificationDataset</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>class GenericMultimodalScalarDataset(GenericMultimodalDataset):\n    \"\"\"GenericMultimodalClassificationDataset\"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        num_classes: int,\n        label_data_root: Path,\n        image_grep: str | None = \"*\",\n        label_grep: str | None = \"*\",\n        split: Path | None = None,\n        image_modalities: list[str] | None = None,\n        rgb_modality: str | None = None,\n        rgb_indices: list[int] | None = None,\n        allow_missing_modalities: bool = False,\n        allow_substring_file_names: bool = False,\n        dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        class_names: list[str] | None = None,\n        constant_scale: dict[float] = 1.0,\n        transform: A.Compose | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = None,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n        channel_position: int = -3,\n        concat_bands: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n                data, with modalities as keys.\n            num_classes (int): Number of classes.\n            label_data_root (Path): Path to data root directory with labels or csv/parquet files with labels.\n            image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n                images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            label_grep (str, optional): Regular expression appended to label_data_root to find labels files.\n                Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            split (Path, optional): Path to file containing samples prefixes to be used for this split.\n                The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n                sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n                files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n                If not specified, search samples based on files in data_root. Defaults to None.\n            image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n                The difference between all modalities and image_modalities are non-image modalities which are treated\n                differently during the transforms and are not modified but only converted into a tensor if possible.\n            rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n                TODO: Currently not implemented on a data module level!\n            allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n                image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n                If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n                Defaults to True.\n            dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n                as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n                that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n                of all modalities. Defaults to None.\n            output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n                provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n            class_names (list[str], optional): Names of the classes. Defaults to None.\n            constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n                keys. Can be subset of all modalities. Defaults to None.\n            transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n                modalities (transformation are shared between image modalities, e.g., similar crop or rotation).\n                Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization.\n                Not supported for multi-temporal data. The transform is not applied to non-image data, which is only\n                converted to tensors if possible. If dict, can include multiple transforms per modality which are\n                applied separately (no shared parameters between modalities).\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input data with this value.\n                If None, does no replacement. Defaults to None.\n            no_label_replace (float | None): Replace nan values in label with this value.\n                If none, does no replacement. Defaults to -1.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n            channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n            concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n                that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n                Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n        \"\"\"\n        assert label_data_root is not None, \"label_data_root must be specified for scalar tasks.\"\n\n        super().__init__(\n            data_root,\n            label_data_root=label_data_root,\n            image_grep=image_grep,\n            label_grep=label_grep,\n            split=split,\n            image_modalities=image_modalities,\n            rgb_modality=rgb_modality,\n            rgb_indices=rgb_indices,\n            allow_missing_modalities=allow_missing_modalities,\n            allow_substring_file_names=allow_substring_file_names,\n            dataset_bands=dataset_bands,\n            output_bands=output_bands,\n            constant_scale=constant_scale,\n            transform=transform,\n            no_data_replace=no_data_replace,\n            no_label_replace=no_label_replace,\n            expand_temporal_dimension=expand_temporal_dimension,\n            reduce_zero_label=reduce_zero_label,\n            channel_position=channel_position,\n            scalar_label=True,\n            concat_bands=concat_bands,\n            *args,\n            **kwargs,\n        )\n\n        self.num_classes = num_classes\n        self.class_names = class_names\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        item = super().__getitem__(index)\n        return item\n\n    def plot(\n        self, sample: dict[str, torch.Tensor], suptitle: str | None = None, show_axes: bool | None = False\n    ) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__`\n            suptitle (str|None): optional string to use as a suptitle\n            show_axes (bool|None): whether to show axes or not\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n\n        .. versionadded:: 0.2\n        \"\"\"\n\n        # TODO: Check plotting code for classification tasks and add it to generic classification dataset as well\n        raise NotImplementedError\n\n        image = sample[\"image\"]\n        if isinstance(image, dict):\n            image = image[self.rgb_modality]\n        if isinstance(image, torch.Tensor):\n            image = image.numpy()\n        image = image.take(self.rgb_indices, axis=0)\n        image = np.transpose(image, (1, 2, 0))\n        image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n        image = np.clip(image, 0, 1)\n\n        label_mask = sample[\"mask\"]\n        if isinstance(label_mask, torch.Tensor):\n            label_mask = label_mask.numpy()\n\n        showing_predictions = \"prediction\" in sample\n        if showing_predictions:\n            prediction_mask = sample[\"prediction\"]\n            if isinstance(prediction_mask, torch.Tensor):\n                prediction_mask = prediction_mask.numpy()\n\n        return self._plot_sample(\n            image,\n            label_mask,\n            prediction=prediction_mask if showing_predictions else None,\n            suptitle=suptitle,\n            show_axes=show_axes,\n        )\n\n    @staticmethod\n    def _plot_sample(image, label, prediction=None, suptitle=None, show_axes=False):\n        num_images = 4 if prediction is not None else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 10), layout=\"compressed\")\n        axes_visibility = \"on\" if show_axes else \"off\"\n\n        norm = mpl.colors.Normalize(vmin=label.min(), vmax=label.max())\n        ax[0].axis(axes_visibility)\n        ax[0].title.set_text(\"Image\")\n        ax[0].imshow(image)\n\n        ax[1].axis(axes_visibility)\n        ax[1].title.set_text(\"Ground Truth Mask\")\n        ax[1].imshow(label, cmap=\"Greens\", norm=norm)\n\n        ax[2].axis(axes_visibility)\n        ax[2].title.set_text(\"GT Mask on Image\")\n        ax[2].imshow(image)\n        ax[2].imshow(label, cmap=\"Greens\", alpha=0.3, norm=norm)\n        # ax[2].legend()\n\n        if prediction is not None:\n            ax[3].axis(axes_visibility)\n            ax[3].title.set_text(\"Predicted Mask\")\n            ax[3].imshow(prediction, cmap=\"Greens\", norm=norm)\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n        return fig\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalScalarDataset.__init__","title":"<code>__init__(data_root, num_classes, label_data_root, image_grep='*', label_grep='*', split=None, image_modalities=None, rgb_modality=None, rgb_indices=None, allow_missing_modalities=False, allow_substring_file_names=False, dataset_bands=None, output_bands=None, class_names=None, constant_scale=1.0, transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=False, reduce_zero_label=False, channel_position=-3, concat_bands=False, *args, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to data root directory or csv/parquet files with image-level data, with modalities as keys.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes.</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with labels or csv/parquet files with labels.</p> required <code>image_grep</code> <code>dict[str]</code> <p>Dictionary with regular expression appended to data_root to find input images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to label_data_root to find labels files. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing samples prefixes to be used for this split. The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise, files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]). If not specified, search samples based on files in data_root. Defaults to None.</p> <code>None</code> <code>image_modalities(list[str],</code> <code>optional</code> <p>List of pixel-level raster modalities. Defaults to data_root.keys(). The difference between all modalities and image_modalities are non-image modalities which are treated differently during the transforms and are not modified but only converted into a tensor if possible.</p> required <code>rgb_modality</code> <code>str</code> <p>Modality used for RGB plots. Defaults to first modality in data_root.keys().</p> <code>None</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>allow_missing_modalities</code> <code>bool</code> <p>Allow missing modalities during data loading. Defaults to False. TODO: Currently not implemented on a data module level!</p> <code>False</code> <code>allow_substring_file_names</code> <code>bool</code> <p>Allow substrings during sample identification by adding image or label grep to the sample prefixes. If False, treats sample prefixes as full file names. If True and no split file is provided, considers the file stem as prefix, otherwise the full file name. Defaults to True.</p> <code>False</code> <code>dataset_bands</code> <code>dict[list]</code> <p>Bands present in the dataset, provided in a dictionary with modalities as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset of all modalities. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>dict[list]</code> <p>Bands that should be output by the dataset as named by dataset_bands, provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.</p> <code>None</code> <code>class_names</code> <code>list[str]</code> <p>Names of the classes. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>dict[float]</code> <p>Factor to multiply data values by, provided as a dictionary with modalities as keys. Can be subset of all modalities. Defaults to None.</p> <code>1.0</code> <code>transform</code> <code>Compose | dict | None</code> <p>Albumentations transform to be applied to all image modalities (transformation are shared between image modalities, e.g., similar crop or rotation). Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. The transform is not applied to non-image data, which is only converted to tensors if possible. If dict, can include multiple transforms per modality which are applied separately (no shared parameters between modalities). Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input data with this value. If None, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>float | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Only works with image modalities. Is only applied to modalities with defined dataset_bands. Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>channel_position</code> <code>int</code> <p>Position of the channel dimension in the image modalities. Defaults to -3.</p> <code>-3</code> <code>concat_bands</code> <code>bool</code> <p>Concatenate all image modalities along the band dimension into a single \"image\", so that it can be processed by single-modal models. Concatenate in the order of provided modalities. Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    num_classes: int,\n    label_data_root: Path,\n    image_grep: str | None = \"*\",\n    label_grep: str | None = \"*\",\n    split: Path | None = None,\n    image_modalities: list[str] | None = None,\n    rgb_modality: str | None = None,\n    rgb_indices: list[int] | None = None,\n    allow_missing_modalities: bool = False,\n    allow_substring_file_names: bool = False,\n    dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    class_names: list[str] | None = None,\n    constant_scale: dict[float] = 1.0,\n    transform: A.Compose | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = None,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n    channel_position: int = -3,\n    concat_bands: bool = False,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n            data, with modalities as keys.\n        num_classes (int): Number of classes.\n        label_data_root (Path): Path to data root directory with labels or csv/parquet files with labels.\n        image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n            images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        label_grep (str, optional): Regular expression appended to label_data_root to find labels files.\n            Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        split (Path, optional): Path to file containing samples prefixes to be used for this split.\n            The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n            sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n            files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n            If not specified, search samples based on files in data_root. Defaults to None.\n        image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n            The difference between all modalities and image_modalities are non-image modalities which are treated\n            differently during the transforms and are not modified but only converted into a tensor if possible.\n        rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n            TODO: Currently not implemented on a data module level!\n        allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n            image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n            If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n            Defaults to True.\n        dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n            as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n            that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n            of all modalities. Defaults to None.\n        output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n            provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n        class_names (list[str], optional): Names of the classes. Defaults to None.\n        constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n            keys. Can be subset of all modalities. Defaults to None.\n        transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n            modalities (transformation are shared between image modalities, e.g., similar crop or rotation).\n            Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization.\n            Not supported for multi-temporal data. The transform is not applied to non-image data, which is only\n            converted to tensors if possible. If dict, can include multiple transforms per modality which are\n            applied separately (no shared parameters between modalities).\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input data with this value.\n            If None, does no replacement. Defaults to None.\n        no_label_replace (float | None): Replace nan values in label with this value.\n            If none, does no replacement. Defaults to -1.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n        channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n        concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n            that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n            Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n    \"\"\"\n    assert label_data_root is not None, \"label_data_root must be specified for scalar tasks.\"\n\n    super().__init__(\n        data_root,\n        label_data_root=label_data_root,\n        image_grep=image_grep,\n        label_grep=label_grep,\n        split=split,\n        image_modalities=image_modalities,\n        rgb_modality=rgb_modality,\n        rgb_indices=rgb_indices,\n        allow_missing_modalities=allow_missing_modalities,\n        allow_substring_file_names=allow_substring_file_names,\n        dataset_bands=dataset_bands,\n        output_bands=output_bands,\n        constant_scale=constant_scale,\n        transform=transform,\n        no_data_replace=no_data_replace,\n        no_label_replace=no_label_replace,\n        expand_temporal_dimension=expand_temporal_dimension,\n        reduce_zero_label=reduce_zero_label,\n        channel_position=channel_position,\n        scalar_label=True,\n        concat_bands=concat_bands,\n        *args,\n        **kwargs,\n    )\n\n    self.num_classes = num_classes\n    self.class_names = class_names\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalScalarDataset.plot","title":"<code>plot(sample, suptitle=None, show_axes=False)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <code>show_axes</code> <code>bool | None</code> <p>whether to show axes or not</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> <p>.. versionadded:: 0.2</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def plot(\n    self, sample: dict[str, torch.Tensor], suptitle: str | None = None, show_axes: bool | None = False\n) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__`\n        suptitle (str|None): optional string to use as a suptitle\n        show_axes (bool|None): whether to show axes or not\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n\n    .. versionadded:: 0.2\n    \"\"\"\n\n    # TODO: Check plotting code for classification tasks and add it to generic classification dataset as well\n    raise NotImplementedError\n\n    image = sample[\"image\"]\n    if isinstance(image, dict):\n        image = image[self.rgb_modality]\n    if isinstance(image, torch.Tensor):\n        image = image.numpy()\n    image = image.take(self.rgb_indices, axis=0)\n    image = np.transpose(image, (1, 2, 0))\n    image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n    image = np.clip(image, 0, 1)\n\n    label_mask = sample[\"mask\"]\n    if isinstance(label_mask, torch.Tensor):\n        label_mask = label_mask.numpy()\n\n    showing_predictions = \"prediction\" in sample\n    if showing_predictions:\n        prediction_mask = sample[\"prediction\"]\n        if isinstance(prediction_mask, torch.Tensor):\n            prediction_mask = prediction_mask.numpy()\n\n    return self._plot_sample(\n        image,\n        label_mask,\n        prediction=prediction_mask if showing_predictions else None,\n        suptitle=suptitle,\n        show_axes=show_axes,\n    )\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalSegmentationDataset","title":"<code>GenericMultimodalSegmentationDataset</code>","text":"<p>               Bases: <code>GenericMultimodalDataset</code></p> <p>GenericNonGeoSegmentationDataset</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>class GenericMultimodalSegmentationDataset(GenericMultimodalDataset):\n    \"\"\"GenericNonGeoSegmentationDataset\"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        num_classes: int,\n        label_data_root: Path,\n        image_grep: str | None = \"*\",\n        label_grep: str | None = \"*\",\n        split: Path | None = None,\n        image_modalities: list[str] | None = None,\n        rgb_modality: str | None = None,\n        rgb_indices: list[str] | None = None,\n        allow_missing_modalities: bool = False,\n        allow_substring_file_names: bool = False,\n        dataset_bands: dict[list] | None = None,\n        output_bands: dict[list] | None = None,\n        class_names: list[str] | None = None,\n        constant_scale: dict[float] = 1.0,\n        transform: A.Compose | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = -1,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n        channel_position: int = -3,\n        concat_bands: bool = False,\n        *args,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n                data, with modalities as keys.\n            num_classes (int): Number of classes.\n            label_data_root (Path): Path to data root directory with mask files.\n            image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n                images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            label_grep (str, optional): Regular expression appended to label_data_root to find mask files.\n                Defaults to \"*\". Ignored when allow_substring_file_names is False.\n            split (Path, optional): Path to file containing samples prefixes to be used for this split.\n                The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n                sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n                files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n                If not specified, search samples based on files in data_root. Defaults to None.\n            image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n                The difference between all modalities and image_modalities are non-image modalities which are treated\n                differently during the transforms and are not modified but only converted into a tensor if possible.\n            rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n                TODO: Currently not implemented on a data module level!\n            allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n                image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n                If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n                Defaults to True.\n            dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n                as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n                that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n                of all modalities. Defaults to None.\n            output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n                provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n            class_names (list[str], optional): Names of the classes. Defaults to None.\n            constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n                keys. Can be subset of all modalities. Defaults to None.\n            transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n                modalities (transformation are shared between image modalities, e.g., similar crop or rotation).\n                Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization.\n                Not supported for multi-temporal data. The transform is not applied to non-image data, which is only\n                converted to tensors if possible. If dict, can include multiple transforms per modality which are\n                applied separately (no shared parameters between modalities).\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input data with this value.\n                If None, does no replacement. Defaults to None.\n            no_label_replace (float | None): Replace nan values in label with this value.\n                If none, does no replacement. Defaults to -1.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n            channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n            concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n                that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n                Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n        \"\"\"\n        assert label_data_root is not None, \"label_data_root must be specified for segmentation tasks.\"\n\n        super().__init__(\n            data_root,\n            label_data_root=label_data_root,\n            image_grep=image_grep,\n            label_grep=label_grep,\n            split=split,\n            image_modalities=image_modalities,\n            rgb_modality=rgb_modality,\n            rgb_indices=rgb_indices,\n            allow_missing_modalities=allow_missing_modalities,\n            allow_substring_file_names=allow_substring_file_names,\n            dataset_bands=dataset_bands,\n            output_bands=output_bands,\n            constant_scale=constant_scale,\n            transform=transform,\n            no_data_replace=no_data_replace,\n            no_label_replace=no_label_replace,\n            expand_temporal_dimension=expand_temporal_dimension,\n            reduce_zero_label=reduce_zero_label,\n            channel_position=channel_position,\n            concat_bands=concat_bands,\n            *args,\n            **kwargs,\n        )\n        self.num_classes = num_classes\n        self.class_names = class_names\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        item = super().__getitem__(index)\n        item[\"mask\"] = item[\"mask\"].long()\n        return item\n\n    def plot(\n        self, sample: dict[str, torch.Tensor], suptitle: str | None = None, show_axes: bool | None = False\n    ) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            suptitle: optional string to use as a suptitle\n            show_axes: whether to show axes or not\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n\n        .. versionadded:: 0.2\n        \"\"\"\n        image = sample[\"image\"]\n        if isinstance(image, dict):\n            image = image[self.rgb_modality]\n        if isinstance(image, torch.Tensor):\n            image = image.numpy()\n        image = image.take(self.rgb_indices, axis=0)\n        image = np.transpose(image, (1, 2, 0))\n        image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n        image = np.clip(image, 0, 1)\n\n        label_mask = sample[\"mask\"]\n        if isinstance(label_mask, torch.Tensor):\n            label_mask = label_mask.numpy()\n\n        showing_predictions = \"prediction\" in sample\n        if showing_predictions:\n            prediction_mask = sample[\"prediction\"]\n            if isinstance(prediction_mask, torch.Tensor):\n                prediction_mask = prediction_mask.numpy()\n\n        return self._plot_sample(\n            image,\n            label_mask,\n            self.num_classes,\n            prediction=prediction_mask if showing_predictions else None,\n            suptitle=suptitle,\n            class_names=self.class_names,\n            show_axes=show_axes,\n        )\n\n    @staticmethod\n    def _plot_sample(image, label, num_classes, prediction=None, suptitle=None, class_names=None, show_axes=False):\n        num_images = 5 if prediction is not None else 4\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 10), layout=\"compressed\")\n        axes_visibility = \"on\" if show_axes else \"off\"\n\n        # for legend\n        ax[0].axis(\"off\")\n\n        norm = mpl.colors.Normalize(vmin=0, vmax=num_classes - 1)\n        ax[1].axis(axes_visibility)\n        ax[1].title.set_text(\"Image\")\n        ax[1].imshow(image)\n\n        ax[2].axis(axes_visibility)\n        ax[2].title.set_text(\"Ground Truth Mask\")\n        ax[2].imshow(label, cmap=\"jet\", norm=norm)\n\n        ax[3].axis(axes_visibility)\n        ax[3].title.set_text(\"GT Mask on Image\")\n        ax[3].imshow(image)\n        ax[3].imshow(label, cmap=\"jet\", alpha=0.3, norm=norm)\n\n        if prediction is not None:\n            ax[4].axis(axes_visibility)\n            ax[4].title.set_text(\"Predicted Mask\")\n            ax[4].imshow(prediction, cmap=\"jet\", norm=norm)\n\n        cmap = plt.get_cmap(\"jet\")\n        legend_data = []\n        for i, _ in enumerate(range(num_classes)):\n            class_name = class_names[i] if class_names else str(i)\n            data = [i, cmap(norm(i)), class_name]\n            legend_data.append(data)\n        handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n        labels = [n for k, c, n in legend_data]\n        ax[0].legend(handles, labels, loc=\"center\")\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n        return fig\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalSegmentationDataset.__init__","title":"<code>__init__(data_root, num_classes, label_data_root, image_grep='*', label_grep='*', split=None, image_modalities=None, rgb_modality=None, rgb_indices=None, allow_missing_modalities=False, allow_substring_file_names=False, dataset_bands=None, output_bands=None, class_names=None, constant_scale=1.0, transform=None, no_data_replace=None, no_label_replace=-1, expand_temporal_dimension=False, reduce_zero_label=False, channel_position=-3, concat_bands=False, *args, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>dict[Path]</code> <p>Dictionary of paths to data root directory or csv/parquet files with image-level data, with modalities as keys.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes.</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with mask files.</p> required <code>image_grep</code> <code>dict[str]</code> <p>Dictionary with regular expression appended to data_root to find input images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to label_data_root to find mask files. Defaults to \"*\". Ignored when allow_substring_file_names is False.</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing samples prefixes to be used for this split. The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise, files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]). If not specified, search samples based on files in data_root. Defaults to None.</p> <code>None</code> <code>image_modalities(list[str],</code> <code>optional</code> <p>List of pixel-level raster modalities. Defaults to data_root.keys(). The difference between all modalities and image_modalities are non-image modalities which are treated differently during the transforms and are not modified but only converted into a tensor if possible.</p> required <code>rgb_modality</code> <code>str</code> <p>Modality used for RGB plots. Defaults to first modality in data_root.keys().</p> <code>None</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>allow_missing_modalities</code> <code>bool</code> <p>Allow missing modalities during data loading. Defaults to False. TODO: Currently not implemented on a data module level!</p> <code>False</code> <code>allow_substring_file_names</code> <code>bool</code> <p>Allow substrings during sample identification by adding image or label grep to the sample prefixes. If False, treats sample prefixes as full file names. If True and no split file is provided, considers the file stem as prefix, otherwise the full file name. Defaults to True.</p> <code>False</code> <code>dataset_bands</code> <code>dict[list]</code> <p>Bands present in the dataset, provided in a dictionary with modalities as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset of all modalities. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>dict[list]</code> <p>Bands that should be output by the dataset as named by dataset_bands, provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.</p> <code>None</code> <code>class_names</code> <code>list[str]</code> <p>Names of the classes. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>dict[float]</code> <p>Factor to multiply data values by, provided as a dictionary with modalities as keys. Can be subset of all modalities. Defaults to None.</p> <code>1.0</code> <code>transform</code> <code>Compose | dict | None</code> <p>Albumentations transform to be applied to all image modalities (transformation are shared between image modalities, e.g., similar crop or rotation). Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. The transform is not applied to non-image data, which is only converted to tensors if possible. If dict, can include multiple transforms per modality which are applied separately (no shared parameters between modalities). Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input data with this value. If None, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>float | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>-1</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Only works with image modalities. Is only applied to modalities with defined dataset_bands. Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>channel_position</code> <code>int</code> <p>Position of the channel dimension in the image modalities. Defaults to -3.</p> <code>-3</code> <code>concat_bands</code> <code>bool</code> <p>Concatenate all image modalities along the band dimension into a single \"image\", so that it can be processed by single-modal models. Concatenate in the order of provided modalities. Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    num_classes: int,\n    label_data_root: Path,\n    image_grep: str | None = \"*\",\n    label_grep: str | None = \"*\",\n    split: Path | None = None,\n    image_modalities: list[str] | None = None,\n    rgb_modality: str | None = None,\n    rgb_indices: list[str] | None = None,\n    allow_missing_modalities: bool = False,\n    allow_substring_file_names: bool = False,\n    dataset_bands: dict[list] | None = None,\n    output_bands: dict[list] | None = None,\n    class_names: list[str] | None = None,\n    constant_scale: dict[float] = 1.0,\n    transform: A.Compose | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = -1,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n    channel_position: int = -3,\n    concat_bands: bool = False,\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (dict[Path]): Dictionary of paths to data root directory or csv/parquet files with image-level\n            data, with modalities as keys.\n        num_classes (int): Number of classes.\n        label_data_root (Path): Path to data root directory with mask files.\n        image_grep (dict[str], optional): Dictionary with regular expression appended to data_root to find input\n            images, with modalities as keys. Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        label_grep (str, optional): Regular expression appended to label_data_root to find mask files.\n            Defaults to \"*\". Ignored when allow_substring_file_names is False.\n        split (Path, optional): Path to file containing samples prefixes to be used for this split.\n            The file can be a csv/parquet file with the prefixes in the index or a txt file with new-line separated\n            sample prefixes. File names must be exact matches if allow_substring_file_names is False. Otherwise,\n            files are searched using glob with the form Path(data_root).glob(prefix + [image or label grep]).\n            If not specified, search samples based on files in data_root. Defaults to None.\n        image_modalities(list[str], optional): List of pixel-level raster modalities. Defaults to data_root.keys().\n            The difference between all modalities and image_modalities are non-image modalities which are treated\n            differently during the transforms and are not modified but only converted into a tensor if possible.\n        rgb_modality (str, optional): Modality used for RGB plots. Defaults to first modality in data_root.keys().\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        allow_missing_modalities (bool, optional): Allow missing modalities during data loading. Defaults to False.\n            TODO: Currently not implemented on a data module level!\n        allow_substring_file_names (bool, optional): Allow substrings during sample identification by adding\n            image or label grep to the sample prefixes. If False, treats sample prefixes as full file names.\n            If True and no split file is provided, considers the file stem as prefix, otherwise the full file name.\n            Defaults to True.\n        dataset_bands (dict[list], optional): Bands present in the dataset, provided in a dictionary with modalities\n            as keys. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so\n            that they can then be referred to by output_bands. Needs to be superset of output_bands. Can be a subset\n            of all modalities. Defaults to None.\n        output_bands (dict[list], optional): Bands that should be output by the dataset as named by dataset_bands,\n            provided as a dictionary with modality keys. Can be subset of all modalities. Defaults to None.\n        class_names (list[str], optional): Names of the classes. Defaults to None.\n        constant_scale (dict[float]): Factor to multiply data values by, provided as a dictionary with modalities as\n            keys. Can be subset of all modalities. Defaults to None.\n        transform (Albumentations.Compose | dict | None): Albumentations transform to be applied to all image\n            modalities (transformation are shared between image modalities, e.g., similar crop or rotation).\n            Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization.\n            Not supported for multi-temporal data. The transform is not applied to non-image data, which is only\n            converted to tensors if possible. If dict, can include multiple transforms per modality which are\n            applied separately (no shared parameters between modalities).\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input data with this value.\n            If None, does no replacement. Defaults to None.\n        no_label_replace (float | None): Replace nan values in label with this value.\n            If none, does no replacement. Defaults to -1.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Only works with image modalities. Is only applied to modalities with defined dataset_bands.\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n        channel_position (int): Position of the channel dimension in the image modalities. Defaults to -3.\n        concat_bands (bool): Concatenate all image modalities along the band dimension into a single \"image\", so\n            that it can be processed by single-modal models. Concatenate in the order of provided modalities.\n            Works with image modalities only. Does not work with allow_missing_modalities. Defaults to False.\n    \"\"\"\n    assert label_data_root is not None, \"label_data_root must be specified for segmentation tasks.\"\n\n    super().__init__(\n        data_root,\n        label_data_root=label_data_root,\n        image_grep=image_grep,\n        label_grep=label_grep,\n        split=split,\n        image_modalities=image_modalities,\n        rgb_modality=rgb_modality,\n        rgb_indices=rgb_indices,\n        allow_missing_modalities=allow_missing_modalities,\n        allow_substring_file_names=allow_substring_file_names,\n        dataset_bands=dataset_bands,\n        output_bands=output_bands,\n        constant_scale=constant_scale,\n        transform=transform,\n        no_data_replace=no_data_replace,\n        no_label_replace=no_label_replace,\n        expand_temporal_dimension=expand_temporal_dimension,\n        reduce_zero_label=reduce_zero_label,\n        channel_position=channel_position,\n        concat_bands=concat_bands,\n        *args,\n        **kwargs,\n    )\n    self.num_classes = num_classes\n    self.class_names = class_names\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_multimodal_dataset.GenericMultimodalSegmentationDataset.plot","title":"<code>plot(sample, suptitle=None, show_axes=False)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <code>show_axes</code> <code>bool | None</code> <p>whether to show axes or not</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> <p>.. versionadded:: 0.2</p> Source code in <code>terratorch/datasets/generic_multimodal_dataset.py</code> <pre><code>def plot(\n    self, sample: dict[str, torch.Tensor], suptitle: str | None = None, show_axes: bool | None = False\n) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        suptitle: optional string to use as a suptitle\n        show_axes: whether to show axes or not\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n\n    .. versionadded:: 0.2\n    \"\"\"\n    image = sample[\"image\"]\n    if isinstance(image, dict):\n        image = image[self.rgb_modality]\n    if isinstance(image, torch.Tensor):\n        image = image.numpy()\n    image = image.take(self.rgb_indices, axis=0)\n    image = np.transpose(image, (1, 2, 0))\n    image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n    image = np.clip(image, 0, 1)\n\n    label_mask = sample[\"mask\"]\n    if isinstance(label_mask, torch.Tensor):\n        label_mask = label_mask.numpy()\n\n    showing_predictions = \"prediction\" in sample\n    if showing_predictions:\n        prediction_mask = sample[\"prediction\"]\n        if isinstance(prediction_mask, torch.Tensor):\n            prediction_mask = prediction_mask.numpy()\n\n    return self._plot_sample(\n        image,\n        label_mask,\n        self.num_classes,\n        prediction=prediction_mask if showing_predictions else None,\n        suptitle=suptitle,\n        class_names=self.class_names,\n        show_axes=show_axes,\n    )\n</code></pre>"},{"location":"glossary/","title":"Glossary of terms used in this Documentation and in the Geospatial AI area","text":""},{"location":"glossary/#encoder","title":"Encoder","text":"<p>The neural network used to map between the inputs and the intermdiary stage (usually referred as embedding or sometimes as latent space) of the forward step. The encoder is also frequently called backbone and, for  finetuning tasks, it is usually the part of the model which is not updated/trained. </p>"},{"location":"glossary/#decoder","title":"Decoder","text":"<p>The neural network employed to map between the intermediary stage (embedding/latent space) and the target output. For finetuning tasks, the decoder is the most essential part, since it is trained to map the embedding produced by a previoulsy trained encoder to a new task. </p>"},{"location":"glossary/#head","title":"Head","text":"<p>A network, usually very small when compared to the encoder and decoder, which is used as final step to adapt the decoder output to a specific task, for example, by applying a determined activation to it. </p>"},{"location":"glossary/#neck","title":"Neck","text":"<p>Necks are operations placed between the encoder and the decoder stages aimed at adjusting possible discrepancies, as incompatible shapes, or applying some specific transform, as a normalization required for the task being executed. </p>"},{"location":"glossary/#factory","title":"Factory","text":"<p>A Factory is a class which organizes the instantiation of a complete model, as a backbone-neck-decoder-head architecture. A class is intended to receive lists and dictionaries containing the required arguments used to build the model and returns a new instance already ready to be used. </p>"},{"location":"heads/","title":"Heads","text":""},{"location":"heads/#terratorch.models.heads.regression_head","title":"<code>terratorch.models.heads.regression_head</code>","text":""},{"location":"heads/#terratorch.models.heads.regression_head.RegressionHead","title":"<code>RegressionHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Regression head</p>"},{"location":"heads/#terratorch.models.heads.regression_head.RegressionHead.__init__","title":"<code>__init__(in_channels, final_act=None, learned_upscale_layers=0, channel_list=None, batch_norm=True, dropout=0)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>final_act</code> <code>Module | None</code> <p>Final activation to be applied. Defaults to None.</p> <code>None</code> <code>learned_upscale_layers</code> <code>int</code> <p>Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0.</p> <code>0</code> <code>channel_list</code> <code>list[int] | None</code> <p>List with number of channels for each Conv layer to be created. Defaults to None.</p> <code>None</code> <code>batch_norm</code> <code>bool</code> <p>Whether to apply batch norm. Defaults to True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout value to apply. Defaults to 0.</p> <code>0</code>"},{"location":"heads/#terratorch.models.heads.segmentation_head","title":"<code>terratorch.models.heads.segmentation_head</code>","text":""},{"location":"heads/#terratorch.models.heads.segmentation_head.SegmentationHead","title":"<code>SegmentationHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Segmentation head</p>"},{"location":"heads/#terratorch.models.heads.segmentation_head.SegmentationHead.__init__","title":"<code>__init__(in_channels, num_classes, channel_list=None, dropout=0)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes</p> required <code>channel_list</code> <code>list[int] | None</code> <p>List with number of channels for each Conv layer to be created. Defaults to None.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout value to apply. Defaults to 0.</p> <code>0</code>"},{"location":"heads/#terratorch.models.heads.classification_head","title":"<code>terratorch.models.heads.classification_head</code>","text":""},{"location":"heads/#terratorch.models.heads.classification_head.ClassificationHead","title":"<code>ClassificationHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Classification head</p>"},{"location":"heads/#terratorch.models.heads.classification_head.ClassificationHead.__init__","title":"<code>__init__(in_dim, num_classes, dim_list=None, dropout=0, linear_after_pool=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input dimensionality</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes</p> required <code>dim_list</code> <code>list[int] | None</code> <p>List with number of dimensions for each Linear layer to be created. Defaults to None.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout value to apply. Defaults to 0.</p> <code>0</code> <code>linear_after_pool</code> <code>bool</code> <p>Apply pooling first, then apply the linear layer. Defaults to False</p> <code>False</code>"},{"location":"license/","title":"Project License","text":""},{"location":"license/#apache-license-20","title":"Apache License 2.0","text":"<p>All code in this repository is licensed under the Apache License, Version 2.0.</p> <p>You may obtain a copy of the license at:</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"license/#mit-licensed-code-blocks","title":"MIT-Licensed Code Blocks","text":"<p>Certain files in this repository contain code that is licensed under the MIT License. These files are explicitly listed in <code>MIT_FILES.txt</code>.</p> <p>The MIT License is as follows:</p> <p>MIT License</p> <p>Copyright (c) YEAR</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"loss/","title":"Loss","text":""},{"location":"loss/#terratorch.tasks.loss_handler","title":"<code>terratorch.tasks.loss_handler</code>","text":""},{"location":"loss/#terratorch.tasks.loss_handler.LossHandler","title":"<code>LossHandler</code>","text":"<p>Class to help handle the computation and logging of loss</p>"},{"location":"loss/#terratorch.tasks.loss_handler.LossHandler.__init__","title":"<code>__init__(loss_prefix)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>loss_prefix</code> <code>str</code> <p>Prefix to be prepended to all the metrics (e.g. training).</p> required"},{"location":"loss/#terratorch.tasks.loss_handler.LossHandler.compute_loss","title":"<code>compute_loss(model_output, ground_truth, criterion, aux_loss_weights)</code>","text":"<p>Compute the loss for the mean decode head as well as other heads</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>ModelOutput</code> <p>Output from the model</p> required <code>ground_truth</code> <code>Tensor</code> <p>Tensor with labels</p> required <code>criterion</code> <code>Callable</code> <p>Loss function to be applied</p> required <code>aux_loss_weights</code> <code>Union[dict[str, float], None]</code> <p>Dictionary of names of model auxiliary heads and their weights</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If the keys in aux_loss_weights and the model output do not match, will raise an exception.</p> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>dict[str, Tensor]: Dictionary of computed losses. Total loss is returned under the key \"loss\". If there are auxiliary heads, the main decode head is returned under the key \"decode_head\". All other heads are returned with the same key as their name.</p>"},{"location":"loss/#terratorch.tasks.loss_handler.LossHandler.log_loss","title":"<code>log_loss(log_function, loss_dict=None, batch_size=None)</code>","text":"<p>Log the loss. If auxiliary heads exist, log the full loss suffix \"loss\", and then all other losses.</p> <p>Parameters:</p> Name Type Description Default <code>log_function</code> <code>Callable</code> <p>description</p> required <code>loss_dict</code> <code>dict[str, Tensor]</code> <p>description. Defaults to None.</p> <code>None</code>"},{"location":"meta_models/","title":"Meta models","text":""},{"location":"meta_models/#terratorch.models.pixel_wise_model.PixelWiseModel","title":"<code>terratorch.models.pixel_wise_model.PixelWiseModel</code>","text":"<p>               Bases: <code>Model</code>, <code>SegmentationModel</code></p> <p>Model that encapsulates encoder and decoder and heads Expects decoder to have a \"forward_features\" method, an embed_dims property and optionally a \"prepare_features_for_image_model\" method.</p> Source code in <code>terratorch/models/pixel_wise_model.py</code> <pre><code>class PixelWiseModel(Model, SegmentationModel):\n    \"\"\"Model that encapsulates encoder and decoder and heads\n    Expects decoder to have a \"forward_features\" method, an embed_dims property\n    and optionally a \"prepare_features_for_image_model\" method.\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        encoder: nn.Module,\n        decoder: nn.Module,\n        head_kwargs: dict,\n        patch_size: int = None, \n        padding: str = None,\n        decoder_includes_head: bool = False,\n        auxiliary_heads: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None = None,\n        neck: nn.Module | None = None,\n        rescale: bool = True,  # noqa: FBT002, FBT001\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            task (str): Task to be performed. One of segmentation or regression.\n            encoder (nn.Module): Encoder to be used\n            decoder (nn.Module): Decoder to be used\n            head_kwargs (dict): Arguments to be passed at instantiation of the head.\n            decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.\n            auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of\n                AuxiliaryHeads with heads to be instantiated. Defaults to None.\n            neck (nn.Module | None): Module applied between backbone and decoder.\n                Defaults to None, which applies the identity.\n            rescale (bool, optional): Rescale the output of the model if it has a different size than the ground truth.\n                Uses bilinear interpolation. Defaults to True.\n        \"\"\"\n        super().__init__()\n\n        self.task = task\n        self.encoder = encoder\n        self.decoder = decoder\n        self.head = (\n            self._get_head(task, decoder.out_channels, head_kwargs) if not decoder_includes_head else nn.Identity()\n        )\n\n        if auxiliary_heads is not None:\n            aux_heads = {}\n            for aux_head_to_be_instantiated in auxiliary_heads:\n                aux_head: nn.Module = self._get_head(\n                    task, aux_head_to_be_instantiated.decoder.out_channels, head_kwargs\n                ) if not aux_head_to_be_instantiated.decoder_includes_head else nn.Identity()\n                aux_head = nn.Sequential(aux_head_to_be_instantiated.decoder, aux_head)\n                aux_heads[aux_head_to_be_instantiated.name] = aux_head\n        else:\n            aux_heads = {}\n        self.aux_heads = nn.ModuleDict(aux_heads)\n\n        self.neck = neck\n        self.rescale = rescale\n        self.patch_size = patch_size\n        self.padding = padding\n\n    def freeze_encoder(self):\n        if hasattr(self.encoder, \"freeze\"):\n            self.encoder.freeze()\n        else:\n            freeze_module(self.encoder)\n\n    def freeze_decoder(self):\n        freeze_module(self.decoder)\n\n    def freeze_head(self):\n        freeze_module(self.head)\n\n    @staticmethod\n    def _check_for_single_channel_and_squeeze(x):\n        if x.shape[1] == 1:\n            x = x.squeeze(1)\n        return x\n\n    def forward(self, x: torch.Tensor, **kwargs) -&gt; ModelOutput:\n        \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\"\n\n        def _get_size(x):\n            if isinstance(x, torch.Tensor):\n                return x.shape[-2:]\n            elif isinstance(x, dict):\n                # Multimodal input in passed as dict (Assuming first modality to be an image)\n                return list(x.values())[0].shape[-2:]\n            elif hasattr(kwargs, 'image_size'):\n                return kwargs['image_size']\n            else:\n                ValueError('Could not infer image shape.')\n\n        image_size = _get_size(x)\n        if isinstance(x, torch.Tensor) and self.patch_size:\n            # Only works for single image modalities\n            x = pad_images(x, self.patch_size, self.padding)\n        input_size = _get_size(x)\n\n        features = self.encoder(x, **kwargs)\n\n        # only for backwards compatibility with pre-neck times.\n        if self.neck:\n            prepare = self.neck\n        else:\n            # for backwards compatibility, if this is defined in the encoder, use it\n            prepare = getattr(self.encoder, \"prepare_features_for_image_model\", lambda x: x)\n\n        features = prepare(features)\n\n        decoder_output = self.decoder([f.clone() for f in features])\n        mask = self.head(decoder_output)\n        if self.rescale and mask.shape[-2:] != input_size:\n            mask = F.interpolate(mask, size=input_size, mode=\"bilinear\")\n        mask = self._check_for_single_channel_and_squeeze(mask)\n        mask = mask[..., :image_size[0], :image_size[1]]\n\n        aux_outputs = {}\n        for name, decoder in self.aux_heads.items():\n            aux_output = decoder([f.clone() for f in features])\n            if self.rescale and aux_output.shape[-2:] != input_size:\n                aux_output = F.interpolate(aux_output, size=input_size, mode=\"bilinear\")\n            aux_output = self._check_for_single_channel_and_squeeze(aux_output)\n            aux_output = aux_output[..., :image_size[0], :image_size[1]]\n            aux_outputs[name] = aux_output\n\n\n        return ModelOutput(output=mask, auxiliary_heads=aux_outputs)\n\n    def _get_head(self, task: str, input_embed_dim: int, head_kwargs):\n        if task == \"segmentation\":\n            if \"num_classes\" not in head_kwargs:\n                msg = \"num_classes must be defined for segmentation task\"\n                raise Exception(msg)\n            return SegmentationHead(input_embed_dim, **head_kwargs)\n        if task == \"regression\":\n            return RegressionHead(input_embed_dim, **head_kwargs)\n        msg = \"Task must be one of segmentation or regression.\"\n        raise Exception(msg)\n</code></pre>"},{"location":"meta_models/#terratorch.models.pixel_wise_model.PixelWiseModel.__init__","title":"<code>__init__(task, encoder, decoder, head_kwargs, patch_size=None, padding=None, decoder_includes_head=False, auxiliary_heads=None, neck=None, rescale=True)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. One of segmentation or regression.</p> required <code>encoder</code> <code>Module</code> <p>Encoder to be used</p> required <code>decoder</code> <code>Module</code> <p>Decoder to be used</p> required <code>head_kwargs</code> <code>dict</code> <p>Arguments to be passed at instantiation of the head.</p> required <code>decoder_includes_head</code> <code>bool</code> <p>Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.</p> <code>False</code> <code>auxiliary_heads</code> <code>list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None</code> <p>List of AuxiliaryHeads with heads to be instantiated. Defaults to None.</p> <code>None</code> <code>neck</code> <code>Module | None</code> <p>Module applied between backbone and decoder. Defaults to None, which applies the identity.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Rescale the output of the model if it has a different size than the ground truth. Uses bilinear interpolation. Defaults to True.</p> <code>True</code> Source code in <code>terratorch/models/pixel_wise_model.py</code> <pre><code>def __init__(\n    self,\n    task: str,\n    encoder: nn.Module,\n    decoder: nn.Module,\n    head_kwargs: dict,\n    patch_size: int = None, \n    padding: str = None,\n    decoder_includes_head: bool = False,\n    auxiliary_heads: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None = None,\n    neck: nn.Module | None = None,\n    rescale: bool = True,  # noqa: FBT002, FBT001\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        task (str): Task to be performed. One of segmentation or regression.\n        encoder (nn.Module): Encoder to be used\n        decoder (nn.Module): Decoder to be used\n        head_kwargs (dict): Arguments to be passed at instantiation of the head.\n        decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.\n        auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of\n            AuxiliaryHeads with heads to be instantiated. Defaults to None.\n        neck (nn.Module | None): Module applied between backbone and decoder.\n            Defaults to None, which applies the identity.\n        rescale (bool, optional): Rescale the output of the model if it has a different size than the ground truth.\n            Uses bilinear interpolation. Defaults to True.\n    \"\"\"\n    super().__init__()\n\n    self.task = task\n    self.encoder = encoder\n    self.decoder = decoder\n    self.head = (\n        self._get_head(task, decoder.out_channels, head_kwargs) if not decoder_includes_head else nn.Identity()\n    )\n\n    if auxiliary_heads is not None:\n        aux_heads = {}\n        for aux_head_to_be_instantiated in auxiliary_heads:\n            aux_head: nn.Module = self._get_head(\n                task, aux_head_to_be_instantiated.decoder.out_channels, head_kwargs\n            ) if not aux_head_to_be_instantiated.decoder_includes_head else nn.Identity()\n            aux_head = nn.Sequential(aux_head_to_be_instantiated.decoder, aux_head)\n            aux_heads[aux_head_to_be_instantiated.name] = aux_head\n    else:\n        aux_heads = {}\n    self.aux_heads = nn.ModuleDict(aux_heads)\n\n    self.neck = neck\n    self.rescale = rescale\n    self.patch_size = patch_size\n    self.padding = padding\n</code></pre>"},{"location":"meta_models/#terratorch.models.pixel_wise_model.PixelWiseModel.forward","title":"<code>forward(x, **kwargs)</code>","text":"<p>Sequentially pass <code>x</code> through model`s encoder, decoder and heads</p> Source code in <code>terratorch/models/pixel_wise_model.py</code> <pre><code>def forward(self, x: torch.Tensor, **kwargs) -&gt; ModelOutput:\n    \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\"\n\n    def _get_size(x):\n        if isinstance(x, torch.Tensor):\n            return x.shape[-2:]\n        elif isinstance(x, dict):\n            # Multimodal input in passed as dict (Assuming first modality to be an image)\n            return list(x.values())[0].shape[-2:]\n        elif hasattr(kwargs, 'image_size'):\n            return kwargs['image_size']\n        else:\n            ValueError('Could not infer image shape.')\n\n    image_size = _get_size(x)\n    if isinstance(x, torch.Tensor) and self.patch_size:\n        # Only works for single image modalities\n        x = pad_images(x, self.patch_size, self.padding)\n    input_size = _get_size(x)\n\n    features = self.encoder(x, **kwargs)\n\n    # only for backwards compatibility with pre-neck times.\n    if self.neck:\n        prepare = self.neck\n    else:\n        # for backwards compatibility, if this is defined in the encoder, use it\n        prepare = getattr(self.encoder, \"prepare_features_for_image_model\", lambda x: x)\n\n    features = prepare(features)\n\n    decoder_output = self.decoder([f.clone() for f in features])\n    mask = self.head(decoder_output)\n    if self.rescale and mask.shape[-2:] != input_size:\n        mask = F.interpolate(mask, size=input_size, mode=\"bilinear\")\n    mask = self._check_for_single_channel_and_squeeze(mask)\n    mask = mask[..., :image_size[0], :image_size[1]]\n\n    aux_outputs = {}\n    for name, decoder in self.aux_heads.items():\n        aux_output = decoder([f.clone() for f in features])\n        if self.rescale and aux_output.shape[-2:] != input_size:\n            aux_output = F.interpolate(aux_output, size=input_size, mode=\"bilinear\")\n        aux_output = self._check_for_single_channel_and_squeeze(aux_output)\n        aux_output = aux_output[..., :image_size[0], :image_size[1]]\n        aux_outputs[name] = aux_output\n\n\n    return ModelOutput(output=mask, auxiliary_heads=aux_outputs)\n</code></pre>"},{"location":"meta_models/#terratorch.models.scalar_output_model.ScalarOutputModel","title":"<code>terratorch.models.scalar_output_model.ScalarOutputModel</code>","text":"<p>               Bases: <code>Model</code>, <code>SegmentationModel</code></p> <p>Model that encapsulates encoder and decoder and heads for a scalar output Expects decoder to have a \"forward_features\" method, an embed_dims property and optionally a \"prepare_features_for_image_model\" method.</p> Source code in <code>terratorch/models/scalar_output_model.py</code> <pre><code>class ScalarOutputModel(Model, SegmentationModel):\n    \"\"\"Model that encapsulates encoder and decoder and heads for a scalar output\n    Expects decoder to have a \"forward_features\" method, an embed_dims property\n    and optionally a \"prepare_features_for_image_model\" method.\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        encoder: nn.Module,\n        decoder: nn.Module,\n        head_kwargs: dict,\n        patch_size: int = None,\n        padding: str = None,\n        decoder_includes_head: bool = False,\n        auxiliary_heads: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None = None,\n        neck: nn.Module | None = None,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            task (str): Task to be performed. Must be \"classification\".\n            encoder (nn.Module): Encoder to be used\n            decoder (nn.Module): Decoder to be used\n            head_kwargs (dict): Arguments to be passed at instantiation of the head.\n            decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.\n            auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of\n                AuxiliaryHeads with heads to be instantiated. Defaults to None.\n            neck (nn.Module | None): Module applied between backbone and decoder.\n                Defaults to None, which applies the identity.\n        \"\"\"\n        super().__init__()\n        self.task = task\n        self.encoder = encoder\n        self.decoder = decoder\n        self.head = (\n            self._get_head(task, decoder.out_channels, head_kwargs) if not decoder_includes_head else nn.Identity()\n        )\n\n        if auxiliary_heads is not None:\n            aux_heads = {}\n            for aux_head_to_be_instantiated in auxiliary_heads:\n                aux_head: nn.Module = self._get_head(\n                    task, aux_head_to_be_instantiated.decoder.out_channels, head_kwargs\n                ) if not aux_head_to_be_instantiated.decoder_includes_head else nn.Identity()\n                aux_head = nn.Sequential(aux_head_to_be_instantiated.decoder, aux_head)\n                aux_heads[aux_head_to_be_instantiated.name] = aux_head\n        else:\n            aux_heads = {}\n        self.aux_heads = nn.ModuleDict(aux_heads)\n\n        self.neck = neck\n        self.patch_size = patch_size\n        self.padding = padding\n\n    def freeze_encoder(self):\n        freeze_module(self.encoder)\n\n    def freeze_decoder(self):\n        freeze_module(self.decoder)\n\n    def freeze_head(self):\n        freeze_module(self.head)\n\n    def forward(self, x: torch.Tensor, **kwargs) -&gt; ModelOutput:\n        \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\"\n\n        if isinstance(x, torch.Tensor) and self.patch_size:\n            # Only works for single image modalities\n            x = pad_images(x, self.patch_size, self.padding)\n        features = self.encoder(x, **kwargs)\n\n        # only for backwards compatibility with pre-neck times.\n        if self.neck:\n            prepare = self.neck\n        else:\n            # for backwards compatibility, if this is defined in the encoder, use it\n            prepare = getattr(self.encoder, \"prepare_features_for_image_model\", lambda x: x)\n\n        features = prepare(features)\n\n        decoder_output = self.decoder([f.clone() for f in features])\n        mask = self.head(decoder_output)\n\n        aux_outputs = {}\n        for name, decoder in self.aux_heads.items():\n            aux_output = decoder([f.clone() for f in features])\n            aux_outputs[name] = aux_output\n\n        return ModelOutput(output=mask, auxiliary_heads=aux_outputs)\n\n    def _get_head(self, task: str, input_embed_dim: int, head_kwargs: dict):\n        if task == \"classification\":\n            if \"num_classes\" not in head_kwargs:\n                msg = \"num_classes must be defined for classification task\"\n                raise Exception(msg)\n            return ClassificationHead(input_embed_dim, **head_kwargs)\n        msg = \"Task must be classification.\"\n        raise Exception(msg)\n</code></pre>"},{"location":"meta_models/#terratorch.models.scalar_output_model.ScalarOutputModel.__init__","title":"<code>__init__(task, encoder, decoder, head_kwargs, patch_size=None, padding=None, decoder_includes_head=False, auxiliary_heads=None, neck=None)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Must be \"classification\".</p> required <code>encoder</code> <code>Module</code> <p>Encoder to be used</p> required <code>decoder</code> <code>Module</code> <p>Decoder to be used</p> required <code>head_kwargs</code> <code>dict</code> <p>Arguments to be passed at instantiation of the head.</p> required <code>decoder_includes_head</code> <code>bool</code> <p>Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.</p> <code>False</code> <code>auxiliary_heads</code> <code>list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None</code> <p>List of AuxiliaryHeads with heads to be instantiated. Defaults to None.</p> <code>None</code> <code>neck</code> <code>Module | None</code> <p>Module applied between backbone and decoder. Defaults to None, which applies the identity.</p> <code>None</code> Source code in <code>terratorch/models/scalar_output_model.py</code> <pre><code>def __init__(\n    self,\n    task: str,\n    encoder: nn.Module,\n    decoder: nn.Module,\n    head_kwargs: dict,\n    patch_size: int = None,\n    padding: str = None,\n    decoder_includes_head: bool = False,\n    auxiliary_heads: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None = None,\n    neck: nn.Module | None = None,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        task (str): Task to be performed. Must be \"classification\".\n        encoder (nn.Module): Encoder to be used\n        decoder (nn.Module): Decoder to be used\n        head_kwargs (dict): Arguments to be passed at instantiation of the head.\n        decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.\n        auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of\n            AuxiliaryHeads with heads to be instantiated. Defaults to None.\n        neck (nn.Module | None): Module applied between backbone and decoder.\n            Defaults to None, which applies the identity.\n    \"\"\"\n    super().__init__()\n    self.task = task\n    self.encoder = encoder\n    self.decoder = decoder\n    self.head = (\n        self._get_head(task, decoder.out_channels, head_kwargs) if not decoder_includes_head else nn.Identity()\n    )\n\n    if auxiliary_heads is not None:\n        aux_heads = {}\n        for aux_head_to_be_instantiated in auxiliary_heads:\n            aux_head: nn.Module = self._get_head(\n                task, aux_head_to_be_instantiated.decoder.out_channels, head_kwargs\n            ) if not aux_head_to_be_instantiated.decoder_includes_head else nn.Identity()\n            aux_head = nn.Sequential(aux_head_to_be_instantiated.decoder, aux_head)\n            aux_heads[aux_head_to_be_instantiated.name] = aux_head\n    else:\n        aux_heads = {}\n    self.aux_heads = nn.ModuleDict(aux_heads)\n\n    self.neck = neck\n    self.patch_size = patch_size\n    self.padding = padding\n</code></pre>"},{"location":"meta_models/#terratorch.models.scalar_output_model.ScalarOutputModel.forward","title":"<code>forward(x, **kwargs)</code>","text":"<p>Sequentially pass <code>x</code> through model`s encoder, decoder and heads</p> Source code in <code>terratorch/models/scalar_output_model.py</code> <pre><code>def forward(self, x: torch.Tensor, **kwargs) -&gt; ModelOutput:\n    \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\"\n\n    if isinstance(x, torch.Tensor) and self.patch_size:\n        # Only works for single image modalities\n        x = pad_images(x, self.patch_size, self.padding)\n    features = self.encoder(x, **kwargs)\n\n    # only for backwards compatibility with pre-neck times.\n    if self.neck:\n        prepare = self.neck\n    else:\n        # for backwards compatibility, if this is defined in the encoder, use it\n        prepare = getattr(self.encoder, \"prepare_features_for_image_model\", lambda x: x)\n\n    features = prepare(features)\n\n    decoder_output = self.decoder([f.clone() for f in features])\n    mask = self.head(decoder_output)\n\n    aux_outputs = {}\n    for name, decoder in self.aux_heads.items():\n        aux_output = decoder([f.clone() for f in features])\n        aux_outputs[name] = aux_output\n\n    return ModelOutput(output=mask, auxiliary_heads=aux_outputs)\n</code></pre>"},{"location":"model_factories/","title":"Model Factories","text":""},{"location":"model_factories/#terratorch.models.model.ModelFactory","title":"<code>terratorch.models.model.ModelFactory</code>","text":"<p>               Bases: <code>Protocol</code></p>"},{"location":"model_factories/#terratorch.models.encoder_decoder_factory.EncoderDecoderFactory","title":"<code>terratorch.models.encoder_decoder_factory.EncoderDecoderFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p>"},{"location":"model_factories/#terratorch.models.encoder_decoder_factory.EncoderDecoderFactory.build_model","title":"<code>build_model(task, backbone, decoder, backbone_kwargs=None, decoder_kwargs=None, head_kwargs=None, num_classes=None, necks=None, aux_decoders=None, rescale=True, peft_config=None, **kwargs)</code>","text":"<p>Generic model factory that combines an encoder and decoder, together with a head, for a specific task.</p> <p>Further arguments to be passed to the backbone, decoder or head. They should be prefixed with <code>backbone_</code>, <code>decoder_</code> and <code>head_</code> respectively.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Currently supports \"segmentation\", \"regression\" and \"classification\".</p> required <code>backbone</code> <code>(str, Module)</code> <p>Backbone to be used. If a string, will look for such models in the different registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it directly. The backbone should have and <code>out_channels</code> attribute and its <code>forward</code> should return a list[Tensor].</p> required <code>decoder</code> <code>Union[str, Module]</code> <p>Decoder to be used for the segmentation model.     If a string, will look for such decoders in the different     registries supported (internal terratorch registry, smp, ...).     If an nn.Module, we expect it to expose a property <code>decoder.out_channels</code>.     Pixel wise tasks will be concatenated with a Conv2d for the final convolution.     Defaults to \"FCNDecoder\".</p> required <code>backbone_kwargs</code> <code>dict, optional) </code> <p>Arguments to be passed to instantiate the backbone.</p> <code>None</code> <code>decoder_kwargs</code> <code>dict, optional) </code> <p>Arguments to be passed to instantiate the decoder.</p> <code>None</code> <code>head_kwargs</code> <code>dict, optional) </code> <p>Arguments to be passed to the head network. </p> <code>None</code> <code>num_classes</code> <code>int</code> <p>Number of classes. None for regression tasks.</p> <code>None</code> <code>necks</code> <code>list[dict]</code> <p>nn.Modules to be called in succession on encoder features before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry. Expects each one to have a key \"name\" and subsequent keys for arguments, if any. Defaults to None, which applies the identity function.</p> <code>None</code> <code>aux_decoders</code> <code>list[AuxiliaryHead] | None</code> <p>List of AuxiliaryHead decoders to be added to the model. These decoders take the input from the encoder as well.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.</p> <code>True</code> <code>peft_config</code> <code>dict</code> <p>Configuration options for using PEFT. The dictionary should have the following keys:</p> <ul> <li>\"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available here.</li> <li>\"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep.   This should be used when the qkv matrices are merged together in a single linear layer and the PEFT   method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in   Q and V matrices). e.g. If using Prithvi this should be \"qkv\"</li> <li>\"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to PeftConfig</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: Full model with encoder, decoder and head.</p>"},{"location":"model_factories/#terratorch.models.clay_model_factory.ClayModelFactory","title":"<code>terratorch.models.clay_model_factory.ClayModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p>"},{"location":"model_factories/#terratorch.models.clay_model_factory.ClayModelFactory.build_model","title":"<code>build_model(task, backbone, decoder, in_channels, bands=[], num_classes=None, pretrained=True, num_frames=1, prepare_features_for_image_model=None, aux_decoders=None, rescale=True, checkpoint_path=None, **kwargs)</code>","text":"<p>Model factory for Clay models.</p> <p>Further arguments to be passed to the backbone, decoder or head. They should be prefixed with <code>backbone_</code>, <code>decoder_</code> and <code>head_</code> respectively.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Currently supports \"segmentation\" and \"regression\".</p> required <code>backbone</code> <code>(str, Module)</code> <p>Backbone to be used. If string, should be able to be parsed by the specified factory. Defaults to \"prithvi_100\".</p> required <code>decoder</code> <code>Union[str, Module]</code> <p>Decoder to be used for the segmentation model.     If a string, it will be created from a class exposed in decoder.init.py with the same name.     If an nn.Module, we expect it to expose a property <code>decoder.out_channels</code>.     Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes. None for regression tasks.</p> <code>None</code> <code>pretrained</code> <code>Union[bool, Path]</code> <p>Whether to load pretrained weights for the backbone, if available. Defaults to True.</p> <code>True</code> <code>num_frames</code> <code>int</code> <p>Number of timesteps for the model to handle. Defaults to 1.</p> <code>1</code> <code>prepare_features_for_image_model</code> <code>Callable | None</code> <p>Function to be called on encoder features before passing them to the decoder. Defaults to None, which applies the identity function.</p> <code>None</code> <code>aux_decoders</code> <code>list[AuxiliaryHead] | None</code> <p>List of AuxiliaryHead deciders to be added to the model. These decoders take the input from the encoder as well.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> <code>DecoderNotFoundException</code> <p>description</p> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: description</p>"},{"location":"model_factories/#terratorch.models.generic_model_factory.GenericModelFactory","title":"<code>terratorch.models.generic_model_factory.GenericModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p>"},{"location":"model_factories/#terratorch.models.generic_model_factory.GenericModelFactory.build_model","title":"<code>build_model(backbone=None, in_channels=6, pretrained=True, **kwargs)</code>","text":"<p>Factory to create models from any custom module.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The name for the model class.</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> <code>6</code> <code>pretrained(str</code> <code>| bool</code> <p>Which weights to use for the backbone. If true, will use \"imagenet\". If false or None, random weights. Defaults to True.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>A wrapped generic model.</p>"},{"location":"model_factories/#terratorch.models.generic_unet_model_factory.GenericUnetModelFactory","title":"<code>terratorch.models.generic_unet_model_factory.GenericUnetModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p>"},{"location":"model_factories/#terratorch.models.generic_unet_model_factory.GenericUnetModelFactory.build_model","title":"<code>build_model(task='segmentation', backbone=None, decoder=None, dilations=(1, 6, 12, 18), in_channels=6, pretrained=True, num_classes=1, regression_relu=False, **kwargs)</code>","text":"<p>Factory to create model based on SMP.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Must be \"segmentation\".</p> <code>'segmentation'</code> <code>model</code> <code>str</code> <p>Decoder architecture. Currently only supports \"unet\".</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> <code>6</code> <code>pretrained(str</code> <code>| bool</code> <p>Which weights to use for the backbone. If true, will use \"imagenet\". If false or None, random weights. Defaults to True.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>SMP model wrapped in SMPModelWrapper.</p>"},{"location":"model_factories/#terratorch.models.prithvi_model_factory.PrithviModelFactory","title":"<code>terratorch.models.prithvi_model_factory.PrithviModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p>"},{"location":"model_factories/#terratorch.models.prithvi_model_factory.PrithviModelFactory.build_model","title":"<code>build_model(task, backbone, decoder, bands, in_channels=None, num_classes=None, pretrained=True, num_frames=1, prepare_features_for_image_model=None, aux_decoders=None, rescale=True, **kwargs)</code>","text":"<p>Model factory for prithvi models.</p> <p>Further arguments to be passed to the backbone, decoder or head. They should be prefixed with <code>backbone_</code>, <code>decoder_</code> and <code>head_</code> respectively.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Currently supports \"segmentation\" and \"regression\".</p> required <code>backbone</code> <code>(str, Module)</code> <p>Backbone to be used. If string, should be able to be parsed by the specified factory. Defaults to \"prithvi_100\".</p> required <code>decoder</code> <code>Union[str, Module]</code> <p>Decoder to be used for the segmentation model.     If a string, it will be created from a class exposed in decoder.init.py with the same name.     If an nn.Module, we expect it to expose a property <code>decoder.out_channels</code>.     Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> <code>None</code> <code>bands</code> <code>list[HLSBands]</code> <p>Bands the model will be trained on.     Should be a list of terratorch.datasets.HLSBands.     Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE].</p> required <code>num_classes</code> <code>int</code> <p>Number of classes. None for regression tasks.</p> <code>None</code> <code>pretrained</code> <code>Union[bool, Path]</code> <p>Whether to load pretrained weights for the backbone, if available. Defaults to True.</p> <code>True</code> <code>num_frames</code> <code>int</code> <p>Number of timesteps for the model to handle. Defaults to 1.</p> <code>1</code> <code>prepare_features_for_image_model</code> <code>Callable | None</code> <p>Function to be called on encoder features before passing them to the decoder. Defaults to None, which applies the identity function.</p> <code>None</code> <code>aux_decoders</code> <code>list[AuxiliaryHead] | None</code> <p>List of AuxiliaryHead deciders to be added to the model. These decoders take the input from the encoder as well.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: Full model with encoder, decoder and head.</p>"},{"location":"model_factories/#terratorch.models.satmae_model_factory.SatMAEModelFactory","title":"<code>terratorch.models.satmae_model_factory.SatMAEModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p>"},{"location":"model_factories/#terratorch.models.satmae_model_factory.SatMAEModelFactory.build_model","title":"<code>build_model(task, backbone, decoder, in_channels, bands, num_classes=None, pretrained=True, num_frames=1, prepare_features_for_image_model=None, aux_decoders=None, rescale=True, checkpoint_path=None, **kwargs)</code>","text":"<p>Model factory for SatMAE  models.</p> <p>Further arguments to be passed to the backbone, decoder or head. They should be prefixed with <code>backbone_</code>, <code>decoder_</code> and <code>head_</code> respectively.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Currently supports \"segmentation\" and \"regression\".</p> required <code>backbone</code> <code>(str, Module)</code> <p>Backbone to be used. If string, should be able to be parsed by the specified factory. Defaults to \"prithvi_100\".</p> required <code>decoder</code> <code>Union[str, Module]</code> <p>Decoder to be used for the segmentation model.     If a string, it will be created from a class exposed in decoder.init.py with the same name.     If an nn.Module, we expect it to expose a property <code>decoder.out_channels</code>.     Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> required <code>bands</code> <code>list[HLSBands]</code> <p>Bands the model will be trained on.     Should be a list of terratorch.datasets.HLSBands.     Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE].</p> required <code>num_classes</code> <code>int</code> <p>Number of classes. None for regression tasks.</p> <code>None</code> <code>pretrained</code> <code>Union[bool, Path]</code> <p>Whether to load pretrained weights for the backbone, if available. Defaults to True.</p> <code>True</code> <code>num_frames</code> <code>int</code> <p>Number of timesteps for the model to handle. Defaults to 1.</p> <code>1</code> <code>prepare_features_for_image_model</code> <code>Callable | None</code> <p>Function to be called on encoder features before passing them to the decoder. Defaults to None, which applies the identity function.</p> <code>None</code> <code>aux_decoders</code> <code>list[AuxiliaryHead] | None</code> <p>List of AuxiliaryHead deciders to be added to the model. These decoders take the input from the encoder as well.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> <code>DecoderNotFoundException</code> <p>description</p> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: description</p>"},{"location":"model_factories/#terratorch.models.smp_model_factory.SMPModelFactory","title":"<code>terratorch.models.smp_model_factory.SMPModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p>"},{"location":"model_factories/#terratorch.models.smp_model_factory.SMPModelFactory.build_model","title":"<code>build_model(task, backbone, model, bands, in_channels=None, num_classes=1, pretrained=True, prepare_features_for_image_model=None, regression_relu=False, **kwargs)</code>","text":"<p>Factory class for creating SMP (Segmentation Models Pytorch) based models with optional customization.</p> <p>This factory handles the instantiation of segmentation and regression models using specified encoders and decoders from the SMP library, along with custom modifications and extensions such as auxiliary decoders or modified encoders.</p> <p>Attributes:</p> Name Type Description <code>task</code> <code>str</code> <p>Specifies the task for which the model is being built. Supported tasks are         \"segmentation\".</p> <code>backbone</code> <code>str</code> <p>Specifies the backbone model to be used.</p> <code>decoder</code> <code>str</code> <p>Specifies the decoder to be used for constructing the         segmentation model.</p> <code>bands</code> <code>list[HLSBands | int]</code> <p>A list specifying the bands that the model         will operate on. These are expected to be from terratorch.datasets.HLSBands.</p> <code>in_channels</code> <code>int</code> <p>Specifies the number of input channels. Defaults to None.</p> <code>num_classes</code> <code>int</code> <p>The number of output classes for the model.</p> <code>pretrained</code> <code>bool | Path</code> <p>Indicates whether to load pretrained weights for the         backbone. Can also specify a path to weights. Defaults to True.</p> <code>num_frames</code> <code>int</code> <p>Specifies the number of timesteps the model should handle. Useful         for temporal models.</p> <code>regression_relu</code> <code>bool</code> <p>Whether to apply ReLU activation in the case of regression tasks.</p> <code>**kwargs</code> <code>bool</code> <p>Additional arguments that might be passed to further customize the backbone, decoder,         or any auxiliary heads. These should be prefixed appropriately</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified decoder is not supported by SMP.</p> <code>Exception</code> <p>If the specified task is not \"segmentation\"</p> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: A model instance wrapped in SMPModelWrapper configured according to the specified     parameters and tasks.</p>"},{"location":"models/","title":"Models","text":"<p>To interface with terratorch tasks correctly, models must inherit from the Model parent class and have a forward method which returns an object ModelOutput:</p>"},{"location":"models/#model-factories","title":"Model Factories","text":"<p>In order to be used by tasks, models must have a Model Factory which builds them. Factories must conform to the ModelFactory parent class. </p> <p>You most likely do not need to implement your own model factory, unless you are wrapping another library which generates full models.</p> <p>For most cases, the encoder decoder factory can be used to combine a backbone with a decoder.</p> <p>To add new backbones or decoders, to be used with the encoder decoder factory they should be registered. </p> <p>To add a new model factory, it should be registered in the <code>MODEL_FACTORY_REGISTRY</code>.</p>"},{"location":"models/#adding-a-new-model","title":"Adding a new model","text":"<p>To add a new backbone, simply create a class and annotate it (or a constructor function that instantiates it) with <code>@TERRATORCH_BACKBONE_FACTORY.register</code>. </p> <p>The model will be registered with the same name as the function. To create many model variants from the same class, the reccomended approach is to annotate a constructor function from each with a fully descriptive name.</p> <pre><code>from terratorch.registry import TERRATORCH_BACKBONE_REGISTRY, BACKBONE_REGISTRY\n\nfrom torch import nn\n\n# make sure this is in the import path for terratorch\n@TERRATORCH_BACKBONE_REGISTRY.register\nclass BasicBackbone(nn.Module):\n    def __init__(self, out_channels=64):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.layer = nn.Linear(224*224, out_channels)\n        self.out_channels = [out_channels]\n\n    def forward(self, x):\n        return self.layer(self.flatten(x))\n\n# you can build directly with the TERRATORCH_BACKBONE_REGISTRY\n# but typically this will be accessed from the BACKBONE_REGISTRY\n&gt;&gt;&gt; BACKBONE_REGISTRY.build(\"BasicBackbone\", out_channels=64)\nBasicBackbone(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (layer): Linear(in_features=50176, out_features=64, bias=True)\n)\n\n@TERRATORCH_BACKBONE_REGISTRY.register\ndef basic_backbone_128():\n    return BasicBackbone(out_channels=128)\n\n&gt;&gt;&gt; BACKBONE_REGISTRY.build(\"basic_backbone_128\")\nBasicBackbone(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (layer): Linear(in_features=50176, out_features=128, bias=True)\n)\n</code></pre> <p>Adding a new decoder can be done in the same way with the <code>TERRATORCH_DECODER_REGISTRY</code>.</p> <p>Info</p> <p>All decoders will be passed the channel_list as the first argument for initialization.</p> <p>To pass your own path from where to load the weights with the PrithviModelFactory, you can make use of timm's <code>pretrained_cfg_overlay</code>. E.g. to pass a local path, you can pass the parameter <code>backbone_pretrained_cfg_overlay = {\"file\": \"&lt;local_path&gt;\"}</code> to the model factory.</p> <p>Besides <code>file</code>, you can also pass <code>url</code>, <code>hf_hub_id</code>, amongst others. Check timm's documentation for full details.</p>"},{"location":"models/#adding-new-model-types","title":"Adding new model types","text":"<p>Adding new model types is as simple as creating a new factory that produces models. See for instance the example below for a potential <code>SMPModelFactory</code> <pre><code>from terratorch.models.model import register_factory\n\n@register_factory\nclass SMPModelFactory(ModelFactory):\n    def build_model(\n        self,\n        task: str,\n        backbone: str | nn.Module,\n        decoder: str | nn.Module,\n        in_channels: int,\n        **kwargs,\n    ) -&gt; Model:\n\n        model = smp.Unet(encoder_name=\"resnet34\", encoder_weights=None, in_channels=in_channels, classes=1)\n        return SMPModelWrapper(model)\n\n@register_factory\nclass SMPModelWrapper(Model, nn.Module):\n    def __init__(self, smp_model) -&gt; None:\n        super().__init__()\n        self.smp_model = smp_model\n\n    def forward(self, *args, **kwargs):\n        return ModelOutput(self.smp_model(*args, **kwargs).squeeze(1))\n\n    def freeze_encoder(self):\n        pass\n\n    def freeze_decoder(self):\n        pass\n</code></pre></p>"},{"location":"models/#custom-modules-with-cli","title":"Custom modules with CLI","text":"<p>Custom modules must be in the import path in order to be registered in the appropriate registries. </p> <p>In order to do this without modifying the code when using the CLI, you may place your modules under a <code>custom_modules</code> directory. This must be in the directory from which you execute terratorch.</p>"},{"location":"necks/","title":"Necks","text":""},{"location":"necks/#terratorch.models.necks.Neck","title":"<code>terratorch.models.necks.Neck</code>","text":"<p>               Bases: <code>ABC</code>, <code>Module</code></p> <p>Base class for Neck</p> <p>A neck must must implement <code>self.process_channel_list</code> which returns the new channel list.</p>"},{"location":"necks/#terratorch.models.necks.SelectIndices","title":"<code>terratorch.models.necks.SelectIndices</code>","text":"<p>               Bases: <code>Neck</code></p>"},{"location":"necks/#terratorch.models.necks.SelectIndices.__init__","title":"<code>__init__(channel_list, indices)</code>","text":"<p>Select indices from the embedding list</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>list[int]</code> <p>list of indices to select.</p> required"},{"location":"necks/#terratorch.models.necks.PermuteDims","title":"<code>terratorch.models.necks.PermuteDims</code>","text":"<p>               Bases: <code>Neck</code></p>"},{"location":"necks/#terratorch.models.necks.PermuteDims.__init__","title":"<code>__init__(channel_list, new_order)</code>","text":"<p>Permute dimensions of each element in the embedding list</p> <p>Parameters:</p> Name Type Description Default <code>new_order</code> <code>list[int]</code> <p>list of indices to be passed to tensor.permute()</p> required"},{"location":"necks/#terratorch.models.necks.InterpolateToPyramidal","title":"<code>terratorch.models.necks.InterpolateToPyramidal</code>","text":"<p>               Bases: <code>Neck</code></p>"},{"location":"necks/#terratorch.models.necks.InterpolateToPyramidal.__init__","title":"<code>__init__(channel_list, scale_factor=2, mode='nearest')</code>","text":"<p>Spatially interpolate embeddings so that embedding[i - 1] is scale_factor times larger than embedding[i]</p> <p>Useful to make non-pyramidal backbones compatible with hierarachical ones Args:     scale_factor (int): Amount to scale embeddings by each layer. Defaults to 2.     mode (str): Interpolation mode to be passed to torch.nn.functional.interpolate. Defaults to 'nearest'.</p>"},{"location":"necks/#terratorch.models.necks.MaxpoolToPyramidal","title":"<code>terratorch.models.necks.MaxpoolToPyramidal</code>","text":"<p>               Bases: <code>Neck</code></p>"},{"location":"necks/#terratorch.models.necks.MaxpoolToPyramidal.__init__","title":"<code>__init__(channel_list, kernel_size=2)</code>","text":"<p>Spatially downsample embeddings so that embedding[i - 1] is scale_factor times smaller than embedding[i]</p> <p>Useful to make non-pyramidal backbones compatible with hierarachical ones Args:     kernel_size (int). Base kernel size to use for maxpool. Defaults to 2.</p>"},{"location":"necks/#terratorch.models.necks.ReshapeTokensToImage","title":"<code>terratorch.models.necks.ReshapeTokensToImage</code>","text":"<p>               Bases: <code>Neck</code></p>"},{"location":"necks/#terratorch.models.necks.ReshapeTokensToImage.__init__","title":"<code>__init__(channel_list, remove_cls_token=True, effective_time_dim=1)</code>","text":"<p>Reshape output of transformer encoder so it can be passed to a conv net.</p> <p>Parameters:</p> Name Type Description Default <code>remove_cls_token</code> <code>bool</code> <p>Whether to remove the cls token from the first position. Defaults to True.</p> <code>True</code> <code>effective_time_dim</code> <code>int</code> <p>The effective temporal dimension the transformer processes. For a ViT, his will be given by <code>num_frames // tubelet size</code>. This is used to determine the temporal dimension of the embedding, which is concatenated with the embedding dimension. For example: - A model which processes 1 frame with a tubelet size of 1 has an effective_time_dim of 1.     The embedding produced by this model has embedding size embed_dim * 1. - A model which processes 3 frames with a tubelet size of 1 has an effective_time_dim of 3.     The embedding produced by this model has embedding size embed_dim * 3. - A model which processes 12 frames with a tubelet size of 4 has an effective_time_dim of 3.     The embedding produced by this model has an embedding size embed_dim * 3. Defaults to 1.</p> <code>1</code>"},{"location":"necks/#terratorch.models.necks.ReshapeTokensToImage.collapse_dims","title":"<code>collapse_dims(x)</code>","text":"<p>When the encoder output has more than 3 dimensions, is necessary to  reshape it.</p>"},{"location":"necks/#terratorch.models.necks.AddBottleneckLayer","title":"<code>terratorch.models.necks.AddBottleneckLayer</code>","text":"<p>               Bases: <code>Neck</code></p> <p>Add a layer that reduces the channel dimension of the final embedding by half, and concatenates it</p> <p>Useful for compatibility with some smp decoders.</p>"},{"location":"necks/#terratorch.models.necks.LearnedInterpolateToPyramidal","title":"<code>terratorch.models.necks.LearnedInterpolateToPyramidal</code>","text":"<p>               Bases: <code>Neck</code></p> <p>Use learned convolutions to transform the output of a non-pyramidal encoder into pyramidal ones</p> <p>Always requires exactly 4 embeddings</p>"},{"location":"quick_start/","title":"Quick start","text":""},{"location":"quick_start/#configuring-the-environment","title":"Configuring the environment","text":""},{"location":"quick_start/#python","title":"Python","text":"<p>TerraTorch is currently tested for Python in <code>3.10 &lt;= Python &lt;= 3.12</code>. </p>"},{"location":"quick_start/#gdal","title":"GDAL","text":"<p>GDAL is required  to read and write TIFF images. It is usually easy to install in Unix/Linux systems, but if it is not your case  we reccomend using a conda environment and installing it with <code>conda install -c conda-forge gdal</code>.</p>"},{"location":"quick_start/#installing-terratorch","title":"Installing TerraTorch","text":"<p>For a stable point-release, use <code>pip install terratorch</code>. If you prefer to get the most recent version of the main branch, install the library with <code>pip install git+https://github.com/IBM/terratorch.git</code>.</p> <p>To install as a developer (e.g. to extend the library) clone this repo, and run <code>pip install -e .</code> .</p>"},{"location":"quick_start/#creating-backbones","title":"Creating Backbones","text":"<p>You can interact with the library at several levels of abstraction. Each deeper level of abstraction trades off some amount of flexibility for ease of use and configuration. In the simplest case, we might only want access a backbone and code all the rest ourselves. In this case, we can simply use the library as a backbone factory:</p> Instantiating a prithvi backbone<pre><code>from terratorch import BACKBONE_REGISTRY\n\n# find available prithvi models\nprint([model_name for model_name in BACKBONE_REGISTRY if \"terratorch_prithvi\" in model_name])\n&gt;&gt;&gt; ['terratorch_prithvi_eo_tiny', 'terratorch_prithvi_eo_v1_100', 'terratorch_prithvi_eo_v2_300', 'terratorch_prithvi_eo_v2_600', 'terratorch_prithvi_eo_v2_300_tl', 'terratorch_prithvi_eo_v2_600_tl']\n\n# show all models with list(BACKBONE_REGISTRY)\n\n# check a model is in the registry\n\"terratorch_prithvi_eo_v2_300\" in BACKBONE_REGISTRY\n&gt;&gt;&gt; True\n\n# without the prefix, all internal registries will be searched until the first match is found\n\"prithvi_eo_v1_100\" in BACKBONE_REGISTRY\n&gt;&gt;&gt; True\n\n# instantiate your desired model\n# the backbone registry prefix (e.g. `terratorch` or `timm`) is optional\n# in this case, the underlying registry is terratorch.\nmodel = BACKBONE_REGISTRY.build(\"prithvi_eo_v1_100\", pretrained=True)\n\n# instantiate your model with more options, for instance, passing weights from your own file\nmodel = BACKBONE_REGISTRY.build(\n    \"prithvi_eo_v2_300\", num_frames=1, ckpt_path='path/to/model.pt'\n)\n# Rest of your PyTorch / PyTorchLightning code\n</code></pre> <p>Internally, terratorch maintains several registries for components such as backbones or decoders. The top-level <code>BACKBONE_REGISTRY</code> collects all of them.</p> <p>The name passed to <code>build</code> is used to find the appropriate model constructor, which will be the first model from the first registry found with that name.</p> <p>To explicitly determine the registry that will build the model, you may prepend a prefix such as <code>timm_</code> to the model name. In this case, the <code>timm</code> model registry will be exclusively searched for the model.</p>"},{"location":"quick_start/#directly-creating-a-full-model","title":"Directly creating a full model","text":"<p>We also provide a model factory for a task specific model built on one a backbones:</p> Building a full model, with task specific decoder<pre><code>import terratorch # even though we don't use the import directly, we need it so that the models are available in the timm registry\nfrom terratorch.models import EncoderDecoderFactory\nfrom terratorch.datasets import HLSBands\n\nmodel_factory = EncoderDecoderFactory()\n\n# Let's build a segmentation model\n# Parameters prefixed with backbone_ get passed to the backbone\n# Parameters prefixed with decoder_ get passed to the decoder\n# Parameters prefixed with head_ get passed to the head\n\nmodel = model_factory.build_model(\n    task=\"segmentation\",\n    backbone=\"prithvi_eo_v2_300\",\n    backbone_pretrained=True,\n    backbone_bands=[\n        HLSBands.BLUE,\n        HLSBands.GREEN,\n        HLSBands.RED,\n        HLSBands.NIR_NARROW,\n        HLSBands.SWIR_1,\n        HLSBands.SWIR_2,\n    ],\n    necks=[{\"name\": \"SelectIndices\", \"indices\": [-1]},\n           {\"name\": \"ReshapeTokensToImage\"}],\n    decoder=\"FCNDecoder\",\n    decoder_channels=128,\n    head_dropout=0.1,\n    num_classes=4,\n)\n\n# Rest of your PyTorch / PyTorchLightning code\n.\n.\n.\n</code></pre>"},{"location":"quick_start/#training-with-lightning-tasks","title":"Training with Lightning Tasks","text":"<p>At the highest level of abstraction, you can directly obtain a LightningModule ready to be trained.</p> <p>Building a full Pixel-Wise Regression task<pre><code>model_args = dict(\n  backbone=\"prithvi_eo_v2_300\",\n  backbone_pretrained=True,\n  backbone_num_frames=1,\n  backbone_bands=[\n      HLSBands.BLUE,\n      HLSBands.GREEN,\n      HLSBands.RED,\n      HLSBands.NIR_NARROW,\n      HLSBands.SWIR_1,\n      HLSBands.SWIR_2,\n  ],\n  necks=[{\"name\": \"SelectIndices\", \"indices\": [-1]},\n               {\"name\": \"ReshapeTokensToImage\"}],\n  decoder=\"FCNDecoder\",\n  decoder_channels=128,\n  head_dropout=0.1\n)\n\ntask = PixelwiseRegressionTask(\n    model_args,\n    \"EncoderDecoderFactory\",\n    loss=\"rmse\",\n    lr=lr,\n    ignore_index=-1,\n    optimizer=\"AdamW\",\n    optimizer_hparams={\"weight_decay\": 0.05},\n)\n\n# Pass this LightningModule to a Lightning Trainer, together with some LightningDataModule\n</code></pre> Alternatively, all the process can be summarized in configuration files written in YAML format, as seen below. Configuration file for a Semantic Segmentation Task<pre><code># lightning.pytorch==2.1.1\nseed_everything: 0\ntrainer:\n  accelerator: auto\n  strategy: auto\n  devices: auto\n  num_nodes: 1\n  precision: bf16\n  logger:\n    class_path: TensorBoardLogger\n    init_args:\n      save_dir: &lt;path_to_experiment_dir&gt;\n      name: &lt;experiment_name&gt;\n  callbacks:\n    - class_path: RichProgressBar\n    - class_path: LearningRateMonitor\n      init_args:\n        logging_interval: epoch\n\n  max_epochs: 200\n  check_val_every_n_epoch: 1\n  log_every_n_steps: 50\n  enable_checkpointing: true\n  default_root_dir: &lt;path_to_experiment_dir&gt;\ndata:\n  class_path: terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule\n  init_args:\n    batch_size: 16\n    num_workers: 8\n  dict_kwargs:\n    data_root: &lt;path_to_data_root&gt;\n    bands:\n      - 1\n      - 2\n      - 3\n      - 8\n      - 11\n      - 12\nmodel:\n  class_path: terratorch.tasks.SemanticSegmentationTask\n  init_args:\n    model_factory: EncoderDecoderFactory\n    model_args:\n      backbone: prithvi_eo_v2_300\n      backbone_img_size: 512\n      backbone_pretrained: True\n      backbone_bands:\n        - BLUE\n        - GREEN\n        - RED\n        - NIR_NARROW\n        - SWIR_1\n        - SWIR_2\n      necks:\n        - name: SelectIndices\n          indices: [5, 11, 17, 23]\n        - name: ReshapeTokensToImage\n        - name: LearnedInterpolateToPyramidal\n      decoder: UperNetDecoder\n      decoder_channels: 256\n      head_channel_list: [256]\n      head_dropout: 0.1\n      num_classes: 2\n    loss: dice\n    ignore_index: -1\n    freeze_backbone: false\n    freeze_decoder: false    \noptimizer:\n  class_path: torch.optim.AdamW\n  init_args:\n    lr: 1.e-4\n    weight_decay: 0.1\nlr_scheduler:\n  class_path: ReduceLROnPlateau\n  init_args:\n    monitor: val/loss\n</code></pre></p> <p>To run this training task using the YAML, simply execute: <pre><code>terratorch fit --config &lt;path_to_config_file&gt;\n</code></pre></p> <p>To test your model on the test set, execute: <pre><code>terratorch test --config  &lt;path_to_config_file&gt; --ckpt_path &lt;path_to_checkpoint_file&gt;\n</code></pre></p> <p>For inference, execute: <pre><code>terratorch predict -c &lt;path_to_config_file&gt; --ckpt_path&lt;path_to_checkpoint&gt; --predict_output_dir &lt;path_to_output_dir&gt; --data.init_args.predict_data_root &lt;path_to_input_dir&gt; --data.init_args.predict_dataset_bands &lt;all bands in the predicted dataset, e.g. [BLUE,GREEN,RED,NIR_NARROW,SWIR_1,SWIR_2,0]&gt;\n</code></pre></p> <p>Experimental feature: Users that want to optimize hyperparameters or repeat best experiment might be interest in in terratorch-iterate, a terratorch's plugin. For instance, to run terratorch-iterate to optimize hyperparameters, one can run:  <pre><code>terratorch iterate --hpo --config &lt;path_to_config_file&gt; \n</code></pre> Please see how to install terratorch-iterate on this link and how to use it on this link.</p>"},{"location":"registry/","title":"Registries","text":"<p>TerraTorch keeps a set of registries which map strings to instances of those strings. They can be imported from <code>terratorch.registry</code>.</p> <p>Info</p> <p>If you are using tasks with existing models, you may never have to interact with registries directly. The model factory will handle interactions with registries.</p> <p>Registries behave like python sets, exposing the usual <code>contains</code> and <code>iter</code> operations. This means you can easily operate on them in a pythonic way, such as  <code>\"model\" in registry</code> or <code>list(registry)</code>.</p> <p>To create the desired instance, registries expose a <code>build</code> method, which accepts the name and the arguments to be passed to the constructor.</p> Using registries<pre><code>from terratorch import BACKBONE_REGISTRY\n\n# find available prithvi models\nprint([model_name for model_name in BACKBONE_REGISTRY if \"terratorch_prithvi\" in model_name])\n&gt;&gt;&gt; ['terratorch_prithvi_eo_tiny', 'terratorch_prithvi_eo_v1_100', 'terratorch_prithvi_eo_v2_300', 'terratorch_prithvi_eo_v2_600', 'terratorch_prithvi_eo_v2_300_tl', 'terratorch_prithvi_eo_v2_600_tl']\n\n# show all models with list(BACKBONE_REGISTRY)\n\n# check a model is in the registry\n\"terratorch_prithvi_eo_v2_300\" in BACKBONE_REGISTRY\n&gt;&gt;&gt; True\n\n# without the prefix, all internal registries will be searched until the first match is found\n\"prithvi_eo_v1_100\" in BACKBONE_REGISTRY\n&gt;&gt;&gt; True\n\n# instantiate your desired model\n# the backbone registry prefix (e.g. `terratorch` or `timm`) is optional\n# in this case, the underlying registry is terratorch.\nmodel = BACKBONE_REGISTRY.build(\"prithvi_eo_v1_100\", pretrained=True)\n\n# instantiate your model with more options, for instance, passing weights from your own file\nmodel = BACKBONE_REGISTRY.build(\n    \"prithvi_eo_v2_300\", num_frames=1, ckpt_path='path/to/model.pt'\n)\n# Rest of your PyTorch / PyTorchLightning code\n</code></pre>"},{"location":"registry/#multisourceregistries","title":"MultiSourceRegistries","text":"<p><code>BACKBONE_REGISTRY</code> and <code>DECODER_REGISTRY</code> are special registries which dynamically aggregate multiple registries. They behave as if they were a single large registry by searching over multiple registries.</p> <p>For instance, the <code>DECODER_REGISTRY</code> holds the <code>TERRATORCH_DECODER_REGISTRY</code>, which is responsible for decoders implemented in terratorch, as well as the <code>SMP_DECODER_REGISTRY</code> and the <code>MMSEG_DECODER_REGISTRY</code> (if mmseg is installed).</p> <p>To make sure you access the object from a particular registry, you may prepend your string with the prefix from that registry.</p> <pre><code>from terratorch import DECODER_REGISTRY\n\n# decoder registries always take at least one extra argument, the channel list with the channel dimension of each embedding passed to it\nDECODER_REGISTRY.build(\"FCNDecoder\", [32, 64, 128])\n\nDECODER_REGISTRY.build(\"terratorch_FCNDecoder\", [32, 64, 128])\n\n# Find all prefixes\nDECODER_REGISTRY.keys()\n&gt;&gt;&gt; odict_keys(['terratorch', 'smp', 'mmseg'])\n</code></pre> <p>If a prefix is not added, the <code>MultiSourceRegistry</code> will search each registry in the order it was added (starting with the <code>TERRATORCH_</code> registry) until it finds the first match.</p> <p>For both of these registries, only <code>TERRATORCH_X_REGISTRY</code> is mutable. To register backbones or decoders to terratorch, you should decorate the constructor function (or the model class itself) with <code>@TERRATORCH_DECODER_REGISTRY.register</code> or <code>@TERRATORCH_BACKBONE_REGISTRY.register</code>.</p> <p>To add a new registry to these top level registries, you should use the <code>.register</code> method, taking the register and the prefix that will be used for it.</p>"},{"location":"registry/#terratorch.registry.registry.MultiSourceRegistry","title":"<code>terratorch.registry.registry.MultiSourceRegistry</code>","text":"<p>               Bases: <code>Mapping[str, T]</code>, <code>Generic[T]</code></p> <p>Registry that searches in multiple sources</p> <p>Correct functioning of this class depends on registries raising a KeyError when the model is not found.</p> Source code in <code>terratorch/registry/registry.py</code> <pre><code>class MultiSourceRegistry(Mapping[str, T], typing.Generic[T]):\n    \"\"\"Registry that searches in multiple sources\n\n        Correct functioning of this class depends on registries raising a KeyError when the model is not found.\n    \"\"\"\n    def __init__(self, **sources) -&gt; None:\n        self._sources: OrderedDict[str, T] = OrderedDict(sources)\n\n    def _parse_prefix(self, name) -&gt; tuple[str, str] | None:\n        split = name.split(\"_\")\n        if len(split) &gt; 1 and split[0] in self._sources:\n            prefix = split[0]\n            name_without_prefix = \"_\".join(split[1:])\n            return prefix, name_without_prefix\n        return None\n\n    def find_registry(self, name: str) -&gt; T:\n        parsed_prefix = self._parse_prefix(name)\n        if parsed_prefix:\n            prefix, name_without_prefix = parsed_prefix\n            registry = self._sources[prefix]\n            return registry\n\n        # if no prefix is given, go through all sources in order\n        for registry in self._sources.values():\n            if name in registry:\n                return registry\n        msg = f\"Model {name} not found in any registry\"\n        raise KeyError(msg)\n\n    def find_class(self, name: str) -&gt; type:\n        parsed_prefix = self._parse_prefix(name)\n        registry = self.find_registry(name)\n        if parsed_prefix:\n            prefix, name_without_prefix = parsed_prefix\n            return registry[name_without_prefix]\n        return registry[name]\n\n    def build(self, name: str, *constructor_args, **constructor_kwargs):\n        parsed_prefix = self._parse_prefix(name)\n        if parsed_prefix:\n            prefix, name_without_prefix = parsed_prefix\n            registry = self._sources[prefix]\n            return registry.build(name_without_prefix, *constructor_args, **constructor_kwargs)\n\n        # if no prefix, try to build in order\n        for source in self._sources.values():\n            with suppress(KeyError):\n                return source.build(name, *constructor_args, **constructor_kwargs)\n\n        msg = f\"Could not instantiate model {name} not from any source.\"\n        raise KeyError(msg)\n\n    def register_source(self, prefix: str, registry: T) -&gt; None:\n        \"\"\"Register a source in the registry\"\"\"\n        if prefix in self._sources:\n            msg = f\"Source for prefix {prefix} already exists.\"\n            raise KeyError(msg)\n        self._sources[prefix] = registry\n\n    def __iter__(self):\n        for prefix in self._sources:\n            for element in self._sources[prefix]:\n                yield prefix + \"_\" + element\n\n    def __len__(self):\n        return sum(len(source) for source in self._sources.values())\n\n    def __getitem__(self, name):\n        return self._sources[name]\n\n    def __contains__(self, name):\n        parsed_prefix = self._parse_prefix(name)\n        if parsed_prefix:\n            prefix, name_without_prefix = parsed_prefix\n            return name_without_prefix in self._sources[prefix]\n        return any(name in source for source in self._sources.values())\n\n    @_recursive_repr()\n    def __repr__(self):\n        args = [f\"{name}={source!r}\" for name, source in self._sources.items()]\n        return f'{self.__class__.__name__}({\", \".join(args)})'\n\n    def __str__(self):\n        sources_str = str(\" | \".join([f\"{prefix}: {source!s}\" for prefix, source in self._sources.items()]))\n        return f\"Multi source registry with {len(self)} items: {sources_str}\"\n\n    def keys(self):\n        return self._sources.keys()\n</code></pre>"},{"location":"registry/#terratorch.registry.registry.MultiSourceRegistry.register_source","title":"<code>register_source(prefix, registry)</code>","text":"<p>Register a source in the registry</p> Source code in <code>terratorch/registry/registry.py</code> <pre><code>def register_source(self, prefix: str, registry: T) -&gt; None:\n    \"\"\"Register a source in the registry\"\"\"\n    if prefix in self._sources:\n        msg = f\"Source for prefix {prefix} already exists.\"\n        raise KeyError(msg)\n    self._sources[prefix] = registry\n</code></pre>"},{"location":"registry/#terratorch.registry.registry.Registry","title":"<code>terratorch.registry.registry.Registry</code>","text":"<p>               Bases: <code>Set</code></p> <p>Registry holding model constructors and multiple additional sources.</p> <p>This registry behaves as a set of strings, which are model names, to model classes or functions which instantiate model classes.</p> <p>In addition, it can instantiate models with the build method.</p> <p>Add constructors to the registry by annotating them with @registry.register. <pre><code>&gt;&gt;&gt; registry = Registry()\n&gt;&gt;&gt; @registry.register\n... def model(*args, **kwargs):\n...     return object()\n&gt;&gt;&gt; \"model\" in registry\nTrue\n&gt;&gt;&gt; model_instance = registry.build(\"model\")\n</code></pre></p> Source code in <code>terratorch/registry/registry.py</code> <pre><code>class Registry(Set):\n    \"\"\"Registry holding model constructors and multiple additional sources.\n\n    This registry behaves as a set of strings, which are model names,\n    to model classes or functions which instantiate model classes.\n\n    In addition, it can instantiate models with the build method.\n\n    Add constructors to the registry by annotating them with @registry.register.\n    ```\n    &gt;&gt;&gt; registry = Registry()\n    &gt;&gt;&gt; @registry.register\n    ... def model(*args, **kwargs):\n    ...     return object()\n    &gt;&gt;&gt; \"model\" in registry\n    True\n    &gt;&gt;&gt; model_instance = registry.build(\"model\")\n    ```\n    \"\"\"\n\n    def __init__(self, **elements) -&gt; None:\n        self._registry: dict[str, Callable] = dict(elements)\n\n    def register(self, constructor: Callable | type) -&gt; Callable:\n        \"\"\"Register a component in the registry. Used as a decorator.\n\n        Args:\n            constructor (Callable | type): Function or class to be decorated with @register.\n        \"\"\"\n        if not callable(constructor):\n            msg = f\"Invalid argument. Decorate a function or class with @{self.__class__.__name__}.register\"\n            raise TypeError(msg)\n        self._registry[constructor.__name__] = constructor\n        return constructor\n\n    def build(self, name: str, *constructor_args, **constructor_kwargs):\n        \"\"\"Build and return the component.\n        Use prefixes ending with _ to forward to a specific source\n        \"\"\"\n        return self._registry[name](*constructor_args, **constructor_kwargs)\n\n    def __iter__(self):\n        return iter(self._registry)\n\n    def __getitem__(self, key):\n        return self._registry[key]\n\n    def __len__(self):\n        return len(self._registry)\n\n    def __contains__(self, key):\n        return key in self._registry\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self._registry!r})\"\n\n    def __str__(self):\n        return f\"Registry with {len(self)} registered items\"\n</code></pre>"},{"location":"registry/#terratorch.registry.registry.Registry.build","title":"<code>build(name, *constructor_args, **constructor_kwargs)</code>","text":"<p>Build and return the component. Use prefixes ending with _ to forward to a specific source</p> Source code in <code>terratorch/registry/registry.py</code> <pre><code>def build(self, name: str, *constructor_args, **constructor_kwargs):\n    \"\"\"Build and return the component.\n    Use prefixes ending with _ to forward to a specific source\n    \"\"\"\n    return self._registry[name](*constructor_args, **constructor_kwargs)\n</code></pre>"},{"location":"registry/#terratorch.registry.registry.Registry.register","title":"<code>register(constructor)</code>","text":"<p>Register a component in the registry. Used as a decorator.</p> <p>Parameters:</p> Name Type Description Default <code>constructor</code> <code>Callable | type</code> <p>Function or class to be decorated with @register.</p> required Source code in <code>terratorch/registry/registry.py</code> <pre><code>def register(self, constructor: Callable | type) -&gt; Callable:\n    \"\"\"Register a component in the registry. Used as a decorator.\n\n    Args:\n        constructor (Callable | type): Function or class to be decorated with @register.\n    \"\"\"\n    if not callable(constructor):\n        msg = f\"Invalid argument. Decorate a function or class with @{self.__class__.__name__}.register\"\n        raise TypeError(msg)\n    self._registry[constructor.__name__] = constructor\n    return constructor\n</code></pre>"},{"location":"registry/#other-registries","title":"Other Registries","text":"<p>Additionally, terratorch has the <code>NECK_REGISTRY</code>, where all necks must be registered, and the <code>MODEL_FACTORY_REGISTRY</code>, where all model factories must be registered.</p>"},{"location":"tasks/","title":"Tasks","text":"<p>Tasks provide a convenient abstraction over the training of a model for a specific downstream task.  They encapsulate the model, optimizer, metrics, loss as well as training, validation and testing steps. The task expects to be passed a model factory, to which the model_args arguments are passed to instantiate the model that will be trained. The models produced by this model factory should output ModelOutput instances and conform to the Model ABC. Tasks are best leveraged using config files, where they are specified in the <code>model</code> section under <code>class_path</code>. You can check out some examples of config files here. Below are the details of the tasks currently implemented in TerraTorch (Pixelwise Regression, Semantic Segmentation and Classification). </p>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask","title":"<code>terratorch.tasks.segmentation_tasks.SemanticSegmentationTask</code>","text":"<p>               Bases: <code>TerraTorchTask</code></p> <p>Semantic Segmentation Task that accepts models from a range of sources.</p> <p>This class is analog in functionality to class SemanticSegmentationTask defined by torchgeo. However, it has some important differences:     - Accepts the specification of a model factory     - Logs metrics per class     - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)     - Allows the setting of optimizers in the constructor     - Allows to evaluate on multiple test dataloaders</p>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.__init__","title":"<code>__init__(model_args, model_factory=None, model=None, loss='ce', aux_heads=None, aux_loss=None, class_weights=None, ignore_index=None, lr=0.001, optimizer=None, optimizer_hparams=None, scheduler=None, scheduler_hparams=None, freeze_backbone=False, freeze_decoder=False, freeze_head=False, plot_on_val=10, class_names=None, tiled_inference_parameters=None, test_dataloaders_names=None, lr_overrides=None, output_on_inference='prediction', output_most_probable=True, path_to_record_metrics=None, tiled_inference_on_testing=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict</code> <p>Arguments passed to the model factory.</p> required <code>model_factory</code> <code>str</code> <p>ModelFactory class to be used to instantiate the model. Is ignored when model is provided.</p> <code>None</code> <code>model</code> <code>Module</code> <p>Custom model.</p> <code>None</code> <code>loss</code> <code>str</code> <p>Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\".</p> <code>'ce'</code> <code>aux_loss</code> <code>dict[str, float] | None</code> <p>Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None.</p> <code>None</code> <code>class_weights</code> <code>Union[list[float], None]</code> <p>List of class weights to be applied to the loss.</p> <code>None</code> <code>class_weights</code> <code>list[float] | None</code> <p>List of class weights to be applied to the loss. Defaults to None.</p> <code>None</code> <code>ignore_index</code> <code>int | None</code> <p>Label to ignore in the loss computation. Defaults to None.</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate to be used. Defaults to 0.001.</p> <code>0.001</code> <code>optimizer</code> <code>str | None</code> <p>Name of optimizer class from torch.optim to be used.</p> <code>None</code> <code>optimizer_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler</code> <code>str</code> <p>Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>freeze_backbone</code> <code>bool</code> <p>Whether to freeze the backbone. Defaults to False.</p> <code>False</code> <code>freeze_decoder</code> <code>bool</code> <p>Whether to freeze the decoder. Defaults to False.</p> <code>False</code> <code>freeze_head</code> <code>bool</code> <p>Whether to freeze the segmentation head. Defaults to False.</p> <code>False</code> <code>plot_on_val</code> <code>bool | int</code> <p>Whether to plot visualizations on validation.</p> <code>10</code> <code>class_names</code> <code>list[str] | None</code> <p>List of class names passed to metrics for better naming. Defaults to numeric ordering.</p> <code>None</code> <code>tiled_inference_parameters</code> <code>TiledInferenceParameters | None</code> <p>Inference parameters used to determine if inference is done on the whole image or through tiling.</p> <code>None</code> <code>test_dataloaders_names</code> <code>list[str] | None</code> <p>Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used.</p> <code>None</code> <code>lr_overrides</code> <code>dict[str, float] | None</code> <p>Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name) and the value should be the new lr. Defaults to None.</p> <code>None</code> <code>output_on_inference</code> <code>str | list[str]</code> <p>A string or a list defining the kind of output to be saved to file during the inference, for example, it can be \"prediction\", to save just the most probable class, or [\"prediction\", \"probabilities\"] to save both prediction and probabilities.</p> <code>'prediction'</code> <code>output_most_probable</code> <code>bool</code> <p>A boolean to define if the prediction step will output just the most probable logit or all of them. This argument has been deprecated and will be replaced with <code>output_on_inference</code>. </p> <code>True</code> <code>tiled_inference_on_testing</code> <code>bool</code> <p>A boolean to define if tiled inference will be used when full inference  fails during the test step. </p> <code>False</code> <code>path_to_record_metrics</code> <code>str</code> <p>A path to save the file containing the metrics log.</p> <code>None</code>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.configure_losses","title":"<code>configure_losses()</code>","text":"<p>Initialize the loss criterion.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If loss is invalid.</p>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.configure_metrics","title":"<code>configure_metrics()</code>","text":"<p>Initialize the performance metrics.</p>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the predicted class probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output predicted probabilities.</p>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the test loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.training_step","title":"<code>training_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the train loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the validation loss and additional metrics. Args:     batch: The output of your DataLoader.     batch_idx: Integer displaying index of this batch.     dataloader_idx: Index of the current dataloader.</p>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask","title":"<code>terratorch.tasks.regression_tasks.PixelwiseRegressionTask</code>","text":"<p>               Bases: <code>TerraTorchTask</code></p> <p>Pixelwise Regression Task that accepts models from a range of sources.</p> <p>This class is analog in functionality to PixelwiseRegressionTask defined by torchgeo. However, it has some important differences:     - Accepts the specification of a model factory     - Logs metrics per class     - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)     - Allows the setting of optimizers in the constructor     - Allows to evaluate on multiple test dataloaders</p>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.__init__","title":"<code>__init__(model_args, model_factory=None, model=None, loss='mse', aux_heads=None, aux_loss=None, class_weights=None, ignore_index=None, lr=0.001, optimizer=None, optimizer_hparams=None, scheduler=None, scheduler_hparams=None, freeze_backbone=False, freeze_decoder=False, freeze_head=False, plot_on_val=10, tiled_inference_parameters=None, test_dataloaders_names=None, lr_overrides=None, tiled_inference_on_testing=None, path_to_record_metrics=None)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict</code> <p>Arguments passed to the model factory.</p> required <code>model_factory</code> <code>str</code> <p>Name of ModelFactory class to be used to instantiate the model. Is ignored when model is provided.</p> <code>None</code> <code>model</code> <code>Module</code> <p>Custom model.</p> <code>None</code> <code>loss</code> <code>str</code> <p>Loss to be used. Currently, supports 'mse', 'rmse', 'mae' or 'huber' loss. Defaults to \"mse\".</p> <code>'mse'</code> <code>aux_loss</code> <code>dict[str, float] | None</code> <p>Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None.</p> <code>None</code> <code>class_weights</code> <code>list[float] | None</code> <p>List of class weights to be applied to the loss. Defaults to None.</p> <code>None</code> <code>ignore_index</code> <code>int | None</code> <p>Label to ignore in the loss computation. Defaults to None.</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate to be used. Defaults to 0.001.</p> <code>0.001</code> <code>optimizer</code> <code>str | None</code> <p>Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>optimizer_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler</code> <code>str</code> <p>Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>freeze_backbone</code> <code>bool</code> <p>Whether to freeze the backbone. Defaults to False.</p> <code>False</code> <code>freeze_decoder</code> <code>bool</code> <p>Whether to freeze the decoder. Defaults to False.</p> <code>False</code> <code>freeze_head</code> <code>bool</code> <p>Whether to freeze the segmentation head. Defaults to False.</p> <code>False</code> <code>plot_on_val</code> <code>bool | int</code> <p>Whether to plot visualizations on validation. If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs.</p> <code>10</code> <code>tiled_inference_parameters</code> <code>TiledInferenceParameters | None</code> <p>Inference parameters used to determine if inference is done on the whole image or through tiling.</p> <code>None</code> <code>test_dataloaders_names</code> <code>list[str] | None</code> <p>Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used.</p> <code>None</code> <code>lr_overrides</code> <code>dict[str, float] | None</code> <p>Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None.</p> <code>None</code> <code>tiled_inference_on_testing</code> <code>bool</code> <p>A boolean to the fine if tiled inference will be used when full inference  fails during the test step. </p> <code>None</code> <code>path_to_record_metrics</code> <code>str</code> <p>A path to save the file containing the metrics log.</p> <code>None</code>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.configure_losses","title":"<code>configure_losses()</code>","text":"<p>Initialize the loss criterion.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If loss is invalid.</p>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.configure_metrics","title":"<code>configure_metrics()</code>","text":"<p>Initialize the performance metrics.</p>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the predicted class probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output predicted probabilities.</p>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the test loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.training_step","title":"<code>training_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the train loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the validation loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask","title":"<code>terratorch.tasks.classification_tasks.ClassificationTask</code>","text":"<p>               Bases: <code>TerraTorchTask</code></p> <p>Classification Task that accepts models from a range of sources.</p> <p>This class is analog in functionality to the class ClassificationTask defined by torchgeo. However, it has some important differences:     - Accepts the specification of a model factory     - Logs metrics per class     - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)     - Allows the setting of optimizers in the constructor     - It provides mIoU with both Micro and Macro averaging     - Allows to evaluate on multiple test dataloaders</p> <p>.. note::        * 'Micro' averaging suits overall performance evaluation but may not reflect          minority class accuracy.        * 'Macro' averaging gives equal weight to each class, useful          for balanced performance assessment across imbalanced classes.</p>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.__init__","title":"<code>__init__(model_args, model_factory=None, model=None, loss='ce', aux_heads=None, aux_loss=None, class_weights=None, ignore_index=None, lr=0.001, optimizer=None, optimizer_hparams=None, scheduler=None, scheduler_hparams=None, freeze_backbone=False, freeze_decoder=False, freeze_head=False, class_names=None, test_dataloaders_names=None, lr_overrides=None, path_to_record_metrics=None)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict</code> <p>Arguments passed to the model factory.</p> required <code>model_factory</code> <code>str</code> <p>ModelFactory class to be used to instantiate the model. Is ignored when model is provided.</p> <code>None</code> <code>model</code> <code>Module</code> <p>Custom model.</p> <code>None</code> <code>loss</code> <code>str</code> <p>Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\".</p> <code>'ce'</code> <code>aux_loss</code> <code>dict[str, float] | None</code> <p>Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None.</p> <code>None</code> <code>class_weights</code> <code>Union[list[float], None]</code> <p>List of class weights to be applied to the loss.</p> <code>None</code> <code>class_weights</code> <code>list[float] | None</code> <p>List of class weights to be applied to the loss. Defaults to None.</p> <code>None</code> <code>ignore_index</code> <code>int | None</code> <p>Label to ignore in the loss computation. Defaults to None.</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate to be used. Defaults to 0.001.</p> <code>0.001</code> <code>optimizer</code> <code>str | None</code> <p>Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>optimizer_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler</code> <code>str</code> <p>Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>freeze_backbone</code> <code>bool</code> <p>Whether to freeze the backbone. Defaults to False.</p> <code>False</code> <code>freeze_decoder</code> <code>bool</code> <p>Whether to freeze the decoder. Defaults to False.</p> <code>False</code> <code>freeze_head</code> <code>bool</code> <p>Whether to freeze the segmentation_head. Defaults to False.</p> <code>False</code> <code>class_names</code> <code>list[str] | None</code> <p>List of class names passed to metrics for better naming. Defaults to numeric ordering.</p> <code>None</code> <code>test_dataloaders_names</code> <code>list[str] | None</code> <p>Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used.</p> <code>None</code> <code>lr_overrides</code> <code>dict[str, float] | None</code> <p>Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None.</p> <code>None</code> <code>path_to_record_metrics</code> <code>str</code> <p>A path to save the file containing the metrics log.</p> <code>None</code>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.configure_losses","title":"<code>configure_losses()</code>","text":"<p>Initialize the loss criterion.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If loss is invalid.</p>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.configure_metrics","title":"<code>configure_metrics()</code>","text":"<p>Initialize the performance metrics.</p>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the predicted class probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>object</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output predicted probabilities.</p>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the test loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>object</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.training_step","title":"<code>training_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the train loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>object</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the validation loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>object</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code>"},{"location":"transforms/","title":"Transforms","text":""},{"location":"transforms/#terratorch.datasets.transforms","title":"<code>terratorch.datasets.transforms</code>","text":""},{"location":"transforms/#terratorch.datasets.transforms.FlattenSamplesIntoChannels","title":"<code>FlattenSamplesIntoChannels</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>FlattenSamplesIntoChannels is an image transformation that merges the sample (and optionally temporal) dimensions into the channel dimension.</p> <p>This transform rearranges an input tensor by flattening the sample dimension, and if specified, also the temporal dimension, thereby concatenating these dimensions into a single channel dimension.</p>"},{"location":"transforms/#terratorch.datasets.transforms.FlattenSamplesIntoChannels.__init__","title":"<code>__init__(time_dim=True)</code>","text":"<p>Initialize the FlattenSamplesIntoChannels transform.</p> <p>Parameters:</p> Name Type Description Default <code>time_dim</code> <code>bool</code> <p>If True, the temporal dimension is included in the flattening process. Default is True.</p> <code>True</code>"},{"location":"transforms/#terratorch.datasets.transforms.FlattenTemporalIntoChannels","title":"<code>FlattenTemporalIntoChannels</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>FlattenTemporalIntoChannels is an image transformation that flattens the temporal dimension into the channel dimension.</p> <p>This transform rearranges an input tensor with a temporal dimension into one where the time and channel dimensions are merged. It expects the input to have a fixed number of dimensions defined by N_DIMS_FOR_TEMPORAL.</p>"},{"location":"transforms/#terratorch.datasets.transforms.FlattenTemporalIntoChannels.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the FlattenTemporalIntoChannels transform.</p>"},{"location":"transforms/#terratorch.datasets.transforms.MultimodalTransforms","title":"<code>MultimodalTransforms</code>","text":"<p>MultimodalTransforms applies albumentations transforms to multiple image modalities.</p> <p>This class supports both shared transformations across modalities and separate transformations for each modality. It also handles non-image modalities by applying a specified non-image transform.</p>"},{"location":"transforms/#terratorch.datasets.transforms.MultimodalTransforms.__init__","title":"<code>__init__(transforms, shared=True, non_image_modalities=None, non_image_transform=None)</code>","text":"<p>Initialize the MultimodalTransforms.</p> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>dict or Compose</code> <p>The transformation(s) to apply to the data.</p> required <code>shared</code> <code>bool</code> <p>If True, the same transform is applied to all modalities; if False, separate transforms are used.</p> <code>True</code> <code>non_image_modalities</code> <code>list[str] | None</code> <p>List of keys corresponding to non-image modalities.</p> <code>None</code> <code>non_image_transform</code> <code>object | None</code> <p>A transform to apply to non-image modalities. If None, a default transform is used.</p> <code>None</code>"},{"location":"transforms/#terratorch.datasets.transforms.Padding","title":"<code>Padding</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>Padding to adjust (slight) discrepancies between input images</p>"},{"location":"transforms/#terratorch.datasets.transforms.Rearrange","title":"<code>Rearrange</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>Rearrange is a generic image transformation that reshapes an input tensor using a custom einops pattern.</p> <p>This transform allows flexible reordering of tensor dimensions based on the provided pattern and arguments.</p>"},{"location":"transforms/#terratorch.datasets.transforms.Rearrange.__init__","title":"<code>__init__(rearrange, rearrange_args=None, always_apply=True, p=1)</code>","text":"<p>Initialize the Rearrange transform.</p> <p>Parameters:</p> Name Type Description Default <code>rearrange</code> <code>str</code> <p>The einops rearrangement pattern to apply.</p> required <code>rearrange_args</code> <code>dict[str, int] | None</code> <p>Additional arguments for the rearrangement pattern.</p> <code>None</code> <code>always_apply</code> <code>bool</code> <p>Whether to always apply this transform. Default is True.</p> <code>True</code> <code>p</code> <code>float</code> <p>The probability of applying the transform. Default is 1.</p> <code>1</code>"},{"location":"transforms/#terratorch.datasets.transforms.SelectBands","title":"<code>SelectBands</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>SelectBands is an image transformation that selects a subset of bands (channels) from an input image.</p> <p>This transform uses specified band indices to filter and output only the desired channels from the image tensor.</p>"},{"location":"transforms/#terratorch.datasets.transforms.SelectBands.__init__","title":"<code>__init__(band_indices)</code>","text":"<p>Initialize the SelectBands transform.</p> <p>Parameters:</p> Name Type Description Default <code>band_indices</code> <code>list[int]</code> <p>A list of indices specifying which bands to select.</p> required"},{"location":"transforms/#terratorch.datasets.transforms.UnflattenSamplesFromChannels","title":"<code>UnflattenSamplesFromChannels</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>UnflattenSamplesFromChannels is an image transformation that restores the sample (and optionally temporal) dimensions from the channel dimension.</p> <p>This transform is designed to reverse the flattening performed by FlattenSamplesIntoChannels and is typically applied after converting images to a channels-first format.</p>"},{"location":"transforms/#terratorch.datasets.transforms.UnflattenSamplesFromChannels.__init__","title":"<code>__init__(time_dim=True, n_samples=None, n_timesteps=None, n_channels=None)</code>","text":"<p>Initialize the UnflattenSamplesFromChannels transform.</p> <p>Parameters:</p> Name Type Description Default <code>time_dim</code> <code>bool</code> <p>If True, the temporal dimension is considered during unflattening.</p> <code>True</code> <code>n_samples</code> <code>int | None</code> <p>The number of samples.</p> <code>None</code> <code>n_timesteps</code> <code>int | None</code> <p>The number of time steps.</p> <code>None</code> <code>n_channels</code> <code>int | None</code> <p>The number of channels per time step.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If time_dim is True and fewer than two of n_channels, n_timesteps, and n_samples are provided.</p> <code>Exception</code> <p>If time_dim is False and neither n_channels nor n_samples is provided.</p>"},{"location":"transforms/#terratorch.datasets.transforms.UnflattenTemporalFromChannels","title":"<code>UnflattenTemporalFromChannels</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>UnflattenTemporalFromChannels is an image transformation that restores the temporal dimension from the channel dimension.</p> <p>This transform is typically applied after converting images to a channels-first format (e.g., after ToTensorV2) and rearranges the flattened temporal information back into separate time and channel dimensions.</p>"},{"location":"tutorials/adding_custom_modules/","title":"How to Add Custom Modules to TerraTorch","text":"<p>TerraTorch is designed to be extensible, allowing you to integrate your own custom components, such as models (backbones, decoders), tasks, datamodules, callbacks, or augmentation transforms, into the fine-tuning pipeline. This is primarily achieved by making your custom Python code discoverable by TerraTorch and then referencing your custom classes within the YAML configuration file.</p> <p>This tutorial outlines the steps required to add and use a custom module. We'll use a simple custom model component as an example.</p>"},{"location":"tutorials/adding_custom_modules/#prerequisites","title":"Prerequisites","text":"<ul> <li>A working installation of TerraTorch.</li> <li>Your custom Python code (e.g., a new model architecture implemented as a <code>torch.nn.Module</code>).</li> </ul>"},{"location":"tutorials/adding_custom_modules/#step-1-create-your-custom-module-files","title":"Step 1: Create Your Custom Module File(s)","text":"<p>First, organize your custom code into Python files (<code>.py</code>). It's recommended to place these files within a dedicated directory. For example, let's create a directory named <code>custom_modules</code> in your project's working directory and place our custom model definition inside a file named <code>my_custom_model.py</code>:</p> <pre><code>my_project_root/\n\u251c\u2500\u2500 custom_modules/\n\u2502   \u251c\u2500\u2500 __init__.py       &lt;-- **Required** to make the directory a Python package\n\u2502   \u2514\u2500\u2500 my_custom_model.py\n\u251c\u2500\u2500 my_config.yaml\n\u2514\u2500\u2500 ... (other project files)\n</code></pre> <p>Inside <code>custom_modules/my_custom_model.py</code>, define your custom class. If you intend for TerraTorch's factories to discover this module (e.g., to use it as a backbone or decoder selected by name), you must register it using the appropriate registry decorator.</p> <p>For instance, to register a simple custom CNN as a backbone:</p> <pre><code># custom_modules/my_custom_model.py\nimport torch\nimport torch.nn as nn\n# Import the relevant registry\nfrom terratorch.registry import TERRATORCH_BACKBONE_REGISTRY\n\n# Register the class with the backbone registry\n@TERRATORCH_BACKBONE_REGISTRY.register\nclass MySimpleCNN(nn.Module):\n    # Note: Backbones typically don't take num_classes directly in __init__\n    # They output features which are then processed by a decoder/head.\n    # This example is simplified for demonstration.\n    def __init__(self, in_channels: int, out_features: int = 512): # Example: output feature size\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1)\n        self.relu = nn.ReLU()\n        self.pool = nn.AdaptiveAvgPool2d((1, 1)) # Example pooling\n        self.fc = nn.Linear(32, out_features) # Example final layer\n        print(f\"Initialized MySimpleCNN backbone with in_channels={in_channels}, out_features={out_features}\")\n\n    def forward(self, x: torch.Tensor) -&gt; list[torch.Tensor]: # Backbones often return a list of features\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        # Return as a list, mimicking multi-stage feature outputs\n        return [x]\n\n# You could also define custom tasks, datamodules, decoders (registering with TERRATORCH_DECODER_REGISTRY), etc. here\n# from terratorch.tasks import SemanticSegmentationTask\n# class MyCustomTask(SemanticSegmentationTask):\n#     # ... override methods ...\n#     pass\n</code></pre> <p>The <code>@TERRATORCH_BACKBONE_REGISTRY.register</code> line makes <code>MySimpleCNN</code> available to be selected by name (i.e., <code>\"MySimpleCNN\"</code>) in the <code>backbone</code> field of your model configuration. Similar registries exist for decoders (<code>TERRATORCH_DECODER_REGISTRY</code>), necks, and full models.</p> <p>Make sure your custom classes inherit from appropriate base classes if needed (e.g., <code>torch.nn.Module</code> for models, <code>lightning.pytorch.LightningModule</code> or <code>terratorch.tasks.BaseTask</code> for tasks, <code>lightning.pytorch.LightningDataModule</code> for datamodules).</p>"},{"location":"tutorials/adding_custom_modules/#step-2-inform-terratorch-about-your-custom-module-directory","title":"Step 2: Inform TerraTorch About Your Custom Module Directory","text":"<p>TerraTorch needs to know where to find your custom code. You can specify the path to your custom modules directory using the <code>custom_modules_path</code> argument either in your YAML configuration file or directly via the command line.</p> <p>Option A: In YAML Configuration (<code>my_config.yaml</code>)</p> <p>Add the <code>custom_modules_path</code> key at the top level of your configuration:</p> <pre><code># my_config.yaml\ncustom_modules_path: ./custom_modules  # Path relative to where you run terratorch\n\n# ... rest of your configuration ...\n\nmodel:\n  # ... other model config ...\ndata:\n  # ... data config ...\ntrainer:\n  # ... trainer config ...\n</code></pre> <p>Option B: Via Command Line</p> <pre><code>terratorch fit --config my_config.yaml --custom_modules_path ./custom_modules\n</code></pre> <p>When provided, TerraTorch will add the specified directory (<code>./custom_modules</code> in this case) to Python's <code>sys.path</code>, making the modules within it importable.</p>"},{"location":"tutorials/adding_custom_modules/#step-3-use-your-custom-component-in-the-configuration","title":"Step 3: Use Your Custom Component in the Configuration","text":"<p>Now that TerraTorch can find and import your custom code, you can reference your custom classes in the YAML configuration.</p> <ul> <li> <p>For registered components (Recommended for backbones, decoders, etc.): If you registered your class (like <code>MySimpleCNN</code> above), you can simply use its class name as the identifier:</p> <pre><code># my_config.yaml\ncustom_modules_path: ./custom_modules\n\nmodel:\n  class_path: terratorch.models.EncoderDecoder # Example using a standard factory\n  init_args:\n    backbone: \"MySimpleCNN\" # Reference the registered custom backbone by name\n    decoder: \"UNetDecoder\" # Example using a standard decoder\n    model_args:\n      backbone_in_channels: 3 # Passed to MySimpleCNN.__init__\n      # backbone_out_features: 512 # Default or specify if needed\n      decoder_num_classes: 5 # Example number of output classes\n      # ... other args for backbone/decoder if needed\n# ... rest of config\n</code></pre> </li> <li> <p>For components not using the registry or for full class path reference: You can always reference a class using its full Python path: <code>&lt;directory_name&gt;.&lt;filename&gt;.&lt;ClassName&gt;</code>. This is useful for custom tasks, callbacks, or if you choose not to register a model component and instead specify its full path.</p> <pre><code># my_config.yaml\ncustom_modules_path: ./custom_modules\n\n# Example: Using the custom model directly via class_path (less common for backbones/decoders)\n# model:\n#   class_path: custom_modules.my_custom_model.MySimpleCNN # Using full path\n#   init_args:\n#     in_channels: 3\n#     out_features: 512\n\n# Example: Using a custom task\ntask:\n  class_path: custom_modules.my_custom_task_module.MyCustomTask # Assuming it's defined elsewhere\n  init_args:\n    # ... task args ...\n\n# Example: Using a custom callback\ntrainer:\n  callbacks:\n    - class_path: custom_modules.my_custom_callbacks.MyCallback\n      init_args:\n        # ... callback args ...\n# ... rest of config\n</code></pre> </li> </ul>"},{"location":"tutorials/adding_custom_modules/#summary","title":"Summary","text":"<p>By following these steps: 1.  Creating your custom Python code in a dedicated directory. 2.  Specifying the path to this directory using <code>custom_modules_path</code>. 3.  Referencing your custom classes via their full Python path in the YAML configuration.</p> <p>You can seamlessly integrate your own modules into the TerraTorch framework. </p>"},{"location":"tutorials/basic_inference_burn_scars/","title":"Performing an inference task with TerraTorch CLI: the Burn Scars test case","text":""},{"location":"tutorials/basic_inference_burn_scars/#step-1-download-the-test-case-from-huggingface","title":"Step 1: Download the test case from HuggingFace","text":"<p>We will use the burn scars identification test case, in which we are interested in estimating the area affected by wildfires using a finetuned model (Prithvi-EO backbone + CNN decoder). To download the complete example, do: <pre><code>git clone https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-300M-BurnScars/\n</code></pre></p>"},{"location":"tutorials/basic_inference_burn_scars/#step-2-run-the-default-inference-case","title":"Step 2: Run the default inference case","text":"<p>The example you downloaded already contains some sample images to be used as input, so you just need to go to the local repository and create a directory to save the outputs: <pre><code>cd Prithvi-EO-2.0-300M-BurnScars\nmkdir outputs\n</code></pre> and to execute a command line like: <pre><code>terratorch predict -c burn_scars_config.yaml --predict_output_dir outputs/ --data.init_args.predict_data_root examples/ --ckpt_path Prithvi_EO_V2_300M_BurnScars.pt\n</code></pre> You will see the outputs being saved in the <code>outputs</code> directory. </p>"},{"location":"tutorials/basic_inference_burn_scars/#input-image-rgb-components","title":"Input image (RGB components)","text":""},{"location":"tutorials/basic_inference_burn_scars/#predicted-mask","title":"Predicted mask","text":"<p>}</p>"},{"location":"tutorials/basic_inference_burn_scars/#more-examples","title":"More examples","text":"<p>For some examples of training using the existing tasks, check out the following pages on our github repo:</p>"},{"location":"tutorials/basic_inference_burn_scars/#from-config-files","title":"From config files","text":"<p>Under <code>examples/confs</code></p> <ul> <li> <p>Flood Segmentation with ViT: <code>sen1floods11_vit.yaml</code></p> </li> <li> <p>Flood Segmentation with ViT and an SMP head: <code>sen1floods11_vit_smp.yaml</code></p> </li> <li> <p>Flood Segmentation with ViT and an MMSeg head: <code>sen1floods11_vit_mmseg.yaml</code></p> </li> <li> <p>Multitemporal Crop Segmentation: <code>multitemporal_crop.yaml</code></p> </li> <li> <p>Burn Scar Segmentation: <code>burn_scars.yaml</code></p> </li> <li> <p>Scene Classification: <code>eurosat.yaml</code></p> </li> </ul> <p>External examples available in Prithvi-EO-2.0</p> <ul> <li>Carbon Flux</li> <li>Landslide</li> <li>Multitemporal Crop</li> </ul>"},{"location":"tutorials/burn_scars_finetuning/","title":"Creating a finetuning workload with the script interface","text":"<p>This tutorial does not intend to create an accurate finetuned example (we are running for a single epoch!), but to describe step-by-step how to instantiate and run this kind of task. </p> <pre><code>from lightning.pytorch import Trainer\nimport terratorch\nimport albumentations\nfrom albumentations.pytorch import ToTensorV2\nfrom terratorch.models import EncoderDecoderFactory\nfrom terratorch.models.necks import SelectIndices, LearnedInterpolateToPyramidal, ReshapeTokensToImage\nfrom terratorch.models.decoders import UNetDecoder\nfrom terratorch.datasets import HLSBands\nfrom terratorch.datamodules import GenericNonGeoSegmentationDataModule\nfrom terratorch.tasks import SemanticSegmentationTask\n</code></pre>"},{"location":"tutorials/burn_scars_finetuning/#defining-fundamental-parameters","title":"Defining fundamental parameters:","text":"<ul> <li><code>lr</code> - learning rate.</li> <li><code>accelerator</code> - The kind of device in which the model will be executed. It is usually <code>gpu</code> or <code>cpu</code>. If we set it as <code>auto</code>, Lightning will select the most appropiate available device.</li> <li><code>max_epochs</code> - The maximum number of epochs used to train the model. </li> </ul> <pre><code>lr = 1e-4\naccelerator = \"auto\"\nmax_epochs = 1\n</code></pre>"},{"location":"tutorials/burn_scars_finetuning/#next-we-will-instantiate-the-datamodule-the-object-we-will-use-to-load-the-files-from-disk-to-memory","title":"Next, we will instantiate the datamodule, the object we will use to load the files from disk to memory.","text":"<pre><code>datamodule = GenericNonGeoSegmentationDataModule(\n    batch_size = 2,\n    num_workers = 8,\n    dataset_bands = [HLSBands.BLUE, HLSBands.GREEN, HLSBands.RED, HLSBands.NIR_NARROW, HLSBands.SWIR_1, HLSBands.SWIR_2],\n    output_bands = [HLSBands.BLUE, HLSBands.GREEN, HLSBands.RED, HLSBands.NIR_NARROW, HLSBands.SWIR_1, HLSBands.SWIR_2],\n    rgb_indices = [2, 1, 0],\n    means = [\n          0.033349706741586264,\n          0.05701185520536176,\n          0.05889748132001316,\n          0.2323245113436119,\n          0.1972854853760658,\n          0.11944914225186566,\n    ],\n    stds = [\n          0.02269135568823774,\n          0.026807560223070237,\n          0.04004109844362779,\n          0.07791732423672691,\n          0.08708738838140137,\n          0.07241979477437814,\n    ],\n    train_data_root = \"../burn_scars/hls_burn_scars/training\",\n    val_data_root = \"../burn_scars/hls_burn_scars/validation\",\n    test_data_root = \"../burn_scars/hls_burn_scars/validation\",\n    img_grep = \"*_merged.tif\",\n    label_grep = \"*.mask.tif\",\n    num_classes = 2,\n    train_transform = [albumentations.D4(), ToTensorV2()],\n    test_transform = [ToTensorV2()],\n    no_data_replace = 0,\n    no_label_replace =  -1,\n)\n</code></pre>"},{"location":"tutorials/burn_scars_finetuning/#a-dictionary-containing-all-the-arguments-necessary-to-instantiate-a-complete-backbone-neck-decoder-head-which-will-be-passed-to-the-task-object","title":"A dictionary containing all the arguments necessary to instantiate a complete <code>backbone-neck-decoder-head</code>, which will be passed to the task object.","text":"<pre><code>model_args = dict(\n  backbone=\"prithvi_eo_v2_300\",\n  backbone_pretrained=True,\n  backbone_num_frames=1,\n  num_classes = 2,\n  backbone_bands=[\n      \"BLUE\",\n      \"GREEN\",\n      \"RED\",\n      \"NIR_NARROW\",\n      \"SWIR_1\",\n      \"WIR_2\",\n  ],\n  decoder = \"UNetDecoder\",\n  decoder_channels = [512, 256, 128, 64],\n  necks=[{\"name\": \"SelectIndices\", \"indices\": [5, 11, 17, 23]},\n         {\"name\": \"ReshapeTokensToImage\"},\n         {\"name\": \"LearnedInterpolateToPyramidal\"}],\n  head_dropout=0.1\n)\n</code></pre>"},{"location":"tutorials/burn_scars_finetuning/#creating-the-task-object-which-will-be-used-to-properly-define-how-the-model-will-be-trained-and-used-after-it","title":"Creating the <code>task</code> object, which will be used to properly define how the model will be trained and used after it.","text":"<pre><code>task = SemanticSegmentationTask(\n    model_args,\n    \"EncoderDecoderFactory\",\n    loss=\"ce\",\n    lr=lr,\n    ignore_index=-1,\n    optimizer=\"AdamW\",\n    optimizer_hparams={\"weight_decay\": 0.05},\n    freeze_backbone = False,\n    plot_on_val = False,\n    class_names = [\"Not burned\", \"Burn scar\"],\n)\n</code></pre>"},{"location":"tutorials/burn_scars_finetuning/#the-object-trainer-manages-all-the-training-process-it-can-be-interpreted-as-an-improved-optimization-loop-in-which-parallelism-and-checkpointing-are-transparently-managed-by-the-system","title":"The object <code>Trainer</code> manages all the training process. It can be interpreted as an improved optimization loop, in which parallelism and checkpointing are transparently managed by the system.","text":"<pre><code>trainer = Trainer(\n    accelerator=accelerator,\n    max_epochs=max_epochs,\n)\n</code></pre>"},{"location":"tutorials/burn_scars_finetuning/#executing-the-training","title":"Executing the training.","text":"<pre><code>trainer.fit(model=task, datamodule=datamodule)\n</code></pre>"},{"location":"tutorials/burn_scars_finetuning/#testing-the-trained-model-extracting-metrics","title":"Testing the trained model (extracting metrics).","text":"<pre><code>trainer.test(dataloaders=datamodule)\n</code></pre> <p>The metrics output:</p> <pre><code>    [{'test/loss': 0.2669268250465393,\n      'test/Multiclass_Accuracy': 0.9274423718452454,\n      'test/multiclassaccuracy_Not burned': 0.9267654418945312,\n      'test/multiclassaccuracy_Burn scar': 0.9340785145759583,\n      'test/Multiclass_F1_Score': 0.9274423718452454,\n      'test/Multiclass_Jaccard_Index': 0.7321492433547974,\n      'test/multiclassjaccardindex_Not burned': 0.9205750226974487,\n      'test/multiclassjaccardindex_Burn scar': 0.5437235236167908,\n      'test/Multiclass_Jaccard_Index_Micro': 0.8647016882896423}]\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/","title":"Performing a simple inference task using the TerraTorch's script interface.","text":""},{"location":"tutorials/burn_scars_inference_simplified/#direct-inference-or-full-image-inference","title":"Direct inference (or full image inference).","text":"<pre><code>import argparse\nimport os\nfrom typing import List, Union\nimport re\nimport datetime\nimport numpy as np\nimport rasterio\nimport torch\nimport rioxarray\nimport yaml\nfrom einops import rearrange\nfrom terratorch.cli_tools import LightningInferenceModel\nfrom terratorch.utils import view_api\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#the-directory-in-which-we-will-save-the-model-output","title":"The directory in which we will save the model output.","text":"<pre><code>output_dir = \"inference_output\"\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#the-path-to-the-configuration-yaml-file","title":"The path to the configuration (YAML) file.","text":"<pre><code>config_file = \"burn_scars_config.yaml\"\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#the-path-to-the-local-checkpoint-a-file-storing-the-model-weights","title":"The path to the local checkpoint (a file storing the model weights).","text":"<pre><code>checkpoint = \"checkpoints/Prithvi_EO_V2_300M_BurnScars.pt\"\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#the-path-for-the-directory-containing-the-input-images","title":"The path for the directory containing the input images.","text":"<pre><code>input_dir = \"data/examples/\"\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#an-image-chosen-to-be-used-in-the-single-file-inference","title":"An image chosen to be used in the single-file inference.","text":"<pre><code>example_file = \"data/examples/subsetted_512x512_HLS.S30.T10SEH.2018190.v1.4_merged.tif\"\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#a-list-indicating-the-bands-contained-in-the-input-files","title":"A list indicating the bands contained in the input files.","text":"<pre><code>predict_dataset_bands=[\n      \"BLUE\",\n      \"GREEN\",\n      \"RED\",\n      \"NIR_NARROW\",\n      \"SWIR_1\",\n      \"SWIR_2\",\n  ]\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#a-subset-of-the-dataset-bands-to-be-used-as-input-for-the-model","title":"A subset of the dataset bands to be used as input for the model.","text":"<pre><code>predict_output_bands = predict_dataset_bands\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#creating-a-directory-to-store-the-output-when-it-does-not-exist","title":"Creating a directory to store the output (when it does not exist).","text":"<pre><code>os.makedirs(output_dir, exist_ok=True)\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#instantiating-the-model-from-the-config-file-and-the-others-arguments-defined-previously","title":"Instantiating the model from the config file and the others arguments defined previously.","text":"<pre><code>lightning_model = LightningInferenceModel.from_config(config_file, checkpoint, predict_dataset_bands, predict_output_bands)\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#performing-the-inference-for-a-single-file-the-output-is-a-tensor-torchtensor","title":"Performing the inference for a single file. The output is a tensor (<code>torch.Tensor</code>).","text":"<pre><code>prediction = lightning_model.inference(example_file)\nprediction\n</code></pre> <pre><code>    tensor([[1, 1, 1,  ..., 0, 0, 0],\n            [1, 1, 1,  ..., 0, 0, 0],\n            [1, 1, 1,  ..., 0, 0, 0],\n            ...,\n            [0, 0, 0,  ..., 1, 1, 1],\n            [0, 0, 0,  ..., 1, 1, 1],\n            [0, 0, 0,  ..., 1, 1, 1]])\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#visualizing-the-input-image","title":"Visualizing the input image.","text":"<p>From the file object <code>fp</code> we select just the bands corresponding to RGB (indexes que correspondem aos \u00edndices 2, 1 and 0 of the TIFF file) for sake of visualization. Notice that we added a shift to white (<code>fp[[2,1,0]]+0.20</code>) in order to lighten the image. </p> <pre><code>import rioxarray\nfp = rioxarray.open_rasterio(example_file)\n(fp[[2,1,0]]+0.20).plot.imshow(rgb=\"band\")\n</code></pre> <p></p> <p>Visualizing the output image. </p> <pre><code>import matplotlib.pyplot as plt\n(fp[[2,1,0]] + 0.10 + 0.5*np.stack(3*[prediction], axis=0)).plot.imshow(rgb=\"band\")\n</code></pre> <p></p> <p>We also can perform inference for an entire directory of images by using the </p> <pre><code>predictions, file_names = lightning_model.inference_on_dir(input_dir)\n</code></pre> <p>This operation will return two lists, one containing predictions and another with the names of the corresponding input files. </p> <pre><code>for pred, input_file in zip(predictions, file_names):\n    fp = rioxarray.open_rasterio(input_file)              \n    f, ax = plt.subplots(1,2, figsize=(14,6))\n    (fp[[2,1,0]]+0.10).plot.imshow(rgb=\"band\", ax=ax[0])\n    (fp[[2,1,0]] + 0.10 + 0.5*np.stack(3*[pred], axis=0)).plot.imshow(rgb=\"band\", ax=ax[1])\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"tutorials/burn_scars_inference_simplified/#tiled-inference","title":"Tiled Inference","text":"<p>Now let's try an alternative form of inference - tiled inference. This type of inference is useful when the GPU (or the RAM associated with the CPU, if applicable) is insufficient to allocate all the information needed to run the model (basic libraries, model and data), because instead of applying the model to the whole image, it divides it into small rectangles, the dimensions of which are defined by the user, applies the model separately and then reconstructs the output figure. To perform this type of inference, we will use the file below. </p> <pre><code>config_file_tiled = \"burn_scars_config_tiled.yaml\"\n</code></pre>"},{"location":"tutorials/burn_scars_inference_simplified/#notice-that-the-content-is-identical-to-the-other-yaml-file-but-the-addition-of-the-subfield","title":"Notice that the content is identical to the other YAML file, but the addition of the subfield:","text":"<p><pre><code>    tiled_inference_parameters:\n      h_crop: 128\n      h_stride: 64\n      w_crop: 128\n      w_stride: 64\n      average_patches: true\n</code></pre> to the variables sent to the field <code>model</code>. The variables containing the suffix <code>_crop</code> refer to the dimensions of the tiles while those ones with the suffix <code>_stride</code> control the distance between them (the tiles can overlap).  </p> <pre><code>lightning_model = LightningInferenceModel.from_config(config_file_tiled, checkpoint, predict_dataset_bands, predict_output_bands)\n</code></pre> <pre><code>predictions, file_names = lightning_model.inference_on_dir(input_dir)\n</code></pre> <pre><code>for pred, input_file in zip(predictions, file_names):\n    fp = rioxarray.open_rasterio(input_file)\n    f, ax = plt.subplots(1,2, figsize=(14,6))\n    (fp[[2,1,0]]+0.10).plot.imshow(rgb=\"band\", ax=ax[0])\n    (fp[[2,1,0]] + 0.10 + 0.5*np.stack(3*[pred], axis=0)).plot.imshow(rgb=\"band\", ax=ax[1])\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"tutorials/the_yaml_config/","title":"The YAML configuration file: an overview","text":"<p>If you are using the command-line interface (CLI) to run jobs using TerraTorch, so you must became familiar with YAML, the format used to configure all the workflow within the toolkit. Writing a YAML file is very similar to coding, because even if you are not direclty handling the classes and others structures defined inside a codebase, you need to know how they work, their input argments and their position in the pipeline. In this way, we could call it a \"low-code\" task. The YAML file used for TerraTorch has an almost closed format, since there are a few fixed fields that must be filled with limited sets of classes, which makes easier for new users to get a pre-existing YAML file and adapt it to their own purposes. </p> <p>In the next sections, we describe each field of a YAML file used for Earth Observation Foundation Models (EOFM) and try to make it clearer for a new user. However, we will not go into detail, since the complementary documentation (Lightning, PyTorch, ...) must fill this gap. The example can be downloaded here. </p>"},{"location":"tutorials/the_yaml_config/#trainer","title":"Trainer","text":"<p>In the section called <code>trainer</code> are defined all the arguments that must be directly sent to the Lightning Trainer object. If you need a deeper explantion about this object, check the Lightning's documentation. In the first lines we have:</p> <p><pre><code>trainer:\n  accelerator: cpu\n  strategy: auto\n  devices: auto\n  num_nodes: 1\n  precision: 16-mixed\n</code></pre> In which:</p> <ul> <li><code>accelerator</code> refers to the kind of device is being used to run the experiment. We are usually more interested in <code>cpu</code> and <code>gpu</code>, but if you set <code>auto</code>, it will automaticaly select allocate the GPU is that is availble or otherwise run on CPU.</li> <li><code>strategy</code> is related to the kind of parallelism is available. As we have usually ran the experiments using a single device for finetuning or inference, we do not care about it and choose the option <code>auto</code> by default. </li> <li><code>devices</code> indicates the list of available devices to use for the experiment. Leave it as <code>auto</code> if you are running with a single device. </li> <li><code>num_nodes</code> is self-explanatory. We have mostly tested TerraTorch for a single-node jobs, so, it is better to set it as <code>1</code> for now. </li> <li><code>precision</code> is the kind of precision used for your model. <code>16-mixed</code> have been an usual choice. </li> </ul> <p>Just below this initial stage, we have <code>logger</code>: <pre><code>  logger:\n    class_path: TensorBoardLogger\n    init_args:\n      save_dir: tests/\n      name: all_ecos_random\n</code></pre> In this field we define the configuration for logging the model state. In this example we are using Tensorboard, and saving all the logs in a directory <code>tests/all_ecos_random</code>.  Others frameworks, as MLFlow are also supported. Check the Lightning documentation about logging for a more complete description. </p> <p>The <code>callbacks</code> field: <pre><code>  callbacks:\n    - class_path: RichProgressBar\n    - class_path: LearningRateMonitor\n      init_args:\n        logging_interval: epoch\n    - class_path: EarlyStopping\n      init_args:\n        monitor: val/loss\n        patience: 100\n</code></pre> Represents a list of operations that can be invoked with determined frequency. The user is free to add others operations from Lightning or custom ones. In the current config we are basically defining: a progress bar to be printed during the model training/validation and a learning rate monitor, determined to call early-stopping when the model shows signals of overfitting.  The rest of the arguments are: <pre><code>  max_epochs: 1\n  check_val_every_n_epoch: 1\n  log_every_n_steps: 20\n  enable_checkpointing: true\n  default_root_dir: tests/\n</code></pre></p> <ul> <li><code>max_epochs</code>: the maximum number of epochs to train the model. Notice that, if you are using early-stopping,     maybe the training will finish before achieving this number. </li> <li><code>check_val_every_n_epoch</code>: the frequency to evaluate the model using the validation dataset. The validation     is important to verify if the model is tending to overfit and  can be used, for example, to define when update the learning rate, or to invoke the early-stopping. </li> <li><code>enable_checkpointing</code>: it enables the checkpointing, the action of periodically saving the state of the     model to a file. </li> <li><code>default_root_dir</code>: the directory used to save the model checkpoints. </li> </ul>"},{"location":"tutorials/the_yaml_config/#datamodule","title":"Datamodule","text":"<p>In this section, we start direclty handling TerraTorch's built-in structures. The field <code>data</code> is expected to receive a generic datamodule or any other datamodule compatible with Lightning Datamodules, as those defined in our collection of datamodules. </p> <p>In the beginning of the field we have: <pre><code>data:\n  class_path: GenericNonGeoPixelwiseRegressionDataModule\n  init_args:\n</code></pre> It means that we have chosen the generic regression datamodule and we will pass all its required arguments below <code>init_args</code> and with one new level of identation. The best practice here is to check the documentation of the datamodule class you are using (in our case, here) and verify all the arguments it expects to receive ant then to fill the lines with <code>&lt;argument_name&gt;: &lt;argument_value&gt;</code>.  As the TerraTorch and Lightning modules were already imported in the CLI script (<code>terratorch/cli_tools.py</code>), you do not need to provide the complete paths for them. Otherwise, if you are using a datamodule defined in an external package, indicate the path to import the model, as <code>package.datamodules.SomeDatamodule</code>. </p>"},{"location":"tutorials/the_yaml_config/#model","title":"Model","text":"<p>The field <code>model</code> is, in fact, the configuration for <code>task + model</code>:  <pre><code>model:\n  class_path: terratorch.tasks.PixelwiseRegressionTask\n  init_args:\n    model_args:\n      decoder: UperNetDecoder\n      pretrained: false\n      backbone: prithvi_eo_v2_600\n      backbone_drop_path_rate: 0.3\n      backbone_window_size: 8\n      decoder_channels: 64\n      num_frames: 1\n      in_channels: 6\n      bands:\n        - BLUE\n        - GREEN\n        - RED\n        - NIR_NARROW\n        - SWIR_1\n        - SWIR_2\n      head_dropout: 0.5708022831486758\n      head_final_act: torch.nn.ReLU\n      head_learned_upscale_layers: 2\n    loss: rmse\n    ignore_index: -1\n    freeze_backbone: true\n    freeze_decoder: false\n    model_factory: PrithviModelFactory\n    tiled_inference_parameters:\n       h_crop: 224\n       h_stride: 192\n       w_crop: 224\n       w_stride: 192\n       average_patches: true\n</code></pre> Notice that there is a field <code>model_args</code>, which it is intended to receive all the necessary configuration to instantiate the model itself, that means, the structure <code>backbone + decoder + head</code>. Inside <code>model_args</code>, it is possible do define which arguments will be sent to each component by including a prefix to the argument names, as <code>backbone_&lt;argument&gt;</code> or <code>decoder_&lt;other_argument&gt;</code>. Alternatively, it is possible to pass the arguments using dictionaries <code>backbone_kwargs</code>, <code>decoder_kwargs</code> and <code>head_kwargs</code>. The same recommendation made for the <code>data</code> field is repeated here, check the documentation of the task and model classes (backbones, decoders and heads) you are using in order to define which arguments to write for each subfield of <code>model</code>. </p>"},{"location":"tutorials/the_yaml_config/#optimizer-and-learning-rate-scheduler","title":"Optimizer and Learning Rate Scheduler","text":"<p>The last two fields of out example are the configuration of the optimizer and the lr scheduler. Those fields are mostly self-explanatory for users already familiar with machine learning:</p> <p><pre><code>optimizer:\n  class_path: torch.optim.AdamW\n  init_args:\n    lr: 0.00013524680528283027\n    weight_decay: 0.047782217873995426\nlr_scheduler:\n  class_path: ReduceLROnPlateau\n  init_args:\n    monitor: val/loss\n</code></pre> Check the PyTorch documentation about optimization to understand them more deeply. </p>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/","title":"Using a dedicated datamodule to perform inference: the crop classification example.","text":""},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#to-run-this-example-the-following-packages-are-necessary","title":"To run this example, the following packages are necessary.","text":"<pre><code>!pip install terratorch gdown tensorboard &gt;&amp; install.log\n</code></pre> <pre><code>import os\nimport sys\nimport torch\nimport gdown\nimport terratorch\nimport albumentations\nimport lightning.pytorch as pl\nimport matplotlib.pyplot as plt\nfrom terratorch.datamodules import MultiTemporalCropClassificationDataModule\nimport warnings\n</code></pre>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#downloading-a-random-subset-of-the-required-dataset-1-gb","title":"Downloading a random subset of the required dataset (~1 GB).","text":"<pre><code>if not os.path.isfile('multi-temporal-crop-classification-subset.tar.gz'):\n    !gdown 1LL6thkuKA0kVyMI39PxgsrJ1FJJDV7-u\n\nif not os.path.isdir('multi-temporal-crop-classification-subset/'):\n    !tar -xzvf multi-temporal-crop-classification-subset.tar.gz\n\ndataset_path = \"multi-temporal-crop-classification-subset\"\n</code></pre>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#instantiating-the-corresponding-datamodule","title":"Instantiating the corresponding datamodule.","text":"<pre><code>datamodule = MultiTemporalCropClassificationDataModule(\n    batch_size=8,\n    num_workers=2,\n    data_root=dataset_path,\n    train_transform=[\n        terratorch.datasets.transforms.FlattenTemporalIntoChannels(),  # Required for temporal data\n        albumentations.D4(), # Random flips and rotation\n        albumentations.pytorch.transforms.ToTensorV2(),\n        terratorch.datasets.transforms.UnflattenTemporalFromChannels(n_timesteps=3),\n    ],\n    val_transform=None,  # Using ToTensor() by default\n    test_transform=None,\n    expand_temporal_dimension=True,\n    use_metadata=False, # The crop dataset has metadata for location and time\n    reduce_zero_label=True,\n)\n\n# Setup train and val datasets\ndatamodule.setup(\"predict\")\n</code></pre>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#visualizing-a-few-samples","title":"Visualizing a few samples.","text":"<pre><code>for i in range(5):\n    datamodule.predict_dataset.plot(datamodule.predict_dataset[i])\n</code></pre>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#downloading-the-best-pretrained-checkpoint","title":"Downloading the best pretrained checkpoint.","text":"<pre><code>best_ckpt_100_epoch_path = \"multicrop_best-epoch=76.ckpt\"\n\nif not os.path.isfile(best_ckpt_100_epoch_path):\n    gdown.download(\"https://drive.google.com/uc?id=1o1Hzd4yyiKyYdzfotQlEOeGTjsM8cHSw\")\n</code></pre>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#instantiating-the-lightning-trainer","title":"Instantiating the Lightning Trainer.","text":"<pre><code>checkpoint_callback = pl.callbacks.ModelCheckpoint(\n    dirpath=\"output/multicrop/checkpoints/\",\n    mode=\"max\",\n    monitor=\"val/Multiclass_Jaccard_Index\", # Variable to monitor\n    filename=\"best-{epoch:02d}\",\n)\ntrainer = pl.Trainer(\n    accelerator=\"auto\",\n    strategy=\"auto\",\n    devices=1, # Lightning multi-gpu often fails in notebooks\n    precision='bf16-mixed',  # Speed up training\n    num_nodes=1,\n    logger=True, # Uses TensorBoard by default\n    max_epochs=1, # For demos\n    log_every_n_steps=5,\n    enable_checkpointing=True,\n    callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n    default_root_dir=\"output/multicrop\",\n)\n</code></pre>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#instantiating-the-task-to-handle-the-model","title":"Instantiating the task to handle the model.","text":"<pre><code>model = terratorch.tasks.SemanticSegmentationTask(\n    model_factory=\"EncoderDecoderFactory\",\n    model_args={\n        # Backbone\n        \"backbone\": \"prithvi_eo_v2_300\", \n        \"backbone_pretrained\": True,\n        \"backbone_num_frames\": 3,\n        \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_NARROW\", \"SWIR_1\", \"SWIR_2\"],\n        \"backbone_coords_encoding\": [], # use [\"time\", \"location\"] for time and location metadata\n\n        # Necks \n        \"necks\": [\n            {\n                \"name\": \"SelectIndices\",\n                \"indices\": [5, 11, 17, 23] \n            },\n            {\n                \"name\": \"ReshapeTokensToImage\",\n                \"effective_time_dim\": 3\n            },\n            {\"name\": \"LearnedInterpolateToPyramidal\"},            \n        ],\n\n        # Decoder\n        \"decoder\": \"UNetDecoder\",\n        \"decoder_channels\": [512, 256, 128, 64],\n\n        # Head\n        \"head_dropout\": 0.1,\n        \"num_classes\": 13,\n    },\n\n    loss=\"ce\",\n    lr=1e-4,\n    optimizer=\"AdamW\",\n    ignore_index=-1,\n    freeze_backbone=True,  \n    freeze_decoder=False,\n    plot_on_val=True,\n\n)\n</code></pre>"},{"location":"tutorials/using_datamodule_multitemporalclassificationModule/#predicting-for-some-samples-in-the-prediction-dataset","title":"Predicting for some samples in the prediction dataset.","text":"<pre><code>preds = trainer.predict(model, datamodule=datamodule, ckpt_path=best_ckpt_100_epoch_path)\n# get data \ndata_loader = trainer.predict_dataloaders\nbatch = next(iter(data_loader))\n\nBATCH_SIZE = 8\nfor i in range(BATCH_SIZE):\n\n    sample = {key: batch[key][i] for key in batch}\n    sample[\"prediction\"] = preds[0][0][i].cpu().numpy()\n\n    datamodule.predict_dataset.plot(sample)\n</code></pre>"}]}