{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to TerraTorch # Overview # The purpose of this package is to build a flexible fine-tuning framework for Geospatial Foundation Models (GFMs) based on TorchGeo and Lightning which can be employed at different abstraction levels. It currently supports models from the Prithvi and Granite series, and also have been tested with others models available on HuggingFace. This library provides: All the functionality in TorchGeo. Easy access to Prithvi, timm and smp backbones. Flexible trainers for Image Segmentation, Pixel Wise Regression and Classification (more in progress). Launching of fine-tuning tasks through powerful configuration files. A good starting place is familiarization with PyTorch Lightning , which this project is built on. TorchGeo is also an important complementary reference. Check out the architecture overview for a general description about how TerraTorch is organized. Quick start # To get started, check out the quick start guide License # TerraTorch is distributed under the terms of License Apache 2.0, see here for more details.","title":"Welcome to TerraTorch"},{"location":"#welcome-to-terratorch","text":"","title":"Welcome to TerraTorch"},{"location":"#overview","text":"The purpose of this package is to build a flexible fine-tuning framework for Geospatial Foundation Models (GFMs) based on TorchGeo and Lightning which can be employed at different abstraction levels. It currently supports models from the Prithvi and Granite series, and also have been tested with others models available on HuggingFace. This library provides: All the functionality in TorchGeo. Easy access to Prithvi, timm and smp backbones. Flexible trainers for Image Segmentation, Pixel Wise Regression and Classification (more in progress). Launching of fine-tuning tasks through powerful configuration files. A good starting place is familiarization with PyTorch Lightning , which this project is built on. TorchGeo is also an important complementary reference. Check out the architecture overview for a general description about how TerraTorch is organized.","title":"Overview"},{"location":"#quick-start","text":"To get started, check out the quick start guide","title":"Quick start"},{"location":"#license","text":"TerraTorch is distributed under the terms of License Apache 2.0, see here for more details.","title":"License"},{"location":"architecture/","text":"Architecture Overview # The main goal of the design is to extend TorchGeo's existing tasks to be able to handle Prithvi backbones with appropriate decoders and heads. At the same time, we wish to keep the existing TorchGeo functionality intact so it can be leveraged with pretrained models that are already included. We achieve this by making new tasks that accept model factory classes, containing a build_model method. This strategy in principle allows arbitrary models to be trained for these tasks, given they respect some reasonable minimal interface. Together with this, we provide the EncoderDecoderFactory , which should enable users to plug together different Encoders and Decoders, with the aid of Necks for intermediate operations. Additionally, we extend TorchGeo with generic datasets and datamodules which can be defined at runtime, rather than requiring classes to be defined beforehand. The glue that holds everything together is LightningCLI , allowing the model, datamodule and Lightning Trainer to be instantiated from a config file or from the CLI. We make extensive use of for training and inference. Initial reading for a full understanding of the platform includes: Familiarity with PyTorch Lightning Familiarity with TorchGeo Familiarity with LightningCLI Tasks # Tasks are the main coordinators for training and inference for specific tasks. They are LightningModules that contain a model and abstract away all the logic for training steps, metric computation and inference. One of the most important design decisions was delegating the model construction to a model factory. This has a few advantages: Avoids code repetition among tasks - different tasks can use the same factory Prefers composition over inheritance Allows new models to be easily added by introducing new factories Models are expected to be torch.nn.Module and implement the Model interface, providing: freeze_encoder() freeze_decoder() forward() Additionally, the forward() method is expected to return an object of type ModelOutput , containing the main head's output, as well as any additional auxiliary outputs. The names of these auxiliary heads are matched with the names of the provided auxiliary losses. Models # Models constructed by the EncoderDecoderFactory have an internal structure explicitly divided into backbones, necks, decoders and heads. This structure is provided by the PixelWiseModel and ScalarOutputModel classes. However, as long as models implement the Model interface, and return ModelOutput in their forward method, they can take on any structure. terratorch.models.pixel_wise_model.PixelWiseModel # Bases: Model , SegmentationModel Model that encapsulates encoder and decoder and heads Expects decoder to have a \"forward_features\" method, an embed_dims property and optionally a \"prepare_features_for_image_model\" method. Source code in terratorch/models/pixel_wise_model.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class PixelWiseModel ( Model , SegmentationModel ): \"\"\"Model that encapsulates encoder and decoder and heads Expects decoder to have a \"forward_features\" method, an embed_dims property and optionally a \"prepare_features_for_image_model\" method. \"\"\" def __init__ ( self , task : str , encoder : nn . Module , decoder : nn . Module , head_kwargs : dict , patch_size : int = None , padding : str = None , decoder_includes_head : bool = False , auxiliary_heads : list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] | None = None , neck : nn . Module | None = None , rescale : bool = True , # noqa: FBT002, FBT001 ) -> None : \"\"\"Constructor Args: task (str): Task to be performed. One of segmentation or regression. encoder (nn.Module): Encoder to be used decoder (nn.Module): Decoder to be used head_kwargs (dict): Arguments to be passed at instantiation of the head. decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False. auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of AuxiliaryHeads with heads to be instantiated. Defaults to None. neck (nn.Module | None): Module applied between backbone and decoder. Defaults to None, which applies the identity. rescale (bool, optional): Rescale the output of the model if it has a different size than the ground truth. Uses bilinear interpolation. Defaults to True. \"\"\" super () . __init__ () self . task = task self . encoder = encoder self . decoder = decoder self . head = ( self . _get_head ( task , decoder . out_channels , head_kwargs ) if not decoder_includes_head else nn . Identity () ) if auxiliary_heads is not None : aux_heads = {} for aux_head_to_be_instantiated in auxiliary_heads : aux_head : nn . Module = self . _get_head ( task , aux_head_to_be_instantiated . decoder . out_channels , head_kwargs ) if not aux_head_to_be_instantiated . decoder_includes_head else nn . Identity () aux_head = nn . Sequential ( aux_head_to_be_instantiated . decoder , aux_head ) aux_heads [ aux_head_to_be_instantiated . name ] = aux_head else : aux_heads = {} self . aux_heads = nn . ModuleDict ( aux_heads ) self . neck = neck self . rescale = rescale self . patch_size = patch_size self . padding = padding def freeze_encoder ( self ): freeze_module ( self . encoder ) def freeze_decoder ( self ): freeze_module ( self . decoder ) def freeze_head ( self ): freeze_module ( self . head ) @staticmethod def _check_for_single_channel_and_squeeze ( x ): if x . shape [ 1 ] == 1 : x = x . squeeze ( 1 ) return x def forward ( self , x : torch . Tensor , ** kwargs ) -> ModelOutput : \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\" def _get_size ( x ): if isinstance ( x , torch . Tensor ): return x . shape [ - 2 :] elif isinstance ( x , dict ): # Multimodal input in passed as dict (Assuming first modality to be an image) return list ( x . values ())[ 0 ] . shape [ - 2 :] elif hasattr ( kwargs , 'image_size' ): return kwargs [ 'image_size' ] else : ValueError ( 'Could not infer image shape.' ) image_size = _get_size ( x ) if isinstance ( x , torch . Tensor ) and self . patch_size : # Only works for single image modalities x = pad_images ( x , self . patch_size , self . padding ) input_size = _get_size ( x ) features = self . encoder ( x , ** kwargs ) # only for backwards compatibility with pre-neck times. if self . neck : prepare = self . neck else : # for backwards compatibility, if this is defined in the encoder, use it prepare = getattr ( self . encoder , \"prepare_features_for_image_model\" , lambda x : x ) print ( f \"neck: { self . neck } \" ) features = prepare ( features ) print ([ f . shape for f in features ]) decoder_output = self . decoder ([ f . clone () for f in features ]) mask = self . head ( decoder_output ) if self . rescale and mask . shape [ - 2 :] != input_size : mask = F . interpolate ( mask , size = input_size , mode = \"bilinear\" ) mask = self . _check_for_single_channel_and_squeeze ( mask ) mask = mask [ ... , : image_size [ 0 ], : image_size [ 1 ]] aux_outputs = {} for name , decoder in self . aux_heads . items (): aux_output = decoder ([ f . clone () for f in features ]) if self . rescale and aux_output . shape [ - 2 :] != input_size : aux_output = F . interpolate ( aux_output , size = input_size , mode = \"bilinear\" ) aux_output = self . _check_for_single_channel_and_squeeze ( aux_output ) aux_output = aux_output [ ... , : image_size [ 0 ], : image_size [ 1 ]] aux_outputs [ name ] = aux_output return ModelOutput ( output = mask , auxiliary_heads = aux_outputs ) def _get_head ( self , task : str , input_embed_dim : int , head_kwargs ): if task == \"segmentation\" : if \"num_classes\" not in head_kwargs : msg = \"num_classes must be defined for segmentation task\" raise Exception ( msg ) return SegmentationHead ( input_embed_dim , ** head_kwargs ) if task == \"regression\" : return RegressionHead ( input_embed_dim , ** head_kwargs ) msg = \"Task must be one of segmentation or regression.\" raise Exception ( msg ) __init__ ( task , encoder , decoder , head_kwargs , patch_size = None , padding = None , decoder_includes_head = False , auxiliary_heads = None , neck = None , rescale = True ) # Constructor Parameters: task ( str ) \u2013 Task to be performed. One of segmentation or regression. encoder ( Module ) \u2013 Encoder to be used decoder ( Module ) \u2013 Decoder to be used head_kwargs ( dict ) \u2013 Arguments to be passed at instantiation of the head. decoder_includes_head ( bool , default: False ) \u2013 Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False. auxiliary_heads ( list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] | None , default: None ) \u2013 List of AuxiliaryHeads with heads to be instantiated. Defaults to None. neck ( Module | None , default: None ) \u2013 Module applied between backbone and decoder. Defaults to None, which applies the identity. rescale ( bool , default: True ) \u2013 Rescale the output of the model if it has a different size than the ground truth. Uses bilinear interpolation. Defaults to True. Source code in terratorch/models/pixel_wise_model.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , task : str , encoder : nn . Module , decoder : nn . Module , head_kwargs : dict , patch_size : int = None , padding : str = None , decoder_includes_head : bool = False , auxiliary_heads : list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] | None = None , neck : nn . Module | None = None , rescale : bool = True , # noqa: FBT002, FBT001 ) -> None : \"\"\"Constructor Args: task (str): Task to be performed. One of segmentation or regression. encoder (nn.Module): Encoder to be used decoder (nn.Module): Decoder to be used head_kwargs (dict): Arguments to be passed at instantiation of the head. decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False. auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of AuxiliaryHeads with heads to be instantiated. Defaults to None. neck (nn.Module | None): Module applied between backbone and decoder. Defaults to None, which applies the identity. rescale (bool, optional): Rescale the output of the model if it has a different size than the ground truth. Uses bilinear interpolation. Defaults to True. \"\"\" super () . __init__ () self . task = task self . encoder = encoder self . decoder = decoder self . head = ( self . _get_head ( task , decoder . out_channels , head_kwargs ) if not decoder_includes_head else nn . Identity () ) if auxiliary_heads is not None : aux_heads = {} for aux_head_to_be_instantiated in auxiliary_heads : aux_head : nn . Module = self . _get_head ( task , aux_head_to_be_instantiated . decoder . out_channels , head_kwargs ) if not aux_head_to_be_instantiated . decoder_includes_head else nn . Identity () aux_head = nn . Sequential ( aux_head_to_be_instantiated . decoder , aux_head ) aux_heads [ aux_head_to_be_instantiated . name ] = aux_head else : aux_heads = {} self . aux_heads = nn . ModuleDict ( aux_heads ) self . neck = neck self . rescale = rescale self . patch_size = patch_size self . padding = padding forward ( x , ** kwargs ) # Sequentially pass x through model`s encoder, decoder and heads Source code in terratorch/models/pixel_wise_model.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def forward ( self , x : torch . Tensor , ** kwargs ) -> ModelOutput : \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\" def _get_size ( x ): if isinstance ( x , torch . Tensor ): return x . shape [ - 2 :] elif isinstance ( x , dict ): # Multimodal input in passed as dict (Assuming first modality to be an image) return list ( x . values ())[ 0 ] . shape [ - 2 :] elif hasattr ( kwargs , 'image_size' ): return kwargs [ 'image_size' ] else : ValueError ( 'Could not infer image shape.' ) image_size = _get_size ( x ) if isinstance ( x , torch . Tensor ) and self . patch_size : # Only works for single image modalities x = pad_images ( x , self . patch_size , self . padding ) input_size = _get_size ( x ) features = self . encoder ( x , ** kwargs ) # only for backwards compatibility with pre-neck times. if self . neck : prepare = self . neck else : # for backwards compatibility, if this is defined in the encoder, use it prepare = getattr ( self . encoder , \"prepare_features_for_image_model\" , lambda x : x ) print ( f \"neck: { self . neck } \" ) features = prepare ( features ) print ([ f . shape for f in features ]) decoder_output = self . decoder ([ f . clone () for f in features ]) mask = self . head ( decoder_output ) if self . rescale and mask . shape [ - 2 :] != input_size : mask = F . interpolate ( mask , size = input_size , mode = \"bilinear\" ) mask = self . _check_for_single_channel_and_squeeze ( mask ) mask = mask [ ... , : image_size [ 0 ], : image_size [ 1 ]] aux_outputs = {} for name , decoder in self . aux_heads . items (): aux_output = decoder ([ f . clone () for f in features ]) if self . rescale and aux_output . shape [ - 2 :] != input_size : aux_output = F . interpolate ( aux_output , size = input_size , mode = \"bilinear\" ) aux_output = self . _check_for_single_channel_and_squeeze ( aux_output ) aux_output = aux_output [ ... , : image_size [ 0 ], : image_size [ 1 ]] aux_outputs [ name ] = aux_output return ModelOutput ( output = mask , auxiliary_heads = aux_outputs ) terratorch.models.scalar_output_model.ScalarOutputModel # Bases: Model , SegmentationModel Model that encapsulates encoder and decoder and heads for a scalar output Expects decoder to have a \"forward_features\" method, an embed_dims property and optionally a \"prepare_features_for_image_model\" method. Source code in terratorch/models/scalar_output_model.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class ScalarOutputModel ( Model , SegmentationModel ): \"\"\"Model that encapsulates encoder and decoder and heads for a scalar output Expects decoder to have a \"forward_features\" method, an embed_dims property and optionally a \"prepare_features_for_image_model\" method. \"\"\" def __init__ ( self , task : str , encoder : nn . Module , decoder : nn . Module , head_kwargs : dict , patch_size : int = None , padding : str = None , decoder_includes_head : bool = False , auxiliary_heads : list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] | None = None , neck : nn . Module | None = None , ) -> None : \"\"\"Constructor Args: task (str): Task to be performed. Must be \"classification\". encoder (nn.Module): Encoder to be used decoder (nn.Module): Decoder to be used head_kwargs (dict): Arguments to be passed at instantiation of the head. decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False. auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of AuxiliaryHeads with heads to be instantiated. Defaults to None. neck (nn.Module | None): Module applied between backbone and decoder. Defaults to None, which applies the identity. \"\"\" super () . __init__ () self . task = task self . encoder = encoder self . decoder = decoder self . head = ( self . _get_head ( task , decoder . out_channels , head_kwargs ) if not decoder_includes_head else nn . Identity () ) if auxiliary_heads is not None : aux_heads = {} for aux_head_to_be_instantiated in auxiliary_heads : aux_head : nn . Module = self . _get_head ( task , aux_head_to_be_instantiated . decoder . out_channels , head_kwargs ) if not aux_head_to_be_instantiated . decoder_includes_head else nn . Identity () aux_head = nn . Sequential ( aux_head_to_be_instantiated . decoder , aux_head ) aux_heads [ aux_head_to_be_instantiated . name ] = aux_head else : aux_heads = {} self . aux_heads = nn . ModuleDict ( aux_heads ) self . neck = neck self . patch_size = patch_size self . padding = padding def freeze_encoder ( self ): freeze_module ( self . encoder ) def freeze_decoder ( self ): freeze_module ( self . decoder ) def freeze_head ( self ): freeze_module ( self . head ) def forward ( self , x : torch . Tensor , ** kwargs ) -> ModelOutput : \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\" if isinstance ( x , torch . Tensor ) and self . patch_size : # Only works for single image modalities x = pad_images ( x , self . patch_size , self . padding ) features = self . encoder ( x , ** kwargs ) # only for backwards compatibility with pre-neck times. if self . neck : prepare = self . neck else : # for backwards compatibility, if this is defined in the encoder, use it prepare = getattr ( self . encoder , \"prepare_features_for_image_model\" , lambda x : x ) features = prepare ( features ) decoder_output = self . decoder ([ f . clone () for f in features ]) mask = self . head ( decoder_output ) aux_outputs = {} for name , decoder in self . aux_heads . items (): aux_output = decoder ([ f . clone () for f in features ]) aux_outputs [ name ] = aux_output return ModelOutput ( output = mask , auxiliary_heads = aux_outputs ) def _get_head ( self , task : str , input_embed_dim : int , head_kwargs : dict ): if task == \"classification\" : if \"num_classes\" not in head_kwargs : msg = \"num_classes must be defined for classification task\" raise Exception ( msg ) return ClassificationHead ( input_embed_dim , ** head_kwargs ) msg = \"Task must be classification.\" raise Exception ( msg ) __init__ ( task , encoder , decoder , head_kwargs , patch_size = None , padding = None , decoder_includes_head = False , auxiliary_heads = None , neck = None ) # Constructor Parameters: task ( str ) \u2013 Task to be performed. Must be \"classification\". encoder ( Module ) \u2013 Encoder to be used decoder ( Module ) \u2013 Decoder to be used head_kwargs ( dict ) \u2013 Arguments to be passed at instantiation of the head. decoder_includes_head ( bool , default: False ) \u2013 Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False. auxiliary_heads ( list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] | None , default: None ) \u2013 List of AuxiliaryHeads with heads to be instantiated. Defaults to None. neck ( Module | None , default: None ) \u2013 Module applied between backbone and decoder. Defaults to None, which applies the identity. Source code in terratorch/models/scalar_output_model.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , task : str , encoder : nn . Module , decoder : nn . Module , head_kwargs : dict , patch_size : int = None , padding : str = None , decoder_includes_head : bool = False , auxiliary_heads : list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] | None = None , neck : nn . Module | None = None , ) -> None : \"\"\"Constructor Args: task (str): Task to be performed. Must be \"classification\". encoder (nn.Module): Encoder to be used decoder (nn.Module): Decoder to be used head_kwargs (dict): Arguments to be passed at instantiation of the head. decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False. auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of AuxiliaryHeads with heads to be instantiated. Defaults to None. neck (nn.Module | None): Module applied between backbone and decoder. Defaults to None, which applies the identity. \"\"\" super () . __init__ () self . task = task self . encoder = encoder self . decoder = decoder self . head = ( self . _get_head ( task , decoder . out_channels , head_kwargs ) if not decoder_includes_head else nn . Identity () ) if auxiliary_heads is not None : aux_heads = {} for aux_head_to_be_instantiated in auxiliary_heads : aux_head : nn . Module = self . _get_head ( task , aux_head_to_be_instantiated . decoder . out_channels , head_kwargs ) if not aux_head_to_be_instantiated . decoder_includes_head else nn . Identity () aux_head = nn . Sequential ( aux_head_to_be_instantiated . decoder , aux_head ) aux_heads [ aux_head_to_be_instantiated . name ] = aux_head else : aux_heads = {} self . aux_heads = nn . ModuleDict ( aux_heads ) self . neck = neck self . patch_size = patch_size self . padding = padding forward ( x , ** kwargs ) # Sequentially pass x through model`s encoder, decoder and heads Source code in terratorch/models/scalar_output_model.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def forward ( self , x : torch . Tensor , ** kwargs ) -> ModelOutput : \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\" if isinstance ( x , torch . Tensor ) and self . patch_size : # Only works for single image modalities x = pad_images ( x , self . patch_size , self . padding ) features = self . encoder ( x , ** kwargs ) # only for backwards compatibility with pre-neck times. if self . neck : prepare = self . neck else : # for backwards compatibility, if this is defined in the encoder, use it prepare = getattr ( self . encoder , \"prepare_features_for_image_model\" , lambda x : x ) features = prepare ( features ) decoder_output = self . decoder ([ f . clone () for f in features ]) mask = self . head ( decoder_output ) aux_outputs = {} for name , decoder in self . aux_heads . items (): aux_output = decoder ([ f . clone () for f in features ]) aux_outputs [ name ] = aux_output return ModelOutput ( output = mask , auxiliary_heads = aux_outputs ) EncoderDecoderFactory # We expect this factory to be widely employed by users. With that in mind, we dive deeper into it here . Loss # For convenience, we provide a loss handler that can be used to compute the full loss (from the main head and auxiliary heads as well). terratorch.tasks.loss_handler # LossHandler # Class to help handle the computation and logging of loss Source code in terratorch/tasks/loss_handler.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class LossHandler : \"\"\"Class to help handle the computation and logging of loss\"\"\" def __init__ ( self , loss_prefix : str ) -> None : \"\"\"Constructor Args: loss_prefix (str): Prefix to be prepended to all the metrics (e.g. training). \"\"\" self . loss_prefix = loss_prefix def compute_loss ( self , model_output : ModelOutput , ground_truth : Tensor , criterion : Callable , aux_loss_weights : dict [ str , float ] | None , ) -> dict [ str , Tensor ]: \"\"\"Compute the loss for the mean decode head as well as other heads Args: model_output (ModelOutput): Output from the model ground_truth (Tensor): Tensor with labels criterion (Callable): Loss function to be applied aux_loss_weights (Union[dict[str, float], None]): Dictionary of names of model auxiliary heads and their weights Raises: Exception: If the keys in aux_loss_weights and the model output do not match, will raise an exception. Returns: dict[str, Tensor]: Dictionary of computed losses. Total loss is returned under the key \"loss\". If there are auxiliary heads, the main decode head is returned under the key \"decode_head\". All other heads are returned with the same key as their name. \"\"\" loss = self . _compute_loss ( model_output . output , ground_truth , criterion ) if not model_output . auxiliary_heads : return { \"loss\" : loss } if aux_loss_weights is None : msg = \"Auxiliary heads given with no aux_loss_weights\" raise Exception ( msg ) all_losses = {} all_losses [ \"decode_head\" ] = loss total_loss = loss . clone () # incorporate aux heads model_output_names = set ( model_output . auxiliary_heads . keys ()) aux_loss_names = set ( aux_loss_weights . keys ()) if aux_loss_names != model_output_names : msg = f \"Found difference in declared auxiliary losses and model outputs. \\n \\ Found in declared losses but not in model output: { aux_loss_names - model_output_names } . \\n \\ Found in model output but not in delcared losses: { model_output_names - aux_loss_names } \" raise Exception ( msg ) for loss_name , loss_weight in aux_loss_weights . items (): output = model_output . auxiliary_heads [ loss_name ] loss_value : Tensor = self . _compute_loss ( output , ground_truth , criterion ) all_losses [ loss_name ] = loss_value total_loss = total_loss + loss_value * loss_weight all_losses [ \"loss\" ] = total_loss return all_losses def _compute_loss ( self , y_hat : Tensor , ground_truth : Tensor , criterion : Callable ): loss : Tensor = criterion ( y_hat , ground_truth ) return loss def log_loss ( self , log_function : Callable , loss_dict : dict [ str , Tensor ] | None = None , batch_size : int | None = None ) -> None : \"\"\"Log the loss. If auxiliary heads exist, log the full loss suffix \"loss\", and then all other losses. Args: log_function (Callable): _description_ loss_dict (dict[str, Tensor], optional): _description_. Defaults to None. \"\"\" # dont alter passed dict all_losses = dict ( loss_dict ) full_loss = all_losses . pop ( \"loss\" ) log_function ( f \" { self . loss_prefix } loss\" , full_loss , sync_dist = True , batch_size = batch_size ) for loss_name , loss_value in all_losses . items (): log_function ( f \" { self . loss_prefix }{ loss_name } \" , loss_value , on_epoch = True , on_step = True , sync_dist = True , batch_size = batch_size , ) __init__ ( loss_prefix ) # Constructor Parameters: loss_prefix ( str ) \u2013 Prefix to be prepended to all the metrics (e.g. training). Source code in terratorch/tasks/loss_handler.py 11 12 13 14 15 16 17 def __init__ ( self , loss_prefix : str ) -> None : \"\"\"Constructor Args: loss_prefix (str): Prefix to be prepended to all the metrics (e.g. training). \"\"\" self . loss_prefix = loss_prefix compute_loss ( model_output , ground_truth , criterion , aux_loss_weights ) # Compute the loss for the mean decode head as well as other heads Parameters: model_output ( ModelOutput ) \u2013 Output from the model ground_truth ( Tensor ) \u2013 Tensor with labels criterion ( Callable ) \u2013 Loss function to be applied aux_loss_weights ( Union [ dict [ str , float ], None] ) \u2013 Dictionary of names of model auxiliary heads and their weights Raises: Exception \u2013 If the keys in aux_loss_weights and the model output do not match, will raise an exception. Returns: dict [ str , Tensor ] \u2013 dict[str, Tensor]: Dictionary of computed losses. Total loss is returned under the key \"loss\". If there are auxiliary heads, the main decode head is returned under the key \"decode_head\". All other heads are returned with the same key as their name. Source code in terratorch/tasks/loss_handler.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def compute_loss ( self , model_output : ModelOutput , ground_truth : Tensor , criterion : Callable , aux_loss_weights : dict [ str , float ] | None , ) -> dict [ str , Tensor ]: \"\"\"Compute the loss for the mean decode head as well as other heads Args: model_output (ModelOutput): Output from the model ground_truth (Tensor): Tensor with labels criterion (Callable): Loss function to be applied aux_loss_weights (Union[dict[str, float], None]): Dictionary of names of model auxiliary heads and their weights Raises: Exception: If the keys in aux_loss_weights and the model output do not match, will raise an exception. Returns: dict[str, Tensor]: Dictionary of computed losses. Total loss is returned under the key \"loss\". If there are auxiliary heads, the main decode head is returned under the key \"decode_head\". All other heads are returned with the same key as their name. \"\"\" loss = self . _compute_loss ( model_output . output , ground_truth , criterion ) if not model_output . auxiliary_heads : return { \"loss\" : loss } if aux_loss_weights is None : msg = \"Auxiliary heads given with no aux_loss_weights\" raise Exception ( msg ) all_losses = {} all_losses [ \"decode_head\" ] = loss total_loss = loss . clone () # incorporate aux heads model_output_names = set ( model_output . auxiliary_heads . keys ()) aux_loss_names = set ( aux_loss_weights . keys ()) if aux_loss_names != model_output_names : msg = f \"Found difference in declared auxiliary losses and model outputs. \\n \\ Found in declared losses but not in model output: { aux_loss_names - model_output_names } . \\n \\ Found in model output but not in delcared losses: { model_output_names - aux_loss_names } \" raise Exception ( msg ) for loss_name , loss_weight in aux_loss_weights . items (): output = model_output . auxiliary_heads [ loss_name ] loss_value : Tensor = self . _compute_loss ( output , ground_truth , criterion ) all_losses [ loss_name ] = loss_value total_loss = total_loss + loss_value * loss_weight all_losses [ \"loss\" ] = total_loss return all_losses log_loss ( log_function , loss_dict = None , batch_size = None ) # Log the loss. If auxiliary heads exist, log the full loss suffix \"loss\", and then all other losses. Parameters: log_function ( Callable ) \u2013 description loss_dict ( dict [ str , Tensor ] , default: None ) \u2013 description . Defaults to None. Source code in terratorch/tasks/loss_handler.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def log_loss ( self , log_function : Callable , loss_dict : dict [ str , Tensor ] | None = None , batch_size : int | None = None ) -> None : \"\"\"Log the loss. If auxiliary heads exist, log the full loss suffix \"loss\", and then all other losses. Args: log_function (Callable): _description_ loss_dict (dict[str, Tensor], optional): _description_. Defaults to None. \"\"\" # dont alter passed dict all_losses = dict ( loss_dict ) full_loss = all_losses . pop ( \"loss\" ) log_function ( f \" { self . loss_prefix } loss\" , full_loss , sync_dist = True , batch_size = batch_size ) for loss_name , loss_value in all_losses . items (): log_function ( f \" { self . loss_prefix }{ loss_name } \" , loss_value , on_epoch = True , on_step = True , sync_dist = True , batch_size = batch_size , ) Generic datasets / datamodules # Refer to the section on data Exporting models # Models are saved using the PyTorch format, which basically serializes the model weights using pickle and store them into a binary file.","title":"Architecture Overview"},{"location":"architecture/#architecture-overview","text":"The main goal of the design is to extend TorchGeo's existing tasks to be able to handle Prithvi backbones with appropriate decoders and heads. At the same time, we wish to keep the existing TorchGeo functionality intact so it can be leveraged with pretrained models that are already included. We achieve this by making new tasks that accept model factory classes, containing a build_model method. This strategy in principle allows arbitrary models to be trained for these tasks, given they respect some reasonable minimal interface. Together with this, we provide the EncoderDecoderFactory , which should enable users to plug together different Encoders and Decoders, with the aid of Necks for intermediate operations. Additionally, we extend TorchGeo with generic datasets and datamodules which can be defined at runtime, rather than requiring classes to be defined beforehand. The glue that holds everything together is LightningCLI , allowing the model, datamodule and Lightning Trainer to be instantiated from a config file or from the CLI. We make extensive use of for training and inference. Initial reading for a full understanding of the platform includes: Familiarity with PyTorch Lightning Familiarity with TorchGeo Familiarity with LightningCLI","title":"Architecture Overview"},{"location":"architecture/#tasks","text":"Tasks are the main coordinators for training and inference for specific tasks. They are LightningModules that contain a model and abstract away all the logic for training steps, metric computation and inference. One of the most important design decisions was delegating the model construction to a model factory. This has a few advantages: Avoids code repetition among tasks - different tasks can use the same factory Prefers composition over inheritance Allows new models to be easily added by introducing new factories Models are expected to be torch.nn.Module and implement the Model interface, providing: freeze_encoder() freeze_decoder() forward() Additionally, the forward() method is expected to return an object of type ModelOutput , containing the main head's output, as well as any additional auxiliary outputs. The names of these auxiliary heads are matched with the names of the provided auxiliary losses.","title":"Tasks"},{"location":"architecture/#models","text":"Models constructed by the EncoderDecoderFactory have an internal structure explicitly divided into backbones, necks, decoders and heads. This structure is provided by the PixelWiseModel and ScalarOutputModel classes. However, as long as models implement the Model interface, and return ModelOutput in their forward method, they can take on any structure.","title":"Models"},{"location":"architecture/#terratorch.models.pixel_wise_model.PixelWiseModel","text":"Bases: Model , SegmentationModel Model that encapsulates encoder and decoder and heads Expects decoder to have a \"forward_features\" method, an embed_dims property and optionally a \"prepare_features_for_image_model\" method. Source code in terratorch/models/pixel_wise_model.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class PixelWiseModel ( Model , SegmentationModel ): \"\"\"Model that encapsulates encoder and decoder and heads Expects decoder to have a \"forward_features\" method, an embed_dims property and optionally a \"prepare_features_for_image_model\" method. \"\"\" def __init__ ( self , task : str , encoder : nn . Module , decoder : nn . Module , head_kwargs : dict , patch_size : int = None , padding : str = None , decoder_includes_head : bool = False , auxiliary_heads : list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] | None = None , neck : nn . Module | None = None , rescale : bool = True , # noqa: FBT002, FBT001 ) -> None : \"\"\"Constructor Args: task (str): Task to be performed. One of segmentation or regression. encoder (nn.Module): Encoder to be used decoder (nn.Module): Decoder to be used head_kwargs (dict): Arguments to be passed at instantiation of the head. decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False. auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of AuxiliaryHeads with heads to be instantiated. Defaults to None. neck (nn.Module | None): Module applied between backbone and decoder. Defaults to None, which applies the identity. rescale (bool, optional): Rescale the output of the model if it has a different size than the ground truth. Uses bilinear interpolation. Defaults to True. \"\"\" super () . __init__ () self . task = task self . encoder = encoder self . decoder = decoder self . head = ( self . _get_head ( task , decoder . out_channels , head_kwargs ) if not decoder_includes_head else nn . Identity () ) if auxiliary_heads is not None : aux_heads = {} for aux_head_to_be_instantiated in auxiliary_heads : aux_head : nn . Module = self . _get_head ( task , aux_head_to_be_instantiated . decoder . out_channels , head_kwargs ) if not aux_head_to_be_instantiated . decoder_includes_head else nn . Identity () aux_head = nn . Sequential ( aux_head_to_be_instantiated . decoder , aux_head ) aux_heads [ aux_head_to_be_instantiated . name ] = aux_head else : aux_heads = {} self . aux_heads = nn . ModuleDict ( aux_heads ) self . neck = neck self . rescale = rescale self . patch_size = patch_size self . padding = padding def freeze_encoder ( self ): freeze_module ( self . encoder ) def freeze_decoder ( self ): freeze_module ( self . decoder ) def freeze_head ( self ): freeze_module ( self . head ) @staticmethod def _check_for_single_channel_and_squeeze ( x ): if x . shape [ 1 ] == 1 : x = x . squeeze ( 1 ) return x def forward ( self , x : torch . Tensor , ** kwargs ) -> ModelOutput : \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\" def _get_size ( x ): if isinstance ( x , torch . Tensor ): return x . shape [ - 2 :] elif isinstance ( x , dict ): # Multimodal input in passed as dict (Assuming first modality to be an image) return list ( x . values ())[ 0 ] . shape [ - 2 :] elif hasattr ( kwargs , 'image_size' ): return kwargs [ 'image_size' ] else : ValueError ( 'Could not infer image shape.' ) image_size = _get_size ( x ) if isinstance ( x , torch . Tensor ) and self . patch_size : # Only works for single image modalities x = pad_images ( x , self . patch_size , self . padding ) input_size = _get_size ( x ) features = self . encoder ( x , ** kwargs ) # only for backwards compatibility with pre-neck times. if self . neck : prepare = self . neck else : # for backwards compatibility, if this is defined in the encoder, use it prepare = getattr ( self . encoder , \"prepare_features_for_image_model\" , lambda x : x ) print ( f \"neck: { self . neck } \" ) features = prepare ( features ) print ([ f . shape for f in features ]) decoder_output = self . decoder ([ f . clone () for f in features ]) mask = self . head ( decoder_output ) if self . rescale and mask . shape [ - 2 :] != input_size : mask = F . interpolate ( mask , size = input_size , mode = \"bilinear\" ) mask = self . _check_for_single_channel_and_squeeze ( mask ) mask = mask [ ... , : image_size [ 0 ], : image_size [ 1 ]] aux_outputs = {} for name , decoder in self . aux_heads . items (): aux_output = decoder ([ f . clone () for f in features ]) if self . rescale and aux_output . shape [ - 2 :] != input_size : aux_output = F . interpolate ( aux_output , size = input_size , mode = \"bilinear\" ) aux_output = self . _check_for_single_channel_and_squeeze ( aux_output ) aux_output = aux_output [ ... , : image_size [ 0 ], : image_size [ 1 ]] aux_outputs [ name ] = aux_output return ModelOutput ( output = mask , auxiliary_heads = aux_outputs ) def _get_head ( self , task : str , input_embed_dim : int , head_kwargs ): if task == \"segmentation\" : if \"num_classes\" not in head_kwargs : msg = \"num_classes must be defined for segmentation task\" raise Exception ( msg ) return SegmentationHead ( input_embed_dim , ** head_kwargs ) if task == \"regression\" : return RegressionHead ( input_embed_dim , ** head_kwargs ) msg = \"Task must be one of segmentation or regression.\" raise Exception ( msg )","title":"PixelWiseModel"},{"location":"architecture/#terratorch.models.pixel_wise_model.PixelWiseModel.__init__","text":"Constructor Parameters: task ( str ) \u2013 Task to be performed. One of segmentation or regression. encoder ( Module ) \u2013 Encoder to be used decoder ( Module ) \u2013 Decoder to be used head_kwargs ( dict ) \u2013 Arguments to be passed at instantiation of the head. decoder_includes_head ( bool , default: False ) \u2013 Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False. auxiliary_heads ( list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] | None , default: None ) \u2013 List of AuxiliaryHeads with heads to be instantiated. Defaults to None. neck ( Module | None , default: None ) \u2013 Module applied between backbone and decoder. Defaults to None, which applies the identity. rescale ( bool , default: True ) \u2013 Rescale the output of the model if it has a different size than the ground truth. Uses bilinear interpolation. Defaults to True. Source code in terratorch/models/pixel_wise_model.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , task : str , encoder : nn . Module , decoder : nn . Module , head_kwargs : dict , patch_size : int = None , padding : str = None , decoder_includes_head : bool = False , auxiliary_heads : list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] | None = None , neck : nn . Module | None = None , rescale : bool = True , # noqa: FBT002, FBT001 ) -> None : \"\"\"Constructor Args: task (str): Task to be performed. One of segmentation or regression. encoder (nn.Module): Encoder to be used decoder (nn.Module): Decoder to be used head_kwargs (dict): Arguments to be passed at instantiation of the head. decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False. auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of AuxiliaryHeads with heads to be instantiated. Defaults to None. neck (nn.Module | None): Module applied between backbone and decoder. Defaults to None, which applies the identity. rescale (bool, optional): Rescale the output of the model if it has a different size than the ground truth. Uses bilinear interpolation. Defaults to True. \"\"\" super () . __init__ () self . task = task self . encoder = encoder self . decoder = decoder self . head = ( self . _get_head ( task , decoder . out_channels , head_kwargs ) if not decoder_includes_head else nn . Identity () ) if auxiliary_heads is not None : aux_heads = {} for aux_head_to_be_instantiated in auxiliary_heads : aux_head : nn . Module = self . _get_head ( task , aux_head_to_be_instantiated . decoder . out_channels , head_kwargs ) if not aux_head_to_be_instantiated . decoder_includes_head else nn . Identity () aux_head = nn . Sequential ( aux_head_to_be_instantiated . decoder , aux_head ) aux_heads [ aux_head_to_be_instantiated . name ] = aux_head else : aux_heads = {} self . aux_heads = nn . ModuleDict ( aux_heads ) self . neck = neck self . rescale = rescale self . patch_size = patch_size self . padding = padding","title":"__init__"},{"location":"architecture/#terratorch.models.pixel_wise_model.PixelWiseModel.forward","text":"Sequentially pass x through model`s encoder, decoder and heads Source code in terratorch/models/pixel_wise_model.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def forward ( self , x : torch . Tensor , ** kwargs ) -> ModelOutput : \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\" def _get_size ( x ): if isinstance ( x , torch . Tensor ): return x . shape [ - 2 :] elif isinstance ( x , dict ): # Multimodal input in passed as dict (Assuming first modality to be an image) return list ( x . values ())[ 0 ] . shape [ - 2 :] elif hasattr ( kwargs , 'image_size' ): return kwargs [ 'image_size' ] else : ValueError ( 'Could not infer image shape.' ) image_size = _get_size ( x ) if isinstance ( x , torch . Tensor ) and self . patch_size : # Only works for single image modalities x = pad_images ( x , self . patch_size , self . padding ) input_size = _get_size ( x ) features = self . encoder ( x , ** kwargs ) # only for backwards compatibility with pre-neck times. if self . neck : prepare = self . neck else : # for backwards compatibility, if this is defined in the encoder, use it prepare = getattr ( self . encoder , \"prepare_features_for_image_model\" , lambda x : x ) print ( f \"neck: { self . neck } \" ) features = prepare ( features ) print ([ f . shape for f in features ]) decoder_output = self . decoder ([ f . clone () for f in features ]) mask = self . head ( decoder_output ) if self . rescale and mask . shape [ - 2 :] != input_size : mask = F . interpolate ( mask , size = input_size , mode = \"bilinear\" ) mask = self . _check_for_single_channel_and_squeeze ( mask ) mask = mask [ ... , : image_size [ 0 ], : image_size [ 1 ]] aux_outputs = {} for name , decoder in self . aux_heads . items (): aux_output = decoder ([ f . clone () for f in features ]) if self . rescale and aux_output . shape [ - 2 :] != input_size : aux_output = F . interpolate ( aux_output , size = input_size , mode = \"bilinear\" ) aux_output = self . _check_for_single_channel_and_squeeze ( aux_output ) aux_output = aux_output [ ... , : image_size [ 0 ], : image_size [ 1 ]] aux_outputs [ name ] = aux_output return ModelOutput ( output = mask , auxiliary_heads = aux_outputs )","title":"forward"},{"location":"architecture/#terratorch.models.scalar_output_model.ScalarOutputModel","text":"Bases: Model , SegmentationModel Model that encapsulates encoder and decoder and heads for a scalar output Expects decoder to have a \"forward_features\" method, an embed_dims property and optionally a \"prepare_features_for_image_model\" method. Source code in terratorch/models/scalar_output_model.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class ScalarOutputModel ( Model , SegmentationModel ): \"\"\"Model that encapsulates encoder and decoder and heads for a scalar output Expects decoder to have a \"forward_features\" method, an embed_dims property and optionally a \"prepare_features_for_image_model\" method. \"\"\" def __init__ ( self , task : str , encoder : nn . Module , decoder : nn . Module , head_kwargs : dict , patch_size : int = None , padding : str = None , decoder_includes_head : bool = False , auxiliary_heads : list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] | None = None , neck : nn . Module | None = None , ) -> None : \"\"\"Constructor Args: task (str): Task to be performed. Must be \"classification\". encoder (nn.Module): Encoder to be used decoder (nn.Module): Decoder to be used head_kwargs (dict): Arguments to be passed at instantiation of the head. decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False. auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of AuxiliaryHeads with heads to be instantiated. Defaults to None. neck (nn.Module | None): Module applied between backbone and decoder. Defaults to None, which applies the identity. \"\"\" super () . __init__ () self . task = task self . encoder = encoder self . decoder = decoder self . head = ( self . _get_head ( task , decoder . out_channels , head_kwargs ) if not decoder_includes_head else nn . Identity () ) if auxiliary_heads is not None : aux_heads = {} for aux_head_to_be_instantiated in auxiliary_heads : aux_head : nn . Module = self . _get_head ( task , aux_head_to_be_instantiated . decoder . out_channels , head_kwargs ) if not aux_head_to_be_instantiated . decoder_includes_head else nn . Identity () aux_head = nn . Sequential ( aux_head_to_be_instantiated . decoder , aux_head ) aux_heads [ aux_head_to_be_instantiated . name ] = aux_head else : aux_heads = {} self . aux_heads = nn . ModuleDict ( aux_heads ) self . neck = neck self . patch_size = patch_size self . padding = padding def freeze_encoder ( self ): freeze_module ( self . encoder ) def freeze_decoder ( self ): freeze_module ( self . decoder ) def freeze_head ( self ): freeze_module ( self . head ) def forward ( self , x : torch . Tensor , ** kwargs ) -> ModelOutput : \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\" if isinstance ( x , torch . Tensor ) and self . patch_size : # Only works for single image modalities x = pad_images ( x , self . patch_size , self . padding ) features = self . encoder ( x , ** kwargs ) # only for backwards compatibility with pre-neck times. if self . neck : prepare = self . neck else : # for backwards compatibility, if this is defined in the encoder, use it prepare = getattr ( self . encoder , \"prepare_features_for_image_model\" , lambda x : x ) features = prepare ( features ) decoder_output = self . decoder ([ f . clone () for f in features ]) mask = self . head ( decoder_output ) aux_outputs = {} for name , decoder in self . aux_heads . items (): aux_output = decoder ([ f . clone () for f in features ]) aux_outputs [ name ] = aux_output return ModelOutput ( output = mask , auxiliary_heads = aux_outputs ) def _get_head ( self , task : str , input_embed_dim : int , head_kwargs : dict ): if task == \"classification\" : if \"num_classes\" not in head_kwargs : msg = \"num_classes must be defined for classification task\" raise Exception ( msg ) return ClassificationHead ( input_embed_dim , ** head_kwargs ) msg = \"Task must be classification.\" raise Exception ( msg )","title":"ScalarOutputModel"},{"location":"architecture/#terratorch.models.scalar_output_model.ScalarOutputModel.__init__","text":"Constructor Parameters: task ( str ) \u2013 Task to be performed. Must be \"classification\". encoder ( Module ) \u2013 Encoder to be used decoder ( Module ) \u2013 Decoder to be used head_kwargs ( dict ) \u2013 Arguments to be passed at instantiation of the head. decoder_includes_head ( bool , default: False ) \u2013 Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False. auxiliary_heads ( list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] | None , default: None ) \u2013 List of AuxiliaryHeads with heads to be instantiated. Defaults to None. neck ( Module | None , default: None ) \u2013 Module applied between backbone and decoder. Defaults to None, which applies the identity. Source code in terratorch/models/scalar_output_model.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , task : str , encoder : nn . Module , decoder : nn . Module , head_kwargs : dict , patch_size : int = None , padding : str = None , decoder_includes_head : bool = False , auxiliary_heads : list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] | None = None , neck : nn . Module | None = None , ) -> None : \"\"\"Constructor Args: task (str): Task to be performed. Must be \"classification\". encoder (nn.Module): Encoder to be used decoder (nn.Module): Decoder to be used head_kwargs (dict): Arguments to be passed at instantiation of the head. decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False. auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of AuxiliaryHeads with heads to be instantiated. Defaults to None. neck (nn.Module | None): Module applied between backbone and decoder. Defaults to None, which applies the identity. \"\"\" super () . __init__ () self . task = task self . encoder = encoder self . decoder = decoder self . head = ( self . _get_head ( task , decoder . out_channels , head_kwargs ) if not decoder_includes_head else nn . Identity () ) if auxiliary_heads is not None : aux_heads = {} for aux_head_to_be_instantiated in auxiliary_heads : aux_head : nn . Module = self . _get_head ( task , aux_head_to_be_instantiated . decoder . out_channels , head_kwargs ) if not aux_head_to_be_instantiated . decoder_includes_head else nn . Identity () aux_head = nn . Sequential ( aux_head_to_be_instantiated . decoder , aux_head ) aux_heads [ aux_head_to_be_instantiated . name ] = aux_head else : aux_heads = {} self . aux_heads = nn . ModuleDict ( aux_heads ) self . neck = neck self . patch_size = patch_size self . padding = padding","title":"__init__"},{"location":"architecture/#terratorch.models.scalar_output_model.ScalarOutputModel.forward","text":"Sequentially pass x through model`s encoder, decoder and heads Source code in terratorch/models/scalar_output_model.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def forward ( self , x : torch . Tensor , ** kwargs ) -> ModelOutput : \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\" if isinstance ( x , torch . Tensor ) and self . patch_size : # Only works for single image modalities x = pad_images ( x , self . patch_size , self . padding ) features = self . encoder ( x , ** kwargs ) # only for backwards compatibility with pre-neck times. if self . neck : prepare = self . neck else : # for backwards compatibility, if this is defined in the encoder, use it prepare = getattr ( self . encoder , \"prepare_features_for_image_model\" , lambda x : x ) features = prepare ( features ) decoder_output = self . decoder ([ f . clone () for f in features ]) mask = self . head ( decoder_output ) aux_outputs = {} for name , decoder in self . aux_heads . items (): aux_output = decoder ([ f . clone () for f in features ]) aux_outputs [ name ] = aux_output return ModelOutput ( output = mask , auxiliary_heads = aux_outputs )","title":"forward"},{"location":"architecture/#encoderdecoderfactory","text":"We expect this factory to be widely employed by users. With that in mind, we dive deeper into it here .","title":"EncoderDecoderFactory"},{"location":"architecture/#loss","text":"For convenience, we provide a loss handler that can be used to compute the full loss (from the main head and auxiliary heads as well).","title":"Loss"},{"location":"architecture/#terratorch.tasks.loss_handler","text":"","title":"loss_handler"},{"location":"architecture/#terratorch.tasks.loss_handler.LossHandler","text":"Class to help handle the computation and logging of loss Source code in terratorch/tasks/loss_handler.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class LossHandler : \"\"\"Class to help handle the computation and logging of loss\"\"\" def __init__ ( self , loss_prefix : str ) -> None : \"\"\"Constructor Args: loss_prefix (str): Prefix to be prepended to all the metrics (e.g. training). \"\"\" self . loss_prefix = loss_prefix def compute_loss ( self , model_output : ModelOutput , ground_truth : Tensor , criterion : Callable , aux_loss_weights : dict [ str , float ] | None , ) -> dict [ str , Tensor ]: \"\"\"Compute the loss for the mean decode head as well as other heads Args: model_output (ModelOutput): Output from the model ground_truth (Tensor): Tensor with labels criterion (Callable): Loss function to be applied aux_loss_weights (Union[dict[str, float], None]): Dictionary of names of model auxiliary heads and their weights Raises: Exception: If the keys in aux_loss_weights and the model output do not match, will raise an exception. Returns: dict[str, Tensor]: Dictionary of computed losses. Total loss is returned under the key \"loss\". If there are auxiliary heads, the main decode head is returned under the key \"decode_head\". All other heads are returned with the same key as their name. \"\"\" loss = self . _compute_loss ( model_output . output , ground_truth , criterion ) if not model_output . auxiliary_heads : return { \"loss\" : loss } if aux_loss_weights is None : msg = \"Auxiliary heads given with no aux_loss_weights\" raise Exception ( msg ) all_losses = {} all_losses [ \"decode_head\" ] = loss total_loss = loss . clone () # incorporate aux heads model_output_names = set ( model_output . auxiliary_heads . keys ()) aux_loss_names = set ( aux_loss_weights . keys ()) if aux_loss_names != model_output_names : msg = f \"Found difference in declared auxiliary losses and model outputs. \\n \\ Found in declared losses but not in model output: { aux_loss_names - model_output_names } . \\n \\ Found in model output but not in delcared losses: { model_output_names - aux_loss_names } \" raise Exception ( msg ) for loss_name , loss_weight in aux_loss_weights . items (): output = model_output . auxiliary_heads [ loss_name ] loss_value : Tensor = self . _compute_loss ( output , ground_truth , criterion ) all_losses [ loss_name ] = loss_value total_loss = total_loss + loss_value * loss_weight all_losses [ \"loss\" ] = total_loss return all_losses def _compute_loss ( self , y_hat : Tensor , ground_truth : Tensor , criterion : Callable ): loss : Tensor = criterion ( y_hat , ground_truth ) return loss def log_loss ( self , log_function : Callable , loss_dict : dict [ str , Tensor ] | None = None , batch_size : int | None = None ) -> None : \"\"\"Log the loss. If auxiliary heads exist, log the full loss suffix \"loss\", and then all other losses. Args: log_function (Callable): _description_ loss_dict (dict[str, Tensor], optional): _description_. Defaults to None. \"\"\" # dont alter passed dict all_losses = dict ( loss_dict ) full_loss = all_losses . pop ( \"loss\" ) log_function ( f \" { self . loss_prefix } loss\" , full_loss , sync_dist = True , batch_size = batch_size ) for loss_name , loss_value in all_losses . items (): log_function ( f \" { self . loss_prefix }{ loss_name } \" , loss_value , on_epoch = True , on_step = True , sync_dist = True , batch_size = batch_size , )","title":"LossHandler"},{"location":"architecture/#terratorch.tasks.loss_handler.LossHandler.__init__","text":"Constructor Parameters: loss_prefix ( str ) \u2013 Prefix to be prepended to all the metrics (e.g. training). Source code in terratorch/tasks/loss_handler.py 11 12 13 14 15 16 17 def __init__ ( self , loss_prefix : str ) -> None : \"\"\"Constructor Args: loss_prefix (str): Prefix to be prepended to all the metrics (e.g. training). \"\"\" self . loss_prefix = loss_prefix","title":"__init__"},{"location":"architecture/#terratorch.tasks.loss_handler.LossHandler.compute_loss","text":"Compute the loss for the mean decode head as well as other heads Parameters: model_output ( ModelOutput ) \u2013 Output from the model ground_truth ( Tensor ) \u2013 Tensor with labels criterion ( Callable ) \u2013 Loss function to be applied aux_loss_weights ( Union [ dict [ str , float ], None] ) \u2013 Dictionary of names of model auxiliary heads and their weights Raises: Exception \u2013 If the keys in aux_loss_weights and the model output do not match, will raise an exception. Returns: dict [ str , Tensor ] \u2013 dict[str, Tensor]: Dictionary of computed losses. Total loss is returned under the key \"loss\". If there are auxiliary heads, the main decode head is returned under the key \"decode_head\". All other heads are returned with the same key as their name. Source code in terratorch/tasks/loss_handler.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def compute_loss ( self , model_output : ModelOutput , ground_truth : Tensor , criterion : Callable , aux_loss_weights : dict [ str , float ] | None , ) -> dict [ str , Tensor ]: \"\"\"Compute the loss for the mean decode head as well as other heads Args: model_output (ModelOutput): Output from the model ground_truth (Tensor): Tensor with labels criterion (Callable): Loss function to be applied aux_loss_weights (Union[dict[str, float], None]): Dictionary of names of model auxiliary heads and their weights Raises: Exception: If the keys in aux_loss_weights and the model output do not match, will raise an exception. Returns: dict[str, Tensor]: Dictionary of computed losses. Total loss is returned under the key \"loss\". If there are auxiliary heads, the main decode head is returned under the key \"decode_head\". All other heads are returned with the same key as their name. \"\"\" loss = self . _compute_loss ( model_output . output , ground_truth , criterion ) if not model_output . auxiliary_heads : return { \"loss\" : loss } if aux_loss_weights is None : msg = \"Auxiliary heads given with no aux_loss_weights\" raise Exception ( msg ) all_losses = {} all_losses [ \"decode_head\" ] = loss total_loss = loss . clone () # incorporate aux heads model_output_names = set ( model_output . auxiliary_heads . keys ()) aux_loss_names = set ( aux_loss_weights . keys ()) if aux_loss_names != model_output_names : msg = f \"Found difference in declared auxiliary losses and model outputs. \\n \\ Found in declared losses but not in model output: { aux_loss_names - model_output_names } . \\n \\ Found in model output but not in delcared losses: { model_output_names - aux_loss_names } \" raise Exception ( msg ) for loss_name , loss_weight in aux_loss_weights . items (): output = model_output . auxiliary_heads [ loss_name ] loss_value : Tensor = self . _compute_loss ( output , ground_truth , criterion ) all_losses [ loss_name ] = loss_value total_loss = total_loss + loss_value * loss_weight all_losses [ \"loss\" ] = total_loss return all_losses","title":"compute_loss"},{"location":"architecture/#terratorch.tasks.loss_handler.LossHandler.log_loss","text":"Log the loss. If auxiliary heads exist, log the full loss suffix \"loss\", and then all other losses. Parameters: log_function ( Callable ) \u2013 description loss_dict ( dict [ str , Tensor ] , default: None ) \u2013 description . Defaults to None. Source code in terratorch/tasks/loss_handler.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def log_loss ( self , log_function : Callable , loss_dict : dict [ str , Tensor ] | None = None , batch_size : int | None = None ) -> None : \"\"\"Log the loss. If auxiliary heads exist, log the full loss suffix \"loss\", and then all other losses. Args: log_function (Callable): _description_ loss_dict (dict[str, Tensor], optional): _description_. Defaults to None. \"\"\" # dont alter passed dict all_losses = dict ( loss_dict ) full_loss = all_losses . pop ( \"loss\" ) log_function ( f \" { self . loss_prefix } loss\" , full_loss , sync_dist = True , batch_size = batch_size ) for loss_name , loss_value in all_losses . items (): log_function ( f \" { self . loss_prefix }{ loss_name } \" , loss_value , on_epoch = True , on_step = True , sync_dist = True , batch_size = batch_size , )","title":"log_loss"},{"location":"architecture/#generic-datasets-datamodules","text":"Refer to the section on data","title":"Generic datasets / datamodules"},{"location":"architecture/#exporting-models","text":"Models are saved using the PyTorch format, which basically serializes the model weights using pickle and store them into a binary file.","title":"Exporting models"},{"location":"contributing/","text":"Contributing to TerraTorch # Contributions to an open source project can came in different ways, but we could summarize them in three main components: adding code (as new models, tasks and auxiliary algorithms or even addressing the solution of a bug), examples using the software (scripts, yaml files and notebooks showcasing the package) and documentation. All these ways are valid for TerraTorch and the users are welcome to contribute in any of these fronts. However, some recommendations and rules are necessary in order to facilitate and organize his process. And this is the matter of the next paragraphs. Contributing with code # It is not a trivial task to determine how a modification in the source code will impact already implemented and established features, in this way, for any modification in the core source code ( terratorch/ ) we automatically execute a pipeline with hundreds of unit and integration tests to verify that the package have not broken after the modification be merged to main . In this way, when an user wants to modify terratorch for adding new features or bufixes, this are the best practices. If you are an user outside the IBM org, create a fork to add your modifications. If you are inside the IBM org or have received writing provileges, create a branch for it. If you are adding new features, we ask you to also add tests for it. These tests are defined in the directory tests/ and are fundamental to check if your feature is working as expected and not breaking anything. If your feature is something more complex, as a new model or auxiliary algorithm, you can also (optionally) to add a complete example, as a notebook, demonstrating how the feature works. After finishing your modifications, we recommend you to test locally using pytest , for example: pytest -s -v tests/ If all the tests are passing, you can open a PR to terratorch:main describing what you are adding and why that is important to be merged. You do not need to choose a reviewer, since the maintainers will check the new open PR and request review for it by themselves. The PR will pass through the tests in GitHub Actions and if the reviewer approve it, it will soon be merged. It is recommended to add a label to your PR. For example bug , when it solves some issue or enhancement when it adds new features. NOTICE : The PR will not be merged if the automatic tests are failing and the user which has sent the PR is responsible for fixing it. Contributing with documentation # Documentation is core for any project, however, most part of the time, the developers do not have the time (or patience) to carefully document all the codebase, in this way, contributions from interested users are always welcome. To add documentation to TerraTorch, you need to be familiar with MarkDown, a clean markup language, and MkDocs, a framework which relies on MarkDowns in order to create webpages as this which you are reading. Install the MkDocs dependencies. Clone the branch dedicated to documentation to a local branch: git fetch origin improve/docs Add your modifications and open a PR to improve/docs . It is recommended to add the label documentation to your PR. The PR will be reviewed and approved if it is considered relevant by the maintainers.","title":"Contribution Guidelines"},{"location":"contributing/#contributing-to-terratorch","text":"Contributions to an open source project can came in different ways, but we could summarize them in three main components: adding code (as new models, tasks and auxiliary algorithms or even addressing the solution of a bug), examples using the software (scripts, yaml files and notebooks showcasing the package) and documentation. All these ways are valid for TerraTorch and the users are welcome to contribute in any of these fronts. However, some recommendations and rules are necessary in order to facilitate and organize his process. And this is the matter of the next paragraphs.","title":"Contributing to TerraTorch"},{"location":"contributing/#contributing-with-code","text":"It is not a trivial task to determine how a modification in the source code will impact already implemented and established features, in this way, for any modification in the core source code ( terratorch/ ) we automatically execute a pipeline with hundreds of unit and integration tests to verify that the package have not broken after the modification be merged to main . In this way, when an user wants to modify terratorch for adding new features or bufixes, this are the best practices. If you are an user outside the IBM org, create a fork to add your modifications. If you are inside the IBM org or have received writing provileges, create a branch for it. If you are adding new features, we ask you to also add tests for it. These tests are defined in the directory tests/ and are fundamental to check if your feature is working as expected and not breaking anything. If your feature is something more complex, as a new model or auxiliary algorithm, you can also (optionally) to add a complete example, as a notebook, demonstrating how the feature works. After finishing your modifications, we recommend you to test locally using pytest , for example: pytest -s -v tests/ If all the tests are passing, you can open a PR to terratorch:main describing what you are adding and why that is important to be merged. You do not need to choose a reviewer, since the maintainers will check the new open PR and request review for it by themselves. The PR will pass through the tests in GitHub Actions and if the reviewer approve it, it will soon be merged. It is recommended to add a label to your PR. For example bug , when it solves some issue or enhancement when it adds new features. NOTICE : The PR will not be merged if the automatic tests are failing and the user which has sent the PR is responsible for fixing it.","title":"Contributing with code"},{"location":"contributing/#contributing-with-documentation","text":"Documentation is core for any project, however, most part of the time, the developers do not have the time (or patience) to carefully document all the codebase, in this way, contributions from interested users are always welcome. To add documentation to TerraTorch, you need to be familiar with MarkDown, a clean markup language, and MkDocs, a framework which relies on MarkDowns in order to create webpages as this which you are reading. Install the MkDocs dependencies. Clone the branch dedicated to documentation to a local branch: git fetch origin improve/docs Add your modifications and open a PR to improve/docs . It is recommended to add the label documentation to your PR. The PR will be reviewed and approved if it is considered relevant by the maintainers.","title":"Contributing with documentation"},{"location":"data/","text":"Data Processing # In our workflow, we leverage TorchGeo to implement datasets and data modules, ensuring robust and flexible data handling. For a deeper dive into working with datasets using TorchGeo, please refer to the TorchGeo tutorials on datasets . In most cases, it\u2019s best to create a custom TorchGeo dataset tailored to your specific data. Doing so gives you complete control over: - Data Loading: Customize how your data is read and organized. - Transforms: Decide which preprocessing or augmentation steps to apply. - Visualization: Define custom plotting methods (for example, when logging with TensorBoard). TorchGeo offers two primary classes to suit different data formats: - NonGeoDataset : Use this if your dataset is already split into neatly tiled pieces ready for neural network consumption. Essentially, NonGeoDataset is a wrapper around a standard PyTorch dataset, making it straightforward to integrate into your pipeline. - GeoDataset : Opt for this class if your data comes in the form of large GeoTiff files from which you need to sample during training. GeoDataset automatically aligns your input data with corresponding labels and supports a range of geo-aware sampling techniques. In addition to these specialized TorchGeo datasets, TerraTorch offers generic datasets and data modules designed to work with directory-based data structures, similar to those used in MMLab libraries. These generic tools simplify data loading when your data is organized in conventional file directories: - The Generic Pixel-wise Dataset is ideal for tasks where each pixel represents a sample (e.g., segmentation or dense prediction problems). - The Generic Scalar Label Dataset is best suited for classification tasks where each sample is associated with a single label. TerraTorch also provides corresponding generic data modules that bundle the dataset with training, validation, and testing splits, integrating seamlessly with PyTorch Lightning. This arrangement makes it easy to manage data loading, batching, and preprocessing with minimal configuration. While generic datasets offer a quick start for common data structures, many projects require more tailored solutions. Custom datasets and data modules give you complete control over the entire data handling process\u2014from fine-tuned data loading and specific transformations to enhanced visualization. By developing your own dataset and data module classes, you ensure that every step\u2014from data ingestion to final model input\u2014is optimized for your particular use case. TerraTorch\u2019s examples provide an excellent starting point to build these custom components and integrate them seamlessly into your training pipeline. For additional examples on fine-tuning a TerraTorch model using these components, please refer to the Prithvi EO Examples repository. Data curation # Generally speaking, all the datamodules work by collecting sets of files and concatenating them into batches with a size determined by the user. TerraTorch automatically checks the dimensionality of the files in order to guarantee that they are stackable, otherwise a stackability error will be raised. If you are sure that your data files are in the proper format and do not want to check for stackability, define check_stackability: false in the field data of your yaml file. If you are using the script interface, you just need to pass it as argument to your dataloader class. Alternatively, if you want to fix discrepancies related to dimensionality in your input files at the data loading stage, you can add a pad correction pipeline, as seen in the example tests/resources/configs/manufactured-finetune_prithvi_eo_v2_300_pad_transform.yaml . Using Datasets already implemented in TorchGeo # Using existing TorchGeo DataModules is very easy! Just plug them in! For instance, to use the EuroSATDataModule , in your config file, set the data as: data : class_path : torchgeo.datamodules.EuroSATDataModule init_args : batch_size : 32 num_workers : 8 dict_kwargs : root : /dccstor/geofm-pre/EuroSat download : True bands : - B02 - B03 - B04 - B08A - B09 - B10 Modifying each parameter as you see fit. You can also do this outside of config files! Simply instantiate the data module as normal and plug it in. Warning To define transforms to be passed to DataModules from TorchGeo from config files, you must use the following format: data : class_path : terratorch.datamodules.TorchNonGeoDataModule init_args : cls : torchgeo.datamodules.EuroSATDataModule transforms : - class_path : albumentations.augmentations.geometric.resize.Resize init_args : height : 224 width : 224 - class_path : ToTensorV2 Note the class_path is TorchNonGeoDataModule and the class to be used is passed through cls (there is also a TorchGeoDataModule for geo modules). This has to be done as the transforms argument is passed through **kwargs in TorchGeo, making it difficult to instantiate with LightningCLI. See more details below. terratorch.datamodules.torchgeo_data_module # Ugly proxy objects so parsing config file works with transforms. These are necessary since, for LightningCLI to instantiate arguments as objects from the config, they must have type annotations In TorchGeo, transforms is passed in **kwargs, so it has no type annotations! To get around that, we create these wrappers that have transforms type annotated. They create the transforms and forward all method and attribute calls to the original TorchGeo datamodule. Additionally, TorchGeo datasets pass the data to the transforms callable as a dict, and as a tensor. Albumentations expects this data not as a dict but as different key-value arguments, and as numpy. We handle that conversion here. TorchGeoDataModule # Bases: GeoDataModule Proxy object for using Geo data modules defined by TorchGeo. Allows for transforms to be defined and passed using config files. The only reason this class exists is so that we can annotate the transforms argument with a type. This is required for lightningcli and config files. As such, all getattr and setattr will be redirected to the underlying class. Source code in terratorch/datamodules/torchgeo_data_module.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 class TorchGeoDataModule ( GeoDataModule ): \"\"\"Proxy object for using Geo data modules defined by TorchGeo. Allows for transforms to be defined and passed using config files. The only reason this class exists is so that we can annotate the transforms argument with a type. This is required for lightningcli and config files. As such, all getattr and setattr will be redirected to the underlying class. \"\"\" def __init__ ( self , cls : type [ GeoDataModule ], batch_size : int | None = None , num_workers : int = 0 , transforms : None | list [ BasicTransform ] = None , ** kwargs : Any , ): \"\"\"Constructor Args: cls (type[GeoDataModule]): TorchGeo DataModule class to be instantiated batch_size (int | None, optional): batch_size. Defaults to None. num_workers (int, optional): num_workers. Defaults to 0. transforms (None | list[BasicTransform], optional): List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None. **kwargs (Any): Arguments passed to instantiate `cls`. \"\"\" if batch_size is not None : kwargs [ \"batch_size\" ] = batch_size if transforms is not None : transforms_as_callable = albumentations_to_callable_with_dict ( transforms ) kwargs [ \"transforms\" ] = build_callable_transform_from_torch_tensor ( transforms_as_callable ) # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs) self . _proxy = cls ( num_workers = num_workers , ** kwargs ) super () . __init__ ( self . _proxy . dataset_class ) # dummy arg @property def collate_fn ( self ): return self . _proxy . collate_fn @collate_fn . setter def collate_fn ( self , value ): self . _proxy . collate_fn = value @property def patch_size ( self ): return self . _proxy . patch_size @property def length ( self ): return self . _proxy . length def setup ( self , stage : str ): return self . _proxy . setup ( stage ) def train_dataloader ( self ): return self . _proxy . train_dataloader () def val_dataloader ( self ): return self . _proxy . val_dataloader () def test_dataloader ( self ): return self . _proxy . test_dataloader () def predict_dataloader ( self ): return self . _proxy . predict_dataloader () def transfer_batch_to_device ( self , batch , device , dataloader_idx ): return self . _proxy . predict_dataloader ( batch , device , dataloader_idx ) __init__ ( cls , batch_size = None , num_workers = 0 , transforms = None , ** kwargs ) # Constructor Parameters: cls ( type [ GeoDataModule ] ) \u2013 TorchGeo DataModule class to be instantiated batch_size ( int | None , default: None ) \u2013 batch_size. Defaults to None. num_workers ( int , default: 0 ) \u2013 num_workers. Defaults to 0. transforms ( None | list [ BasicTransform ] , default: None ) \u2013 List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None. **kwargs ( Any , default: {} ) \u2013 Arguments passed to instantiate cls . Source code in terratorch/datamodules/torchgeo_data_module.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def __init__ ( self , cls : type [ GeoDataModule ], batch_size : int | None = None , num_workers : int = 0 , transforms : None | list [ BasicTransform ] = None , ** kwargs : Any , ): \"\"\"Constructor Args: cls (type[GeoDataModule]): TorchGeo DataModule class to be instantiated batch_size (int | None, optional): batch_size. Defaults to None. num_workers (int, optional): num_workers. Defaults to 0. transforms (None | list[BasicTransform], optional): List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None. **kwargs (Any): Arguments passed to instantiate `cls`. \"\"\" if batch_size is not None : kwargs [ \"batch_size\" ] = batch_size if transforms is not None : transforms_as_callable = albumentations_to_callable_with_dict ( transforms ) kwargs [ \"transforms\" ] = build_callable_transform_from_torch_tensor ( transforms_as_callable ) # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs) self . _proxy = cls ( num_workers = num_workers , ** kwargs ) super () . __init__ ( self . _proxy . dataset_class ) # dummy arg TorchNonGeoDataModule # Bases: NonGeoDataModule Proxy object for using NonGeo data modules defined by TorchGeo. Allows for transforms to be defined and passed using config files. The only reason this class exists is so that we can annotate the transforms argument with a type. This is required for lightningcli and config files. As such, all getattr and setattr will be redirected to the underlying class. Source code in terratorch/datamodules/torchgeo_data_module.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class TorchNonGeoDataModule ( NonGeoDataModule ): \"\"\"Proxy object for using NonGeo data modules defined by TorchGeo. Allows for transforms to be defined and passed using config files. The only reason this class exists is so that we can annotate the transforms argument with a type. This is required for lightningcli and config files. As such, all getattr and setattr will be redirected to the underlying class. \"\"\" def __init__ ( self , cls : type [ NonGeoDataModule ], batch_size : int | None = None , num_workers : int = 0 , transforms : None | list [ BasicTransform ] = None , ** kwargs : Any , ): \"\"\"Constructor Args: cls (type[NonGeoDataModule]): TorchGeo DataModule class to be instantiated batch_size (int | None, optional): batch_size. Defaults to None. num_workers (int, optional): num_workers. Defaults to 0. transforms (None | list[BasicTransform], optional): List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None. **kwargs (Any): Arguments passed to instantiate `cls`. \"\"\" if batch_size is not None : kwargs [ \"batch_size\" ] = batch_size if transforms is not None : transforms_as_callable = albumentations_to_callable_with_dict ( transforms ) kwargs [ \"transforms\" ] = build_callable_transform_from_torch_tensor ( transforms_as_callable ) # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs) self . _proxy = cls ( num_workers = num_workers , ** kwargs ) super () . __init__ ( self . _proxy . dataset_class ) # dummy arg @property def collate_fn ( self ): return self . _proxy . collate_fn @collate_fn . setter def collate_fn ( self , value ): self . _proxy . collate_fn = value def setup ( self , stage : str ): return self . _proxy . setup ( stage ) def train_dataloader ( self ): return self . _proxy . train_dataloader () def val_dataloader ( self ): return self . _proxy . val_dataloader () def test_dataloader ( self ): return self . _proxy . test_dataloader () def predict_dataloader ( self ): return self . _proxy . predict_dataloader () __init__ ( cls , batch_size = None , num_workers = 0 , transforms = None , ** kwargs ) # Constructor Parameters: cls ( type [ NonGeoDataModule ] ) \u2013 TorchGeo DataModule class to be instantiated batch_size ( int | None , default: None ) \u2013 batch_size. Defaults to None. num_workers ( int , default: 0 ) \u2013 num_workers. Defaults to 0. transforms ( None | list [ BasicTransform ] , default: None ) \u2013 List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None. **kwargs ( Any , default: {} ) \u2013 Arguments passed to instantiate cls . Source code in terratorch/datamodules/torchgeo_data_module.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , cls : type [ NonGeoDataModule ], batch_size : int | None = None , num_workers : int = 0 , transforms : None | list [ BasicTransform ] = None , ** kwargs : Any , ): \"\"\"Constructor Args: cls (type[NonGeoDataModule]): TorchGeo DataModule class to be instantiated batch_size (int | None, optional): batch_size. Defaults to None. num_workers (int, optional): num_workers. Defaults to 0. transforms (None | list[BasicTransform], optional): List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None. **kwargs (Any): Arguments passed to instantiate `cls`. \"\"\" if batch_size is not None : kwargs [ \"batch_size\" ] = batch_size if transforms is not None : transforms_as_callable = albumentations_to_callable_with_dict ( transforms ) kwargs [ \"transforms\" ] = build_callable_transform_from_torch_tensor ( transforms_as_callable ) # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs) self . _proxy = cls ( num_workers = num_workers , ** kwargs ) super () . __init__ ( self . _proxy . dataset_class ) # dummy arg Generic datasets and data modules # For the NonGeoDataset case, we also provide \"generic\" datasets and datamodules. These can be used when you would like to load data from given directories, in a style similar to the MMLab libraries. Generic Datasets # terratorch.datasets.generic_pixel_wise_dataset # Module containing generic dataset classes GenericNonGeoPixelwiseRegressionDataset # Bases: GenericPixelWiseDataset GenericNonGeoPixelwiseRegressionDataset Source code in terratorch/datasets/generic_pixel_wise_dataset.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 class GenericNonGeoPixelwiseRegressionDataset ( GenericPixelWiseDataset ): \"\"\"GenericNonGeoPixelwiseRegressionDataset\"\"\" def __init__ ( self , data_root : Path , label_data_root : Path | None = None , image_grep : str | None = \"*\" , label_grep : str | None = \"*\" , split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , rgb_indices : list [ int ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory label_data_root (Path, optional): Path to data root directory with labels. If not specified, will use the same as for images. image_grep (str, optional): Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep (str, optional): Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. \"\"\" super () . __init__ ( data_root , label_data_root = label_data_root , image_grep = image_grep , label_grep = label_grep , split = split , ignore_split_file_extensions = ignore_split_file_extensions , allow_substring_split_file = allow_substring_split_file , rgb_indices = rgb_indices , dataset_bands = dataset_bands , output_bands = output_bands , constant_scale = constant_scale , transform = transform , no_data_replace = no_data_replace , no_label_replace = no_label_replace , expand_temporal_dimension = expand_temporal_dimension , reduce_zero_label = reduce_zero_label , ) def __getitem__ ( self , index : int ) -> dict [ str , Any ]: item = super () . __getitem__ ( index ) item [ \"mask\" ] = item [ \"mask\" ] . float () return item def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__` suptitle (str|None): optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample .. versionadded:: 0.2 \"\"\" image = sample [ \"image\" ] if len ( image . shape ) == 5 : return if isinstance ( image , Tensor ): image = image . numpy () image = image . take ( self . rgb_indices , axis = 0 ) image = np . transpose ( image , ( 1 , 2 , 0 )) image = ( image - image . min ( axis = ( 0 , 1 ))) * ( 1 / image . max ( axis = ( 0 , 1 ))) image = np . clip ( image , 0 , 1 ) label_mask = sample [ \"mask\" ] if isinstance ( label_mask , Tensor ): label_mask = label_mask . numpy () showing_predictions = \"prediction\" in sample if showing_predictions : prediction_mask = sample [ \"prediction\" ] if isinstance ( prediction_mask , Tensor ): prediction_mask = prediction_mask . numpy () return self . _plot_sample ( image , label_mask , prediction = prediction_mask if showing_predictions else None , suptitle = suptitle , ) @staticmethod def _plot_sample ( image , label , prediction = None , suptitle = None ): num_images = 4 if prediction is not None else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 10 ), layout = \"compressed\" ) norm = mpl . colors . Normalize ( vmin = label . min (), vmax = label . max ()) ax [ 0 ] . axis ( \"off\" ) ax [ 0 ] . title . set_text ( \"Image\" ) ax [ 0 ] . imshow ( image ) ax [ 1 ] . axis ( \"off\" ) ax [ 1 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ 1 ] . imshow ( label , cmap = \"Greens\" , norm = norm ) ax [ 2 ] . axis ( \"off\" ) ax [ 2 ] . title . set_text ( \"GT Mask on Image\" ) ax [ 2 ] . imshow ( image ) ax [ 2 ] . imshow ( label , cmap = \"Greens\" , alpha = 0.3 , norm = norm ) # ax[2].legend() if prediction is not None : ax [ 3 ] . title . set_text ( \"Predicted Mask\" ) ax [ 3 ] . imshow ( prediction , cmap = \"Greens\" , norm = norm ) if suptitle is not None : plt . suptitle ( suptitle ) return fig __init__ ( data_root , label_data_root = None , image_grep = '*' , label_grep = '*' , split = None , ignore_split_file_extensions = True , allow_substring_split_file = True , rgb_indices = None , dataset_bands = None , output_bands = None , constant_scale = 1 , transform = None , no_data_replace = None , no_label_replace = None , expand_temporal_dimension = False , reduce_zero_label = False ) # Constructor Parameters: data_root ( Path ) \u2013 Path to data root directory label_data_root ( Path , default: None ) \u2013 Path to data root directory with labels. If not specified, will use the same as for images. image_grep ( str , default: '*' ) \u2013 Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep ( str , default: '*' ) \u2013 Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split ( Path , default: None ) \u2013 Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices ( list [ str ] , default: None ) \u2013 Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands present in the dataset. output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands that should be output by the dataset. constant_scale ( float , default: 1 ) \u2013 Factor to multiply image values by. Defaults to 1. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float | None , default: None ) \u2013 Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace ( int | None , default: None ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label ( bool , default: False ) \u2013 Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. Source code in terratorch/datasets/generic_pixel_wise_dataset.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 def __init__ ( self , data_root : Path , label_data_root : Path | None = None , image_grep : str | None = \"*\" , label_grep : str | None = \"*\" , split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , rgb_indices : list [ int ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory label_data_root (Path, optional): Path to data root directory with labels. If not specified, will use the same as for images. image_grep (str, optional): Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep (str, optional): Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. \"\"\" super () . __init__ ( data_root , label_data_root = label_data_root , image_grep = image_grep , label_grep = label_grep , split = split , ignore_split_file_extensions = ignore_split_file_extensions , allow_substring_split_file = allow_substring_split_file , rgb_indices = rgb_indices , dataset_bands = dataset_bands , output_bands = output_bands , constant_scale = constant_scale , transform = transform , no_data_replace = no_data_replace , no_label_replace = no_label_replace , expand_temporal_dimension = expand_temporal_dimension , reduce_zero_label = reduce_zero_label , ) plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 a sample returned by :meth: __getitem__ suptitle ( str | None , default: None ) \u2013 optional string to use as a suptitle Returns: Figure \u2013 a matplotlib Figure with the rendered sample .. versionadded:: 0.2 Source code in terratorch/datasets/generic_pixel_wise_dataset.py 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__` suptitle (str|None): optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample .. versionadded:: 0.2 \"\"\" image = sample [ \"image\" ] if len ( image . shape ) == 5 : return if isinstance ( image , Tensor ): image = image . numpy () image = image . take ( self . rgb_indices , axis = 0 ) image = np . transpose ( image , ( 1 , 2 , 0 )) image = ( image - image . min ( axis = ( 0 , 1 ))) * ( 1 / image . max ( axis = ( 0 , 1 ))) image = np . clip ( image , 0 , 1 ) label_mask = sample [ \"mask\" ] if isinstance ( label_mask , Tensor ): label_mask = label_mask . numpy () showing_predictions = \"prediction\" in sample if showing_predictions : prediction_mask = sample [ \"prediction\" ] if isinstance ( prediction_mask , Tensor ): prediction_mask = prediction_mask . numpy () return self . _plot_sample ( image , label_mask , prediction = prediction_mask if showing_predictions else None , suptitle = suptitle , ) GenericNonGeoSegmentationDataset # Bases: GenericPixelWiseDataset GenericNonGeoSegmentationDataset Source code in terratorch/datasets/generic_pixel_wise_dataset.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 class GenericNonGeoSegmentationDataset ( GenericPixelWiseDataset ): \"\"\"GenericNonGeoSegmentationDataset\"\"\" def __init__ ( self , data_root : Path , num_classes : int , label_data_root : Path | None = None , image_grep : str | None = \"*\" , label_grep : str | None = \"*\" , split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , rgb_indices : list [ str ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , class_names : list [ str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory num_classes (int): Number of classes in the dataset label_data_root (Path, optional): Path to data root directory with labels. If not specified, will use the same as for images. image_grep (str, optional): Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep (str, optional): Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. class_names (list[str], optional): Class names. Defaults to None. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. \"\"\" super () . __init__ ( data_root , label_data_root = label_data_root , image_grep = image_grep , label_grep = label_grep , split = split , ignore_split_file_extensions = ignore_split_file_extensions , allow_substring_split_file = allow_substring_split_file , rgb_indices = rgb_indices , dataset_bands = dataset_bands , output_bands = output_bands , constant_scale = constant_scale , transform = transform , no_data_replace = no_data_replace , no_label_replace = no_label_replace , expand_temporal_dimension = expand_temporal_dimension , reduce_zero_label = reduce_zero_label , ) self . num_classes = num_classes self . class_names = class_names def __getitem__ ( self , index : int ) -> dict [ str , Any ]: item = super () . __getitem__ ( index ) item [ \"mask\" ] = item [ \"mask\" ] . long () return item def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample .. versionadded:: 0.2 \"\"\" image = sample [ \"image\" ] if len ( image . shape ) == 5 : return if isinstance ( image , Tensor ): image = image . numpy () image = image . take ( self . rgb_indices , axis = 0 ) image = np . transpose ( image , ( 1 , 2 , 0 )) image = ( image - image . min ( axis = ( 0 , 1 ))) * ( 1 / image . max ( axis = ( 0 , 1 ))) image = np . clip ( image , 0 , 1 ) label_mask = sample [ \"mask\" ] if isinstance ( label_mask , Tensor ): label_mask = label_mask . numpy () showing_predictions = \"prediction\" in sample if showing_predictions : prediction_mask = sample [ \"prediction\" ] if isinstance ( prediction_mask , Tensor ): prediction_mask = prediction_mask . numpy () return self . _plot_sample ( image , label_mask , self . num_classes , prediction = prediction_mask if showing_predictions else None , suptitle = suptitle , class_names = self . class_names , ) @staticmethod def _plot_sample ( image , label , num_classes , prediction = None , suptitle = None , class_names = None ): num_images = 5 if prediction is not None else 4 fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 10 ), layout = \"compressed\" ) # for legend ax [ 0 ] . axis ( \"off\" ) norm = mpl . colors . Normalize ( vmin = 0 , vmax = num_classes - 1 ) ax [ 1 ] . axis ( \"off\" ) ax [ 1 ] . title . set_text ( \"Image\" ) ax [ 1 ] . imshow ( image ) ax [ 2 ] . axis ( \"off\" ) ax [ 2 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ 2 ] . imshow ( label , cmap = \"jet\" , norm = norm ) ax [ 3 ] . axis ( \"off\" ) ax [ 3 ] . title . set_text ( \"GT Mask on Image\" ) ax [ 3 ] . imshow ( image ) ax [ 3 ] . imshow ( label , cmap = \"jet\" , alpha = 0.3 , norm = norm ) if prediction is not None : ax [ 4 ] . title . set_text ( \"Predicted Mask\" ) ax [ 4 ] . imshow ( prediction , cmap = \"jet\" , norm = norm ) cmap = plt . get_cmap ( \"jet\" ) legend_data = [] for i , _ in enumerate ( range ( num_classes )): class_name = class_names [ i ] if class_names else str ( i ) data = [ i , cmap ( norm ( i )), class_name ] legend_data . append ( data ) handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = tuple ( v for v in c )) for k , c , n in legend_data ] labels = [ n for k , c , n in legend_data ] ax [ 0 ] . legend ( handles , labels , loc = \"center\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig __init__ ( data_root , num_classes , label_data_root = None , image_grep = '*' , label_grep = '*' , split = None , ignore_split_file_extensions = True , allow_substring_split_file = True , rgb_indices = None , dataset_bands = None , output_bands = None , class_names = None , constant_scale = 1 , transform = None , no_data_replace = None , no_label_replace = None , expand_temporal_dimension = False , reduce_zero_label = False ) # Constructor Parameters: data_root ( Path ) \u2013 Path to data root directory num_classes ( int ) \u2013 Number of classes in the dataset label_data_root ( Path , default: None ) \u2013 Path to data root directory with labels. If not specified, will use the same as for images. image_grep ( str , default: '*' ) \u2013 Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep ( str , default: '*' ) \u2013 Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split ( Path , default: None ) \u2013 Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices ( list [ str ] , default: None ) \u2013 Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands present in the dataset. output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands that should be output by the dataset. class_names ( list [ str ] , default: None ) \u2013 Class names. Defaults to None. constant_scale ( float , default: 1 ) \u2013 Factor to multiply image values by. Defaults to 1. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float | None , default: None ) \u2013 Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace ( int | None , default: None ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label ( bool , default: False ) \u2013 Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. Source code in terratorch/datasets/generic_pixel_wise_dataset.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def __init__ ( self , data_root : Path , num_classes : int , label_data_root : Path | None = None , image_grep : str | None = \"*\" , label_grep : str | None = \"*\" , split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , rgb_indices : list [ str ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , class_names : list [ str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory num_classes (int): Number of classes in the dataset label_data_root (Path, optional): Path to data root directory with labels. If not specified, will use the same as for images. image_grep (str, optional): Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep (str, optional): Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. class_names (list[str], optional): Class names. Defaults to None. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. \"\"\" super () . __init__ ( data_root , label_data_root = label_data_root , image_grep = image_grep , label_grep = label_grep , split = split , ignore_split_file_extensions = ignore_split_file_extensions , allow_substring_split_file = allow_substring_split_file , rgb_indices = rgb_indices , dataset_bands = dataset_bands , output_bands = output_bands , constant_scale = constant_scale , transform = transform , no_data_replace = no_data_replace , no_label_replace = no_label_replace , expand_temporal_dimension = expand_temporal_dimension , reduce_zero_label = reduce_zero_label , ) self . num_classes = num_classes self . class_names = class_names plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 a sample returned by :meth: __getitem__ suptitle ( str | None , default: None ) \u2013 optional string to use as a suptitle Returns: Figure \u2013 a matplotlib Figure with the rendered sample .. versionadded:: 0.2 Source code in terratorch/datasets/generic_pixel_wise_dataset.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample .. versionadded:: 0.2 \"\"\" image = sample [ \"image\" ] if len ( image . shape ) == 5 : return if isinstance ( image , Tensor ): image = image . numpy () image = image . take ( self . rgb_indices , axis = 0 ) image = np . transpose ( image , ( 1 , 2 , 0 )) image = ( image - image . min ( axis = ( 0 , 1 ))) * ( 1 / image . max ( axis = ( 0 , 1 ))) image = np . clip ( image , 0 , 1 ) label_mask = sample [ \"mask\" ] if isinstance ( label_mask , Tensor ): label_mask = label_mask . numpy () showing_predictions = \"prediction\" in sample if showing_predictions : prediction_mask = sample [ \"prediction\" ] if isinstance ( prediction_mask , Tensor ): prediction_mask = prediction_mask . numpy () return self . _plot_sample ( image , label_mask , self . num_classes , prediction = prediction_mask if showing_predictions else None , suptitle = suptitle , class_names = self . class_names , ) GenericPixelWiseDataset # Bases: NonGeoDataset , ABC This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset. Source code in terratorch/datasets/generic_pixel_wise_dataset.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 class GenericPixelWiseDataset ( NonGeoDataset , ABC ): \"\"\" This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset. \"\"\" def __init__ ( self , data_root : Path , label_data_root : Path | None = None , image_grep : str | None = \"*\" , label_grep : str | None = \"*\" , split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , rgb_indices : list [ int ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory label_data_root (Path, optional): Path to data root directory with labels. If not specified, will use the same as for images. image_grep (str, optional): Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep (str, optional): Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be refered to by output_bands. Defaults to None. output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the dataset as named by dataset_bands. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. \"\"\" super () . __init__ () self . split_file = split label_data_root = label_data_root if label_data_root is not None else data_root self . image_files = sorted ( glob . glob ( os . path . join ( data_root , image_grep ))) self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( label_data_root , label_grep ))) self . reduce_zero_label = reduce_zero_label self . expand_temporal_dimension = expand_temporal_dimension if self . expand_temporal_dimension and output_bands is None : msg = \"Please provide output_bands when expand_temporal_dimension is True\" raise Exception ( msg ) if self . split_file is not None : with open ( self . split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = ignore_split_file_extensions , allow_substring = allow_substring_split_file , ) self . segmentation_mask_files = filter_valid_files ( self . segmentation_mask_files , valid_files = valid_files , ignore_extensions = ignore_split_file_extensions , allow_substring = allow_substring_split_file , ) self . rgb_indices = [ 0 , 1 , 2 ] if rgb_indices is None else rgb_indices self . dataset_bands = generate_bands_intervals ( dataset_bands ) self . output_bands = generate_bands_intervals ( output_bands ) if self . output_bands and not self . dataset_bands : msg = \"If output bands provided, dataset_bands must also be provided\" return Exception ( msg ) # noqa: PLE0101 # There is a special condition if the bands are defined as simple strings. if self . output_bands : if len ( set ( self . output_bands ) & set ( self . dataset_bands )) != len ( self . output_bands ): msg = \"Output bands must be a subset of dataset bands\" raise Exception ( msg ) self . filter_indices = [ self . dataset_bands . index ( band ) for band in self . output_bands ] else : self . filter_indices = None # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform # self.transform = transform if transform else ToTensorV2() import warnings import rasterio warnings . filterwarnings ( \"ignore\" , category = rasterio . errors . NotGeoreferencedWarning ) def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , Any ]: image = self . _load_file ( self . image_files [ index ], nan_replace = self . no_data_replace ) . to_numpy () # to channels last if self . expand_temporal_dimension : image = rearrange ( image , \"(channels time) h w -> channels time h w\" , channels = len ( self . output_bands )) image = np . moveaxis ( image , 0 , - 1 ) if self . filter_indices : image = image [ ... , self . filter_indices ] output = { \"image\" : image . astype ( np . float32 ) * self . constant_scale , \"mask\" : self . _load_file ( self . segmentation_mask_files [ index ], nan_replace = self . no_label_replace ) . to_numpy ()[ 0 ] } if self . reduce_zero_label : output [ \"mask\" ] -= 1 if self . transform : output = self . transform ( ** output ) output [ \"filename\" ] = self . image_files [ index ] return output def _load_file ( self , path , nan_replace : int | float | None = None ) -> xr . DataArray : data = rioxarray . open_rasterio ( path , masked = True ) if nan_replace is not None : data = data . fillna ( nan_replace ) return data __init__ ( data_root , label_data_root = None , image_grep = '*' , label_grep = '*' , split = None , ignore_split_file_extensions = True , allow_substring_split_file = True , rgb_indices = None , dataset_bands = None , output_bands = None , constant_scale = 1 , transform = None , no_data_replace = None , no_label_replace = None , expand_temporal_dimension = False , reduce_zero_label = False ) # Constructor Parameters: data_root ( Path ) \u2013 Path to data root directory label_data_root ( Path , default: None ) \u2013 Path to data root directory with labels. If not specified, will use the same as for images. image_grep ( str , default: '*' ) \u2013 Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep ( str , default: '*' ) \u2013 Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split ( Path , default: None ) \u2013 Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices ( list [ str ] , default: None ) \u2013 Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands ( list [ HLSBands | int | tuple [ int , int ] | str ] | None , default: None ) \u2013 Bands present in the dataset. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be refered to by output_bands. Defaults to None. output_bands ( list [ HLSBands | int | tuple [ int , int ] | str ] | None , default: None ) \u2013 Bands that should be output by the dataset as named by dataset_bands. constant_scale ( float , default: 1 ) \u2013 Factor to multiply image values by. Defaults to 1. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float | None , default: None ) \u2013 Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace ( int | None , default: None ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to -1. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label ( bool , default: False ) \u2013 Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. Source code in terratorch/datasets/generic_pixel_wise_dataset.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def __init__ ( self , data_root : Path , label_data_root : Path | None = None , image_grep : str | None = \"*\" , label_grep : str | None = \"*\" , split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , rgb_indices : list [ int ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory label_data_root (Path, optional): Path to data root directory with labels. If not specified, will use the same as for images. image_grep (str, optional): Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep (str, optional): Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be refered to by output_bands. Defaults to None. output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the dataset as named by dataset_bands. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. \"\"\" super () . __init__ () self . split_file = split label_data_root = label_data_root if label_data_root is not None else data_root self . image_files = sorted ( glob . glob ( os . path . join ( data_root , image_grep ))) self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( label_data_root , label_grep ))) self . reduce_zero_label = reduce_zero_label self . expand_temporal_dimension = expand_temporal_dimension if self . expand_temporal_dimension and output_bands is None : msg = \"Please provide output_bands when expand_temporal_dimension is True\" raise Exception ( msg ) if self . split_file is not None : with open ( self . split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = ignore_split_file_extensions , allow_substring = allow_substring_split_file , ) self . segmentation_mask_files = filter_valid_files ( self . segmentation_mask_files , valid_files = valid_files , ignore_extensions = ignore_split_file_extensions , allow_substring = allow_substring_split_file , ) self . rgb_indices = [ 0 , 1 , 2 ] if rgb_indices is None else rgb_indices self . dataset_bands = generate_bands_intervals ( dataset_bands ) self . output_bands = generate_bands_intervals ( output_bands ) if self . output_bands and not self . dataset_bands : msg = \"If output bands provided, dataset_bands must also be provided\" return Exception ( msg ) # noqa: PLE0101 # There is a special condition if the bands are defined as simple strings. if self . output_bands : if len ( set ( self . output_bands ) & set ( self . dataset_bands )) != len ( self . output_bands ): msg = \"Output bands must be a subset of dataset bands\" raise Exception ( msg ) self . filter_indices = [ self . dataset_bands . index ( band ) for band in self . output_bands ] else : self . filter_indices = None # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform # self.transform = transform if transform else ToTensorV2() import warnings import rasterio warnings . filterwarnings ( \"ignore\" , category = rasterio . errors . NotGeoreferencedWarning ) terratorch.datasets.generic_scalar_label_dataset # Module containing generic dataset classes GenericNonGeoClassificationDataset # Bases: GenericScalarLabelDataset GenericNonGeoClassificationDataset Source code in terratorch/datasets/generic_scalar_label_dataset.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 class GenericNonGeoClassificationDataset ( GenericScalarLabelDataset ): \"\"\"GenericNonGeoClassificationDataset\"\"\" def __init__ ( self , data_root : Path , num_classes : int , split : Path | None = None , ignore_split_file_extensions : bool = True , # noqa: FBT001, FBT002 allow_substring_split_file : bool = True , # noqa: FBT001, FBT002 rgb_indices : list [ str ] | None = None , dataset_bands : list [ HLSBands | int ] | None = None , output_bands : list [ HLSBands | int ] | None = None , class_names : list [ str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float = 0 , expand_temporal_dimension : bool = False , # noqa: FBT001, FBT002 ) -> None : \"\"\"A generic Non-Geo dataset for classification. Args: data_root (Path): Path to data root directory num_classes (int): Number of classes in the dataset split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. class_names (list[str], optional): Class names. Defaults to None. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float): Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. \"\"\" super () . __init__ ( data_root , split = split , ignore_split_file_extensions = ignore_split_file_extensions , allow_substring_split_file = allow_substring_split_file , rgb_indices = rgb_indices , dataset_bands = dataset_bands , output_bands = output_bands , constant_scale = constant_scale , transform = transform , no_data_replace = no_data_replace , expand_temporal_dimension = expand_temporal_dimension , ) self . num_classes = num_classes self . class_names = class_names def __getitem__ ( self , index : int ) -> dict [ str , Any ]: item = super () . __getitem__ ( index ) item [ \"label\" ] = torch . tensor ( item [ \"label\" ]) . long () return item def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : pass __init__ ( data_root , num_classes , split = None , ignore_split_file_extensions = True , allow_substring_split_file = True , rgb_indices = None , dataset_bands = None , output_bands = None , class_names = None , constant_scale = 1 , transform = None , no_data_replace = 0 , expand_temporal_dimension = False ) # A generic Non-Geo dataset for classification. Parameters: data_root ( Path ) \u2013 Path to data root directory num_classes ( int ) \u2013 Number of classes in the dataset split ( Path , default: None ) \u2013 Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices ( list [ str ] , default: None ) \u2013 Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands present in the dataset. output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands that should be output by the dataset. class_names ( list [ str ] , default: None ) \u2013 Class names. Defaults to None. constant_scale ( float , default: 1 ) \u2013 Factor to multiply image values by. Defaults to 1. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float , default: 0 ) \u2013 Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. Source code in terratorch/datasets/generic_scalar_label_dataset.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def __init__ ( self , data_root : Path , num_classes : int , split : Path | None = None , ignore_split_file_extensions : bool = True , # noqa: FBT001, FBT002 allow_substring_split_file : bool = True , # noqa: FBT001, FBT002 rgb_indices : list [ str ] | None = None , dataset_bands : list [ HLSBands | int ] | None = None , output_bands : list [ HLSBands | int ] | None = None , class_names : list [ str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float = 0 , expand_temporal_dimension : bool = False , # noqa: FBT001, FBT002 ) -> None : \"\"\"A generic Non-Geo dataset for classification. Args: data_root (Path): Path to data root directory num_classes (int): Number of classes in the dataset split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. class_names (list[str], optional): Class names. Defaults to None. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float): Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. \"\"\" super () . __init__ ( data_root , split = split , ignore_split_file_extensions = ignore_split_file_extensions , allow_substring_split_file = allow_substring_split_file , rgb_indices = rgb_indices , dataset_bands = dataset_bands , output_bands = output_bands , constant_scale = constant_scale , transform = transform , no_data_replace = no_data_replace , expand_temporal_dimension = expand_temporal_dimension , ) self . num_classes = num_classes self . class_names = class_names GenericScalarLabelDataset # Bases: NonGeoDataset , ImageFolder , ABC This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset. Source code in terratorch/datasets/generic_scalar_label_dataset.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 class GenericScalarLabelDataset ( NonGeoDataset , ImageFolder , ABC ): \"\"\" This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset. \"\"\" def __init__ ( self , data_root : Path , split : Path | None = None , ignore_split_file_extensions : bool = True , # noqa: FBT001, FBT002 allow_substring_split_file : bool = True , # noqa: FBT001, FBT002 rgb_indices : list [ int ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float = 0 , expand_temporal_dimension : bool = False , # noqa: FBT001, FBT002 ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This parameter gives identifiers to input channels (bands) so that they can then be refered to by output_bands. Can use the HLSBands enum, ints, int ranges, or strings. Defaults to None. output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the dataset as named by dataset_bands. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float): Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. \"\"\" self . split_file = split self . image_files = sorted ( glob . glob ( os . path . join ( data_root , \"**\" ), recursive = True )) self . image_files = [ f for f in self . image_files if not os . path . isdir ( f )] self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . expand_temporal_dimension = expand_temporal_dimension if self . expand_temporal_dimension and output_bands is None : msg = \"Please provide output_bands when expand_temporal_dimension is True\" raise Exception ( msg ) if self . split_file is not None : with open ( self . split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = ignore_split_file_extensions , allow_substring = allow_substring_split_file , ) def is_valid_file ( x ): return x in self . image_files else : def is_valid_file ( x ): return True super () . __init__ ( root = data_root , transform = None , target_transform = None , loader = rasterio_loader , is_valid_file = is_valid_file ) self . rgb_indices = [ 0 , 1 , 2 ] if rgb_indices is None else rgb_indices self . dataset_bands = generate_bands_intervals ( dataset_bands ) self . output_bands = generate_bands_intervals ( output_bands ) if self . output_bands and not self . dataset_bands : msg = \"If output bands provided, dataset_bands must also be provided\" return Exception ( msg ) # noqa: PLE0101 # There is a special condition if the bands are defined as simple strings. if self . output_bands : if len ( set ( self . output_bands ) & set ( self . dataset_bands )) != len ( self . output_bands ): msg = \"Output bands must be a subset of dataset bands\" raise Exception ( msg ) self . filter_indices = [ self . dataset_bands . index ( band ) for band in self . output_bands ] else : self . filter_indices = None # If no transform is given, apply only to transform to torch tensor self . transforms = transform if transform else default_transform # self.transform = transform if transform else ToTensorV2() import warnings import rasterio warnings . filterwarnings ( \"ignore\" , category = rasterio . errors . NotGeoreferencedWarning ) def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , Any ]: image , label = ImageFolder . __getitem__ ( self , index ) if self . expand_temporal_dimension : image = rearrange ( image , \"h w (channels time) -> time h w channels\" , channels = len ( self . output_bands )) if self . filter_indices : image = image [ ... , self . filter_indices ] image = image . astype ( np . float32 ) * self . constant_scale if self . transforms : image = self . transforms ( image = image )[ \"image\" ] # albumentations returns dict output = { \"image\" : image , \"label\" : label , # samples is an attribute of ImageFolder. Contains a tuple of (Path, Target) \"filename\" : self . image_files [ index ] } return output def _generate_bands_intervals ( self , bands_intervals : list [ int | str | HLSBands | tuple [ int ]] | None = None ): if bands_intervals is None : return None bands = [] for element in bands_intervals : # if its an interval if isinstance ( element , tuple ): if len ( element ) != 2 : # noqa: PLR2004 msg = \"When defining an interval, a tuple of two integers should be passed, \\ defining start and end indices inclusive\" raise Exception ( msg ) expanded_element = list ( range ( element [ 0 ], element [ 1 ] + 1 )) bands . extend ( expanded_element ) else : bands . append ( element ) return bands def _load_file ( self , path ) -> xr . DataArray : data = rioxarray . open_rasterio ( path , masked = True ) data = data . fillna ( self . no_data_replace ) return data __init__ ( data_root , split = None , ignore_split_file_extensions = True , allow_substring_split_file = True , rgb_indices = None , dataset_bands = None , output_bands = None , constant_scale = 1 , transform = None , no_data_replace = 0 , expand_temporal_dimension = False ) # Constructor Parameters: data_root ( Path ) \u2013 Path to data root directory split ( Path , default: None ) \u2013 Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices ( list [ str ] , default: None ) \u2013 Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands ( list [ HLSBands | int | tuple [ int , int ] | str ] | None , default: None ) \u2013 Bands present in the dataset. This parameter gives identifiers to input channels (bands) so that they can then be refered to by output_bands. Can use the HLSBands enum, ints, int ranges, or strings. Defaults to None. output_bands ( list [ HLSBands | int | tuple [ int , int ] | str ] | None , default: None ) \u2013 Bands that should be output by the dataset as named by dataset_bands. constant_scale ( float , default: 1 ) \u2013 Factor to multiply image values by. Defaults to 1. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float , default: 0 ) \u2013 Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. Source code in terratorch/datasets/generic_scalar_label_dataset.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def __init__ ( self , data_root : Path , split : Path | None = None , ignore_split_file_extensions : bool = True , # noqa: FBT001, FBT002 allow_substring_split_file : bool = True , # noqa: FBT001, FBT002 rgb_indices : list [ int ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float = 0 , expand_temporal_dimension : bool = False , # noqa: FBT001, FBT002 ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This parameter gives identifiers to input channels (bands) so that they can then be refered to by output_bands. Can use the HLSBands enum, ints, int ranges, or strings. Defaults to None. output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the dataset as named by dataset_bands. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float): Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. \"\"\" self . split_file = split self . image_files = sorted ( glob . glob ( os . path . join ( data_root , \"**\" ), recursive = True )) self . image_files = [ f for f in self . image_files if not os . path . isdir ( f )] self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . expand_temporal_dimension = expand_temporal_dimension if self . expand_temporal_dimension and output_bands is None : msg = \"Please provide output_bands when expand_temporal_dimension is True\" raise Exception ( msg ) if self . split_file is not None : with open ( self . split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = ignore_split_file_extensions , allow_substring = allow_substring_split_file , ) def is_valid_file ( x ): return x in self . image_files else : def is_valid_file ( x ): return True super () . __init__ ( root = data_root , transform = None , target_transform = None , loader = rasterio_loader , is_valid_file = is_valid_file ) self . rgb_indices = [ 0 , 1 , 2 ] if rgb_indices is None else rgb_indices self . dataset_bands = generate_bands_intervals ( dataset_bands ) self . output_bands = generate_bands_intervals ( output_bands ) if self . output_bands and not self . dataset_bands : msg = \"If output bands provided, dataset_bands must also be provided\" return Exception ( msg ) # noqa: PLE0101 # There is a special condition if the bands are defined as simple strings. if self . output_bands : if len ( set ( self . output_bands ) & set ( self . dataset_bands )) != len ( self . output_bands ): msg = \"Output bands must be a subset of dataset bands\" raise Exception ( msg ) self . filter_indices = [ self . dataset_bands . index ( band ) for band in self . output_bands ] else : self . filter_indices = None # If no transform is given, apply only to transform to torch tensor self . transforms = transform if transform else default_transform # self.transform = transform if transform else ToTensorV2() import warnings import rasterio warnings . filterwarnings ( \"ignore\" , category = rasterio . errors . NotGeoreferencedWarning ) Generic Data Modules # terratorch.datamodules.generic_pixel_wise_data_module # This module contains generic data modules for instantiation at runtime. GenericNonGeoPixelwiseRegressionDataModule # Bases: NonGeoDataModule This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoPixelwiseRegressionDataset Source code in terratorch/datamodules/generic_pixel_wise_data_module.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 class GenericNonGeoPixelwiseRegressionDataModule ( NonGeoDataModule ): \"\"\"This is a generic datamodule class for instantiating data modules at runtime. Composes several [GenericNonGeoPixelwiseRegressionDataset][terratorch.datasets.GenericNonGeoPixelwiseRegressionDataset] \"\"\" def __init__ ( self , batch_size : int , num_workers : int , train_data_root : Path , val_data_root : Path , test_data_root : Path , means : list [ float ] | str , stds : list [ float ] | str , predict_data_root : Path | None = None , img_grep : str | None = \"*\" , label_grep : str | None = \"*\" , train_label_data_root : Path | None = None , val_label_data_root : Path | None = None , test_label_data_root : Path | None = None , train_split : Path | None = None , val_split : Path | None = None , test_split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , rgb_indices : list [ int ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , no_data_replace : float | None = None , no_label_replace : int | None = None , drop_last : bool = True , pin_memory : bool = False , check_stackability : bool = True , ** kwargs : Any , ) -> None : \"\"\"Constructor Args: batch_size (int): _description_ num_workers (int): _description_ train_data_root (Path): _description_ val_data_root (Path): _description_ test_data_root (Path): _description_ predict_data_root (Path): _description_ img_grep (str): _description_ label_grep (str): _description_ means (list[float]): _description_ stds (list[float]): _description_ train_label_data_root (Path | None, optional): _description_. Defaults to None. val_label_data_root (Path | None, optional): _description_. Defaults to None. test_label_data_root (Path | None, optional): _description_. Defaults to None. train_split (Path | None, optional): _description_. Defaults to None. val_split (Path | None, optional): _description_. Defaults to None. test_split (Path | None, optional): _description_. Defaults to None. ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None. predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite. predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite. constant_scale (float, optional): _description_. Defaults to 1. rgb_indices (list[int] | None, optional): _description_. Defaults to None. train_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. drop_last (bool): Drop the last batch if it is not complete. Defaults to True. pin_memory (bool): If ``True``, the data loader will copy Tensors into device/CUDA pinned memory before returning them. Defaults to False. check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked. \"\"\" super () . __init__ ( GenericNonGeoPixelwiseRegressionDataset , batch_size , num_workers , ** kwargs ) self . img_grep = img_grep self . label_grep = label_grep self . train_root = train_data_root self . val_root = val_data_root self . test_root = test_data_root self . predict_root = predict_data_root self . train_split = train_split self . val_split = val_split self . test_split = test_split self . ignore_split_file_extensions = ignore_split_file_extensions self . allow_substring_split_file = allow_substring_split_file self . drop_last = drop_last self . pin_memory = pin_memory self . expand_temporal_dimension = expand_temporal_dimension self . reduce_zero_label = reduce_zero_label self . train_label_data_root = train_label_data_root self . val_label_data_root = val_label_data_root self . test_label_data_root = test_label_data_root self . constant_scale = constant_scale self . dataset_bands = dataset_bands self . predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands self . predict_output_bands = predict_output_bands if predict_output_bands else output_bands self . output_bands = output_bands self . rgb_indices = rgb_indices # self.aug = AugmentationSequential( # K.Normalize(means, stds), # data_keys=[\"image\"], # ) means = load_from_file_or_attribute ( means ) stds = load_from_file_or_attribute ( stds ) self . aug = Normalize ( means , stds ) self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . check_stackability = check_stackability def setup ( self , stage : str ) -> None : if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( self . train_root , image_grep = self . img_grep , label_grep = self . label_grep , label_data_root = self . train_label_data_root , split = self . train_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . train_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( self . val_root , image_grep = self . img_grep , label_grep = self . label_grep , label_data_root = self . val_label_data_root , split = self . val_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . val_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( self . test_root , image_grep = self . img_grep , label_grep = self . label_grep , label_data_root = self . test_label_data_root , split = self . test_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . test_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) if stage in [ \"predict\" ] and self . predict_root : self . predict_dataset = self . dataset_class ( self . predict_root , image_grep = self . img_grep , dataset_bands = self . predict_dataset_bands , output_bands = self . predict_output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . test_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) def _dataloader_factory ( self , split : str ) -> DataLoader [ dict [ str , Tensor ]]: \"\"\"Implement one or more PyTorch DataLoaders. Args: split: Either 'train', 'val', 'test', or 'predict'. Returns: A collection of data loaders specifying samples. Raises: MisconfigurationException: If :meth:`setup` does not define a dataset or sampler, or if the dataset or sampler has length 0. \"\"\" dataset = self . _valid_attribute ( f \" { split } _dataset\" , \"dataset\" ) batch_size = self . _valid_attribute ( f \" { split } _batch_size\" , \"batch_size\" ) if self . check_stackability : print ( \"Checking stackability.\" ) batch_size = check_dataset_stackability ( dataset , batch_size ) return DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = split == \"train\" , num_workers = self . num_workers , collate_fn = self . collate_fn , drop_last = split == \"train\" and self . drop_last , pin_memory = self . pin_memory , ) __init__ ( batch_size , num_workers , train_data_root , val_data_root , test_data_root , means , stds , predict_data_root = None , img_grep = '*' , label_grep = '*' , train_label_data_root = None , val_label_data_root = None , test_label_data_root = None , train_split = None , val_split = None , test_split = None , ignore_split_file_extensions = True , allow_substring_split_file = True , dataset_bands = None , output_bands = None , predict_dataset_bands = None , predict_output_bands = None , constant_scale = 1 , rgb_indices = None , train_transform = None , val_transform = None , test_transform = None , expand_temporal_dimension = False , reduce_zero_label = False , no_data_replace = None , no_label_replace = None , drop_last = True , pin_memory = False , check_stackability = True , ** kwargs ) # Constructor Parameters: batch_size ( int ) \u2013 description num_workers ( int ) \u2013 description train_data_root ( Path ) \u2013 description val_data_root ( Path ) \u2013 description test_data_root ( Path ) \u2013 description predict_data_root ( Path , default: None ) \u2013 description img_grep ( str , default: '*' ) \u2013 description label_grep ( str , default: '*' ) \u2013 description means ( list [ float ] ) \u2013 description stds ( list [ float ] ) \u2013 description train_label_data_root ( Path | None , default: None ) \u2013 description . Defaults to None. val_label_data_root ( Path | None , default: None ) \u2013 description . Defaults to None. test_label_data_root ( Path | None , default: None ) \u2013 description . Defaults to None. train_split ( Path | None , default: None ) \u2013 description . Defaults to None. val_split ( Path | None , default: None ) \u2013 description . Defaults to None. test_split ( Path | None , default: None ) \u2013 description . Defaults to None. ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands present in the dataset. Defaults to None. output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None. predict_dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite. predict_output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite. constant_scale ( float , default: 1 ) \u2013 description . Defaults to 1. rgb_indices ( list [ int ] | None , default: None ) \u2013 description . Defaults to None. train_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float | None , default: None ) \u2013 Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace ( int | None , default: None ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label ( bool , default: False ) \u2013 Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. drop_last ( bool , default: True ) \u2013 Drop the last batch if it is not complete. Defaults to True. pin_memory ( bool , default: False ) \u2013 If True , the data loader will copy Tensors check_stackability ( bool , default: True ) \u2013 Check if all the files in the dataset has the same size and can be stacked. Source code in terratorch/datamodules/generic_pixel_wise_data_module.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 def __init__ ( self , batch_size : int , num_workers : int , train_data_root : Path , val_data_root : Path , test_data_root : Path , means : list [ float ] | str , stds : list [ float ] | str , predict_data_root : Path | None = None , img_grep : str | None = \"*\" , label_grep : str | None = \"*\" , train_label_data_root : Path | None = None , val_label_data_root : Path | None = None , test_label_data_root : Path | None = None , train_split : Path | None = None , val_split : Path | None = None , test_split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , rgb_indices : list [ int ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , no_data_replace : float | None = None , no_label_replace : int | None = None , drop_last : bool = True , pin_memory : bool = False , check_stackability : bool = True , ** kwargs : Any , ) -> None : \"\"\"Constructor Args: batch_size (int): _description_ num_workers (int): _description_ train_data_root (Path): _description_ val_data_root (Path): _description_ test_data_root (Path): _description_ predict_data_root (Path): _description_ img_grep (str): _description_ label_grep (str): _description_ means (list[float]): _description_ stds (list[float]): _description_ train_label_data_root (Path | None, optional): _description_. Defaults to None. val_label_data_root (Path | None, optional): _description_. Defaults to None. test_label_data_root (Path | None, optional): _description_. Defaults to None. train_split (Path | None, optional): _description_. Defaults to None. val_split (Path | None, optional): _description_. Defaults to None. test_split (Path | None, optional): _description_. Defaults to None. ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None. predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite. predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite. constant_scale (float, optional): _description_. Defaults to 1. rgb_indices (list[int] | None, optional): _description_. Defaults to None. train_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. drop_last (bool): Drop the last batch if it is not complete. Defaults to True. pin_memory (bool): If ``True``, the data loader will copy Tensors into device/CUDA pinned memory before returning them. Defaults to False. check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked. \"\"\" super () . __init__ ( GenericNonGeoPixelwiseRegressionDataset , batch_size , num_workers , ** kwargs ) self . img_grep = img_grep self . label_grep = label_grep self . train_root = train_data_root self . val_root = val_data_root self . test_root = test_data_root self . predict_root = predict_data_root self . train_split = train_split self . val_split = val_split self . test_split = test_split self . ignore_split_file_extensions = ignore_split_file_extensions self . allow_substring_split_file = allow_substring_split_file self . drop_last = drop_last self . pin_memory = pin_memory self . expand_temporal_dimension = expand_temporal_dimension self . reduce_zero_label = reduce_zero_label self . train_label_data_root = train_label_data_root self . val_label_data_root = val_label_data_root self . test_label_data_root = test_label_data_root self . constant_scale = constant_scale self . dataset_bands = dataset_bands self . predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands self . predict_output_bands = predict_output_bands if predict_output_bands else output_bands self . output_bands = output_bands self . rgb_indices = rgb_indices # self.aug = AugmentationSequential( # K.Normalize(means, stds), # data_keys=[\"image\"], # ) means = load_from_file_or_attribute ( means ) stds = load_from_file_or_attribute ( stds ) self . aug = Normalize ( means , stds ) self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . check_stackability = check_stackability GenericNonGeoSegmentationDataModule # Bases: NonGeoDataModule This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoSegmentationDatasets Source code in terratorch/datamodules/generic_pixel_wise_data_module.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 class GenericNonGeoSegmentationDataModule ( NonGeoDataModule ): \"\"\" This is a generic datamodule class for instantiating data modules at runtime. Composes several [GenericNonGeoSegmentationDatasets][terratorch.datasets.GenericNonGeoSegmentationDataset] \"\"\" def __init__ ( self , batch_size : int , num_workers : int , train_data_root : Path , val_data_root : Path , test_data_root : Path , img_grep : str , label_grep : str , means : list [ float ] | str , stds : list [ float ] | str , num_classes : int , predict_data_root : Path | None = None , train_label_data_root : Path | None = None , val_label_data_root : Path | None = None , test_label_data_root : Path | None = None , train_split : Path | None = None , val_split : Path | None = None , test_split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , rgb_indices : list [ int ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , no_data_replace : float | None = None , no_label_replace : int | None = None , drop_last : bool = True , pin_memory : bool = False , ** kwargs : Any , ) -> None : \"\"\"Constructor Args: batch_size (int): _description_ num_workers (int): _description_ train_data_root (Path): _description_ val_data_root (Path): _description_ test_data_root (Path): _description_ predict_data_root (Path): _description_ img_grep (str): _description_ label_grep (str): _description_ means (list[float]): _description_ stds (list[float]): _description_ num_classes (int): _description_ train_label_data_root (Path | None, optional): _description_. Defaults to None. val_label_data_root (Path | None, optional): _description_. Defaults to None. test_label_data_root (Path | None, optional): _description_. Defaults to None. train_split (Path | None, optional): _description_. Defaults to None. val_split (Path | None, optional): _description_. Defaults to None. test_split (Path | None, optional): _description_. Defaults to None. ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None. predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite. predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite. constant_scale (float, optional): _description_. Defaults to 1. rgb_indices (list[int] | None, optional): _description_. Defaults to None. train_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. drop_last (bool): Drop the last batch if it is not complete. Defaults to True. pin_memory (bool): If ``True``, the data loader will copy Tensors into device/CUDA pinned memory before returning them. Defaults to False. \"\"\" super () . __init__ ( GenericNonGeoSegmentationDataset , batch_size , num_workers , ** kwargs ) self . num_classes = num_classes self . img_grep = img_grep self . label_grep = label_grep self . train_root = train_data_root self . val_root = val_data_root self . test_root = test_data_root self . predict_root = predict_data_root self . train_split = train_split self . val_split = val_split self . test_split = test_split self . ignore_split_file_extensions = ignore_split_file_extensions self . allow_substring_split_file = allow_substring_split_file self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . drop_last = drop_last self . pin_memory = pin_memory self . train_label_data_root = train_label_data_root self . val_label_data_root = val_label_data_root self . test_label_data_root = test_label_data_root self . dataset_bands = dataset_bands self . predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands self . predict_output_bands = predict_output_bands if predict_output_bands else output_bands self . output_bands = output_bands self . rgb_indices = rgb_indices self . expand_temporal_dimension = expand_temporal_dimension self . reduce_zero_label = reduce_zero_label self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) # self.aug = AugmentationSequential( # K.Normalize(means, stds), # data_keys=[\"image\"], # ) means = load_from_file_or_attribute ( means ) stds = load_from_file_or_attribute ( stds ) self . aug = Normalize ( means , stds ) # self.aug = Normalize(means, stds) # self.collate_fn = collate_fn_list_dicts def setup ( self , stage : str ) -> None : if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( self . train_root , self . num_classes , image_grep = self . img_grep , label_grep = self . label_grep , label_data_root = self . train_label_data_root , split = self . train_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . train_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( self . val_root , self . num_classes , image_grep = self . img_grep , label_grep = self . label_grep , label_data_root = self . val_label_data_root , split = self . val_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . val_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( self . test_root , self . num_classes , image_grep = self . img_grep , label_grep = self . label_grep , label_data_root = self . test_label_data_root , split = self . test_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . test_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) if stage in [ \"predict\" ] and self . predict_root : self . predict_dataset = self . dataset_class ( self . predict_root , self . num_classes , dataset_bands = self . predict_dataset_bands , output_bands = self . predict_output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . test_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) def _dataloader_factory ( self , split : str ) -> DataLoader [ dict [ str , Tensor ]]: \"\"\"Implement one or more PyTorch DataLoaders. Args: split: Either 'train', 'val', 'test', or 'predict'. Returns: A collection of data loaders specifying samples. Raises: MisconfigurationException: If :meth:`setup` does not define a dataset or sampler, or if the dataset or sampler has length 0. \"\"\" dataset = self . _valid_attribute ( f \" { split } _dataset\" , \"dataset\" ) batch_size = self . _valid_attribute ( f \" { split } _batch_size\" , \"batch_size\" ) batch_size = check_dataset_stackability ( dataset , batch_size ) return DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = split == \"train\" , num_workers = self . num_workers , collate_fn = self . collate_fn , drop_last = split == \"train\" and self . drop_last , pin_memory = self . pin_memory , ) __init__ ( batch_size , num_workers , train_data_root , val_data_root , test_data_root , img_grep , label_grep , means , stds , num_classes , predict_data_root = None , train_label_data_root = None , val_label_data_root = None , test_label_data_root = None , train_split = None , val_split = None , test_split = None , ignore_split_file_extensions = True , allow_substring_split_file = True , dataset_bands = None , output_bands = None , predict_dataset_bands = None , predict_output_bands = None , constant_scale = 1 , rgb_indices = None , train_transform = None , val_transform = None , test_transform = None , expand_temporal_dimension = False , reduce_zero_label = False , no_data_replace = None , no_label_replace = None , drop_last = True , pin_memory = False , ** kwargs ) # Constructor Parameters: batch_size ( int ) \u2013 description num_workers ( int ) \u2013 description train_data_root ( Path ) \u2013 description val_data_root ( Path ) \u2013 description test_data_root ( Path ) \u2013 description predict_data_root ( Path , default: None ) \u2013 description img_grep ( str ) \u2013 description label_grep ( str ) \u2013 description means ( list [ float ] ) \u2013 description stds ( list [ float ] ) \u2013 description num_classes ( int ) \u2013 description train_label_data_root ( Path | None , default: None ) \u2013 description . Defaults to None. val_label_data_root ( Path | None , default: None ) \u2013 description . Defaults to None. test_label_data_root ( Path | None , default: None ) \u2013 description . Defaults to None. train_split ( Path | None , default: None ) \u2013 description . Defaults to None. val_split ( Path | None , default: None ) \u2013 description . Defaults to None. test_split ( Path | None , default: None ) \u2013 description . Defaults to None. ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands present in the dataset. Defaults to None. output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None. predict_dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite. predict_output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite. constant_scale ( float , default: 1 ) \u2013 description . Defaults to 1. rgb_indices ( list [ int ] | None , default: None ) \u2013 description . Defaults to None. train_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float | None , default: None ) \u2013 Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace ( int | None , default: None ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label ( bool , default: False ) \u2013 Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. drop_last ( bool , default: True ) \u2013 Drop the last batch if it is not complete. Defaults to True. pin_memory ( bool , default: False ) \u2013 If True , the data loader will copy Tensors Source code in terratorch/datamodules/generic_pixel_wise_data_module.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def __init__ ( self , batch_size : int , num_workers : int , train_data_root : Path , val_data_root : Path , test_data_root : Path , img_grep : str , label_grep : str , means : list [ float ] | str , stds : list [ float ] | str , num_classes : int , predict_data_root : Path | None = None , train_label_data_root : Path | None = None , val_label_data_root : Path | None = None , test_label_data_root : Path | None = None , train_split : Path | None = None , val_split : Path | None = None , test_split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , rgb_indices : list [ int ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , no_data_replace : float | None = None , no_label_replace : int | None = None , drop_last : bool = True , pin_memory : bool = False , ** kwargs : Any , ) -> None : \"\"\"Constructor Args: batch_size (int): _description_ num_workers (int): _description_ train_data_root (Path): _description_ val_data_root (Path): _description_ test_data_root (Path): _description_ predict_data_root (Path): _description_ img_grep (str): _description_ label_grep (str): _description_ means (list[float]): _description_ stds (list[float]): _description_ num_classes (int): _description_ train_label_data_root (Path | None, optional): _description_. Defaults to None. val_label_data_root (Path | None, optional): _description_. Defaults to None. test_label_data_root (Path | None, optional): _description_. Defaults to None. train_split (Path | None, optional): _description_. Defaults to None. val_split (Path | None, optional): _description_. Defaults to None. test_split (Path | None, optional): _description_. Defaults to None. ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None. predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite. predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite. constant_scale (float, optional): _description_. Defaults to 1. rgb_indices (list[int] | None, optional): _description_. Defaults to None. train_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. drop_last (bool): Drop the last batch if it is not complete. Defaults to True. pin_memory (bool): If ``True``, the data loader will copy Tensors into device/CUDA pinned memory before returning them. Defaults to False. \"\"\" super () . __init__ ( GenericNonGeoSegmentationDataset , batch_size , num_workers , ** kwargs ) self . num_classes = num_classes self . img_grep = img_grep self . label_grep = label_grep self . train_root = train_data_root self . val_root = val_data_root self . test_root = test_data_root self . predict_root = predict_data_root self . train_split = train_split self . val_split = val_split self . test_split = test_split self . ignore_split_file_extensions = ignore_split_file_extensions self . allow_substring_split_file = allow_substring_split_file self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . drop_last = drop_last self . pin_memory = pin_memory self . train_label_data_root = train_label_data_root self . val_label_data_root = val_label_data_root self . test_label_data_root = test_label_data_root self . dataset_bands = dataset_bands self . predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands self . predict_output_bands = predict_output_bands if predict_output_bands else output_bands self . output_bands = output_bands self . rgb_indices = rgb_indices self . expand_temporal_dimension = expand_temporal_dimension self . reduce_zero_label = reduce_zero_label self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) # self.aug = AugmentationSequential( # K.Normalize(means, stds), # data_keys=[\"image\"], # ) means = load_from_file_or_attribute ( means ) stds = load_from_file_or_attribute ( stds ) self . aug = Normalize ( means , stds ) terratorch.datamodules.generic_scalar_label_data_module # This module contains generic data modules for instantiation at runtime. GenericNonGeoClassificationDataModule # Bases: NonGeoDataModule This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoClassificationDatasets Source code in terratorch/datamodules/generic_scalar_label_data_module.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 class GenericNonGeoClassificationDataModule ( NonGeoDataModule ): \"\"\" This is a generic datamodule class for instantiating data modules at runtime. Composes several [GenericNonGeoClassificationDatasets][terratorch.datasets.GenericNonGeoClassificationDataset] \"\"\" def __init__ ( self , batch_size : int , num_workers : int , train_data_root : Path , val_data_root : Path , test_data_root : Path , means : list [ float ] | str , stds : list [ float ] | str , num_classes : int , predict_data_root : Path | None = None , train_split : Path | None = None , val_split : Path | None = None , test_split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , dataset_bands : list [ HLSBands | int ] | None = None , predict_dataset_bands : list [ HLSBands | int ] | None = None , output_bands : list [ HLSBands | int ] | None = None , constant_scale : float = 1 , rgb_indices : list [ int ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , expand_temporal_dimension : bool = False , no_data_replace : float = 0 , drop_last : bool = True , check_stackability : bool = True , ** kwargs : Any , ) -> None : \"\"\"Constructor Args: batch_size (int): _description_ num_workers (int): _description_ train_data_root (Path): _description_ val_data_root (Path): _description_ test_data_root (Path): _description_ means (list[float]): _description_ stds (list[float]): _description_ num_classes (int): _description_ predict_data_root (Path): _description_ train_split (Path | None, optional): _description_. Defaults to None. val_split (Path | None, optional): _description_. Defaults to None. test_split (Path | None, optional): _description_. Defaults to None. ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None. predict_dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None. output_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None. constant_scale (float, optional): _description_. Defaults to 1. rgb_indices (list[int] | None, optional): _description_. Defaults to None. train_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float): Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. drop_last (bool): Drop the last batch if it is not complete. Defaults to True. check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked. \"\"\" super () . __init__ ( GenericNonGeoClassificationDataset , batch_size , num_workers , ** kwargs ) self . num_classes = num_classes self . train_root = train_data_root self . val_root = val_data_root self . test_root = test_data_root self . predict_root = predict_data_root self . train_split = train_split self . val_split = val_split self . test_split = test_split self . ignore_split_file_extensions = ignore_split_file_extensions self . allow_substring_split_file = allow_substring_split_file self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . drop_last = drop_last self . dataset_bands = dataset_bands self . predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands self . output_bands = output_bands self . rgb_indices = rgb_indices self . expand_temporal_dimension = expand_temporal_dimension self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) # self.aug = AugmentationSequential( # K.Normalize(means, stds), # data_keys=[\"image\"], # ) means = load_from_file_or_attribute ( means ) stds = load_from_file_or_attribute ( stds ) self . aug = Normalize ( means , stds ) # self.aug = Normalize(means, stds) # self.collate_fn = collate_fn_list_dicts self . check_stackability = check_stackability def setup ( self , stage : str ) -> None : if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( self . train_root , self . num_classes , split = self . train_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . train_transform , no_data_replace = self . no_data_replace , expand_temporal_dimension = self . expand_temporal_dimension , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( self . val_root , self . num_classes , split = self . val_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . val_transform , no_data_replace = self . no_data_replace , expand_temporal_dimension = self . expand_temporal_dimension , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( self . test_root , self . num_classes , split = self . test_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . test_transform , no_data_replace = self . no_data_replace , expand_temporal_dimension = self . expand_temporal_dimension , ) if stage in [ \"predict\" ] and self . predict_root : self . predict_dataset = self . dataset_class ( self . predict_root , self . num_classes , dataset_bands = self . predict_dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . test_transform , no_data_replace = self . no_data_replace , expand_temporal_dimension = self . expand_temporal_dimension , ) def _dataloader_factory ( self , split : str ) -> DataLoader [ dict [ str , Tensor ]]: \"\"\"Implement one or more PyTorch DataLoaders. Args: split: Either 'train', 'val', 'test', or 'predict'. Returns: A collection of data loaders specifying samples. Raises: MisconfigurationException: If :meth:`setup` does not define a dataset or sampler, or if the dataset or sampler has length 0. \"\"\" dataset = self . _valid_attribute ( f \" { split } _dataset\" , \"dataset\" ) batch_size = self . _valid_attribute ( f \" { split } _batch_size\" , \"batch_size\" ) if self . check_stackability : print ( \"Checking stackability.\" ) batch_size = check_dataset_stackability ( dataset , batch_size ) return DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = split == \"train\" , num_workers = self . num_workers , collate_fn = self . collate_fn , drop_last = split == \"train\" and self . drop_last , ) __init__ ( batch_size , num_workers , train_data_root , val_data_root , test_data_root , means , stds , num_classes , predict_data_root = None , train_split = None , val_split = None , test_split = None , ignore_split_file_extensions = True , allow_substring_split_file = True , dataset_bands = None , predict_dataset_bands = None , output_bands = None , constant_scale = 1 , rgb_indices = None , train_transform = None , val_transform = None , test_transform = None , expand_temporal_dimension = False , no_data_replace = 0 , drop_last = True , check_stackability = True , ** kwargs ) # Constructor Parameters: batch_size ( int ) \u2013 description num_workers ( int ) \u2013 description train_data_root ( Path ) \u2013 description val_data_root ( Path ) \u2013 description test_data_root ( Path ) \u2013 description means ( list [ float ] ) \u2013 description stds ( list [ float ] ) \u2013 description num_classes ( int ) \u2013 description predict_data_root ( Path , default: None ) \u2013 description train_split ( Path | None , default: None ) \u2013 description . Defaults to None. val_split ( Path | None , default: None ) \u2013 description . Defaults to None. test_split ( Path | None , default: None ) \u2013 description . Defaults to None. ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 description . Defaults to None. predict_dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 description . Defaults to None. output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 description . Defaults to None. constant_scale ( float , default: 1 ) \u2013 description . Defaults to 1. rgb_indices ( list [ int ] | None , default: None ) \u2013 description . Defaults to None. train_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float , default: 0 ) \u2013 Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. drop_last ( bool , default: True ) \u2013 Drop the last batch if it is not complete. Defaults to True. check_stackability ( bool , default: True ) \u2013 Check if all the files in the dataset has the same size and can be stacked. Source code in terratorch/datamodules/generic_scalar_label_data_module.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def __init__ ( self , batch_size : int , num_workers : int , train_data_root : Path , val_data_root : Path , test_data_root : Path , means : list [ float ] | str , stds : list [ float ] | str , num_classes : int , predict_data_root : Path | None = None , train_split : Path | None = None , val_split : Path | None = None , test_split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , dataset_bands : list [ HLSBands | int ] | None = None , predict_dataset_bands : list [ HLSBands | int ] | None = None , output_bands : list [ HLSBands | int ] | None = None , constant_scale : float = 1 , rgb_indices : list [ int ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , expand_temporal_dimension : bool = False , no_data_replace : float = 0 , drop_last : bool = True , check_stackability : bool = True , ** kwargs : Any , ) -> None : \"\"\"Constructor Args: batch_size (int): _description_ num_workers (int): _description_ train_data_root (Path): _description_ val_data_root (Path): _description_ test_data_root (Path): _description_ means (list[float]): _description_ stds (list[float]): _description_ num_classes (int): _description_ predict_data_root (Path): _description_ train_split (Path | None, optional): _description_. Defaults to None. val_split (Path | None, optional): _description_. Defaults to None. test_split (Path | None, optional): _description_. Defaults to None. ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None. predict_dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None. output_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None. constant_scale (float, optional): _description_. Defaults to 1. rgb_indices (list[int] | None, optional): _description_. Defaults to None. train_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float): Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. drop_last (bool): Drop the last batch if it is not complete. Defaults to True. check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked. \"\"\" super () . __init__ ( GenericNonGeoClassificationDataset , batch_size , num_workers , ** kwargs ) self . num_classes = num_classes self . train_root = train_data_root self . val_root = val_data_root self . test_root = test_data_root self . predict_root = predict_data_root self . train_split = train_split self . val_split = val_split self . test_split = test_split self . ignore_split_file_extensions = ignore_split_file_extensions self . allow_substring_split_file = allow_substring_split_file self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . drop_last = drop_last self . dataset_bands = dataset_bands self . predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands self . output_bands = output_bands self . rgb_indices = rgb_indices self . expand_temporal_dimension = expand_temporal_dimension self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) # self.aug = AugmentationSequential( # K.Normalize(means, stds), # data_keys=[\"image\"], # ) means = load_from_file_or_attribute ( means ) stds = load_from_file_or_attribute ( stds ) self . aug = Normalize ( means , stds ) # self.aug = Normalize(means, stds) # self.collate_fn = collate_fn_list_dicts self . check_stackability = check_stackability Custom datasets and data modules # Our custom datasets and data modules are crafted to handle specific data, offering enhanced control and flexibility throughout the workflow. In case you want to use TerraTorch on your specific data, we invite you to develop your own dataset and data module classes by following the examples below. Datasets # terratorch.datasets.biomassters # BioMasstersNonGeo # Bases: BioMassters BioMassters Dataset for Aboveground Biomass prediction. Dataset intended for Aboveground Biomass (AGB) prediction over Finnish forests based on Sentinel 1 and 2 data with corresponding target AGB mask values generated by Light Detection and Ranging (LiDAR). Dataset Format: .tif files for Sentinel 1 and 2 data .tif file for pixel wise AGB target mask .csv files for metadata regarding features and targets Dataset Features: 13,000 target AGB masks of size (256x256px) 12 months of data per target mask Sentinel 1 and Sentinel 2 data for each location Sentinel 1 available for every month Sentinel 2 available for almost every month (not available for every month due to ESA acquisition halt over the region during particular periods) If you use this dataset in your research, please cite the following paper: https://nascetti-a.github.io/BioMasster/ .. versionadded:: 0.5 Source code in terratorch/datasets/biomassters.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 class BioMasstersNonGeo ( BioMassters ): \"\"\"[BioMassters Dataset](https://huggingface.co/datasets/ibm-nasa-geospatial/BioMassters) for Aboveground Biomass prediction. Dataset intended for Aboveground Biomass (AGB) prediction over Finnish forests based on Sentinel 1 and 2 data with corresponding target AGB mask values generated by Light Detection and Ranging (LiDAR). Dataset Format: * .tif files for Sentinel 1 and 2 data * .tif file for pixel wise AGB target mask * .csv files for metadata regarding features and targets Dataset Features: * 13,000 target AGB masks of size (256x256px) * 12 months of data per target mask * Sentinel 1 and Sentinel 2 data for each location * Sentinel 1 available for every month * Sentinel 2 available for almost every month (not available for every month due to ESA acquisition halt over the region during particular periods) If you use this dataset in your research, please cite the following paper: * https://nascetti-a.github.io/BioMasster/ .. versionadded:: 0.5 \"\"\" S1_BAND_NAMES = [ \"VV_Asc\" , \"VH_Asc\" , \"VV_Desc\" , \"VH_Desc\" , \"RVI_Asc\" , \"RVI_Desc\" ] S2_BAND_NAMES = [ \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"NIR_NARROW\" , \"SWIR_1\" , \"SWIR_2\" , \"CLOUD_PROBABILITY\" , ] all_band_names = { \"S1\" : S1_BAND_NAMES , \"S2\" : S2_BAND_NAMES , } rgb_bands = { \"S1\" : [], \"S2\" : [ \"RED\" , \"GREEN\" , \"BLUE\" ], } valid_splits = ( \"train\" , \"test\" ) valid_sensors = ( \"S1\" , \"S2\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } default_metadata_filename = \"The_BioMassters_-_features_metadata.csv.csv\" def __init__ ( self , root = \"data\" , split : str = \"train\" , bands : dict [ str , Sequence [ str ]] | Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , mask_mean : float | None = 63.4584 , mask_std : float | None = 72.21242 , sensors : Sequence [ str ] = [ \"S1\" , \"S2\" ], as_time_series : bool = False , metadata_filename : str = default_metadata_filename , max_cloud_percentage : float | None = None , max_red_mean : float | None = None , include_corrupt : bool = True , subset : float = 1 , seed : int = 42 , use_four_frames : bool = False ) -> None : \"\"\"Initialize a new instance of BioMassters dataset. If ``as_time_series=False`` (the default), each time step becomes its own sample with the target being shared across multiple samples. Args: root: root directory where dataset can be found split: train or test split sensors: which sensors to consider for the sample, Sentinel 1 and/or Sentinel 2 ('S1', 'S2') as_time_series: whether or not to return all available time-steps or just a single one for a given target location metadata_filename: metadata file to be used max_cloud_percentage: maximum allowed cloud percentage for images max_red_mean: maximum allowed red_mean value for images include_corrupt: whether to include images marked as corrupted Raises: AssertionError: if ``split`` or ``sensors`` is invalid DatasetNotFoundError: If dataset is not found. \"\"\" self . root = root self . sensors = sensors self . bands = bands assert ( split in self . valid_splits ), f \"Please choose one of the valid splits: { self . valid_splits } .\" self . split = split assert set ( sensors ) . issubset ( set ( self . valid_sensors ) ), f \"Please choose a subset of valid sensors: { self . valid_sensors } .\" if len ( self . sensors ) == 1 : sens = self . sensors [ 0 ] self . band_indices = [ self . all_band_names [ sens ] . index ( band ) for band in self . bands [ sens ] ] else : self . band_indices = { sens : [ self . all_band_names [ sens ] . index ( band ) for band in self . bands [ sens ]] for sens in self . sensors } self . mask_mean = mask_mean self . mask_std = mask_std self . as_time_series = as_time_series self . metadata_filename = metadata_filename self . max_cloud_percentage = max_cloud_percentage self . max_red_mean = max_red_mean self . include_corrupt = include_corrupt self . subset = subset self . seed = seed self . use_four_frames = use_four_frames self . _verify () # open metadata csv files self . df = pd . read_csv ( os . path . join ( self . root , self . metadata_filename )) # Filter sensors self . df = self . df [ self . df [ \"satellite\" ] . isin ( self . sensors )] # Filter split self . df = self . df [ self . df [ \"split\" ] == self . split ] # Optional filtering self . _filter_and_select_data () # Optional subsampling self . _random_subsample () # generate numerical month from filename since first month is September # and has numerical index of 0 self . df [ \"num_month\" ] = ( self . df [ \"filename\" ] . str . split ( \"_\" , expand = True )[ 2 ] . str . split ( \".\" , expand = True )[ 0 ] . astype ( int ) ) # Set dataframe index depending on the task for easier indexing if self . as_time_series : self . df [ \"num_index\" ] = self . df . groupby ([ \"chip_id\" ]) . ngroup () else : filter_df = ( self . df . groupby ([ \"chip_id\" , \"month\" ])[ \"satellite\" ] . count () . reset_index () ) filter_df = filter_df [ filter_df [ \"satellite\" ] == len ( self . sensors ) ] . drop ( \"satellite\" , axis = 1 ) # Guarantee that each sample has corresponding number of images available self . df = self . df . merge ( filter_df , on = [ \"chip_id\" , \"month\" ], how = \"inner\" ) self . df [ \"num_index\" ] = self . df . groupby ([ \"chip_id\" , \"month\" ]) . ngroup () # Adjust transforms based on the number of sensors if len ( self . sensors ) == 1 : self . transform = transform if transform else default_transform elif transform is None : self . transform = MultimodalToTensor ( self . sensors ) else : transform = { s : transform [ s ] if s in transform else default_transform for s in self . sensors } self . transform = MultimodalTransforms ( transform , shared = False ) if self . use_four_frames : self . _select_4_frames () def __len__ ( self ) -> int : return len ( self . df [ \"num_index\" ] . unique ()) def _load_input ( self , filenames : list [ Path ]) -> Tensor : \"\"\"Load the input imagery at the index. Args: filenames: list of filenames corresponding to input Returns: input image \"\"\" filepaths = [ os . path . join ( self . root , f \" { self . split } _features\" , f ) for f in filenames ] arr_list = [ rasterio . open ( fp ) . read () for fp in filepaths ] if self . as_time_series : arr = np . stack ( arr_list , axis = 0 ) # (T, C, H, W) else : arr = np . concatenate ( arr_list , axis = 0 ) return arr . astype ( np . int32 ) def _load_target ( self , filename : Path ) -> Tensor : \"\"\"Load the target mask at the index. Args: filename: filename of target to index Returns: target mask \"\"\" with rasterio . open ( os . path . join ( self . root , f \" { self . split } _agbm\" , filename ), \"r\" ) as src : arr : np . typing . NDArray [ np . float64 ] = src . read () return arr def _compute_rvi ( self , img : np . ndarray , linear : np . ndarray , sens : str ) -> np . ndarray : \"\"\"Compute the RVI indices for S1 data.\"\"\" rvi_channels = [] if self . as_time_series : if \"RVI_Asc\" in self . bands [ sens ]: try : vv_asc_index = self . all_band_names [ \"S1\" ] . index ( \"VV_Asc\" ) vh_asc_index = self . all_band_names [ \"S1\" ] . index ( \"VH_Asc\" ) except ValueError as e : msg = f \"RVI_Asc needs band: { e } \" raise ValueError ( msg ) from e VV = linear [:, vv_asc_index , :, :] VH = linear [:, vh_asc_index , :, :] rvi_asc = 4 * VH / ( VV + VH + 1e-6 ) rvi_asc = np . expand_dims ( rvi_asc , axis = 1 ) rvi_channels . append ( rvi_asc ) if \"RVI_Desc\" in self . bands [ sens ]: try : vv_desc_index = self . all_band_names [ \"S1\" ] . index ( \"VV_Desc\" ) vh_desc_index = self . all_band_names [ \"S1\" ] . index ( \"VH_Desc\" ) except ValueError as e : msg = f \"RVI_Desc needs band: { e } \" raise ValueError ( msg ) from e VV_desc = linear [:, vv_desc_index , :, :] VH_desc = linear [:, vh_desc_index , :, :] rvi_desc = 4 * VH_desc / ( VV_desc + VH_desc + 1e-6 ) rvi_desc = np . expand_dims ( rvi_desc , axis = 1 ) rvi_channels . append ( rvi_desc ) if rvi_channels : rvi_concat = np . concatenate ( rvi_channels , axis = 1 ) img = np . concatenate ([ img , rvi_concat ], axis = 1 ) else : if \"RVI_Asc\" in self . bands [ sens ]: if linear . shape [ 0 ] < 2 : msg = f \"Not enough bands to calculate RVI_Asc. Available bands: { linear . shape [ 0 ] } \" raise ValueError ( msg ) VV = linear [ 0 ] VH = linear [ 1 ] rvi_asc = 4 * VH / ( VV + VH + 1e-6 ) rvi_asc = np . expand_dims ( rvi_asc , axis = 0 ) rvi_channels . append ( rvi_asc ) if \"RVI_Desc\" in self . bands [ sens ]: if linear . shape [ 0 ] < 4 : msg = f \"Not enough bands to calculate RVI_Desc. Available bands: { linear . shape [ 0 ] } \" raise ValueError ( msg ) VV_desc = linear [ 2 ] VH_desc = linear [ 3 ] rvi_desc = 4 * VH_desc / ( VV_desc + VH_desc + 1e-6 ) rvi_desc = np . expand_dims ( rvi_desc , axis = 0 ) rvi_channels . append ( rvi_desc ) if rvi_channels : rvi_concat = np . concatenate ( rvi_channels , axis = 0 ) img = np . concatenate ([ linear , rvi_concat ], axis = 0 ) return img def _select_4_frames ( self ): \"\"\"Filter the dataset to select only 4 frames per sample.\"\"\" if \"cloud_percentage\" in self . df . columns : self . df = self . df . sort_values ( by = [ \"chip_id\" , \"cloud_percentage\" ]) else : self . df = self . df . sort_values ( by = [ \"chip_id\" , \"num_month\" ]) self . df = ( self . df . groupby ( \"chip_id\" ) . head ( 4 ) # Select the first 4 frames per chip . reset_index ( drop = True ) ) def _process_sensor_images ( self , sens : str , sens_filepaths : list [ str ]) -> np . ndarray : \"\"\"Process images for a given sensor.\"\"\" img = self . _load_input ( sens_filepaths ) if sens == \"S1\" : img = img . astype ( np . float32 ) linear = 10 ** ( img / 10 ) img = self . _compute_rvi ( img , linear , sens ) if self . as_time_series : img = img . transpose ( 0 , 2 , 3 , 1 ) # (T, H, W, C) else : img = img . transpose ( 1 , 2 , 0 ) # (H, W, C) if len ( self . sensors ) == 1 : img = img [ ... , self . band_indices ] else : img = img [ ... , self . band_indices [ sens ]] return img def __getitem__ ( self , index : int ) -> dict : sample_df = self . df [ self . df [ \"num_index\" ] == index ] . copy () # Sort by satellite and month sample_df . sort_values ( by = [ \"satellite\" , \"num_month\" ], inplace = True , ascending = True ) filepaths = sample_df [ \"filename\" ] . tolist () output = {} if len ( self . sensors ) == 1 : sens = self . sensors [ 0 ] sens_filepaths = [ fp for fp in filepaths if sens in fp ] img = self . _process_sensor_images ( sens , sens_filepaths ) output [ \"image\" ] = img . astype ( np . float32 ) else : for sens in self . sensors : sens_filepaths = [ fp for fp in filepaths if sens in fp ] img = self . _process_sensor_images ( sens , sens_filepaths ) output [ sens ] = img . astype ( np . float32 ) # Load target target_filename = sample_df [ \"corresponding_agbm\" ] . unique ()[ 0 ] target = np . array ( self . _load_target ( Path ( target_filename ))) target = target . transpose ( 1 , 2 , 0 ) output [ \"mask\" ] = target if self . transform : if len ( self . sensors ) == 1 : output = self . transform ( ** output ) else : output = self . transform ( output ) output [ \"mask\" ] = output [ \"mask\" ] . squeeze () . float () return output def _filter_and_select_data ( self ): if ( self . max_cloud_percentage is not None and \"cloud_percentage\" in self . df . columns ): self . df = self . df [ self . df [ \"cloud_percentage\" ] <= self . max_cloud_percentage ] if self . max_red_mean is not None and \"red_mean\" in self . df . columns : self . df = self . df [ self . df [ \"red_mean\" ] <= self . max_red_mean ] if not self . include_corrupt and \"corrupt_values\" in self . df . columns : self . df = self . df [ self . df [ \"corrupt_values\" ] is False ] def _random_subsample ( self ): if self . split == \"train\" and self . subset < 1.0 : num_samples = int ( len ( self . df [ \"num_index\" ] . unique ()) * self . subset ) if self . seed is not None : random . seed ( self . seed ) selected_indices = random . sample ( list ( self . df [ \"num_index\" ] . unique ()), num_samples ) self . df = self . df [ self . df [ \"num_index\" ] . isin ( selected_indices )] self . df . reset_index ( drop = True , inplace = True ) def plot ( self , sample : dict [ str , Tensor ], show_titles : bool = True , suptitle : str | None = None , ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` show_titles: flag indicating whether to show titles above each panel suptitle: optional suptitle to use for figure Returns: a matplotlib Figure with the rendered sample \"\"\" # Determine if the sample contains multiple sensors or a single sensor if isinstance ( sample [ \"image\" ], dict ): ncols = len ( self . sensors ) + 1 else : ncols = 2 # One for the image and one for the mask showing_predictions = \"prediction\" in sample if showing_predictions : ncols += 1 fig , axs = plt . subplots ( 1 , ncols = ncols , figsize = ( 5 * ncols , 10 )) if isinstance ( sample [ \"image\" ], dict ): # Multiple sensors case for idx , sens in enumerate ( self . sensors ): img = sample [ \"image\" ][ sens ] . numpy () if self . as_time_series : # Plot last time step img = img [:, - 1 , ... ] if sens == \"S2\" : img = img [[ 2 , 1 , 0 ], ... ] . transpose ( 1 , 2 , 0 ) img = percentile_normalization ( img ) else : co_polarization = img [ 0 ] # transmit == receive cross_polarization = img [ 1 ] # transmit != receive ratio = co_polarization / ( cross_polarization + 1e-6 ) co_polarization = np . clip ( co_polarization / 0.3 , 0 , 1 ) cross_polarization = np . clip ( cross_polarization / 0.05 , 0 , 1 ) ratio = np . clip ( ratio / 25 , 0 , 1 ) img = np . stack ( ( co_polarization , cross_polarization , ratio ), axis = 0 ) img = img . transpose ( 1 , 2 , 0 ) # Convert to (H, W, 3) axs [ idx ] . imshow ( img ) axs [ idx ] . axis ( \"off\" ) if show_titles : axs [ idx ] . set_title ( sens ) mask_idx = len ( self . sensors ) else : # Single sensor case sens = self . sensors [ 0 ] img = sample [ \"image\" ] . numpy () if self . as_time_series : # Plot last time step img = img [:, - 1 , ... ] if sens == \"S2\" : img = img [[ 2 , 1 , 0 ], ... ] . transpose ( 1 , 2 , 0 ) img = percentile_normalization ( img ) else : co_polarization = img [ 0 ] # transmit == receive cross_polarization = img [ 1 ] # transmit != receive ratio = co_polarization / ( cross_polarization + 1e-6 ) co_polarization = np . clip ( co_polarization / 0.3 , 0 , 1 ) cross_polarization = np . clip ( cross_polarization / 0.05 , 0 , 1 ) ratio = np . clip ( ratio / 25 , 0 , 1 ) img = np . stack ( ( co_polarization , cross_polarization , ratio ), axis = 0 ) img = img . transpose ( 1 , 2 , 0 ) # Convert to (H, W, 3) axs [ 0 ] . imshow ( img ) axs [ 0 ] . axis ( \"off\" ) if show_titles : axs [ 0 ] . set_title ( sens ) mask_idx = 1 # Plot target mask if \"mask\" in sample : target = sample [ \"mask\" ] . squeeze () target_im = axs [ mask_idx ] . imshow ( target , cmap = \"YlGn\" ) plt . colorbar ( target_im , ax = axs [ mask_idx ], fraction = 0.046 , pad = 0.04 ) axs [ mask_idx ] . axis ( \"off\" ) if show_titles : axs [ mask_idx ] . set_title ( \"Target\" ) # Plot prediction if available if showing_predictions : pred_idx = mask_idx + 1 prediction = sample [ \"prediction\" ] . squeeze () pred_im = axs [ pred_idx ] . imshow ( prediction , cmap = \"YlGn\" ) plt . colorbar ( pred_im , ax = axs [ pred_idx ], fraction = 0.046 , pad = 0.04 ) axs [ pred_idx ] . axis ( \"off\" ) if show_titles : axs [ pred_idx ] . set_title ( \"Prediction\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig __init__ ( root = 'data' , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , mask_mean = 63.4584 , mask_std = 72.21242 , sensors = [ 'S1' , 'S2' ], as_time_series = False , metadata_filename = default_metadata_filename , max_cloud_percentage = None , max_red_mean = None , include_corrupt = True , subset = 1 , seed = 42 , use_four_frames = False ) # Initialize a new instance of BioMassters dataset. If as_time_series=False (the default), each time step becomes its own sample with the target being shared across multiple samples. Parameters: root \u2013 root directory where dataset can be found split ( str , default: 'train' ) \u2013 train or test split sensors ( Sequence [ str ] , default: ['S1', 'S2'] ) \u2013 which sensors to consider for the sample, Sentinel 1 and/or Sentinel 2 ('S1', 'S2') as_time_series ( bool , default: False ) \u2013 whether or not to return all available time-steps or just a single one for a given target location metadata_filename ( str , default: default_metadata_filename ) \u2013 metadata file to be used max_cloud_percentage ( float | None , default: None ) \u2013 maximum allowed cloud percentage for images max_red_mean ( float | None , default: None ) \u2013 maximum allowed red_mean value for images include_corrupt ( bool , default: True ) \u2013 whether to include images marked as corrupted Raises: AssertionError \u2013 if split or sensors is invalid DatasetNotFoundError \u2013 If dataset is not found. Source code in terratorch/datasets/biomassters.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def __init__ ( self , root = \"data\" , split : str = \"train\" , bands : dict [ str , Sequence [ str ]] | Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , mask_mean : float | None = 63.4584 , mask_std : float | None = 72.21242 , sensors : Sequence [ str ] = [ \"S1\" , \"S2\" ], as_time_series : bool = False , metadata_filename : str = default_metadata_filename , max_cloud_percentage : float | None = None , max_red_mean : float | None = None , include_corrupt : bool = True , subset : float = 1 , seed : int = 42 , use_four_frames : bool = False ) -> None : \"\"\"Initialize a new instance of BioMassters dataset. If ``as_time_series=False`` (the default), each time step becomes its own sample with the target being shared across multiple samples. Args: root: root directory where dataset can be found split: train or test split sensors: which sensors to consider for the sample, Sentinel 1 and/or Sentinel 2 ('S1', 'S2') as_time_series: whether or not to return all available time-steps or just a single one for a given target location metadata_filename: metadata file to be used max_cloud_percentage: maximum allowed cloud percentage for images max_red_mean: maximum allowed red_mean value for images include_corrupt: whether to include images marked as corrupted Raises: AssertionError: if ``split`` or ``sensors`` is invalid DatasetNotFoundError: If dataset is not found. \"\"\" self . root = root self . sensors = sensors self . bands = bands assert ( split in self . valid_splits ), f \"Please choose one of the valid splits: { self . valid_splits } .\" self . split = split assert set ( sensors ) . issubset ( set ( self . valid_sensors ) ), f \"Please choose a subset of valid sensors: { self . valid_sensors } .\" if len ( self . sensors ) == 1 : sens = self . sensors [ 0 ] self . band_indices = [ self . all_band_names [ sens ] . index ( band ) for band in self . bands [ sens ] ] else : self . band_indices = { sens : [ self . all_band_names [ sens ] . index ( band ) for band in self . bands [ sens ]] for sens in self . sensors } self . mask_mean = mask_mean self . mask_std = mask_std self . as_time_series = as_time_series self . metadata_filename = metadata_filename self . max_cloud_percentage = max_cloud_percentage self . max_red_mean = max_red_mean self . include_corrupt = include_corrupt self . subset = subset self . seed = seed self . use_four_frames = use_four_frames self . _verify () # open metadata csv files self . df = pd . read_csv ( os . path . join ( self . root , self . metadata_filename )) # Filter sensors self . df = self . df [ self . df [ \"satellite\" ] . isin ( self . sensors )] # Filter split self . df = self . df [ self . df [ \"split\" ] == self . split ] # Optional filtering self . _filter_and_select_data () # Optional subsampling self . _random_subsample () # generate numerical month from filename since first month is September # and has numerical index of 0 self . df [ \"num_month\" ] = ( self . df [ \"filename\" ] . str . split ( \"_\" , expand = True )[ 2 ] . str . split ( \".\" , expand = True )[ 0 ] . astype ( int ) ) # Set dataframe index depending on the task for easier indexing if self . as_time_series : self . df [ \"num_index\" ] = self . df . groupby ([ \"chip_id\" ]) . ngroup () else : filter_df = ( self . df . groupby ([ \"chip_id\" , \"month\" ])[ \"satellite\" ] . count () . reset_index () ) filter_df = filter_df [ filter_df [ \"satellite\" ] == len ( self . sensors ) ] . drop ( \"satellite\" , axis = 1 ) # Guarantee that each sample has corresponding number of images available self . df = self . df . merge ( filter_df , on = [ \"chip_id\" , \"month\" ], how = \"inner\" ) self . df [ \"num_index\" ] = self . df . groupby ([ \"chip_id\" , \"month\" ]) . ngroup () # Adjust transforms based on the number of sensors if len ( self . sensors ) == 1 : self . transform = transform if transform else default_transform elif transform is None : self . transform = MultimodalToTensor ( self . sensors ) else : transform = { s : transform [ s ] if s in transform else default_transform for s in self . sensors } self . transform = MultimodalTransforms ( transform , shared = False ) if self . use_four_frames : self . _select_4_frames () plot ( sample , show_titles = True , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 a sample returned by :meth: __getitem__ show_titles ( bool , default: True ) \u2013 flag indicating whether to show titles above each panel suptitle ( str | None , default: None ) \u2013 optional suptitle to use for figure Returns: Figure \u2013 a matplotlib Figure with the rendered sample Source code in terratorch/datasets/biomassters.py 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 def plot ( self , sample : dict [ str , Tensor ], show_titles : bool = True , suptitle : str | None = None , ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` show_titles: flag indicating whether to show titles above each panel suptitle: optional suptitle to use for figure Returns: a matplotlib Figure with the rendered sample \"\"\" # Determine if the sample contains multiple sensors or a single sensor if isinstance ( sample [ \"image\" ], dict ): ncols = len ( self . sensors ) + 1 else : ncols = 2 # One for the image and one for the mask showing_predictions = \"prediction\" in sample if showing_predictions : ncols += 1 fig , axs = plt . subplots ( 1 , ncols = ncols , figsize = ( 5 * ncols , 10 )) if isinstance ( sample [ \"image\" ], dict ): # Multiple sensors case for idx , sens in enumerate ( self . sensors ): img = sample [ \"image\" ][ sens ] . numpy () if self . as_time_series : # Plot last time step img = img [:, - 1 , ... ] if sens == \"S2\" : img = img [[ 2 , 1 , 0 ], ... ] . transpose ( 1 , 2 , 0 ) img = percentile_normalization ( img ) else : co_polarization = img [ 0 ] # transmit == receive cross_polarization = img [ 1 ] # transmit != receive ratio = co_polarization / ( cross_polarization + 1e-6 ) co_polarization = np . clip ( co_polarization / 0.3 , 0 , 1 ) cross_polarization = np . clip ( cross_polarization / 0.05 , 0 , 1 ) ratio = np . clip ( ratio / 25 , 0 , 1 ) img = np . stack ( ( co_polarization , cross_polarization , ratio ), axis = 0 ) img = img . transpose ( 1 , 2 , 0 ) # Convert to (H, W, 3) axs [ idx ] . imshow ( img ) axs [ idx ] . axis ( \"off\" ) if show_titles : axs [ idx ] . set_title ( sens ) mask_idx = len ( self . sensors ) else : # Single sensor case sens = self . sensors [ 0 ] img = sample [ \"image\" ] . numpy () if self . as_time_series : # Plot last time step img = img [:, - 1 , ... ] if sens == \"S2\" : img = img [[ 2 , 1 , 0 ], ... ] . transpose ( 1 , 2 , 0 ) img = percentile_normalization ( img ) else : co_polarization = img [ 0 ] # transmit == receive cross_polarization = img [ 1 ] # transmit != receive ratio = co_polarization / ( cross_polarization + 1e-6 ) co_polarization = np . clip ( co_polarization / 0.3 , 0 , 1 ) cross_polarization = np . clip ( cross_polarization / 0.05 , 0 , 1 ) ratio = np . clip ( ratio / 25 , 0 , 1 ) img = np . stack ( ( co_polarization , cross_polarization , ratio ), axis = 0 ) img = img . transpose ( 1 , 2 , 0 ) # Convert to (H, W, 3) axs [ 0 ] . imshow ( img ) axs [ 0 ] . axis ( \"off\" ) if show_titles : axs [ 0 ] . set_title ( sens ) mask_idx = 1 # Plot target mask if \"mask\" in sample : target = sample [ \"mask\" ] . squeeze () target_im = axs [ mask_idx ] . imshow ( target , cmap = \"YlGn\" ) plt . colorbar ( target_im , ax = axs [ mask_idx ], fraction = 0.046 , pad = 0.04 ) axs [ mask_idx ] . axis ( \"off\" ) if show_titles : axs [ mask_idx ] . set_title ( \"Target\" ) # Plot prediction if available if showing_predictions : pred_idx = mask_idx + 1 prediction = sample [ \"prediction\" ] . squeeze () pred_im = axs [ pred_idx ] . imshow ( prediction , cmap = \"YlGn\" ) plt . colorbar ( pred_im , ax = axs [ pred_idx ], fraction = 0.046 , pad = 0.04 ) axs [ pred_idx ] . axis ( \"off\" ) if show_titles : axs [ pred_idx ] . set_title ( \"Prediction\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig terratorch.datasets.burn_intensity # BurnIntensityNonGeo # Bases: NonGeoDataset Dataset implementation for Burn Intensity classification . Source code in terratorch/datasets/burn_intensity.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 class BurnIntensityNonGeo ( NonGeoDataset ): \"\"\"Dataset implementation for [Burn Intensity classification](https://huggingface.co/datasets/ibm-nasa-geospatial/burn_intensity).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" , \"NIR\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } class_names = ( \"No burn\" , \"Unburned to Very Low\" , \"Low Severity\" , \"Moderate Severity\" , \"High Severity\" ) CSV_FILES = { \"limited\" : \"BS_files_with_less_than_25_percent_zeros.csv\" , \"full\" : \"BS_files_raw.csv\" , } num_classes = 5 splits = { \"train\" : \"train\" , \"val\" : \"val\" } time_steps = [ \"pre\" , \"during\" , \"post\" ] def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , use_full_data : bool = True , no_data_replace : float | None = 0.0001 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ) -> None : \"\"\"Initialize the BurnIntensity dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train' or 'val'. bands (Sequence[str]): Bands to output. Defaults to all bands. transform (Optional[A.Compose]): Albumentations transform to be applied. use_metadata (bool): Whether to return metadata info (location). use_full_data (bool): Wheter to use full data or data with less than 25 percent zeros. no_data_replace (Optional[float]): Value to replace NaNs in images. no_label_replace (Optional[int]): Value to replace NaNs in labels. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) # Read the CSV file to get the list of cases to include csv_file_key = \"full\" if use_full_data else \"limited\" csv_path = self . data_root / self . CSV_FILES [ csv_file_key ] df = pd . read_csv ( csv_path ) casenames = df [ \"Case_Name\" ] . tolist () split_file = self . data_root / f \" { split } .txt\" with open ( split_file ) as f : split_images = [ line . strip () for line in f . readlines ()] split_images = [ img for img in split_images if self . _extract_casename ( img ) in casenames ] # Build the samples list self . samples = [] for image_filename in split_images : image_files = [] for time_step in self . time_steps : image_file = self . data_root / time_step / image_filename image_files . append ( str ( image_file )) mask_filename = image_filename . replace ( \"HLS_\" , \"BS_\" ) mask_file = self . data_root / \"pre\" / mask_filename self . samples . append ({ \"image_files\" : image_files , \"mask_file\" : str ( mask_file ), \"casename\" : self . _extract_casename ( image_filename ), }) self . use_metadata = use_metadata self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . transform = transform if transform else default_transform def _extract_basename ( self , filepath : str ) -> str : \"\"\"Extract the base filename without extension.\"\"\" return os . path . splitext ( os . path . basename ( filepath ))[ 0 ] def _extract_casename ( self , filename : str ) -> str : \"\"\"Extract the casename from the filename.\"\"\" basename = self . _extract_basename ( filename ) # Remove 'HLS_' or 'BS_' prefix casename = basename . replace ( \"HLS_\" , \"\" ) . replace ( \"BS_\" , \"\" ) return casename def __len__ ( self ) -> int : return len ( self . samples ) def _get_coords ( self , image : DataArray ) -> torch . Tensor : pixel_scale = image . rio . resolution () width , height = image . rio . width , image . rio . height left , bottom , right , top = image . rio . bounds () tie_point_x , tie_point_y = left , top center_col = width / 2 center_row = height / 2 center_lon = tie_point_x + ( center_col * pixel_scale [ 0 ]) center_lat = tie_point_y - ( center_row * pixel_scale [ 1 ]) lat_lon = np . asarray ([ center_lat , center_lon ]) return torch . tensor ( lat_lon , dtype = torch . float32 ) def __getitem__ ( self , index : int ) -> dict [ str , Any ]: sample = self . samples [ index ] image_files = sample [ \"image_files\" ] mask_file = sample [ \"mask_file\" ] images = [] for idx , image_file in enumerate ( image_files ): image = self . _load_file ( Path ( image_file ), nan_replace = self . no_data_replace ) if idx == 0 and self . use_metadata : location_coords = self . _get_coords ( image ) image = image . to_numpy () image = np . moveaxis ( image , 0 , - 1 ) image = image [ ... , self . band_indices ] images . append ( image ) images = np . stack ( images , axis = 0 ) # (T, H, W, C) output = { \"image\" : images . astype ( np . float32 ), \"mask\" : self . _load_file ( Path ( mask_file ), nan_replace = self . no_label_replace ) . to_numpy ()[ 0 ] } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () if self . use_metadata : output [ \"location_coords\" ] = location_coords return output def _load_file ( self , path : Path , nan_replace : float | int | None = None ) -> DataArray : data = rioxarray . open_rasterio ( path , masked = True ) if nan_replace is not None : data = data . fillna ( nan_replace ) return data def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Any : \"\"\"Plot a sample from the dataset. Args: sample: A sample returned by `__getitem__`. suptitle: Optional string to use as a suptitle. Returns: A matplotlib Figure with the rendered sample. \"\"\" num_images = len ( self . time_steps ) + 2 if \"prediction\" in sample : num_images += 1 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) images = sample [ \"image\" ] # (C, T, H, W) mask = sample [ \"mask\" ] . numpy () num_classes = len ( np . unique ( mask )) fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 5 , 5 )) for i in range ( len ( self . time_steps )): image = images [:, i , :, :] # (C, H, W) image = np . transpose ( image , ( 1 , 2 , 0 )) # (H, W, C) rgb_image = image [ ... , rgb_indices ] rgb_image = ( rgb_image - rgb_image . min ()) / ( rgb_image . max () - rgb_image . min () + 1e-8 ) rgb_image = np . clip ( rgb_image , 0 , 1 ) ax [ i ] . imshow ( rgb_image ) ax [ i ] . axis ( \"off\" ) ax [ i ] . set_title ( f \" { self . time_steps [ i ] . capitalize () } Image\" ) cmap = plt . get_cmap ( \"jet\" , num_classes ) norm = Normalize ( vmin = 0 , vmax = num_classes - 1 ) mask_ax_index = len ( self . time_steps ) ax [ mask_ax_index ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ mask_ax_index ] . axis ( \"off\" ) ax [ mask_ax_index ] . set_title ( \"Ground Truth Mask\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () pred_ax_index = mask_ax_index + 1 ax [ pred_ax_index ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ pred_ax_index ] . axis ( \"off\" ) ax [ pred_ax_index ] . set_title ( \"Predicted Mask\" ) legend_ax_index = - 1 class_names = sample . get ( \"class_names\" , self . class_names ) positions = np . linspace ( 0 , 1 , num_classes ) if num_classes > 1 else [ 0.5 ] legend_handles = [ mpatches . Patch ( color = cmap ( pos ), label = class_names [ i ]) for i , pos in enumerate ( positions ) ] ax [ legend_ax_index ] . legend ( handles = legend_handles , loc = \"center\" ) ax [ legend_ax_index ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) plt . tight_layout () return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , use_full_data = True , no_data_replace = 0.0001 , no_label_replace =- 1 , use_metadata = False ) # Initialize the BurnIntensity dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train' or 'val'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to output. Defaults to all bands. transform ( Optional [ Compose ] , default: None ) \u2013 Albumentations transform to be applied. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (location). use_full_data ( bool , default: True ) \u2013 Wheter to use full data or data with less than 25 percent zeros. no_data_replace ( Optional [ float ] , default: 0.0001 ) \u2013 Value to replace NaNs in images. no_label_replace ( Optional [ int ] , default: -1 ) \u2013 Value to replace NaNs in labels. Source code in terratorch/datasets/burn_intensity.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , use_full_data : bool = True , no_data_replace : float | None = 0.0001 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ) -> None : \"\"\"Initialize the BurnIntensity dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train' or 'val'. bands (Sequence[str]): Bands to output. Defaults to all bands. transform (Optional[A.Compose]): Albumentations transform to be applied. use_metadata (bool): Whether to return metadata info (location). use_full_data (bool): Wheter to use full data or data with less than 25 percent zeros. no_data_replace (Optional[float]): Value to replace NaNs in images. no_label_replace (Optional[int]): Value to replace NaNs in labels. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) # Read the CSV file to get the list of cases to include csv_file_key = \"full\" if use_full_data else \"limited\" csv_path = self . data_root / self . CSV_FILES [ csv_file_key ] df = pd . read_csv ( csv_path ) casenames = df [ \"Case_Name\" ] . tolist () split_file = self . data_root / f \" { split } .txt\" with open ( split_file ) as f : split_images = [ line . strip () for line in f . readlines ()] split_images = [ img for img in split_images if self . _extract_casename ( img ) in casenames ] # Build the samples list self . samples = [] for image_filename in split_images : image_files = [] for time_step in self . time_steps : image_file = self . data_root / time_step / image_filename image_files . append ( str ( image_file )) mask_filename = image_filename . replace ( \"HLS_\" , \"BS_\" ) mask_file = self . data_root / \"pre\" / mask_filename self . samples . append ({ \"image_files\" : image_files , \"mask_file\" : str ( mask_file ), \"casename\" : self . _extract_casename ( image_filename ), }) self . use_metadata = use_metadata self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Any \u2013 A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/burn_intensity.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Any : \"\"\"Plot a sample from the dataset. Args: sample: A sample returned by `__getitem__`. suptitle: Optional string to use as a suptitle. Returns: A matplotlib Figure with the rendered sample. \"\"\" num_images = len ( self . time_steps ) + 2 if \"prediction\" in sample : num_images += 1 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) images = sample [ \"image\" ] # (C, T, H, W) mask = sample [ \"mask\" ] . numpy () num_classes = len ( np . unique ( mask )) fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 5 , 5 )) for i in range ( len ( self . time_steps )): image = images [:, i , :, :] # (C, H, W) image = np . transpose ( image , ( 1 , 2 , 0 )) # (H, W, C) rgb_image = image [ ... , rgb_indices ] rgb_image = ( rgb_image - rgb_image . min ()) / ( rgb_image . max () - rgb_image . min () + 1e-8 ) rgb_image = np . clip ( rgb_image , 0 , 1 ) ax [ i ] . imshow ( rgb_image ) ax [ i ] . axis ( \"off\" ) ax [ i ] . set_title ( f \" { self . time_steps [ i ] . capitalize () } Image\" ) cmap = plt . get_cmap ( \"jet\" , num_classes ) norm = Normalize ( vmin = 0 , vmax = num_classes - 1 ) mask_ax_index = len ( self . time_steps ) ax [ mask_ax_index ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ mask_ax_index ] . axis ( \"off\" ) ax [ mask_ax_index ] . set_title ( \"Ground Truth Mask\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () pred_ax_index = mask_ax_index + 1 ax [ pred_ax_index ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ pred_ax_index ] . axis ( \"off\" ) ax [ pred_ax_index ] . set_title ( \"Predicted Mask\" ) legend_ax_index = - 1 class_names = sample . get ( \"class_names\" , self . class_names ) positions = np . linspace ( 0 , 1 , num_classes ) if num_classes > 1 else [ 0.5 ] legend_handles = [ mpatches . Patch ( color = cmap ( pos ), label = class_names [ i ]) for i , pos in enumerate ( positions ) ] ax [ legend_ax_index ] . legend ( handles = legend_handles , loc = \"center\" ) ax [ legend_ax_index ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) plt . tight_layout () return fig terratorch.datasets.carbonflux # CarbonFluxNonGeo # Bases: NonGeoDataset Dataset for Carbon Flux regression from HLS images and MERRA data. Source code in terratorch/datasets/carbonflux.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 class CarbonFluxNonGeo ( NonGeoDataset ): \"\"\"Dataset for [Carbon Flux](https://huggingface.co/datasets/ibm-nasa-geospatial/hls_merra2_gppFlux) regression from HLS images and MERRA data.\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" , \"NIR\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" , ) merra_var_names = ( \"T2MIN\" , \"T2MAX\" , \"T2MEAN\" , \"TSMDEWMEAN\" , \"GWETROOT\" , \"LHLAND\" , \"SHLAND\" , \"SWLAND\" , \"PARDFLAND\" , \"PRECTOTLAND\" ) splits = { \"train\" : \"train\" , \"test\" : \"test\" } BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } metadata_file = \"data_train_hls_37sites_v0_1.csv\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , gpp_mean : float | None = None , gpp_std : float | None = None , no_data_replace : float | None = 0.0001 , use_metadata : bool = False , modalities : Sequence [ str ] = ( \"image\" , \"merra_vars\" ) ) -> None : \"\"\"Initialize the CarbonFluxNonGeo dataset. Args: data_root (str): Path to the data root directory. split (str): 'train' or 'test'. bands (Sequence[str]): Bands to use. Defaults to all bands. transform (Optional[A.Compose]): Albumentations transform to be applied. use_metadata (bool): Whether to return metadata (coordinates and date). merra_means (Sequence[float]): Means for MERRA data normalization. merra_stds (Sequence[float]): Standard deviations for MERRA data normalization. gpp_mean (float): Mean for GPP normalization. gpp_std (float): Standard deviation for GPP normalization. no_data_replace (Optional[float]): Value to replace NO_DATA values in images. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( band ) for band in bands ] self . data_root = Path ( data_root ) # Load the CSV file with metadata csv_file = self . data_root / self . metadata_file df = pd . read_csv ( csv_file ) # Get list of image filenames in the split directory image_dir = self . data_root / self . split image_files = [ f . name for f in image_dir . glob ( \"*.tiff\" )] df [ \"Chip\" ] = df [ \"Chip\" ] . str . replace ( \".tif$\" , \".tiff\" , regex = True ) # Filter the DataFrame to include only rows with 'Chip' in image_files df = df [ df [ \"Chip\" ] . isin ( image_files )] # Build the samples list self . samples = [] for _ , row in df . iterrows (): image_filename = row [ \"Chip\" ] image_path = image_dir / image_filename # MERRA vectors merra_vars = row [ list ( self . merra_var_names )] . values . astype ( np . float32 ) # GPP target gpp = row [ \"GPP\" ] image_path = image_dir / row [ \"Chip\" ] merra_vars = row [ list ( self . merra_var_names )] . values . astype ( np . float32 ) gpp = row [ \"GPP\" ] self . samples . append ({ \"image_path\" : str ( image_path ), \"merra_vars\" : merra_vars , \"gpp\" : gpp , }) if gpp_mean is None or gpp_std is None : msg = \"Mean and standard deviation for GPP must be provided.\" raise ValueError ( msg ) self . gpp_mean = gpp_mean self . gpp_std = gpp_std self . use_metadata = use_metadata self . modalities = modalities self . no_data_replace = no_data_replace if transform is None : self . transform = MultimodalToTensor ( self . modalities ) else : transform = { m : transform [ m ] if m in transform else default_transform for m in self . modalities } self . transform = MultimodalTransforms ( transform , shared = False ) def __len__ ( self ) -> int : return len ( self . samples ) def _load_file ( self , path : str , nan_replace : float | int | None = None ): data = rioxarray . open_rasterio ( path , masked = True ) if nan_replace is not None : data = data . fillna ( nan_replace ) return data def _get_coords ( self , image ) -> torch . Tensor : \"\"\"Extract the center coordinates from the image geospatial metadata.\"\"\" pixel_scale = image . rio . resolution () width , height = image . rio . width , image . rio . height left , bottom , right , top = image . rio . bounds () tie_point_x , tie_point_y = left , top center_col = width / 2 center_row = height / 2 center_lon = tie_point_x + ( center_col * pixel_scale [ 0 ]) center_lat = tie_point_y - ( center_row * pixel_scale [ 1 ]) src_crs = image . rio . crs dst_crs = \"EPSG:4326\" transformer = pyproj . Transformer . from_crs ( src_crs , dst_crs , always_xy = True ) lon , lat = transformer . transform ( center_lon , center_lat ) coords = np . array ([ lat , lon ], dtype = np . float32 ) return torch . from_numpy ( coords ) def _get_date ( self , filename : str ) -> torch . Tensor : \"\"\"Extract the date from the filename.\"\"\" base_filename = os . path . basename ( filename ) pattern = r \"HLS\\.. {3} \\.[A-Z0-9] {6} \\.(?P<date>\\d {7} T\\d {6} )\\..*\\.tiff$\" match = re . match ( pattern , base_filename ) if not match : msg = f \"Filename { filename } does not match expected pattern.\" raise ValueError ( msg ) date_str = match . group ( \"date\" ) year = int ( date_str [: 4 ]) julian_day = int ( date_str [ 4 : 7 ]) date_tensor = torch . tensor ([ year , julian_day ], dtype = torch . int32 ) return date_tensor def __getitem__ ( self , idx : int ) -> dict [ str , Any ]: sample = self . samples [ idx ] image_path = sample [ \"image_path\" ] image = self . _load_file ( image_path , nan_replace = self . no_data_replace ) if self . use_metadata : location_coords = self . _get_coords ( image ) temporal_coords = self . _get_date ( os . path . basename ( image_path )) image = image . to_numpy () # (C, H, W) image = image [ self . band_indices , ... ] image = np . moveaxis ( image , 0 , - 1 ) # (H, W, C) merra_vars = np . array ( sample [ \"merra_vars\" ]) target = np . array ( sample [ \"gpp\" ]) target_norm = ( target - self . gpp_mean ) / self . gpp_std target_norm = torch . tensor ( target_norm , dtype = torch . float32 ) output = { \"image\" : image . astype ( np . float32 ), \"merra_vars\" : merra_vars , } if self . transform : output = self . transform ( output ) output = { \"image\" : { m : output [ m ] for m in self . modalities if m in output }, \"mask\" : target_norm } if self . use_metadata : output [ \"location_coords\" ] = location_coords output [ \"temporal_coords\" ] = temporal_coords return output def plot ( self , sample : dict [ str , Any ], suptitle : str | None = None ) -> Any : \"\"\"Plot a sample from the dataset. Args: sample: A sample returned by `__getitem__`. suptitle: Optional title for the figure. Returns: A matplotlib figure with the rendered sample. \"\"\" image = sample [ \"image\" ] . numpy () image = np . transpose ( image , ( 1 , 2 , 0 )) # (H, W, C) rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) rgb_image = image [ ... , rgb_indices ] rgb_image = ( rgb_image - rgb_image . min ()) / ( rgb_image . max () - rgb_image . min () + 1e-8 ) rgb_image = np . clip ( rgb_image , 0 , 1 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( \"Image\" ) if suptitle : plt . suptitle ( suptitle ) plt . tight_layout () return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , gpp_mean = None , gpp_std = None , no_data_replace = 0.0001 , use_metadata = False , modalities = ( 'image' , 'merra_vars' )) # Initialize the CarbonFluxNonGeo dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 'train' or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to use. Defaults to all bands. transform ( Optional [ Compose ] , default: None ) \u2013 Albumentations transform to be applied. use_metadata ( bool , default: False ) \u2013 Whether to return metadata (coordinates and date). merra_means ( Sequence [ float ] ) \u2013 Means for MERRA data normalization. merra_stds ( Sequence [ float ] ) \u2013 Standard deviations for MERRA data normalization. gpp_mean ( float , default: None ) \u2013 Mean for GPP normalization. gpp_std ( float , default: None ) \u2013 Standard deviation for GPP normalization. no_data_replace ( Optional [ float ] , default: 0.0001 ) \u2013 Value to replace NO_DATA values in images. Source code in terratorch/datasets/carbonflux.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , gpp_mean : float | None = None , gpp_std : float | None = None , no_data_replace : float | None = 0.0001 , use_metadata : bool = False , modalities : Sequence [ str ] = ( \"image\" , \"merra_vars\" ) ) -> None : \"\"\"Initialize the CarbonFluxNonGeo dataset. Args: data_root (str): Path to the data root directory. split (str): 'train' or 'test'. bands (Sequence[str]): Bands to use. Defaults to all bands. transform (Optional[A.Compose]): Albumentations transform to be applied. use_metadata (bool): Whether to return metadata (coordinates and date). merra_means (Sequence[float]): Means for MERRA data normalization. merra_stds (Sequence[float]): Standard deviations for MERRA data normalization. gpp_mean (float): Mean for GPP normalization. gpp_std (float): Standard deviation for GPP normalization. no_data_replace (Optional[float]): Value to replace NO_DATA values in images. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( band ) for band in bands ] self . data_root = Path ( data_root ) # Load the CSV file with metadata csv_file = self . data_root / self . metadata_file df = pd . read_csv ( csv_file ) # Get list of image filenames in the split directory image_dir = self . data_root / self . split image_files = [ f . name for f in image_dir . glob ( \"*.tiff\" )] df [ \"Chip\" ] = df [ \"Chip\" ] . str . replace ( \".tif$\" , \".tiff\" , regex = True ) # Filter the DataFrame to include only rows with 'Chip' in image_files df = df [ df [ \"Chip\" ] . isin ( image_files )] # Build the samples list self . samples = [] for _ , row in df . iterrows (): image_filename = row [ \"Chip\" ] image_path = image_dir / image_filename # MERRA vectors merra_vars = row [ list ( self . merra_var_names )] . values . astype ( np . float32 ) # GPP target gpp = row [ \"GPP\" ] image_path = image_dir / row [ \"Chip\" ] merra_vars = row [ list ( self . merra_var_names )] . values . astype ( np . float32 ) gpp = row [ \"GPP\" ] self . samples . append ({ \"image_path\" : str ( image_path ), \"merra_vars\" : merra_vars , \"gpp\" : gpp , }) if gpp_mean is None or gpp_std is None : msg = \"Mean and standard deviation for GPP must be provided.\" raise ValueError ( msg ) self . gpp_mean = gpp_mean self . gpp_std = gpp_std self . use_metadata = use_metadata self . modalities = modalities self . no_data_replace = no_data_replace if transform is None : self . transform = MultimodalToTensor ( self . modalities ) else : transform = { m : transform [ m ] if m in transform else default_transform for m in self . modalities } self . transform = MultimodalTransforms ( transform , shared = False ) plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Any ] ) \u2013 A sample returned by __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional title for the figure. Returns: Any \u2013 A matplotlib figure with the rendered sample. Source code in terratorch/datasets/carbonflux.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def plot ( self , sample : dict [ str , Any ], suptitle : str | None = None ) -> Any : \"\"\"Plot a sample from the dataset. Args: sample: A sample returned by `__getitem__`. suptitle: Optional title for the figure. Returns: A matplotlib figure with the rendered sample. \"\"\" image = sample [ \"image\" ] . numpy () image = np . transpose ( image , ( 1 , 2 , 0 )) # (H, W, C) rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) rgb_image = image [ ... , rgb_indices ] rgb_image = ( rgb_image - rgb_image . min ()) / ( rgb_image . max () - rgb_image . min () + 1e-8 ) rgb_image = np . clip ( rgb_image , 0 , 1 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( \"Image\" ) if suptitle : plt . suptitle ( suptitle ) plt . tight_layout () return fig terratorch.datasets.forestnet # ForestNetNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for ForestNet . Source code in terratorch/datasets/forestnet.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 class ForestNetNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [ForestNet](https://huggingface.co/datasets/ibm-nasa-geospatial/ForestNet).\"\"\" all_band_names = ( \"RED\" , \"GREEN\" , \"BLUE\" , \"NIR\" , \"SWIR_1\" , \"SWIR_2\" ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" , ) splits = ( \"train\" , \"test\" , \"val\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } default_label_map = { # noqa: RUF012 \"Plantation\" : 0 , \"Smallholder agriculture\" : 1 , \"Grassland shrubland\" : 2 , \"Other\" : 3 , } def __init__ ( self , data_root : str , split : str = \"train\" , label_map : dict [ str , int ] = default_label_map , transform : A . Compose | None = None , fraction : float = 1.0 , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], use_metadata : bool = False , ) -> None : \"\"\" Initialize the ForestNetNonGeo dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. label_map (Dict[str, int]): Mapping from label names to integer labels. transform: Transformations to be applied to the images. fraction (float): Fraction of the dataset to use. Defaults to 1.0 (use all data). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits ) } .\" raise ValueError ( msg ) self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . label_map = label_map # Load the CSV file corresponding to the split csv_file = self . data_root / f \" { split } _filtered.csv\" original_df = pd . read_csv ( csv_file ) # Apply stratified sampling if fraction < 1.0 if fraction < 1.0 : sss = StratifiedShuffleSplit ( n_splits = 1 , test_size = 1 - fraction , random_state = 47 ) stratified_indices , _ = next ( sss . split ( original_df , original_df [ \"merged_label\" ])) self . dataset = original_df . iloc [ stratified_indices ] . reset_index ( drop = True ) else : self . dataset = original_df self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . dataset ) def _get_coords ( self , event_path : Path ) -> torch . Tensor : auxiliary_path = event_path / \"auxiliary\" osm_json_path = auxiliary_path / \"osm.json\" with open ( osm_json_path ) as f : osm_data = json . load ( f ) lat = float ( osm_data [ \"closest_city\" ][ \"lat\" ]) lon = float ( osm_data [ \"closest_city\" ][ \"lon\" ]) lat_lon = np . asarray ([ lat , lon ]) return torch . tensor ( lat_lon , dtype = torch . float32 ) def _get_dates ( self , image_files : list ) -> list : dates = [] pattern = re . compile ( r \"(\\d {4} )_(\\d {2} )_(\\d {2} )_cloud_\\d+\\.(png|npy)\" ) for img_path in image_files : match = pattern . search ( img_path ) year , month , day = int ( match . group ( 1 )), int ( match . group ( 2 )), int ( match . group ( 3 )) date_obj = datetime . datetime ( year , month , day ) # noqa: DTZ001 julian_day = date_obj . timetuple () . tm_yday date_tensor = torch . tensor ([ year , julian_day ], dtype = torch . int32 ) dates . append ( date_tensor ) return torch . stack ( dates , dim = 0 ) def __getitem__ ( self , index : int ): path = self . data_root / self . dataset [ \"example_path\" ][ index ] label = self . map_label ( index ) visible_images , infrared_images , temporal_coords = self . _load_images ( path ) visible_images = np . stack ( visible_images , axis = 0 ) infrared_images = np . stack ( infrared_images , axis = 0 ) merged_images = np . concatenate ([ visible_images , infrared_images ], axis =- 1 ) merged_images = merged_images [ ... , self . band_indices ] # (T, H, W, 2C) output = { \"image\" : merged_images . astype ( np . float32 ) } if self . transform : output = self . transform ( ** output ) if self . use_metadata : location_coords = self . _get_coords ( path ) output [ \"location_coords\" ] = location_coords output [ \"temporal_coords\" ] = temporal_coords output [ \"label\" ] = label return output def _load_images ( self , path : str ): \"\"\"Load visible and infrared images from the given event path\"\"\" visible_image_files = glob . glob ( os . path . join ( path , \"images/visible/*_cloud_*.png\" )) infra_image_files = glob . glob ( os . path . join ( path , \"images/infrared/*_cloud_*.npy\" )) selected_visible_images = self . select_images ( visible_image_files ) selected_infra_images = self . select_images ( infra_image_files ) dates = None if self . use_metadata : dates = self . _get_dates ( selected_visible_images ) vis_images = [ np . array ( Image . open ( img )) for img in selected_visible_images ] # (T, H, W, C) inf_images = [ np . load ( img , allow_pickle = True ) for img in selected_infra_images ] # (T, H, W, C) return vis_images , inf_images , dates def least_cloudy_image ( self , image_files ): pattern = re . compile ( r \"(\\d {4} )_\\d {2} _\\d {2} _cloud_(\\d+)\\.(png|npy)\" ) lowest_cloud_images = defaultdict ( lambda : { \"path\" : None , \"cloud_value\" : float ( \"inf\" )}) for path in image_files : match = pattern . search ( path ) if match : year , cloud_value = match . group ( 1 ), int ( match . group ( 2 )) if cloud_value < lowest_cloud_images [ year ][ \"cloud_value\" ]: lowest_cloud_images [ year ] = { \"path\" : path , \"cloud_value\" : cloud_value } return [ info [ \"path\" ] for info in lowest_cloud_images . values ()] def match_timesteps ( self , image_files , selected_images ): if len ( selected_images ) < 3 : extra_imgs = [ img for img in image_files if img not in selected_images ] selected_images += extra_imgs [: 3 - len ( selected_images )] while len ( selected_images ) < 3 : selected_images . append ( selected_images [ - 1 ]) return selected_images [: 3 ] def select_images ( self , image_files ): selected = self . least_cloudy_image ( image_files ) return self . match_timesteps ( image_files , selected ) def map_label ( self , index : int ) -> torch . Tensor : \"\"\"Map the label name to an integer label.\"\"\" label_name = self . dataset [ \"merged_label\" ][ index ] label = self . label_map [ label_name ] return label def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ): num_images = sample [ \"image\" ] . shape [ 1 ] + 1 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) fig , ax = plt . subplots ( 1 , num_images , figsize = ( 15 , 5 )) for i in range ( sample [ \"image\" ] . shape [ 1 ]): image = sample [ \"image\" ][:, i , :, :] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [ ... , rgb_indices ] rgb_image = ( rgb_image - rgb_image . min ()) / ( rgb_image . max () - rgb_image . min () + 1e-8 ) rgb_image = np . clip ( rgb_image , 0 , 1 ) ax [ i ] . imshow ( rgb_image ) ax [ i ] . axis ( \"off\" ) ax [ i ] . set_title ( f \"Timestep { i + 1 } \" ) legend_handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = \"blue\" )] legend_label = [ self . label_map . get ( sample [ \"label\" ], \"Unknown Label\" )] ax [ - 1 ] . legend ( legend_handles , legend_label , loc = \"center\" ) ax [ - 1 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) plt . tight_layout () return fig __init__ ( data_root , split = 'train' , label_map = default_label_map , transform = None , fraction = 1.0 , bands = BAND_SETS [ 'all' ], use_metadata = False ) # Initialize the ForestNetNonGeo dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. label_map ( Dict [ str , int ] , default: default_label_map ) \u2013 Mapping from label names to integer labels. transform ( Compose | None , default: None ) \u2013 Transformations to be applied to the images. fraction ( float , default: 1.0 ) \u2013 Fraction of the dataset to use. Defaults to 1.0 (use all data). Source code in terratorch/datasets/forestnet.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def __init__ ( self , data_root : str , split : str = \"train\" , label_map : dict [ str , int ] = default_label_map , transform : A . Compose | None = None , fraction : float = 1.0 , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], use_metadata : bool = False , ) -> None : \"\"\" Initialize the ForestNetNonGeo dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. label_map (Dict[str, int]): Mapping from label names to integer labels. transform: Transformations to be applied to the images. fraction (float): Fraction of the dataset to use. Defaults to 1.0 (use all data). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits ) } .\" raise ValueError ( msg ) self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . label_map = label_map # Load the CSV file corresponding to the split csv_file = self . data_root / f \" { split } _filtered.csv\" original_df = pd . read_csv ( csv_file ) # Apply stratified sampling if fraction < 1.0 if fraction < 1.0 : sss = StratifiedShuffleSplit ( n_splits = 1 , test_size = 1 - fraction , random_state = 47 ) stratified_indices , _ = next ( sss . split ( original_df , original_df [ \"merged_label\" ])) self . dataset = original_df . iloc [ stratified_indices ] . reset_index ( drop = True ) else : self . dataset = original_df self . transform = transform if transform else default_transform map_label ( index ) # Map the label name to an integer label. Source code in terratorch/datasets/forestnet.py 189 190 191 192 193 def map_label ( self , index : int ) -> torch . Tensor : \"\"\"Map the label name to an integer label.\"\"\" label_name = self . dataset [ \"merged_label\" ][ index ] label = self . label_map [ label_name ] return label terratorch.datasets.fire_scars # FireScarsHLS # Bases: RasterDataset RasterDataset implementation for fire scars input images. Source code in terratorch/datasets/fire_scars.py 216 217 218 219 220 221 222 223 224 225 class FireScarsHLS ( RasterDataset ): \"\"\"RasterDataset implementation for fire scars input images.\"\"\" filename_glob = \"subsetted*_merged.tif\" filename_regex = r \"subsetted_512x512_HLS\\..30\\.. {6} \\.(?P<date>[0-9]*)\\.v1.4_merged.tif\" date_format = \"%Y%j\" is_image = True separate_files = False all_bands = dataclasses . field ( default_factory = [ \"B02\" , \"B03\" , \"B04\" , \"B8A\" , \"B11\" , \"B12\" ]) rgb_bands = dataclasses . field ( default_factory = [ \"B04\" , \"B03\" , \"B02\" ]) FireScarsNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for fire scars . Source code in terratorch/datasets/fire_scars.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 class FireScarsNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [fire scars](https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" , \"NIR_NARROW\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } num_classes = 2 splits = { \"train\" : \"training\" , \"val\" : \"validation\" } # Only train and val splits available def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ) -> None : \"\"\"Constructor Args: data_root (str): Path to the data root directory. bands (list[str]): Bands that should be output by the dataset. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If None, does no replacement. Defaults to 0. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1. use_metadata (bool): whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { self . splits } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) input_dir = self . data_root / split_name self . image_files = sorted ( glob . glob ( os . path . join ( input_dir , \"*_merged.tif\" ))) self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( input_dir , \"*.mask.tif\" ))) self . use_metadata = use_metadata self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def _get_date ( self , index : int ) -> torch . Tensor : file_name = self . image_files [ index ] base_filename = os . path . basename ( file_name ) filename_regex = r \"subsetted_512x512_HLS\\.S30\\.T[0-9A-Z] {5} \\.(?P<date>[0-9]+)\\.v1\\.4_merged\\.tif\" match = re . match ( filename_regex , base_filename ) date_str = match . group ( \"date\" ) year = int ( date_str [: 4 ]) julian_day = int ( date_str [ 4 :]) return torch . tensor ([[ year , julian_day ]], dtype = torch . float32 ) def _get_coords ( self , image : DataArray ) -> torch . Tensor : px = image . x . shape [ 0 ] // 2 py = image . y . shape [ 0 ] // 2 # get center point to reproject to lat/lon point = image . isel ( band = 0 , x = slice ( px , px + 1 ), y = slice ( py , py + 1 )) point = point . rio . reproject ( \"epsg:4326\" ) lat_lon = np . asarray ([ point . y [ 0 ], point . x [ 0 ]]) return torch . tensor ( lat_lon , dtype = torch . float32 ) def __getitem__ ( self , index : int ) -> dict [ str , Any ]: image = self . _load_file ( self . image_files [ index ], nan_replace = self . no_data_replace ) location_coords , temporal_coords = None , None if self . use_metadata : location_coords = self . _get_coords ( image ) temporal_coords = self . _get_date ( index ) # to channels last image = image . to_numpy () image = np . moveaxis ( image , 0 , - 1 ) # filter bands image = image [ ... , self . band_indices ] output = { \"image\" : image . astype ( np . float32 ), \"mask\" : self . _load_file ( self . segmentation_mask_files [ index ], nan_replace = self . no_label_replace ) . to_numpy ()[ 0 ], } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () if self . use_metadata : output [ \"location_coords\" ] = location_coords output [ \"temporal_coords\" ] = temporal_coords return output def _load_file ( self , path : Path , nan_replace : int | float | None = None ) -> DataArray : data = rioxarray . open_rasterio ( path , masked = True ) if nan_replace is not None : data = data . fillna ( nan_replace ) return data def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample \"\"\" num_images = 4 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) # RGB -> channels-last image = sample [ \"image\" ][ rgb_indices , ... ] . permute ( 1 , 2 , 0 ) . numpy () mask = sample [ \"mask\" ] . numpy () image = clip_image_percentile ( image ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] num_images += 1 else : prediction = None fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 5 ), layout = \"compressed\" ) ax [ 0 ] . axis ( \"off\" ) norm = mpl . colors . Normalize ( vmin = 0 , vmax = self . num_classes - 1 ) ax [ 1 ] . axis ( \"off\" ) ax [ 1 ] . title . set_text ( \"Image\" ) ax [ 1 ] . imshow ( image ) ax [ 2 ] . axis ( \"off\" ) ax [ 2 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ 2 ] . imshow ( mask , cmap = \"jet\" , norm = norm ) ax [ 3 ] . axis ( \"off\" ) ax [ 3 ] . title . set_text ( \"GT Mask on Image\" ) ax [ 3 ] . imshow ( image ) ax [ 3 ] . imshow ( mask , cmap = \"jet\" , alpha = 0.3 , norm = norm ) if \"prediction\" in sample : ax [ 4 ] . title . set_text ( \"Predicted Mask\" ) ax [ 4 ] . imshow ( prediction , cmap = \"jet\" , norm = norm ) cmap = plt . get_cmap ( \"jet\" ) legend_data = [[ i , cmap ( norm ( i )), str ( i )] for i in range ( self . num_classes )] handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = tuple ( v for v in c )) for k , c , n in legend_data ] labels = [ n for k , c , n in legend_data ] ax [ 0 ] . legend ( handles , labels , loc = \"center\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , no_data_replace = 0 , no_label_replace =- 1 , use_metadata = False ) # Constructor Parameters: data_root ( str ) \u2013 Path to the data root directory. bands ( list [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands that should be output by the dataset. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2(). no_data_replace ( float | None , default: 0 ) \u2013 Replace nan values in input images with this value. If None, does no replacement. Defaults to 0. no_label_replace ( int | None , default: -1 ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to -1. use_metadata ( bool , default: False ) \u2013 whether to return metadata info (time and location). Source code in terratorch/datasets/fire_scars.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ) -> None : \"\"\"Constructor Args: data_root (str): Path to the data root directory. bands (list[str]): Bands that should be output by the dataset. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If None, does no replacement. Defaults to 0. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1. use_metadata (bool): whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { self . splits } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) input_dir = self . data_root / split_name self . image_files = sorted ( glob . glob ( os . path . join ( input_dir , \"*_merged.tif\" ))) self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( input_dir , \"*.mask.tif\" ))) self . use_metadata = use_metadata self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 a sample returned by :meth: __getitem__ suptitle ( str | None , default: None ) \u2013 optional string to use as a suptitle Returns: Figure \u2013 a matplotlib Figure with the rendered sample Source code in terratorch/datasets/fire_scars.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample \"\"\" num_images = 4 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) # RGB -> channels-last image = sample [ \"image\" ][ rgb_indices , ... ] . permute ( 1 , 2 , 0 ) . numpy () mask = sample [ \"mask\" ] . numpy () image = clip_image_percentile ( image ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] num_images += 1 else : prediction = None fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 5 ), layout = \"compressed\" ) ax [ 0 ] . axis ( \"off\" ) norm = mpl . colors . Normalize ( vmin = 0 , vmax = self . num_classes - 1 ) ax [ 1 ] . axis ( \"off\" ) ax [ 1 ] . title . set_text ( \"Image\" ) ax [ 1 ] . imshow ( image ) ax [ 2 ] . axis ( \"off\" ) ax [ 2 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ 2 ] . imshow ( mask , cmap = \"jet\" , norm = norm ) ax [ 3 ] . axis ( \"off\" ) ax [ 3 ] . title . set_text ( \"GT Mask on Image\" ) ax [ 3 ] . imshow ( image ) ax [ 3 ] . imshow ( mask , cmap = \"jet\" , alpha = 0.3 , norm = norm ) if \"prediction\" in sample : ax [ 4 ] . title . set_text ( \"Predicted Mask\" ) ax [ 4 ] . imshow ( prediction , cmap = \"jet\" , norm = norm ) cmap = plt . get_cmap ( \"jet\" ) legend_data = [[ i , cmap ( norm ( i )), str ( i )] for i in range ( self . num_classes )] handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = tuple ( v for v in c )) for k , c , n in legend_data ] labels = [ n for k , c , n in legend_data ] ax [ 0 ] . legend ( handles , labels , loc = \"center\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig FireScarsSegmentationMask # Bases: RasterDataset RasterDataset implementation for fire scars segmentation mask. Can be easily merged with input images using the & operator. Source code in terratorch/datasets/fire_scars.py 228 229 230 231 232 233 234 235 236 237 class FireScarsSegmentationMask ( RasterDataset ): \"\"\"RasterDataset implementation for fire scars segmentation mask. Can be easily merged with input images using the & operator. \"\"\" filename_glob = \"subsetted*.mask.tif\" filename_regex = r \"subsetted_512x512_HLS\\..30\\.. {6} \\.(?P<date>[0-9]*)\\.v1.4.mask.tif\" date_format = \"%Y%j\" is_image = False separate_files = False terratorch.datasets.landslide4sense # Landslide4SenseNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for Landslide4Sense . Source code in terratorch/datasets/landslide4sense.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 class Landslide4SenseNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [Landslide4Sense](https://huggingface.co/datasets/ibm-nasa-geospatial/Landslide4sense).\"\"\" all_band_names = ( \"COASTAL AEROSOL\" , \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"WATER_VAPOR\" , \"CIRRUS\" , \"SWIR_1\" , \"SWIR_2\" , \"SLOPE\" , \"DEM\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"validation\" , \"test\" : \"test\" } def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , ) -> None : \"\"\"Initialize the Landslide4Sense dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'validation', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . data_directory = Path ( data_root ) images_dir = self . data_directory / \"images\" / split_name annotations_dir = self . data_directory / \"annotations\" / split_name self . image_files = sorted ( images_dir . glob ( \"image_*.h5\" )) self . mask_files = sorted ( annotations_dir . glob ( \"mask_*.h5\" )) self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: image_file = self . image_files [ index ] mask_file = self . mask_files [ index ] with h5py . File ( image_file , \"r\" ) as h5file : image = np . array ( h5file [ \"img\" ])[ ... , self . band_indices ] with h5py . File ( mask_file , \"r\" ) as h5file : mask = np . array ( h5file [ \"mask\" ]) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = ( rgb_image - rgb_image . min ( axis = ( 0 , 1 ))) * ( 1 / rgb_image . max ( axis = ( 0 , 1 ))) rgb_image = np . clip ( rgb_image , 0 , 1 ) num_classes = len ( np . unique ( mask )) cmap = colormaps [ \"jet\" ] norm = Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if sample . get ( \"class_names\" ): class_names = sample [ \"class_names\" ] legend_handles = [ mpatches . Patch ( color = cmap ( i ), label = class_names [ i ]) for i in range ( num_classes ) ] ax [ 0 ] . legend ( handles = legend_handles , bbox_to_anchor = ( 1.05 , 1 ), loc = \"upper left\" ) if suptitle : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None ) # Initialize the Landslide4Sense dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'validation', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). Source code in terratorch/datasets/landslide4sense.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , ) -> None : \"\"\"Initialize the Landslide4Sense dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'validation', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . data_directory = Path ( data_root ) images_dir = self . data_directory / \"images\" / split_name annotations_dir = self . data_directory / \"annotations\" / split_name self . image_files = sorted ( images_dir . glob ( \"image_*.h5\" )) self . mask_files = sorted ( annotations_dir . glob ( \"mask_*.h5\" )) self . transform = transform if transform else default_transform terratorch.datasets.m_eurosat # MEuroSATNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for M-EuroSAT . Source code in terratorch/datasets/m_eurosat.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class MEuroSATNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-EuroSAT](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"COASTAL_AEROSOL\" , \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"NIR_NARROW\" , \"WATER_VAPOR\" , \"CIRRUS\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-eurosat\" partition_file_template = \" {partition} _partition.json\" label_map_file = \"label_map.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] \\ self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir label_map_path = self . data_directory / self . label_map_file with open ( label_map_path ) as file : self . label_map = json . load ( file ) self . id_to_class = { img_id : cls for cls , ids in self . label_map . items () for img_id in ids } partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] image_id = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) label_class = self . id_to_class [ image_id ] label_index = list ( self . label_map . keys ()) . index ( label_class ) output = { \"image\" : image . astype ( np . float32 )} if self . transform : output = self . transform ( ** output ) output [ \"label\" ] = label_index return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label_index = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) class_names = list ( self . label_map . keys ()) class_name = class_names [ label_index ] fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { class_name } \" ) if suptitle : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , partition = 'default' ) # Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. Source code in terratorch/datasets/m_eurosat.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] \\ self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir label_map_path = self . data_directory / self . label_map_file with open ( label_map_path ) as file : self . label_map = json . load ( file ) self . id_to_class = { img_id : cls for cls , ids in self . label_map . items () for img_id in ids } partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_eurosat.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label_index = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) class_names = list ( self . label_map . keys ()) class_name = class_names [ label_index ] fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { class_name } \" ) if suptitle : plt . suptitle ( suptitle ) return fig terratorch.datasets.m_bigearthnet # MBigEarthNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for M-BigEarthNet . Source code in terratorch/datasets/m_bigearthnet.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class MBigEarthNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-BigEarthNet](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"COASTAL_AEROSOL\" , \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"NIR_NARROW\" , \"WATER_VAPOR\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-bigearthnet\" label_map_file = \"label_stats.json\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir label_map_path = self . data_directory / self . label_map_file with open ( label_map_path ) as file : self . label_map = json . load ( file ) self . num_classes = len ( next ( iter ( self . label_map . values ()))) partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] image_id = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) labels_vector = self . label_map [ image_id ] labels_tensor = torch . tensor ( labels_vector , dtype = torch . float ) output = { \"image\" : image } if self . transform : output = self . transform ( ** output ) output [ \"label\" ] = labels_tensor return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # Convert to (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) active_labels = [ i for i , lbl in enumerate ( label ) if lbl == 1 ] fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Active Labels: { active_labels } \" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , partition = 'default' ) # Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. Source code in terratorch/datasets/m_bigearthnet.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir label_map_path = self . data_directory / self . label_map_file with open ( label_map_path ) as file : self . label_map = json . load ( file ) self . num_classes = len ( next ( iter ( self . label_map . values ()))) partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_bigearthnet.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # Convert to (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) active_labels = [ i for i , lbl in enumerate ( label ) if lbl == 1 ] fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Active Labels: { active_labels } \" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig terratorch.datasets.m_brick_kiln # MBrickKilnNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for M-BrickKiln . Source code in terratorch/datasets/m_brick_kiln.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class MBrickKilnNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-BrickKiln](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"COASTAL_AEROSOL\" , \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"NIR_NARROW\" , \"WATER_VAPOR\" , \"CIRRUS\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-brick-kiln\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] image_id = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) attr_dict = pickle . loads ( ast . literal_eval ( h5file . attrs [ \"pickle\" ])) class_index = attr_dict [ \"label\" ] output = { \"image\" : image . astype ( np . float32 )} if self . transform : output = self . transform ( ** output ) output [ \"label\" ] = class_index return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # Convert to (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { label } \" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , partition = 'default' ) # Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. Source code in terratorch/datasets/m_brick_kiln.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_brick_kiln.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # Convert to (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { label } \" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig terratorch.datasets.m_forestnet # MForestNetNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for M-ForestNet . Source code in terratorch/datasets/m_forestnet.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 class MForestNetNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-ForestNet](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" , \"NIR\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-forestnet\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] image_id = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) attr_dict = pickle . loads ( ast . literal_eval ( h5file . attrs [ \"pickle\" ])) # noqa: S301 class_index = attr_dict [ \"label\" ] output = { \"image\" : image . astype ( np . float32 )} if self . transform : output = self . transform ( ** output ) output [ \"label\" ] = class_index if self . use_metadata : temporal_coords = self . _get_date ( image_id ) location_coords = self . _get_coords ( image_id ) output [ \"temporal_coords\" ] = temporal_coords output [ \"location_coords\" ] = location_coords return output def _get_coords ( self , image_id : str ) -> torch . Tensor : \"\"\"Extract spatial coordinates from the image ID. Args: image_id (str): The ID of the image. Returns: torch.Tensor: Tensor containing latitude and longitude. \"\"\" lat_str , lon_str , _ = image_id . split ( \"_\" , 2 ) latitude = float ( lat_str ) longitude = float ( lon_str ) return torch . tensor ([ latitude , longitude ], dtype = torch . float32 ) def _get_date ( self , image_id : str ) -> torch . Tensor : _ , _ , date_str = image_id . split ( \"_\" , 2 ) date = pd . to_datetime ( date_str , format = \"%Y_%m_ %d \" ) return torch . tensor ([[ date . year , date . dayofyear - 1 ]], dtype = torch . float32 ) def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { label } \" ) if suptitle : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , partition = 'default' , use_metadata = False ) # Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (time and location). Source code in terratorch/datasets/m_forestnet.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_forestnet.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { label } \" ) if suptitle : plt . suptitle ( suptitle ) return fig terratorch.datasets.m_so2sat # MSo2SatNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for M-So2Sat . Source code in terratorch/datasets/m_so2sat.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class MSo2SatNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-So2Sat](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"VH_REAL\" , \"BLUE\" , \"VH_IMAGINARY\" , \"GREEN\" , \"VV_REAL\" , \"RED\" , \"VV_IMAGINARY\" , \"VH_LEE_FILTERED\" , \"RED_EDGE_1\" , \"VV_LEE_FILTERED\" , \"RED_EDGE_2\" , \"VH_LEE_FILTERED_REAL\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"VV_LEE_FILTERED_IMAGINARY\" , \"NIR_NARROW\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-so2sat\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] image_id = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) attr_dict = pickle . loads ( ast . literal_eval ( h5file . attrs [ \"pickle\" ])) class_index = attr_dict [ \"label\" ] output = { \"image\" : image . astype ( np . float32 )} if self . transform : output = self . transform ( ** output ) output [ \"label\" ] = class_index return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label_index = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) class_name = str ( label_index ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { class_name } \" ) if suptitle : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , partition = 'default' ) # Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. Source code in terratorch/datasets/m_so2sat.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_so2sat.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label_index = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) class_name = str ( label_index ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { class_name } \" ) if suptitle : plt . suptitle ( suptitle ) return fig terratorch.datasets.m_pv4ger # MPv4gerNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for M-PV4GER . Source code in terratorch/datasets/m_pv4ger.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class MPv4gerNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-PV4GER](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-pv4ger\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (location coordinates). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] image_id = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) attr_dict = pickle . loads ( ast . literal_eval ( h5file . attrs [ \"pickle\" ])) # noqa: S301 class_index = attr_dict [ \"label\" ] output = { \"image\" : image . astype ( np . float32 )} if self . transform : output = self . transform ( ** output ) output [ \"label\" ] = class_index if self . use_metadata : output [ \"location_coords\" ] = self . _get_coords ( image_id ) return output def _get_coords ( self , image_id : str ) -> torch . Tensor : \"\"\"Extract spatial coordinates from the image ID.\"\"\" lat_str , lon_str = image_id . split ( \",\" ) latitude = float ( lat_str ) longitude = float ( lon_str ) return torch . tensor ([ latitude , longitude ], dtype = torch . float32 ) def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { label } \" ) if suptitle : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , partition = 'default' , use_metadata = False ) # Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (location coordinates). Source code in terratorch/datasets/m_pv4ger.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (location coordinates). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_pv4ger.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { label } \" ) if suptitle : plt . suptitle ( suptitle ) return fig terratorch.datasets.m_cashew_plantation # MBeninSmallHolderCashewsNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for M-BeninSmallHolderCashews . Source code in terratorch/datasets/m_cashew_plantation.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 class MBeninSmallHolderCashewsNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-BeninSmallHolderCashews](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"COASTAL_AEROSOL\" , \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"NIR_NARROW\" , \"WATER_VAPOR\" , \"SWIR_1\" , \"SWIR_2\" , \"CLOUD_PROBABILITY\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-cashew-plant\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (time). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def _get_date ( self , keys ) -> torch . Tensor : date_pattern = re . compile ( r \"\\d {4} -\\d {2} -\\d {2} \" ) date_str = None for key in keys : match = date_pattern . search ( key ) if match : date_str = match . group () break date = torch . zeros (( 1 , 2 ), dtype = torch . float32 ) if date_str : date = pd . to_datetime ( date_str , format = \"%Y-%m- %d \" ) date = torch . tensor ([[ date . year , date . dayofyear - 1 ]], dtype = torch . float32 ) return date def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) temporal_coords = self . _get_date ( h5file ) mask = np . array ( h5file [ \"label\" ]) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () if self . use_metadata : output [ \"temporal_coords\" ] = temporal_coords return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , partition = 'default' , use_metadata = False ) # Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (time). Source code in terratorch/datasets/m_cashew_plantation.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (time). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_cashew_plantation.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig terratorch.datasets.m_nz_cattle # MNzCattleNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for M-NZ-Cattle . Source code in terratorch/datasets/m_nz_cattle.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 class MNzCattleNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-NZ-Cattle](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-nz-cattle\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] file_name = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) data_keys = [ key for key in keys if \"label\" not in key ] label_keys = [ key for key in keys if \"label\" in key ] temporal_coords = self . _get_date ( data_keys [ 0 ]) bands = [ np . array ( h5file [ key ]) for key in data_keys ] image = np . stack ( bands , axis =- 1 ) mask = np . array ( h5file [ label_keys [ 0 ]]) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () if self . use_metadata : location_coords = self . _get_coords ( file_name ) output [ \"location_coords\" ] = location_coords output [ \"temporal_coords\" ] = temporal_coords return output def _get_coords ( self , file_name : str ) -> torch . Tensor : \"\"\"Extract spatial coordinates from the file name.\"\"\" match = re . search ( r \"_(\\-?\\d+\\.\\d+),(\\-?\\d+\\.\\d+)\" , file_name ) if match : longitude , latitude = map ( float , match . groups ()) return torch . tensor ([ latitude , longitude ], dtype = torch . float32 ) def _get_date ( self , band_name : str ) -> torch . Tensor : date_str = band_name . split ( \"_\" )[ - 1 ] date = pd . to_datetime ( date_str , format = \"%Y-%m- %d \" ) return torch . tensor ([[ date . year , date . dayofyear - 1 ]], dtype = torch . float32 ) def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , partition = 'default' , use_metadata = False ) # Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (time and location). Source code in terratorch/datasets/m_nz_cattle.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_nz_cattle.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig terratorch.datasets.m_chesapeake_landcover # MChesapeakeLandcoverNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for M-ChesapeakeLandcover . Source code in terratorch/datasets/m_chesapeake_landcover.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class MChesapeakeLandcoverNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-ChesapeakeLandcover](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"NIR\" , \"RED\" ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-chesapeake\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) mask = np . array ( h5file [ \"label\" ]) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , partition = 'default' ) # Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. Source code in terratorch/datasets/m_chesapeake_landcover.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_chesapeake_landcover.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig terratorch.datasets.m_pv4ger_seg # MPv4gerSegNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for M-PV4GER-SEG . Source code in terratorch/datasets/m_pv4ger_seg.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 class MPv4gerSegNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-PV4GER-SEG](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-pv4ger-seg\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (location coordinates). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] image_id = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) mask = np . array ( h5file [ \"label\" ]) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () if self . use_metadata : output [ \"location_coords\" ] = self . _get_coords ( image_id ) return output def _get_coords ( self , image_id : str ) -> torch . Tensor : \"\"\"Extract spatial coordinates from the image ID.\"\"\" lat_str , lon_str = image_id . split ( \",\" ) latitude = float ( lat_str ) longitude = float ( lon_str ) return torch . tensor ([ latitude , longitude ], dtype = torch . float32 ) def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , partition = 'default' , use_metadata = False ) # Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (location coordinates). Source code in terratorch/datasets/m_pv4ger_seg.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (location coordinates). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_pv4ger_seg.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig terratorch.datasets.m_SA_crop_type # MSACropTypeNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for M-SA-Crop-Type . Source code in terratorch/datasets/m_SA_crop_type.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class MSACropTypeNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-SA-Crop-Type](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"COASTAL_AEROSOL\" , \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"NIR_NARROW\" , \"WATER_VAPOR\" , \"SWIR_1\" , \"SWIR_2\" , \"CLOUD_PROBABILITY\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-SA-crop-type\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) mask = np . array ( h5file [ \"label\" ]) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , partition = 'default' ) # Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. Source code in terratorch/datasets/m_SA_crop_type.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_SA_crop_type.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig terratorch.datasets.m_neontree # MNeonTreeNonGeo # Bases: NonGeoDataset NonGeo dataset implementation for M-NeonTree . Source code in terratorch/datasets/m_neontree.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 class MNeonTreeNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-NeonTree](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"BLUE\" , \"CANOPY_HEIGHT_MODEL\" , \"GREEN\" , \"NEON\" , \"RED\" ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-NeonTree\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = rgb_bands , transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to RGB bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) mask = np . array ( h5file [ \"label\" ]) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = rgb_bands , transform = None , partition = 'default' ) # Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: rgb_bands ) \u2013 Bands to be used. Defaults to RGB bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. Source code in terratorch/datasets/m_neontree.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = rgb_bands , transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to RGB bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_neontree.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig terratorch.datasets.multi_temporal_crop_classification # MultiTemporalCropClassification # Bases: NonGeoDataset NonGeo dataset implementation for multi-temporal crop classification . Source code in terratorch/datasets/multi_temporal_crop_classification.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 class MultiTemporalCropClassification ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [multi-temporal crop classification](https://huggingface.co/datasets/ibm-nasa-geospatial/multi-temporal-crop-classification).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" , \"NIR_NARROW\" , \"SWIR_1\" , \"SWIR_2\" , ) class_names = ( \"Natural Vegetation\" , \"Forest\" , \"Corn\" , \"Soybeans\" , \"Wetlands\" , \"Developed / Barren\" , \"Open Water\" , \"Winter Wheat\" , \"Alfalfa\" , \"Fallow / Idle Cropland\" , \"Cotton\" , \"Sorghum\" , \"Other\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } num_classes = 13 time_steps = 3 splits = { \"train\" : \"training\" , \"val\" : \"validation\" } # Only train and val splits available metadata_file_name = \"chips_df.csv\" col_name = \"chip_id\" date_columns = [ \"first_img_date\" , \"middle_img_date\" , \"last_img_date\" ] def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = True , reduce_zero_label : bool = True , use_metadata : bool = False , ) -> None : \"\"\"Constructor Args: data_root (str): Path to the data root directory. split (str): one of 'train' or 'val'. bands (list[str]): Bands that should be output by the dataset. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If None, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True. use_metadata (bool): whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { self . splits } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) data_dir = self . data_root / f \" { split_name } _chips\" self . image_files = sorted ( glob . glob ( os . path . join ( data_dir , \"*_merged.tif\" ))) self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( data_dir , \"*.mask.tif\" ))) split_file = self . data_root / f \" { split_name } _data.txt\" with open ( split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . segmentation_mask_files = filter_valid_files ( self . segmentation_mask_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . reduce_zero_label = reduce_zero_label self . expand_temporal_dimension = expand_temporal_dimension self . use_metadata = use_metadata self . metadata = None if self . use_metadata : metadata_file = self . data_root / self . metadata_file_name self . metadata = pd . read_csv ( metadata_file ) self . _build_image_metadata_mapping () # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform def _build_image_metadata_mapping ( self ): \"\"\"Build a mapping from image filenames to metadata indices.\"\"\" self . image_to_metadata_index = dict () for idx , image_file in enumerate ( self . image_files ): image_filename = Path ( image_file ) . name image_id = image_filename . replace ( \"_merged.tif\" , \"\" ) . replace ( \".tif\" , \"\" ) metadata_indices = self . metadata . index [ self . metadata [ self . col_name ] == image_id ] . tolist () self . image_to_metadata_index [ idx ] = metadata_indices [ 0 ] def __len__ ( self ) -> int : return len ( self . image_files ) def _get_date ( self , row : pd . Series ) -> torch . Tensor : \"\"\"Extract and format temporal coordinates (T, date) from metadata.\"\"\" temporal_coords = [] for col in self . date_columns : date_str = row [ col ] date = pd . to_datetime ( date_str , format = \"%Y-%m- %d \" ) temporal_coords . append ([ date . year , date . dayofyear - 1 ]) return torch . tensor ( temporal_coords , dtype = torch . float32 ) def _get_coords ( self , image : DataArray ) -> torch . Tensor : px = image . x . shape [ 0 ] // 2 py = image . y . shape [ 0 ] // 2 # get center point to reproject to lat/lon point = image . isel ( band = 0 , x = slice ( px , px + 1 ), y = slice ( py , py + 1 )) point = point . rio . reproject ( \"epsg:4326\" ) lat_lon = np . asarray ([ point . y [ 0 ], point . x [ 0 ]]) return torch . tensor ( lat_lon , dtype = torch . float32 ) def __getitem__ ( self , index : int ) -> dict [ str , Any ]: image = self . _load_file ( self . image_files [ index ], nan_replace = self . no_data_replace ) location_coords , temporal_coords = None , None if self . use_metadata : location_coords = self . _get_coords ( image ) metadata_idx = self . image_to_metadata_index . get ( index , None ) if metadata_idx is not None : row = self . metadata . iloc [ metadata_idx ] temporal_coords = self . _get_date ( row ) # to channels last image = image . to_numpy () if self . expand_temporal_dimension : image = rearrange ( image , \"(channels time) h w -> channels time h w\" , channels = len ( self . bands )) image = np . moveaxis ( image , 0 , - 1 ) # filter bands image = image [ ... , self . band_indices ] output = { \"image\" : image . astype ( np . float32 ), \"mask\" : self . _load_file ( self . segmentation_mask_files [ index ], nan_replace = self . no_label_replace ) . to_numpy ()[ 0 ], } if self . reduce_zero_label : output [ \"mask\" ] -= 1 if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () if self . use_metadata : output [ \"location_coords\" ] = location_coords output [ \"temporal_coords\" ] = temporal_coords return output def _load_file ( self , path : Path , nan_replace : int | float | None = None ) -> DataArray : data = rioxarray . open_rasterio ( path , masked = True ) if nan_replace is not None : data = data . fillna ( nan_replace ) return data def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample \"\"\" num_images = self . time_steps + 2 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) images = sample [ \"image\" ] images = images [ rgb_indices , ... ] # Shape: (T, 3, H, W) processed_images = [] for t in range ( self . time_steps ): img = images [ t ] img = img . permute ( 1 , 2 , 0 ) img = img . numpy () img = clip_image ( img ) processed_images . append ( img ) mask = sample [ \"mask\" ] . numpy () if \"prediction\" in sample : num_images += 1 fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 5 ), layout = \"compressed\" ) ax [ 0 ] . axis ( \"off\" ) norm = mpl . colors . Normalize ( vmin = 0 , vmax = self . num_classes - 1 ) for i , img in enumerate ( processed_images ): ax [ i + 1 ] . axis ( \"off\" ) ax [ i + 1 ] . title . set_text ( f \"T { i } \" ) ax [ i + 1 ] . imshow ( img ) ax [ self . time_steps + 1 ] . axis ( \"off\" ) ax [ self . time_steps + 1 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ self . time_steps + 1 ] . imshow ( mask , cmap = \"jet\" , norm = norm ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] ax [ self . time_steps + 2 ] . axis ( \"off\" ) ax [ self . time_steps + 2 ] . title . set_text ( \"Predicted Mask\" ) ax [ self . time_steps + 2 ] . imshow ( prediction , cmap = \"jet\" , norm = norm ) cmap = plt . get_cmap ( \"jet\" ) legend_data = [[ i , cmap ( norm ( i )), self . class_names [ i ]] for i in range ( self . num_classes )] handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = tuple ( v for v in c )) for k , c , n in legend_data ] labels = [ n for k , c , n in legend_data ] ax [ 0 ] . legend ( handles , labels , loc = \"center\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , no_data_replace = None , no_label_replace = None , expand_temporal_dimension = True , reduce_zero_label = True , use_metadata = False ) # Constructor Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 one of 'train' or 'val'. bands ( list [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands that should be output by the dataset. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2(). no_data_replace ( float | None , default: None ) \u2013 Replace nan values in input images with this value. If None, does no replacement. Defaults to None. no_label_replace ( int | None , default: None ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension ( bool , default: True ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True. reduce_zero_label ( bool , default: True ) \u2013 Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True. use_metadata ( bool , default: False ) \u2013 whether to return metadata info (time and location). Source code in terratorch/datasets/multi_temporal_crop_classification.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = True , reduce_zero_label : bool = True , use_metadata : bool = False , ) -> None : \"\"\"Constructor Args: data_root (str): Path to the data root directory. split (str): one of 'train' or 'val'. bands (list[str]): Bands that should be output by the dataset. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If None, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True. use_metadata (bool): whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { self . splits } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) data_dir = self . data_root / f \" { split_name } _chips\" self . image_files = sorted ( glob . glob ( os . path . join ( data_dir , \"*_merged.tif\" ))) self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( data_dir , \"*.mask.tif\" ))) split_file = self . data_root / f \" { split_name } _data.txt\" with open ( split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . segmentation_mask_files = filter_valid_files ( self . segmentation_mask_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . reduce_zero_label = reduce_zero_label self . expand_temporal_dimension = expand_temporal_dimension self . use_metadata = use_metadata self . metadata = None if self . use_metadata : metadata_file = self . data_root / self . metadata_file_name self . metadata = pd . read_csv ( metadata_file ) self . _build_image_metadata_mapping () # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 a sample returned by :meth: __getitem__ suptitle ( str | None , default: None ) \u2013 optional string to use as a suptitle Returns: Figure \u2013 a matplotlib Figure with the rendered sample Source code in terratorch/datasets/multi_temporal_crop_classification.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample \"\"\" num_images = self . time_steps + 2 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) images = sample [ \"image\" ] images = images [ rgb_indices , ... ] # Shape: (T, 3, H, W) processed_images = [] for t in range ( self . time_steps ): img = images [ t ] img = img . permute ( 1 , 2 , 0 ) img = img . numpy () img = clip_image ( img ) processed_images . append ( img ) mask = sample [ \"mask\" ] . numpy () if \"prediction\" in sample : num_images += 1 fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 5 ), layout = \"compressed\" ) ax [ 0 ] . axis ( \"off\" ) norm = mpl . colors . Normalize ( vmin = 0 , vmax = self . num_classes - 1 ) for i , img in enumerate ( processed_images ): ax [ i + 1 ] . axis ( \"off\" ) ax [ i + 1 ] . title . set_text ( f \"T { i } \" ) ax [ i + 1 ] . imshow ( img ) ax [ self . time_steps + 1 ] . axis ( \"off\" ) ax [ self . time_steps + 1 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ self . time_steps + 1 ] . imshow ( mask , cmap = \"jet\" , norm = norm ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] ax [ self . time_steps + 2 ] . axis ( \"off\" ) ax [ self . time_steps + 2 ] . title . set_text ( \"Predicted Mask\" ) ax [ self . time_steps + 2 ] . imshow ( prediction , cmap = \"jet\" , norm = norm ) cmap = plt . get_cmap ( \"jet\" ) legend_data = [[ i , cmap ( norm ( i )), self . class_names [ i ]] for i in range ( self . num_classes )] handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = tuple ( v for v in c )) for k , c , n in legend_data ] labels = [ n for k , c , n in legend_data ] ax [ 0 ] . legend ( handles , labels , loc = \"center\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig terratorch.datasets.open_sentinel_map # OpenSentinelMap # Bases: NonGeoDataset Pytorch Dataset class to load samples from the OpenSentinelMap dataset, supporting multiple bands and temporal sampling strategies. Source code in terratorch/datasets/open_sentinel_map.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 class OpenSentinelMap ( NonGeoDataset ): \"\"\" Pytorch Dataset class to load samples from the [OpenSentinelMap](https://visionsystemsinc.github.io/open-sentinel-map/) dataset, supporting multiple bands and temporal sampling strategies. \"\"\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : list [ str ] | None = None , transform : A . Compose | None = None , spatial_interpolate_and_stack_temporally : bool = True , # noqa: FBT001, FBT002 pad_image : int | None = None , truncate_image : int | None = None , target : int = 0 , pick_random_pair : bool = True , # noqa: FBT002, FBT001 ) -> None : \"\"\" Args: data_root (str): Path to the root directory of the dataset. split (str): Dataset split to load. Options are 'train', 'val', or 'test'. Defaults to 'train'. bands (list of str, optional): List of band names to load. Defaults to ['gsd_10', 'gsd_20', 'gsd_60']. transform (albumentations.Compose, optional): Albumentations transformations to apply to the data. spatial_interpolate_and_stack_temporally (bool): If True, the bands are interpolated and concatenated over time. Default is True. pad_image (int, optional): Number of timesteps to pad the time dimension of the image. If None, no padding is applied. truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed. target (int): Specifies which target class to use from the mask. Default is 0. pick_random_pair (bool): If True, selects two random images from the temporal sequence. Default is True. \"\"\" split = \"test\" if bands is None : bands = [ \"gsd_10\" , \"gsd_20\" , \"gsd_60\" ] allowed_bands = { \"gsd_10\" , \"gsd_20\" , \"gsd_60\" } for band in bands : if band not in allowed_bands : msg = f \"Band ' { band } ' is not recognized. Available values are: { ', ' . join ( allowed_bands ) } \" raise ValueError ( msg ) if split not in [ \"train\" , \"val\" , \"test\" ]: msg = f \"Split ' { split } ' not recognized. Use 'train', 'val', or 'test'.\" raise ValueError ( msg ) self . data_root = Path ( data_root ) split_mapping = { \"train\" : \"training\" , \"val\" : \"validation\" , \"test\" : \"testing\" } split = split_mapping [ split ] self . imagery_root = self . data_root / \"osm_sentinel_imagery\" self . label_root = self . data_root / \"osm_label_images_v10\" self . auxiliary_data = pd . read_csv ( self . data_root / \"spatial_cell_info.csv\" ) self . auxiliary_data = self . auxiliary_data [ self . auxiliary_data [ \"split\" ] == split ] self . bands = bands self . transform = transform if transform else lambda ** batch : to_tensor ( batch ) self . label_mappings = self . _load_label_mappings () self . split_data = self . auxiliary_data [ self . auxiliary_data [ \"split\" ] == split ] self . spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally self . pad_image = pad_image self . truncate_image = truncate_image self . target = target self . pick_random_pair = pick_random_pair self . image_files = [] self . label_files = [] for _ , row in self . split_data . iterrows (): mgrs_tile = row [ \"MGRS_tile\" ] spatial_cell = str ( row [ \"cell_id\" ]) label_file = self . label_root / mgrs_tile / f \" { spatial_cell } .png\" if label_file . exists (): self . image_files . append (( mgrs_tile , spatial_cell )) self . label_files . append ( label_file ) def _load_label_mappings ( self ): with open ( self . data_root / \"osm_categories.json\" ) as f : return json . load ( f ) def _extract_date_from_filename ( self , filename : str ) -> str : match = re . search ( r \"(\\d {8} )\" , filename ) if match : return match . group ( 1 ) else : msg = f \"Date not found in filename { filename } \" raise ValueError ( msg ) def __len__ ( self ) -> int : return len ( self . image_files ) def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : if \"gsd_10\" not in self . bands : return None num_images = len ([ key for key in sample if key . startswith ( \"image\" )]) images = [] for i in range ( 1 , num_images + 1 ): image_dict = sample [ f \"image { i } \" ] image = image_dict [ \"gsd_10\" ] if isinstance ( image , Tensor ): image = image . numpy () image = image . take ( range ( 3 ), axis = 2 ) image = image . squeeze () image = ( image - image . min ( axis = ( 0 , 1 ))) * ( 1 / image . max ( axis = ( 0 , 1 ))) image = np . clip ( image , 0 , 1 ) images . append ( image ) label_mask = sample [ \"mask\" ] if isinstance ( label_mask , Tensor ): label_mask = label_mask . numpy () return self . _plot_sample ( images , label_mask , suptitle = suptitle ) def _plot_sample ( self , images : list [ np . ndarray ], label : np . ndarray , suptitle : str | None = None , ) -> Figure : num_images = len ( images ) fig , ax = plt . subplots ( 1 , num_images + 1 , figsize = ( 15 , 5 )) for i , image in enumerate ( images ): ax [ i ] . imshow ( image ) ax [ i ] . set_title ( f \"Image { i + 1 } \" ) ax [ i ] . axis ( \"off\" ) ax [ - 1 ] . imshow ( label , cmap = \"gray\" ) ax [ - 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ - 1 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: mgrs_tile , spatial_cell = self . image_files [ index ] spatial_cell_path = self . imagery_root / mgrs_tile / spatial_cell npz_files = list ( spatial_cell_path . glob ( \"*.npz\" )) npz_files . sort ( key = lambda x : self . _extract_date_from_filename ( x . stem )) if self . pick_random_pair : npz_files = random . sample ( npz_files , 2 ) npz_files . sort ( key = lambda x : self . _extract_date_from_filename ( x . stem )) output = {} if self . spatial_interpolate_and_stack_temporally : images_over_time = [] for _ , npz_file in enumerate ( npz_files ): data = np . load ( npz_file ) interpolated_bands = [] for band in self . bands : band_frame = data [ band ] band_frame = torch . from_numpy ( band_frame ) . float () band_frame = band_frame . permute ( 2 , 0 , 1 ) interpolated = F . interpolate ( band_frame . unsqueeze ( 0 ), size = MAX_TEMPORAL_IMAGE_SIZE , mode = \"bilinear\" , align_corners = False ) . squeeze ( 0 ) interpolated_bands . append ( interpolated ) concatenated_bands = torch . cat ( interpolated_bands , dim = 0 ) images_over_time . append ( concatenated_bands ) images = torch . stack ( images_over_time , dim = 0 ) . numpy () if self . truncate_image : images = images [ - self . truncate_image :] if self . pad_image : images = pad_numpy ( images , self . pad_image ) output [ \"image\" ] = images . transpose ( 0 , 2 , 3 , 1 ) else : image_dict = { band : [] for band in self . bands } for _ , npz_file in enumerate ( npz_files ): data = np . load ( npz_file ) for band in self . bands : band_frames = data [ band ] band_frames = band_frames . astype ( np . float32 ) band_frames = np . transpose ( band_frames , ( 2 , 0 , 1 )) image_dict [ band ] . append ( band_frames ) final_image_dict = {} for band in self . bands : band_images = image_dict [ band ] if self . truncate_image : band_images = band_images [ - self . truncate_image :] if self . pad_image : band_images = [ pad_numpy ( img , self . pad_image ) for img in band_images ] band_images = np . stack ( band_images , axis = 0 ) final_image_dict [ band ] = band_images output [ \"image\" ] = final_image_dict label_file = self . label_files [ index ] mask = np . array ( Image . open ( label_file )) . astype ( int ) # Map 'unlabel' (254) and 'none' (255) to unused classes 15 and 16 for processing mask [ mask == 254 ] = 15 # noqa: PLR2004 mask [ mask == 255 ] = 16 # noqa: PLR2004 output [ \"mask\" ] = mask [:, :, self . target ] if self . transform : output = self . transform ( ** output ) return output __init__ ( data_root , split = 'train' , bands = None , transform = None , spatial_interpolate_and_stack_temporally = True , pad_image = None , truncate_image = None , target = 0 , pick_random_pair = True ) # Parameters: data_root ( str ) \u2013 Path to the root directory of the dataset. split ( str , default: 'train' ) \u2013 Dataset split to load. Options are 'train', 'val', or 'test'. Defaults to 'train'. bands ( list of str , default: None ) \u2013 List of band names to load. Defaults to ['gsd_10', 'gsd_20', 'gsd_60']. transform ( Compose , default: None ) \u2013 Albumentations transformations to apply to the data. spatial_interpolate_and_stack_temporally ( bool , default: True ) \u2013 If True, the bands are interpolated and concatenated over time. Default is True. pad_image ( int , default: None ) \u2013 Number of timesteps to pad the time dimension of the image. If None, no padding is applied. truncate_image ( int , default: None ) \u2013 Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed. target ( int , default: 0 ) \u2013 Specifies which target class to use from the mask. Default is 0. pick_random_pair ( bool , default: True ) \u2013 If True, selects two random images from the temporal sequence. Default is True. Source code in terratorch/datasets/open_sentinel_map.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , data_root : str , split : str = \"train\" , bands : list [ str ] | None = None , transform : A . Compose | None = None , spatial_interpolate_and_stack_temporally : bool = True , # noqa: FBT001, FBT002 pad_image : int | None = None , truncate_image : int | None = None , target : int = 0 , pick_random_pair : bool = True , # noqa: FBT002, FBT001 ) -> None : \"\"\" Args: data_root (str): Path to the root directory of the dataset. split (str): Dataset split to load. Options are 'train', 'val', or 'test'. Defaults to 'train'. bands (list of str, optional): List of band names to load. Defaults to ['gsd_10', 'gsd_20', 'gsd_60']. transform (albumentations.Compose, optional): Albumentations transformations to apply to the data. spatial_interpolate_and_stack_temporally (bool): If True, the bands are interpolated and concatenated over time. Default is True. pad_image (int, optional): Number of timesteps to pad the time dimension of the image. If None, no padding is applied. truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed. target (int): Specifies which target class to use from the mask. Default is 0. pick_random_pair (bool): If True, selects two random images from the temporal sequence. Default is True. \"\"\" split = \"test\" if bands is None : bands = [ \"gsd_10\" , \"gsd_20\" , \"gsd_60\" ] allowed_bands = { \"gsd_10\" , \"gsd_20\" , \"gsd_60\" } for band in bands : if band not in allowed_bands : msg = f \"Band ' { band } ' is not recognized. Available values are: { ', ' . join ( allowed_bands ) } \" raise ValueError ( msg ) if split not in [ \"train\" , \"val\" , \"test\" ]: msg = f \"Split ' { split } ' not recognized. Use 'train', 'val', or 'test'.\" raise ValueError ( msg ) self . data_root = Path ( data_root ) split_mapping = { \"train\" : \"training\" , \"val\" : \"validation\" , \"test\" : \"testing\" } split = split_mapping [ split ] self . imagery_root = self . data_root / \"osm_sentinel_imagery\" self . label_root = self . data_root / \"osm_label_images_v10\" self . auxiliary_data = pd . read_csv ( self . data_root / \"spatial_cell_info.csv\" ) self . auxiliary_data = self . auxiliary_data [ self . auxiliary_data [ \"split\" ] == split ] self . bands = bands self . transform = transform if transform else lambda ** batch : to_tensor ( batch ) self . label_mappings = self . _load_label_mappings () self . split_data = self . auxiliary_data [ self . auxiliary_data [ \"split\" ] == split ] self . spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally self . pad_image = pad_image self . truncate_image = truncate_image self . target = target self . pick_random_pair = pick_random_pair self . image_files = [] self . label_files = [] for _ , row in self . split_data . iterrows (): mgrs_tile = row [ \"MGRS_tile\" ] spatial_cell = str ( row [ \"cell_id\" ]) label_file = self . label_root / mgrs_tile / f \" { spatial_cell } .png\" if label_file . exists (): self . image_files . append (( mgrs_tile , spatial_cell )) self . label_files . append ( label_file ) terratorch.datasets.openearthmap # OpenEarthMapNonGeo # Bases: NonGeoDataset OpenEarthMapNonGeo Dataset for non-georeferenced imagery. This dataset class handles non-georeferenced image data from the OpenEarthMap dataset. It supports configurable band sets and transformations, and performs cropping operations to ensure that the images conform to the required input dimensions. The dataset is split into \"train\", \"test\", and \"val\" subsets based on the provided split parameter. Source code in terratorch/datasets/openearthmap.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 class OpenEarthMapNonGeo ( NonGeoDataset ): \"\"\" [OpenEarthMapNonGeo](https://open-earth-map.org/) Dataset for non-georeferenced imagery. This dataset class handles non-georeferenced image data from the OpenEarthMap dataset. It supports configurable band sets and transformations, and performs cropping operations to ensure that the images conform to the required input dimensions. The dataset is split into \"train\", \"test\", and \"val\" subsets based on the provided split parameter. \"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } def __init__ ( self , data_root : str , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , split = \"train\" , crop_size : int = 256 , random_crop : bool = True ) -> None : \"\"\" Initialize a new instance of the OpenEarthMapNonGeo dataset. Args: data_root (str): The root directory containing the dataset files. bands (Sequence[str], optional): A list of band names to be used. Default is BAND_SETS[\"all\"]. transform (A.Compose or None, optional): A transformation pipeline to be applied to the data. If None, a default transform converting the data to a tensor is applied. split (str, optional): The dataset split to use (\"train\", \"test\", or \"val\"). Default is \"train\". crop_size (int, optional): The size (in pixels) of the crop to apply to images. Must be greater than 0. Default is 256. random_crop (bool, optional): If True, performs a random crop; otherwise, performs a center crop. Default is True. Raises: Exception: If the provided split is not one of \"train\", \"test\", or \"val\". AssertionError: If crop_size is not greater than 0. \"\"\" super () . __init__ () if split not in [ \"train\" , \"test\" , \"val\" ]: msg = \"Split must be one of train, test, val.\" raise Exception ( msg ) self . transform = transform if transform else lambda ** batch : to_tensor ( batch , transpose = False ) self . split = split self . data_root = data_root # images in openearthmap are not all 1024x1024 and must be cropped self . crop_size = crop_size self . random_crop = random_crop assert self . crop_size > 0 , \"Crop size must be greater than 0\" self . image_files = self . _get_file_paths ( Path ( self . data_root , f \" { split } .txt\" )) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: image_path , label_path = self . image_files [ index ] with rasterio . open ( image_path ) as src : image = src . read () with rasterio . open ( label_path ) as src : mask = src . read () # some images in the dataset are not perfect squares # cropping to fit to the prepare_features_for_image_model call if self . random_crop : image , mask = self . _random_crop ( image , mask ) else : image , mask = self . _center_crop ( image , mask ) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } output = self . transform ( ** output ) output [ 'mask' ] = output [ 'mask' ] . long () return output def _parse_file_name ( self , file_name : str ): underscore_pos = file_name . rfind ( '_' ) folder_name = file_name [: underscore_pos ] region_path = Path ( self . data_root , folder_name ) image_path = Path ( region_path , \"images\" , file_name ) label_path = Path ( region_path , \"labels\" , file_name ) return image_path , label_path def _get_file_paths ( self , text_file_path : str ): with open ( text_file_path , 'r' ) as file : lines = file . readlines () file_paths = [ self . _parse_file_name ( line . strip ()) for line in lines ] return file_paths def __len__ ( self ): return len ( self . image_files ) def _random_crop ( self , image , mask ): h , w = image . shape [ 1 :] top = np . random . randint ( 0 , h - self . crop_size ) left = np . random . randint ( 0 , w - self . crop_size ) image = image [:, top : top + self . crop_size , left : left + self . crop_size ] mask = mask [:, top : top + self . crop_size , left : left + self . crop_size ] return image , mask def _center_crop ( self , image , mask ): h , w = image . shape [ 1 :] top = ( h - self . crop_size ) // 2 left = ( w - self . crop_size ) // 2 image = image [:, top : top + self . crop_size , left : left + self . crop_size ] mask = mask [:, top : top + self . crop_size , left : left + self . crop_size ] return image , mask def plot ( self , arg , suptitle : str | None = None ) -> None : pass def plot_sample ( self , sample , prediction = None , suptitle : str | None = None , class_names = None ): pass __init__ ( data_root , bands = BAND_SETS [ 'all' ], transform = None , split = 'train' , crop_size = 256 , random_crop = True ) # Initialize a new instance of the OpenEarthMapNonGeo dataset. Parameters: data_root ( str ) \u2013 The root directory containing the dataset files. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 A list of band names to be used. Default is BAND_SETS[\"all\"]. transform ( Compose or None , default: None ) \u2013 A transformation pipeline to be applied to the data. If None, a default transform converting the data to a tensor is applied. split ( str , default: 'train' ) \u2013 The dataset split to use (\"train\", \"test\", or \"val\"). Default is \"train\". crop_size ( int , default: 256 ) \u2013 The size (in pixels) of the crop to apply to images. Must be greater than 0. Default is 256. random_crop ( bool , default: True ) \u2013 If True, performs a random crop; otherwise, performs a center crop. Default is True. Raises: Exception \u2013 If the provided split is not one of \"train\", \"test\", or \"val\". AssertionError \u2013 If crop_size is not greater than 0. Source code in terratorch/datasets/openearthmap.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , data_root : str , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , split = \"train\" , crop_size : int = 256 , random_crop : bool = True ) -> None : \"\"\" Initialize a new instance of the OpenEarthMapNonGeo dataset. Args: data_root (str): The root directory containing the dataset files. bands (Sequence[str], optional): A list of band names to be used. Default is BAND_SETS[\"all\"]. transform (A.Compose or None, optional): A transformation pipeline to be applied to the data. If None, a default transform converting the data to a tensor is applied. split (str, optional): The dataset split to use (\"train\", \"test\", or \"val\"). Default is \"train\". crop_size (int, optional): The size (in pixels) of the crop to apply to images. Must be greater than 0. Default is 256. random_crop (bool, optional): If True, performs a random crop; otherwise, performs a center crop. Default is True. Raises: Exception: If the provided split is not one of \"train\", \"test\", or \"val\". AssertionError: If crop_size is not greater than 0. \"\"\" super () . __init__ () if split not in [ \"train\" , \"test\" , \"val\" ]: msg = \"Split must be one of train, test, val.\" raise Exception ( msg ) self . transform = transform if transform else lambda ** batch : to_tensor ( batch , transpose = False ) self . split = split self . data_root = data_root # images in openearthmap are not all 1024x1024 and must be cropped self . crop_size = crop_size self . random_crop = random_crop assert self . crop_size > 0 , \"Crop size must be greater than 0\" self . image_files = self . _get_file_paths ( Path ( self . data_root , f \" { split } .txt\" )) terratorch.datasets.pastis # PASTIS # Bases: NonGeoDataset \" Pytorch Dataset class to load samples from the PASTIS dataset, for semantic and panoptic segmentation. Source code in terratorch/datasets/pastis.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 class PASTIS ( NonGeoDataset ): \"\"\"\" Pytorch Dataset class to load samples from the [PASTIS](https://github.com/VSainteuf/pastis-benchmark) dataset, for semantic and panoptic segmentation. \"\"\" def __init__ ( self , data_root , norm = True , # noqa: FBT002 target = \"semantic\" , folds = None , reference_date = \"2018-09-01\" , date_interval = ( - 200 , 600 ), class_mapping = None , transform = None , truncate_image = None , pad_image = None , satellites = [ \"S2\" ], # noqa: B006 ): \"\"\" Args: data_root (str): Path to the dataset. norm (bool): If true, images are standardised using pre-computed channel-wise means and standard deviations. reference_date (str, Format : 'YYYY-MM-DD'): Defines the reference date based on which all observation dates are expressed. Along with the image time series and the target tensor, this dataloader yields the sequence of observation dates (in terms of number of days since the reference date). This sequence of dates is used for instance for the positional encoding in attention based approaches. target (str): 'semantic' or 'instance'. Defines which type of target is returned by the dataloader. * If 'semantic' the target tensor is a tensor containing the class of each pixel. * If 'instance' the target tensor is the concatenation of several signals, necessary to train the Parcel-as-Points module: - the centerness heatmap, - the instance ids, - the voronoi partitioning of the patch with regards to the parcels' centers, - the (height, width) size of each parcel, - the semantic label of each parcel, - the semantic label of each pixel. folds (list, optional): List of ints specifying which of the 5 official folds to load. By default (when None is specified), all folds are loaded. class_mapping (dict, optional): A dictionary to define a mapping between the default 18 class nomenclature and another class grouping. If not provided, the default class mapping is used. transform (callable, optional): A transform to apply to the loaded data (images, dates, and masks). By default, no transformation is applied. truncate_image (int, optional): Truncate the time dimension of the image to a specified number of timesteps. If None, no truncation is performed. pad_image (int, optional): Pad the time dimension of the image to a specified number of timesteps. If None, no padding is applied. satellites (list): Defines the satellites to use. If you are using PASTIS-R, you have access to Sentinel-2 imagery and Sentinel-1 observations in Ascending and Descending orbits, respectively S2, S1A, and S1D. For example, use satellites=['S2', 'S1A'] for Sentinel-2 + Sentinel-1 ascending time series, or satellites=['S2', 'S1A', 'S1D'] to retrieve all time series. If you are using PASTIS, only S2 observations are available. \"\"\" if target not in [ \"semantic\" , \"instance\" ]: msg = f \"Target ' { target } ' not recognized. Use 'semantic', or 'instance'.\" raise ValueError ( msg ) valid_satellites = { \"S2\" , \"S1A\" , \"S1D\" } for sat in satellites : if sat not in valid_satellites : msg = f \"Satellite ' { sat } ' not recognized. Valid options are { valid_satellites } .\" raise ValueError ( msg ) super () . __init__ () self . data_root = data_root self . norm = norm self . reference_date = datetime ( * map ( int , reference_date . split ( \"-\" )), tzinfo = timezone . utc ) self . class_mapping = ( np . vectorize ( lambda x : class_mapping [ x ]) if class_mapping is not None else class_mapping ) self . target = target self . satellites = satellites self . transform = transform self . truncate_image = truncate_image self . pad_image = pad_image # loads patches metadata self . meta_patch = gpd . read_file ( os . path . join ( data_root , \"metadata.geojson\" )) self . meta_patch . index = self . meta_patch [ \"ID_PATCH\" ] . astype ( int ) self . meta_patch . sort_index ( inplace = True ) # stores table for each satalite date self . date_tables = { s : None for s in satellites } # date interval used in the PASTIS benchmark paper. date_interval_begin , date_interval_end = date_interval self . date_range = np . array ( range ( date_interval_begin , date_interval_end )) for s in satellites : # maps patches to its observation dates dates = self . meta_patch [ f \"dates- { s } \" ] date_table = pd . DataFrame ( index = self . meta_patch . index , columns = self . date_range , dtype = int ) for pid , date_seq in dates . items (): if type ( date_seq ) is str : date_seq = json . loads ( date_seq ) # noqa: PLW2901 # convert date to days since obersavation format d = pd . DataFrame () . from_dict ( date_seq , orient = \"index\" ) d = d [ 0 ] . apply ( lambda x : ( datetime ( int ( str ( x )[: 4 ]), int ( str ( x )[ 4 : 6 ]), int ( str ( x )[ 6 :]), tzinfo = timezone . utc ) - self . reference_date ) . days ) date_table . loc [ pid , d . values ] = 1 date_table = date_table . fillna ( 0 ) self . date_tables [ s ] = { index : np . array ( list ( d . values ())) for index , d in date_table . to_dict ( orient = \"index\" ) . items () } # selects patches correspondig to selected folds if folds is not None : self . meta_patch = pd . concat ( [ self . meta_patch [ self . meta_patch [ \"Fold\" ] == f ] for f in folds ] ) self . len = self . meta_patch . shape [ 0 ] self . id_patches = self . meta_patch . index # loads normalization values if norm : self . norm = {} for s in self . satellites : with open ( os . path . join ( data_root , f \"NORM_ { s } _patch.json\" ) ) as file : normvals = json . loads ( file . read ()) selected_folds = folds if folds is not None else range ( 1 , 6 ) means = [ normvals [ f \"Fold_ { f } \" ][ \"mean\" ] for f in selected_folds ] stds = [ normvals [ f \"Fold_ { f } \" ][ \"std\" ] for f in selected_folds ] self . norm [ s ] = np . stack ( means ) . mean ( axis = 0 ), np . stack ( stds ) . mean ( axis = 0 ) self . norm [ s ] = ( self . norm [ s ][ 0 ], self . norm [ s ][ 1 ], ) else : self . norm = None def __len__ ( self ): return self . len def get_dates ( self , id_patch , sat ): return self . date_range [ np . where ( self . date_tables [ sat ][ id_patch ] == 1 )[ 0 ]] def __getitem__ ( self , item ): id_patch = self . id_patches [ item ] output = {} satellites = {} for satellite in self . satellites : data = np . load ( os . path . join ( self . data_root , f \"DATA_ { satellite } \" , f \" { satellite } _ { id_patch } .npy\" , ) ) . astype ( np . float32 ) if self . norm is not None : data = data - self . norm [ satellite ][ 0 ][ None , :, None , None ] data = data / self . norm [ satellite ][ 1 ][ None , :, None , None ] if self . truncate_image and data . shape [ 0 ] > self . truncate_image : data = data [ - self . truncate_image :] if self . pad_image and data . shape [ 0 ] < self . pad_image : data = pad_numpy ( data , self . pad_image ) satellites [ satellite ] = data . astype ( np . float32 ) if self . target == \"semantic\" : target = np . load ( os . path . join ( self . data_root , \"ANNOTATIONS\" , f \"TARGET_ { id_patch } .npy\" ) ) target = target [ 0 ] . astype ( int ) if self . class_mapping is not None : target = self . class_mapping ( target ) elif self . target == \"instance\" : heatmap = np . load ( os . path . join ( self . data_root , \"INSTANCE_ANNOTATIONS\" , f \"HEATMAP_ { id_patch } .npy\" )) instance_ids = np . load ( os . path . join ( self . data_root , \"INSTANCE_ANNOTATIONS\" , f \"INSTANCES_ { id_patch } .npy\" )) zones_path = os . path . join ( self . data_root , \"INSTANCE_ANNOTATIONS\" , f \"ZONES_ { id_patch } .npy\" ) pixel_to_object_mapping = np . load ( zones_path ) pixel_semantic_annotation = np . load ( os . path . join ( self . data_root , \"ANNOTATIONS\" , f \"TARGET_ { id_patch } .npy\" )) if self . class_mapping is not None : pixel_semantic_annotation = self . class_mapping ( pixel_semantic_annotation [ 0 ]) else : pixel_semantic_annotation = pixel_semantic_annotation [ 0 ] size = np . zeros (( * instance_ids . shape , 2 )) object_semantic_annotation = np . zeros ( instance_ids . shape ) for instance_id in np . unique ( instance_ids ): if instance_id != 0 : h = ( instance_ids == instance_id ) . any ( axis =- 1 ) . sum () w = ( instance_ids == instance_id ) . any ( axis =- 2 ) . sum () size [ pixel_to_object_mapping == instance_id ] = ( h , w ) semantic_value = pixel_semantic_annotation [ instance_ids == instance_id ][ 0 ] object_semantic_annotation [ pixel_to_object_mapping == instance_id ] = semantic_value target = np . concatenate ( [ heatmap [:, :, None ], instance_ids [:, :, None ], pixel_to_object_mapping [:, :, None ], size , object_semantic_annotation [:, :, None ], pixel_semantic_annotation [:, :, None ], ], axis =- 1 ) . astype ( np . float32 ) dates = {} for satellite in self . satellites : date = np . array ( self . get_dates ( id_patch , satellite )) if self . truncate_image and len ( date ) > self . truncate_image : date = date [ - self . truncate_image :] if self . pad_image and len ( date ) < self . pad_image : date = pad_dates_numpy ( date , self . pad_image ) dates [ satellite ] = torch . from_numpy ( date ) output [ \"image\" ] = satellites [ \"S2\" ] . transpose ( 0 , 2 , 3 , 1 ) output [ \"mask\" ] = target if self . transform : output = self . transform ( ** output ) output . update ( satellites ) output [ \"dates\" ] = dates return output def plot ( self , sample , suptitle = None ): dates = sample [ \"dates\" ] target = sample [ \"target\" ] if \"S2\" not in sample : warnings . warn ( \"No RGB image.\" , stacklevel = 2 ) return None image_data = sample [ \"S2\" ] date_data = dates [ \"S2\" ] rgb_images = [] for i in range ( image_data . shape [ 0 ]): rgb_image = image_data [ i , : 3 , :, :] . numpy () . transpose ( 1 , 2 , 0 ) rgb_min = rgb_image . min ( axis = ( 0 , 1 ), keepdims = True ) rgb_max = rgb_image . max ( axis = ( 0 , 1 ), keepdims = True ) denom = rgb_max - rgb_min denom [ denom == 0 ] = 1 rgb_image = ( rgb_image - rgb_min ) / denom rgb_images . append ( np . clip ( rgb_image , 0 , 1 )) return self . _plot_sample ( rgb_images , date_data , target , suptitle = suptitle ) def _plot_sample ( self , images : list [ np . ndarray ], dates : torch . Tensor , target : torch . Tensor | None , suptitle : str | None = None ): num_images = len ( images ) cols = 5 rows = ( num_images + cols ) // cols fig , ax = plt . subplots ( rows , cols , figsize = ( 20 , 4 * rows )) for i , image in enumerate ( images ): ax [ i // cols , i % cols ] . imshow ( image ) ax [ i // cols , i % cols ] . set_title ( f \"Image { i + 1 } - Day { dates [ i ] . item () } \" ) ax [ i // cols , i % cols ] . axis ( \"off\" ) if target is not None : if rows * cols > num_images : target_ax = ax [( num_images ) // cols , ( num_images ) % cols ] else : fig . add_subplot ( rows + 1 , 1 , 1 ) target_ax = fig . gca () target_ax . imshow ( target . numpy (), cmap = \"tab20\" ) target_ax . set_title ( \"Target\" ) target_ax . axis ( \"off\" ) for k in range ( num_images + 1 , rows * cols ): ax [ k // cols , k % cols ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) plt . tight_layout () return fig __init__ ( data_root , norm = True , target = 'semantic' , folds = None , reference_date = '2018-09-01' , date_interval = ( - 200 , 600 ), class_mapping = None , transform = None , truncate_image = None , pad_image = None , satellites = [ 'S2' ]) # Parameters: data_root ( str ) \u2013 Path to the dataset. norm ( bool , default: True ) \u2013 If true, images are standardised using pre-computed channel-wise means and standard deviations. reference_date ( ( str , Format ) , default: '2018-09-01' ) \u2013 'YYYY-MM-DD'): Defines the reference date based on which all observation dates are expressed. Along with the image time series and the target tensor, this dataloader yields the sequence of observation dates (in terms of number of days since the reference date). This sequence of dates is used for instance for the positional encoding in attention based approaches. target ( str , default: 'semantic' ) \u2013 'semantic' or 'instance'. Defines which type of target is returned by the dataloader. * If 'semantic' the target tensor is a tensor containing the class of each pixel. * If 'instance' the target tensor is the concatenation of several signals, necessary to train the Parcel-as-Points module: - the centerness heatmap, - the instance ids, - the voronoi partitioning of the patch with regards to the parcels' centers, - the (height, width) size of each parcel, - the semantic label of each parcel, - the semantic label of each pixel. folds ( list , default: None ) \u2013 List of ints specifying which of the 5 official folds to load. By default (when None is specified), all folds are loaded. class_mapping ( dict , default: None ) \u2013 A dictionary to define a mapping between the default 18 class nomenclature and another class grouping. If not provided, the default class mapping is used. transform ( callable , default: None ) \u2013 A transform to apply to the loaded data (images, dates, and masks). By default, no transformation is applied. truncate_image ( int , default: None ) \u2013 Truncate the time dimension of the image to a specified number of timesteps. If None, no truncation is performed. pad_image ( int , default: None ) \u2013 Pad the time dimension of the image to a specified number of timesteps. If None, no padding is applied. satellites ( list , default: ['S2'] ) \u2013 Defines the satellites to use. If you are using PASTIS-R, you have access to Sentinel-2 imagery and Sentinel-1 observations in Ascending and Descending orbits, respectively S2, S1A, and S1D. For example, use satellites=['S2', 'S1A'] for Sentinel-2 + Sentinel-1 ascending time series, or satellites=['S2', 'S1A', 'S1D'] to retrieve all time series. If you are using PASTIS, only S2 observations are available. Source code in terratorch/datasets/pastis.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def __init__ ( self , data_root , norm = True , # noqa: FBT002 target = \"semantic\" , folds = None , reference_date = \"2018-09-01\" , date_interval = ( - 200 , 600 ), class_mapping = None , transform = None , truncate_image = None , pad_image = None , satellites = [ \"S2\" ], # noqa: B006 ): \"\"\" Args: data_root (str): Path to the dataset. norm (bool): If true, images are standardised using pre-computed channel-wise means and standard deviations. reference_date (str, Format : 'YYYY-MM-DD'): Defines the reference date based on which all observation dates are expressed. Along with the image time series and the target tensor, this dataloader yields the sequence of observation dates (in terms of number of days since the reference date). This sequence of dates is used for instance for the positional encoding in attention based approaches. target (str): 'semantic' or 'instance'. Defines which type of target is returned by the dataloader. * If 'semantic' the target tensor is a tensor containing the class of each pixel. * If 'instance' the target tensor is the concatenation of several signals, necessary to train the Parcel-as-Points module: - the centerness heatmap, - the instance ids, - the voronoi partitioning of the patch with regards to the parcels' centers, - the (height, width) size of each parcel, - the semantic label of each parcel, - the semantic label of each pixel. folds (list, optional): List of ints specifying which of the 5 official folds to load. By default (when None is specified), all folds are loaded. class_mapping (dict, optional): A dictionary to define a mapping between the default 18 class nomenclature and another class grouping. If not provided, the default class mapping is used. transform (callable, optional): A transform to apply to the loaded data (images, dates, and masks). By default, no transformation is applied. truncate_image (int, optional): Truncate the time dimension of the image to a specified number of timesteps. If None, no truncation is performed. pad_image (int, optional): Pad the time dimension of the image to a specified number of timesteps. If None, no padding is applied. satellites (list): Defines the satellites to use. If you are using PASTIS-R, you have access to Sentinel-2 imagery and Sentinel-1 observations in Ascending and Descending orbits, respectively S2, S1A, and S1D. For example, use satellites=['S2', 'S1A'] for Sentinel-2 + Sentinel-1 ascending time series, or satellites=['S2', 'S1A', 'S1D'] to retrieve all time series. If you are using PASTIS, only S2 observations are available. \"\"\" if target not in [ \"semantic\" , \"instance\" ]: msg = f \"Target ' { target } ' not recognized. Use 'semantic', or 'instance'.\" raise ValueError ( msg ) valid_satellites = { \"S2\" , \"S1A\" , \"S1D\" } for sat in satellites : if sat not in valid_satellites : msg = f \"Satellite ' { sat } ' not recognized. Valid options are { valid_satellites } .\" raise ValueError ( msg ) super () . __init__ () self . data_root = data_root self . norm = norm self . reference_date = datetime ( * map ( int , reference_date . split ( \"-\" )), tzinfo = timezone . utc ) self . class_mapping = ( np . vectorize ( lambda x : class_mapping [ x ]) if class_mapping is not None else class_mapping ) self . target = target self . satellites = satellites self . transform = transform self . truncate_image = truncate_image self . pad_image = pad_image # loads patches metadata self . meta_patch = gpd . read_file ( os . path . join ( data_root , \"metadata.geojson\" )) self . meta_patch . index = self . meta_patch [ \"ID_PATCH\" ] . astype ( int ) self . meta_patch . sort_index ( inplace = True ) # stores table for each satalite date self . date_tables = { s : None for s in satellites } # date interval used in the PASTIS benchmark paper. date_interval_begin , date_interval_end = date_interval self . date_range = np . array ( range ( date_interval_begin , date_interval_end )) for s in satellites : # maps patches to its observation dates dates = self . meta_patch [ f \"dates- { s } \" ] date_table = pd . DataFrame ( index = self . meta_patch . index , columns = self . date_range , dtype = int ) for pid , date_seq in dates . items (): if type ( date_seq ) is str : date_seq = json . loads ( date_seq ) # noqa: PLW2901 # convert date to days since obersavation format d = pd . DataFrame () . from_dict ( date_seq , orient = \"index\" ) d = d [ 0 ] . apply ( lambda x : ( datetime ( int ( str ( x )[: 4 ]), int ( str ( x )[ 4 : 6 ]), int ( str ( x )[ 6 :]), tzinfo = timezone . utc ) - self . reference_date ) . days ) date_table . loc [ pid , d . values ] = 1 date_table = date_table . fillna ( 0 ) self . date_tables [ s ] = { index : np . array ( list ( d . values ())) for index , d in date_table . to_dict ( orient = \"index\" ) . items () } # selects patches correspondig to selected folds if folds is not None : self . meta_patch = pd . concat ( [ self . meta_patch [ self . meta_patch [ \"Fold\" ] == f ] for f in folds ] ) self . len = self . meta_patch . shape [ 0 ] self . id_patches = self . meta_patch . index # loads normalization values if norm : self . norm = {} for s in self . satellites : with open ( os . path . join ( data_root , f \"NORM_ { s } _patch.json\" ) ) as file : normvals = json . loads ( file . read ()) selected_folds = folds if folds is not None else range ( 1 , 6 ) means = [ normvals [ f \"Fold_ { f } \" ][ \"mean\" ] for f in selected_folds ] stds = [ normvals [ f \"Fold_ { f } \" ][ \"std\" ] for f in selected_folds ] self . norm [ s ] = np . stack ( means ) . mean ( axis = 0 ), np . stack ( stds ) . mean ( axis = 0 ) self . norm [ s ] = ( self . norm [ s ][ 0 ], self . norm [ s ][ 1 ], ) else : self . norm = None terratorch.datasets.sen1floods11 # Sen1Floods11NonGeo # Bases: NonGeoDataset NonGeo dataset implementation for sen1floods11 . Source code in terratorch/datasets/sen1floods11.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 class Sen1Floods11NonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [sen1floods11](https://github.com/cloudtostreet/Sen1Floods11).\"\"\" all_band_names = ( \"COASTAL_AEROSOL\" , \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"NIR_NARROW\" , \"WATER_VAPOR\" , \"CIRRUS\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } num_classes = 2 splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"v1.1/data/flood_events/HandLabeled/S2Hand\" label_dir = \"v1.1/data/flood_events/HandLabeled/LabelHand\" split_dir = \"v1.1/splits/flood_handlabeled\" metadata_file = \"v1.1/Sen1Floods11_Metadata.geojson\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , constant_scale : float = 0.0001 , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , # noqa: FBT001, FBT002 ) -> None : \"\"\"Constructor Args: data_root (str): Path to the data root directory. split (str): one of 'train', 'val' or 'test'. bands (list[str]): Bands that should be output by the dataset. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). Defaults to None, which applies ToTensorV2(). constant_scale (float): Factor to multiply image values by. Defaults to 0.0001. no_data_replace (float | None): Replace nan values in input images with this value. If None, does no replacement. Defaults to 0. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1. use_metadata (bool): whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { self . splits } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . constant_scale = constant_scale self . data_root = Path ( data_root ) data_dir = self . data_root / self . data_dir label_dir = self . data_root / self . label_dir self . image_files = sorted ( glob . glob ( os . path . join ( data_dir , \"*_S2Hand.tif\" ))) self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( label_dir , \"*_LabelHand.tif\" ))) split_file = self . data_root / self . split_dir / f \"flood_ { split_name } _data.txt\" with open ( split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . segmentation_mask_files = filter_valid_files ( self . segmentation_mask_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata self . metadata = None if self . use_metadata : self . metadata = geopandas . read_file ( self . data_root / self . metadata_file ) # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def _get_date ( self , index : int ) -> torch . Tensor : file_name = self . image_files [ index ] location = os . path . basename ( file_name ) . split ( \"_\" )[ 0 ] if self . metadata [ self . metadata [ \"location\" ] == location ] . shape [ 0 ] != 1 : date = pd . to_datetime ( \"13-10-1998\" , dayfirst = True ) else : date = pd . to_datetime ( self . metadata [ self . metadata [ \"location\" ] == location ][ \"s2_date\" ] . item ()) return torch . tensor ([[ date . year , date . dayofyear - 1 ]], dtype = torch . float32 ) # (n_timesteps, coords) def _get_coords ( self , image : DataArray ) -> torch . Tensor : center_lat = image . y [ image . y . shape [ 0 ] // 2 ] center_lon = image . x [ image . x . shape [ 0 ] // 2 ] lat_lon = np . asarray ([ center_lat , center_lon ]) return torch . tensor ( lat_lon , dtype = torch . float32 ) def __getitem__ ( self , index : int ) -> dict [ str , Any ]: image = self . _load_file ( self . image_files [ index ], nan_replace = self . no_data_replace ) location_coords , temporal_coords = None , None if self . use_metadata : location_coords = self . _get_coords ( image ) temporal_coords = self . _get_date ( index ) # to channels last image = image . to_numpy () image = np . moveaxis ( image , 0 , - 1 ) # filter bands image = image [ ... , self . band_indices ] output = { \"image\" : image . astype ( np . float32 ) * self . constant_scale , \"mask\" : self . _load_file ( self . segmentation_mask_files [ index ], nan_replace = self . no_label_replace ) . to_numpy ()[ 0 ], } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () if self . use_metadata : output [ \"location_coords\" ] = location_coords output [ \"temporal_coords\" ] = temporal_coords return output def _load_file ( self , path : Path , nan_replace : int | float | None = None ) -> DataArray : data = rioxarray . open_rasterio ( path , masked = True ) if nan_replace is not None : data = data . fillna ( nan_replace ) return data def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample \"\"\" num_images = 4 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) # RGB -> channels-last image = sample [ \"image\" ][ rgb_indices , ... ] . permute ( 1 , 2 , 0 ) . numpy () mask = sample [ \"mask\" ] . numpy () image = clip_image ( image ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] num_images += 1 else : prediction = None fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 5 ), layout = \"compressed\" ) ax [ 0 ] . axis ( \"off\" ) norm = mpl . colors . Normalize ( vmin = 0 , vmax = self . num_classes - 1 ) ax [ 1 ] . axis ( \"off\" ) ax [ 1 ] . title . set_text ( \"Image\" ) ax [ 1 ] . imshow ( image ) ax [ 2 ] . axis ( \"off\" ) ax [ 2 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ 2 ] . imshow ( mask , cmap = \"jet\" , norm = norm ) ax [ 3 ] . axis ( \"off\" ) ax [ 3 ] . title . set_text ( \"GT Mask on Image\" ) ax [ 3 ] . imshow ( image ) ax [ 3 ] . imshow ( mask , cmap = \"jet\" , alpha = 0.3 , norm = norm ) if \"prediction\" in sample : ax [ 4 ] . title . set_text ( \"Predicted Mask\" ) ax [ 4 ] . imshow ( prediction , cmap = \"jet\" , norm = norm ) cmap = plt . get_cmap ( \"jet\" ) legend_data = [[ i , cmap ( norm ( i )), str ( i )] for i in range ( self . num_classes )] handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = tuple ( v for v in c )) for k , c , n in legend_data ] labels = [ n for k , c , n in legend_data ] ax [ 0 ] . legend ( handles , labels , loc = \"center\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig __init__ ( data_root , split = 'train' , bands = BAND_SETS [ 'all' ], transform = None , constant_scale = 0.0001 , no_data_replace = 0 , no_label_replace =- 1 , use_metadata = False ) # Constructor Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 one of 'train', 'val' or 'test'. bands ( list [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands that should be output by the dataset. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). Defaults to None, which applies ToTensorV2(). constant_scale ( float , default: 0.0001 ) \u2013 Factor to multiply image values by. Defaults to 0.0001. no_data_replace ( float | None , default: 0 ) \u2013 Replace nan values in input images with this value. If None, does no replacement. Defaults to 0. no_label_replace ( int | None , default: -1 ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to -1. use_metadata ( bool , default: False ) \u2013 whether to return metadata info (time and location). Source code in terratorch/datasets/sen1floods11.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , constant_scale : float = 0.0001 , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , # noqa: FBT001, FBT002 ) -> None : \"\"\"Constructor Args: data_root (str): Path to the data root directory. split (str): one of 'train', 'val' or 'test'. bands (list[str]): Bands that should be output by the dataset. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). Defaults to None, which applies ToTensorV2(). constant_scale (float): Factor to multiply image values by. Defaults to 0.0001. no_data_replace (float | None): Replace nan values in input images with this value. If None, does no replacement. Defaults to 0. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1. use_metadata (bool): whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { self . splits } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . constant_scale = constant_scale self . data_root = Path ( data_root ) data_dir = self . data_root / self . data_dir label_dir = self . data_root / self . label_dir self . image_files = sorted ( glob . glob ( os . path . join ( data_dir , \"*_S2Hand.tif\" ))) self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( label_dir , \"*_LabelHand.tif\" ))) split_file = self . data_root / self . split_dir / f \"flood_ { split_name } _data.txt\" with open ( split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . segmentation_mask_files = filter_valid_files ( self . segmentation_mask_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata self . metadata = None if self . use_metadata : self . metadata = geopandas . read_file ( self . data_root / self . metadata_file ) # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform plot ( sample , suptitle = None ) # Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 a sample returned by :meth: __getitem__ suptitle ( str | None , default: None ) \u2013 optional string to use as a suptitle Returns: Figure \u2013 a matplotlib Figure with the rendered sample Source code in terratorch/datasets/sen1floods11.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample \"\"\" num_images = 4 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) # RGB -> channels-last image = sample [ \"image\" ][ rgb_indices , ... ] . permute ( 1 , 2 , 0 ) . numpy () mask = sample [ \"mask\" ] . numpy () image = clip_image ( image ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] num_images += 1 else : prediction = None fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 5 ), layout = \"compressed\" ) ax [ 0 ] . axis ( \"off\" ) norm = mpl . colors . Normalize ( vmin = 0 , vmax = self . num_classes - 1 ) ax [ 1 ] . axis ( \"off\" ) ax [ 1 ] . title . set_text ( \"Image\" ) ax [ 1 ] . imshow ( image ) ax [ 2 ] . axis ( \"off\" ) ax [ 2 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ 2 ] . imshow ( mask , cmap = \"jet\" , norm = norm ) ax [ 3 ] . axis ( \"off\" ) ax [ 3 ] . title . set_text ( \"GT Mask on Image\" ) ax [ 3 ] . imshow ( image ) ax [ 3 ] . imshow ( mask , cmap = \"jet\" , alpha = 0.3 , norm = norm ) if \"prediction\" in sample : ax [ 4 ] . title . set_text ( \"Predicted Mask\" ) ax [ 4 ] . imshow ( prediction , cmap = \"jet\" , norm = norm ) cmap = plt . get_cmap ( \"jet\" ) legend_data = [[ i , cmap ( norm ( i )), str ( i )] for i in range ( self . num_classes )] handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = tuple ( v for v in c )) for k , c , n in legend_data ] labels = [ n for k , c , n in legend_data ] ax [ 0 ] . legend ( handles , labels , loc = \"center\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig terratorch.datasets.sen4agrinet # Sen4AgriNet # Bases: NonGeoDataset Source code in terratorch/datasets/sen4agrinet.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 class Sen4AgriNet ( NonGeoDataset ): def __init__ ( self , data_root : str , bands : list [ str ] | None = None , scenario : str = \"random\" , split : str = \"train\" , transform : A . Compose = None , truncate_image : int | None = 4 , pad_image : int | None = 4 , spatial_interpolate_and_stack_temporally : bool = True , # noqa: FBT001, FBT002 seed : int = 42 , ): \"\"\" Pytorch Dataset class to load samples from the [Sen4AgriNet](https://github.com/Orion-AI-Lab/S4A) dataset, supporting multiple scenarios for splitting the data. Args: data_root (str): Root directory of the dataset. bands (list of str, optional): List of band names to load. Defaults to all available bands. scenario (str): Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020). split (str): Specifies the dataset split. Options are 'train', 'val', or 'test'. transform (albumentations.Compose, optional): Albumentations transformations to apply to the data. truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image. If None, no truncation is applied. Default is 4. pad_image (int, optional): Number of timesteps to pad the time dimension of the image. If None, no padding is applied. Default is 4. spatial_interpolate_and_stack_temporally (bool): Whether to interpolate bands and concatenate them over time seed (int): Random seed used for data splitting. \"\"\" self . data_root = Path ( data_root ) / \"data\" self . transform = transform if transform else lambda ** batch : to_tensor ( batch ) self . scenario = scenario self . seed = seed self . truncate_image = truncate_image self . pad_image = pad_image self . spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally if bands is None : bands = [ \"B01\" , \"B02\" , \"B03\" , \"B04\" , \"B05\" , \"B06\" , \"B07\" , \"B08\" , \"B09\" , \"B10\" , \"B11\" , \"B12\" , \"B8A\" ] self . bands = bands self . image_files = list ( self . data_root . glob ( \"**/*.nc\" )) self . train_files , self . val_files , self . test_files = self . split_data () if split == \"train\" : self . image_files = self . train_files elif split == \"val\" : self . image_files = self . val_files elif split == \"test\" : self . image_files = self . test_files def __len__ ( self ): return len ( self . image_files ) def split_data ( self ): random . seed ( self . seed ) if self . scenario == \"random\" : random . shuffle ( self . image_files ) total_files = len ( self . image_files ) train_split = int ( 0.6 * total_files ) val_split = int ( 0.8 * total_files ) train_files = self . image_files [: train_split ] val_files = self . image_files [ train_split : val_split ] test_files = self . image_files [ val_split :] elif self . scenario == \"spatial\" : catalonia_files = [ f for f in self . image_files if any ( tile in f . stem for tile in CAT_TILES )] france_files = [ f for f in self . image_files if any ( tile in f . stem for tile in FR_TILES )] val_split_cat = int ( 0.2 * len ( catalonia_files )) train_files = catalonia_files [ val_split_cat :] val_files = catalonia_files [: val_split_cat ] test_files = france_files elif self . scenario == \"spatio-temporal\" : france_files = [ f for f in self . image_files if any ( tile in f . stem for tile in FR_TILES )] catalonia_files = [ f for f in self . image_files if any ( tile in f . stem for tile in CAT_TILES )] france_2019_files = [ f for f in france_files if \"2019\" in f . stem ] catalonia_2020_files = [ f for f in catalonia_files if \"2020\" in f . stem ] val_split_france_2019 = int ( 0.2 * len ( france_2019_files )) train_files = france_2019_files [ val_split_france_2019 :] val_files = france_2019_files [: val_split_france_2019 ] test_files = catalonia_2020_files return train_files , val_files , test_files def __getitem__ ( self , index : int ): patch_file = self . image_files [ index ] with h5py . File ( patch_file , \"r\" ) as patch_data : output = {} images_over_time = [] for band in self . bands : band_group = patch_data [ band ] band_data = band_group [ f \" { band } \" ][:] time_vector = band_group [ \"time\" ][:] sorted_indices = np . argsort ( time_vector ) band_data = band_data [ sorted_indices ] . astype ( np . float32 ) if self . truncate_image : band_data = band_data [ - self . truncate_image :] if self . pad_image : band_data = pad_numpy ( band_data , self . pad_image ) if self . spatial_interpolate_and_stack_temporally : band_data = torch . from_numpy ( band_data ) band_data = band_data . clone () . detach () interpolated = F . interpolate ( band_data . unsqueeze ( 0 ), size = MAX_TEMPORAL_IMAGE_SIZE , mode = \"bilinear\" , align_corners = False ) . squeeze ( 0 ) images_over_time . append ( interpolated ) else : output [ band ] = band_data if self . spatial_interpolate_and_stack_temporally : images = torch . stack ( images_over_time , dim = 0 ) . numpy () output [ \"image\" ] = images labels = patch_data [ \"labels\" ][ \"labels\" ][:] . astype ( int ) parcels = patch_data [ \"parcels\" ][ \"parcels\" ][:] . astype ( int ) output [ \"mask\" ] = labels image_shape = output [ \"image\" ] . shape [ - 2 :] mask_shape = output [ \"mask\" ] . shape if image_shape != mask_shape : diff_h = mask_shape [ 0 ] - image_shape [ 0 ] diff_w = mask_shape [ 1 ] - image_shape [ 1 ] output [ \"image\" ] = np . pad ( output [ \"image\" ], [( 0 , 0 ), ( 0 , 0 ), ( diff_h // 2 , diff_h - diff_h // 2 ), ( diff_w // 2 , diff_w - diff_w // 2 )], mode = \"constant\" , constant_values = 0 ) linear_encoder = { val : i + 1 for i , val in enumerate ( sorted ( SELECTED_CLASSES ))} linear_encoder [ 0 ] = 0 output [ \"image\" ] = output [ \"image\" ] . transpose ( 0 , 2 , 3 , 1 ) output [ \"mask\" ] = self . map_mask_to_discrete_classes ( output [ \"mask\" ], linear_encoder ) if self . transform : output = self . transform ( ** output ) output [ \"parcels\" ] = parcels return output def plot ( self , sample , suptitle = None ): rgb_bands = [ \"B04\" , \"B03\" , \"B02\" ] if not all ( band in sample for band in rgb_bands ): warnings . warn ( \"No RGB image.\" ) # noqa: B028 return None rgb_images = [] for t in range ( sample [ \"B04\" ] . shape [ 0 ]): rgb_image = torch . stack ([ sample [ band ][ t ] for band in rgb_bands ]) # Normalization rgb_min = rgb_image . min ( dim = 1 , keepdim = True ) . values . min ( dim = 2 , keepdim = True ) . values rgb_max = rgb_image . max ( dim = 1 , keepdim = True ) . values . max ( dim = 2 , keepdim = True ) . values denom = rgb_max - rgb_min denom [ denom == 0 ] = 1 rgb_image = ( rgb_image - rgb_min ) / denom rgb_image = rgb_image . permute ( 1 , 2 , 0 ) . numpy () rgb_images . append ( np . clip ( rgb_image , 0 , 1 )) dates = torch . arange ( sample [ \"B04\" ] . shape [ 0 ]) return self . _plot_sample ( rgb_images , dates , sample . get ( \"labels\" ), suptitle = suptitle ) def _plot_sample ( self , images , dates , labels = None , suptitle = None ): num_images = len ( images ) cols = 5 rows = ( num_images + cols - 1 ) // cols fig , ax = plt . subplots ( rows , cols , figsize = ( 20 , 4 * rows )) for i , image in enumerate ( images ): ax [ i // cols , i % cols ] . imshow ( image ) ax [ i // cols , i % cols ] . set_title ( f \"T { i + 1 } - Day { dates [ i ] . item () } \" ) ax [ i // cols , i % cols ] . axis ( \"off\" ) if labels is not None : if rows * cols > num_images : target_ax = ax [( num_images ) // cols , ( num_images ) % cols ] else : fig . add_subplot ( rows + 1 , 1 , 1 ) target_ax = fig . gca () target_ax . imshow ( labels . numpy (), cmap = \"tab20\" ) target_ax . set_title ( \"Labels\" ) target_ax . axis ( \"off\" ) for k in range ( num_images , rows * cols ): ax [ k // cols , k % cols ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) plt . tight_layout () plt . show () def map_mask_to_discrete_classes ( self , mask , encoder ): map_func = np . vectorize ( lambda x : encoder . get ( x , 0 )) return map_func ( mask ) __init__ ( data_root , bands = None , scenario = 'random' , split = 'train' , transform = None , truncate_image = 4 , pad_image = 4 , spatial_interpolate_and_stack_temporally = True , seed = 42 ) # Pytorch Dataset class to load samples from the Sen4AgriNet dataset, supporting multiple scenarios for splitting the data. Parameters: data_root ( str ) \u2013 Root directory of the dataset. bands ( list of str , default: None ) \u2013 List of band names to load. Defaults to all available bands. scenario ( str , default: 'random' ) \u2013 Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020). split ( str , default: 'train' ) \u2013 Specifies the dataset split. Options are 'train', 'val', or 'test'. transform ( Compose , default: None ) \u2013 Albumentations transformations to apply to the data. truncate_image ( int , default: 4 ) \u2013 Number of timesteps to truncate the time dimension of the image. If None, no truncation is applied. Default is 4. pad_image ( int , default: 4 ) \u2013 Number of timesteps to pad the time dimension of the image. If None, no padding is applied. Default is 4. spatial_interpolate_and_stack_temporally ( bool , default: True ) \u2013 Whether to interpolate bands and concatenate them over time seed ( int , default: 42 ) \u2013 Random seed used for data splitting. Source code in terratorch/datasets/sen4agrinet.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , data_root : str , bands : list [ str ] | None = None , scenario : str = \"random\" , split : str = \"train\" , transform : A . Compose = None , truncate_image : int | None = 4 , pad_image : int | None = 4 , spatial_interpolate_and_stack_temporally : bool = True , # noqa: FBT001, FBT002 seed : int = 42 , ): \"\"\" Pytorch Dataset class to load samples from the [Sen4AgriNet](https://github.com/Orion-AI-Lab/S4A) dataset, supporting multiple scenarios for splitting the data. Args: data_root (str): Root directory of the dataset. bands (list of str, optional): List of band names to load. Defaults to all available bands. scenario (str): Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020). split (str): Specifies the dataset split. Options are 'train', 'val', or 'test'. transform (albumentations.Compose, optional): Albumentations transformations to apply to the data. truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image. If None, no truncation is applied. Default is 4. pad_image (int, optional): Number of timesteps to pad the time dimension of the image. If None, no padding is applied. Default is 4. spatial_interpolate_and_stack_temporally (bool): Whether to interpolate bands and concatenate them over time seed (int): Random seed used for data splitting. \"\"\" self . data_root = Path ( data_root ) / \"data\" self . transform = transform if transform else lambda ** batch : to_tensor ( batch ) self . scenario = scenario self . seed = seed self . truncate_image = truncate_image self . pad_image = pad_image self . spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally if bands is None : bands = [ \"B01\" , \"B02\" , \"B03\" , \"B04\" , \"B05\" , \"B06\" , \"B07\" , \"B08\" , \"B09\" , \"B10\" , \"B11\" , \"B12\" , \"B8A\" ] self . bands = bands self . image_files = list ( self . data_root . glob ( \"**/*.nc\" )) self . train_files , self . val_files , self . test_files = self . split_data () if split == \"train\" : self . image_files = self . train_files elif split == \"val\" : self . image_files = self . val_files elif split == \"test\" : self . image_files = self . test_files terratorch.datasets.sen4map # Sen4MapDatasetMonthlyComposites # Bases: Dataset Sen4Map Dataset for Monthly Composites. Dataset intended for land-cover and crop classification tasks based on monthly composites derived from multi-temporal satellite data stored in HDF5 files. Dataset Format: HDF5 files containing multi-temporal acquisitions with spectral bands (e.g., B2, B3, \u2026, B12) Composite images computed as the median across available acquisitions for each month. Classification labels provided via HDF5 attributes (e.g., 'lc1') with mappings defined for: Land-cover: using land_cover_classification_map Crops: using crop_classification_map Dataset Features: Supports two classification tasks: \"land-cover\" (default) and \"crops\". Pre-processing options include center cropping, reverse tiling, and resizing. Option to save the keys HDF5 for later filtering. Input channel selection via a mapping between available bands and input bands. Source code in terratorch/datasets/sen4map.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 class Sen4MapDatasetMonthlyComposites ( Dataset ): \"\"\"[Sen4Map](https://gitlab.jsc.fz-juelich.de/sdlrs/sen4map-benchmark-dataset) Dataset for Monthly Composites. Dataset intended for land-cover and crop classification tasks based on monthly composites derived from multi-temporal satellite data stored in HDF5 files. Dataset Format: * HDF5 files containing multi-temporal acquisitions with spectral bands (e.g., B2, B3, \u2026, B12) * Composite images computed as the median across available acquisitions for each month. * Classification labels provided via HDF5 attributes (e.g., 'lc1') with mappings defined for: - Land-cover: using `land_cover_classification_map` - Crops: using `crop_classification_map` Dataset Features: * Supports two classification tasks: \"land-cover\" (default) and \"crops\". * Pre-processing options include center cropping, reverse tiling, and resizing. * Option to save the keys HDF5 for later filtering. * Input channel selection via a mapping between available bands and input bands. \"\"\" land_cover_classification_map = { 'A10' : 0 , 'A11' : 0 , 'A12' : 0 , 'A13' : 0 , 'A20' : 0 , 'A21' : 0 , 'A30' : 0 , 'A22' : 1 , 'F10' : 1 , 'F20' : 1 , 'F30' : 1 , 'F40' : 1 , 'E10' : 2 , 'E20' : 2 , 'E30' : 2 , 'B50' : 2 , 'B51' : 2 , 'B52' : 2 , 'B53' : 2 , 'B54' : 2 , 'B55' : 2 , 'B10' : 3 , 'B11' : 3 , 'B12' : 3 , 'B13' : 3 , 'B14' : 3 , 'B15' : 3 , 'B16' : 3 , 'B17' : 3 , 'B18' : 3 , 'B19' : 3 , 'B10' : 3 , 'B20' : 3 , 'B21' : 3 , 'B22' : 3 , 'B23' : 3 , 'B30' : 3 , 'B31' : 3 , 'B32' : 3 , 'B33' : 3 , 'B34' : 3 , 'B35' : 3 , 'B30' : 3 , 'B36' : 3 , 'B37' : 3 , 'B40' : 3 , 'B41' : 3 , 'B42' : 3 , 'B43' : 3 , 'B44' : 3 , 'B45' : 3 , 'B70' : 3 , 'B71' : 3 , 'B72' : 3 , 'B73' : 3 , 'B74' : 3 , 'B75' : 3 , 'B76' : 3 , 'B77' : 3 , 'B80' : 3 , 'B81' : 3 , 'B82' : 3 , 'B83' : 3 , 'B84' : 3 , 'BX1' : 3 , 'BX2' : 3 , 'C10' : 4 , 'C20' : 5 , 'C21' : 5 , 'C22' : 5 , 'C23' : 5 , 'C30' : 5 , 'C31' : 5 , 'C32' : 5 , 'C33' : 5 , 'CXX1' : 5 , 'CXX2' : 5 , 'CXX3' : 5 , 'CXX4' : 5 , 'CXX5' : 5 , 'CXX5' : 5 , 'CXX6' : 5 , 'CXX7' : 5 , 'CXX8' : 5 , 'CXX9' : 5 , 'CXXA' : 5 , 'CXXB' : 5 , 'CXXC' : 5 , 'CXXD' : 5 , 'CXXE' : 5 , 'D10' : 6 , 'D20' : 6 , 'D10' : 6 , 'G10' : 7 , 'G11' : 7 , 'G12' : 7 , 'G20' : 7 , 'G21' : 7 , 'G22' : 7 , 'G30' : 7 , 'G40' : 7 , 'G50' : 7 , 'H10' : 8 , 'H11' : 8 , 'H12' : 8 , 'H11' : 8 , 'H20' : 8 , 'H21' : 8 , 'H22' : 8 , 'H23' : 8 , '' : 9 } # This dictionary maps the LUCAS classes to crop classes. crop_classification_map = { \"B11\" : 0 , \"B12\" : 0 , \"B13\" : 0 , \"B14\" : 0 , \"B15\" : 0 , \"B16\" : 0 , \"B17\" : 0 , \"B18\" : 0 , \"B19\" : 0 , # Cereals \"B21\" : 1 , \"B22\" : 1 , \"B23\" : 1 , # Root Crops \"B31\" : 2 , \"B32\" : 2 , \"B33\" : 2 , \"B34\" : 2 , \"B35\" : 2 , \"B36\" : 2 , \"B37\" : 2 , # Nonpermanent Industrial Crops \"B41\" : 3 , \"B42\" : 3 , \"B43\" : 3 , \"B44\" : 3 , \"B45\" : 3 , # Dry Pulses, Vegetables and Flowers \"B51\" : 4 , \"B52\" : 4 , \"B53\" : 4 , \"B54\" : 4 , # Fodder Crops \"F10\" : 5 , \"F20\" : 5 , \"F30\" : 5 , \"F40\" : 5 , # Bareland \"B71\" : 6 , \"B72\" : 6 , \"B73\" : 6 , \"B74\" : 6 , \"B75\" : 6 , \"B76\" : 6 , \"B77\" : 6 , \"B81\" : 6 , \"B82\" : 6 , \"B83\" : 6 , \"B84\" : 6 , \"C10\" : 6 , \"C21\" : 6 , \"C22\" : 6 , \"C23\" : 6 , \"C31\" : 6 , \"C32\" : 6 , \"C33\" : 6 , \"D10\" : 6 , \"D20\" : 6 , # Woodland and Shrubland \"B55\" : 7 , \"E10\" : 7 , \"E20\" : 7 , \"E30\" : 7 , # Grassland } def __init__ ( self , h5py_file_object : h5py . File , h5data_keys = None , crop_size : None | int = None , dataset_bands : list [ HLSBands | int ] | None = None , input_bands : list [ HLSBands | int ] | None = None , resize = False , resize_to = [ 224 , 224 ], resize_interpolation = InterpolationMode . BILINEAR , resize_antialiasing = True , reverse_tile = False , reverse_tile_size = 3 , save_keys_path = None , classification_map = \"land-cover\" ): \"\"\"Initialize a new instance of Sen4MapDatasetMonthlyComposites. This dataset loads data from an HDF5 file object containing multi-temporal satellite data and computes monthly composite images by aggregating acquisitions (via median). Args: h5py_file_object: An open h5py.File object containing the dataset. h5data_keys: Optional list of keys to select a subset of data samples from the HDF5 file. If None, all keys are used. crop_size: Optional integer specifying the square crop size for the output image. dataset_bands: Optional list of bands available in the dataset. input_bands: Optional list of bands to be used as input channels. Must be provided along with `dataset_bands`. resize: Boolean flag indicating whether the image should be resized. Default is False. resize_to: Target dimensions [height, width] for resizing. Default is [224, 224]. resize_interpolation: Interpolation mode used for resizing. Default is InterpolationMode.BILINEAR. resize_antialiasing: Boolean flag to apply antialiasing during resizing. Default is True. reverse_tile: Boolean flag indicating whether to apply reverse tiling to the image. Default is False. reverse_tile_size: Kernel size for the reverse tiling operation. Must be an odd number >= 3. Default is 3. save_keys_path: Optional file path to save the list of dataset keys. classification_map: String specifying the classification mapping to use (\"land-cover\" or \"crops\"). Default is \"land-cover\". Raises: ValueError: If `input_bands` is provided without specifying `dataset_bands`. ValueError: If an invalid `classification_map` is provided. \"\"\" self . h5data = h5py_file_object if h5data_keys is None : if classification_map == \"crops\" : print ( f \"Crop classification task chosen but no keys supplied. Will fail unless dataset hdf5 files have been filtered. Either filter dataset files or create a filtered set of keys.\" ) self . h5data_keys = list ( self . h5data . keys ()) if save_keys_path is not None : with open ( save_keys_path , \"wb\" ) as file : pickle . dump ( self . h5data_keys , file ) else : self . h5data_keys = h5data_keys self . crop_size = crop_size if input_bands and not dataset_bands : raise ValueError ( f \"input_bands was provided without specifying the dataset_bands\" ) # self.dataset_bands = dataset_bands # self.input_bands = input_bands if input_bands and dataset_bands : self . input_channels = [ dataset_bands . index ( band_ind ) for band_ind in input_bands if band_ind in dataset_bands ] else : self . input_channels = None classification_maps = { \"land-cover\" : Sen4MapDatasetMonthlyComposites . land_cover_classification_map , \"crops\" : Sen4MapDatasetMonthlyComposites . crop_classification_map } if classification_map not in classification_maps . keys (): raise ValueError ( f \"Provided classification_map of: { classification_map } , is not from the list of valid ones: { classification_maps } \" ) self . classification_map = classification_maps [ classification_map ] self . resize = resize self . resize_to = resize_to self . resize_interpolation = resize_interpolation self . resize_antialiasing = resize_antialiasing self . reverse_tile = reverse_tile self . reverse_tile_size = reverse_tile_size def __getitem__ ( self , index ): # we can call dataset with an index, eg. dataset[0] im = self . h5data [ self . h5data_keys [ index ]] Image , Label = self . get_data ( im ) Image = self . min_max_normalize ( Image , [ 67.0 , 122.0 , 93.27 , 158.5 , 160.77 , 174.27 , 162.27 , 149.0 , 84.5 , 66.27 ], [ 2089.0 , 2598.45 , 3214.5 , 3620.45 , 4033.61 , 4613.0 , 4825.45 , 4945.72 , 5140.84 , 4414.45 ]) Image = Image . clip ( 0 , 1 ) Label = torch . LongTensor ( Label ) if self . input_channels : Image = Image [ self . input_channels , ... ] return { \"image\" : Image , \"label\" : Label } def __len__ ( self ): return len ( self . h5data_keys ) def get_data ( self , im ): mask = im [ 'SCL' ] < 9 B2 = np . where ( mask == 1 , im [ 'B2' ], 0 ) B3 = np . where ( mask == 1 , im [ 'B3' ], 0 ) B4 = np . where ( mask == 1 , im [ 'B4' ], 0 ) B5 = np . where ( mask == 1 , im [ 'B5' ], 0 ) B6 = np . where ( mask == 1 , im [ 'B6' ], 0 ) B7 = np . where ( mask == 1 , im [ 'B7' ], 0 ) B8 = np . where ( mask == 1 , im [ 'B8' ], 0 ) B8A = np . where ( mask == 1 , im [ 'B8A' ], 0 ) B11 = np . where ( mask == 1 , im [ 'B11' ], 0 ) B12 = np . where ( mask == 1 , im [ 'B12' ], 0 ) Image = np . stack (( B2 , B3 , B4 , B5 , B6 , B7 , B8 , B8A , B11 , B12 ), axis = 0 , dtype = \"float32\" ) Image = np . moveaxis ( Image , [ 0 ],[ 1 ]) Image = torch . from_numpy ( Image ) # Composites: n1 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201801' in s ] n2 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201802' in s ] n3 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201803' in s ] n4 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201804' in s ] n5 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201805' in s ] n6 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201806' in s ] n7 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201807' in s ] n8 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201808' in s ] n9 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201809' in s ] n10 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201810' in s ] n11 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201811' in s ] n12 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201812' in s ] Jan = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n1 else n1 Feb = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n2 else n2 Mar = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n3 else n3 Apr = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n4 else n4 May = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n5 else n5 Jun = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n6 else n6 Jul = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n7 else n7 Aug = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n8 else n8 Sep = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n9 else n9 Oct = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n10 else n10 Nov = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n11 else n11 Dec = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n12 else n12 month_indices = [ Jan , Feb , Mar , Apr , May , Jun , Jul , Aug , Sep , Oct , Nov , Dec ] month_medians = [ torch . stack ([ Image [ month_indices [ i ][ j ]] for j in range ( len ( month_indices [ i ]))]) . median ( dim = 0 ) . values for i in range ( 12 )] Image = torch . stack ( month_medians , dim = 0 ) Image = torch . moveaxis ( Image , 0 , 1 ) if self . crop_size : Image = self . crop_center ( Image , self . crop_size , self . crop_size ) if self . reverse_tile : Image = self . reverse_tiling_pytorch ( Image , kernel_size = self . reverse_tile_size ) if self . resize : Image = resize ( Image , size = self . resize_to , interpolation = self . resize_interpolation , antialias = self . resize_antialiasing ) Label = im . attrs [ 'lc1' ] Label = self . classification_map [ Label ] Label = np . array ( Label ) Label = Label . astype ( 'float32' ) return Image , Label def crop_center ( self , img_b : torch . Tensor , cropx , cropy ) -> torch . Tensor : c , t , y , x = img_b . shape startx = x // 2 - ( cropx // 2 ) starty = y // 2 - ( cropy // 2 ) return img_b [ 0 : c , 0 : t , starty : starty + cropy , startx : startx + cropx ] def reverse_tiling_pytorch ( self , img_tensor : torch . Tensor , kernel_size : int = 3 ): \"\"\" Upscales an image where every pixel is expanded into `kernel_size`*`kernel_size` pixels. Used to test whether the benefit of resizing images to the pre-trained size comes from the bilnearly interpolated pixels, or if the same would be realized with no interpolated pixels. \"\"\" assert kernel_size % 2 == 1 assert kernel_size >= 3 padding = ( kernel_size - 1 ) // 2 # img_tensor shape: (batch_size, channels, H, W) batch_size , channels , H , W = img_tensor . shape # Unfold: Extract 3x3 patches with padding of 1 to cover borders img_tensor = F . pad ( img_tensor , pad = ( padding , padding , padding , padding ), mode = \"replicate\" ) patches = F . unfold ( img_tensor , kernel_size = kernel_size , padding = 0 ) # Shape: (batch_size, channels*9, H*W) # Reshape to organize the 9 values from each 3x3 neighborhood patches = patches . view ( batch_size , channels , kernel_size * kernel_size , H , W ) # Shape: (batch_size, channels, 9, H, W) # Rearrange the patches into (batch_size, channels, 3, 3, H, W) patches = patches . view ( batch_size , channels , kernel_size , kernel_size , H , W ) # Permute to have the spatial dimensions first and unfold them patches = patches . permute ( 0 , 1 , 4 , 2 , 5 , 3 ) # Shape: (batch_size, channels, H, 3, W, 3) # Reshape to get the final expanded image of shape (batch_size, channels, H*3, W*3) expanded_img = patches . reshape ( batch_size , channels , H * kernel_size , W * kernel_size ) return expanded_img def min_max_normalize ( self , tensor : torch . Tensor , q_low : list [ float ], q_hi : list [ float ]) -> torch . Tensor : dtype = tensor . dtype q_low = torch . as_tensor ( q_low , dtype = dtype , device = tensor . device ) q_hi = torch . as_tensor ( q_hi , dtype = dtype , device = tensor . device ) x = torch . tensor ( - 12.0 ) y = torch . exp ( x ) tensor . sub_ ( q_low [:, None , None , None ]) . div_ (( q_hi [:, None , None , None ] . sub_ ( q_low [:, None , None , None ])) . add ( y )) return tensor __init__ ( h5py_file_object , h5data_keys = None , crop_size = None , dataset_bands = None , input_bands = None , resize = False , resize_to = [ 224 , 224 ], resize_interpolation = InterpolationMode . BILINEAR , resize_antialiasing = True , reverse_tile = False , reverse_tile_size = 3 , save_keys_path = None , classification_map = 'land-cover' ) # Initialize a new instance of Sen4MapDatasetMonthlyComposites. This dataset loads data from an HDF5 file object containing multi-temporal satellite data and computes monthly composite images by aggregating acquisitions (via median). Parameters: h5py_file_object ( File ) \u2013 An open h5py.File object containing the dataset. h5data_keys \u2013 Optional list of keys to select a subset of data samples from the HDF5 file. If None, all keys are used. crop_size ( None | int , default: None ) \u2013 Optional integer specifying the square crop size for the output image. dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Optional list of bands available in the dataset. input_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Optional list of bands to be used as input channels. Must be provided along with dataset_bands . resize \u2013 Boolean flag indicating whether the image should be resized. Default is False. resize_to \u2013 Target dimensions [height, width] for resizing. Default is [224, 224]. resize_interpolation \u2013 Interpolation mode used for resizing. Default is InterpolationMode.BILINEAR. resize_antialiasing \u2013 Boolean flag to apply antialiasing during resizing. Default is True. reverse_tile \u2013 Boolean flag indicating whether to apply reverse tiling to the image. Default is False. reverse_tile_size \u2013 Kernel size for the reverse tiling operation. Must be an odd number >= 3. Default is 3. save_keys_path \u2013 Optional file path to save the list of dataset keys. classification_map \u2013 String specifying the classification mapping to use (\"land-cover\" or \"crops\"). Default is \"land-cover\". Raises: ValueError \u2013 If input_bands is provided without specifying dataset_bands . ValueError \u2013 If an invalid classification_map is provided. Source code in terratorch/datasets/sen4map.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def __init__ ( self , h5py_file_object : h5py . File , h5data_keys = None , crop_size : None | int = None , dataset_bands : list [ HLSBands | int ] | None = None , input_bands : list [ HLSBands | int ] | None = None , resize = False , resize_to = [ 224 , 224 ], resize_interpolation = InterpolationMode . BILINEAR , resize_antialiasing = True , reverse_tile = False , reverse_tile_size = 3 , save_keys_path = None , classification_map = \"land-cover\" ): \"\"\"Initialize a new instance of Sen4MapDatasetMonthlyComposites. This dataset loads data from an HDF5 file object containing multi-temporal satellite data and computes monthly composite images by aggregating acquisitions (via median). Args: h5py_file_object: An open h5py.File object containing the dataset. h5data_keys: Optional list of keys to select a subset of data samples from the HDF5 file. If None, all keys are used. crop_size: Optional integer specifying the square crop size for the output image. dataset_bands: Optional list of bands available in the dataset. input_bands: Optional list of bands to be used as input channels. Must be provided along with `dataset_bands`. resize: Boolean flag indicating whether the image should be resized. Default is False. resize_to: Target dimensions [height, width] for resizing. Default is [224, 224]. resize_interpolation: Interpolation mode used for resizing. Default is InterpolationMode.BILINEAR. resize_antialiasing: Boolean flag to apply antialiasing during resizing. Default is True. reverse_tile: Boolean flag indicating whether to apply reverse tiling to the image. Default is False. reverse_tile_size: Kernel size for the reverse tiling operation. Must be an odd number >= 3. Default is 3. save_keys_path: Optional file path to save the list of dataset keys. classification_map: String specifying the classification mapping to use (\"land-cover\" or \"crops\"). Default is \"land-cover\". Raises: ValueError: If `input_bands` is provided without specifying `dataset_bands`. ValueError: If an invalid `classification_map` is provided. \"\"\" self . h5data = h5py_file_object if h5data_keys is None : if classification_map == \"crops\" : print ( f \"Crop classification task chosen but no keys supplied. Will fail unless dataset hdf5 files have been filtered. Either filter dataset files or create a filtered set of keys.\" ) self . h5data_keys = list ( self . h5data . keys ()) if save_keys_path is not None : with open ( save_keys_path , \"wb\" ) as file : pickle . dump ( self . h5data_keys , file ) else : self . h5data_keys = h5data_keys self . crop_size = crop_size if input_bands and not dataset_bands : raise ValueError ( f \"input_bands was provided without specifying the dataset_bands\" ) # self.dataset_bands = dataset_bands # self.input_bands = input_bands if input_bands and dataset_bands : self . input_channels = [ dataset_bands . index ( band_ind ) for band_ind in input_bands if band_ind in dataset_bands ] else : self . input_channels = None classification_maps = { \"land-cover\" : Sen4MapDatasetMonthlyComposites . land_cover_classification_map , \"crops\" : Sen4MapDatasetMonthlyComposites . crop_classification_map } if classification_map not in classification_maps . keys (): raise ValueError ( f \"Provided classification_map of: { classification_map } , is not from the list of valid ones: { classification_maps } \" ) self . classification_map = classification_maps [ classification_map ] self . resize = resize self . resize_to = resize_to self . resize_interpolation = resize_interpolation self . resize_antialiasing = resize_antialiasing self . reverse_tile = reverse_tile self . reverse_tile_size = reverse_tile_size reverse_tiling_pytorch ( img_tensor , kernel_size = 3 ) # Upscales an image where every pixel is expanded into kernel_size * kernel_size pixels. Used to test whether the benefit of resizing images to the pre-trained size comes from the bilnearly interpolated pixels, or if the same would be realized with no interpolated pixels. Source code in terratorch/datasets/sen4map.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def reverse_tiling_pytorch ( self , img_tensor : torch . Tensor , kernel_size : int = 3 ): \"\"\" Upscales an image where every pixel is expanded into `kernel_size`*`kernel_size` pixels. Used to test whether the benefit of resizing images to the pre-trained size comes from the bilnearly interpolated pixels, or if the same would be realized with no interpolated pixels. \"\"\" assert kernel_size % 2 == 1 assert kernel_size >= 3 padding = ( kernel_size - 1 ) // 2 # img_tensor shape: (batch_size, channels, H, W) batch_size , channels , H , W = img_tensor . shape # Unfold: Extract 3x3 patches with padding of 1 to cover borders img_tensor = F . pad ( img_tensor , pad = ( padding , padding , padding , padding ), mode = \"replicate\" ) patches = F . unfold ( img_tensor , kernel_size = kernel_size , padding = 0 ) # Shape: (batch_size, channels*9, H*W) # Reshape to organize the 9 values from each 3x3 neighborhood patches = patches . view ( batch_size , channels , kernel_size * kernel_size , H , W ) # Shape: (batch_size, channels, 9, H, W) # Rearrange the patches into (batch_size, channels, 3, 3, H, W) patches = patches . view ( batch_size , channels , kernel_size , kernel_size , H , W ) # Permute to have the spatial dimensions first and unfold them patches = patches . permute ( 0 , 1 , 4 , 2 , 5 , 3 ) # Shape: (batch_size, channels, H, 3, W, 3) # Reshape to get the final expanded image of shape (batch_size, channels, H*3, W*3) expanded_img = patches . reshape ( batch_size , channels , H * kernel_size , W * kernel_size ) return expanded_img Datamodules # terratorch.datamodules.biomassters # BioMasstersNonGeoDataModule # Bases: NonGeoDataModule NonGeo LightningDataModule implementation for BioMassters datamodule. Source code in terratorch/datamodules/biomassters.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 class BioMasstersNonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for BioMassters datamodule.\"\"\" default_metadata_filename = \"The_BioMassters_-_features_metadata.csv.csv\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : dict [ str , Sequence [ str ]] | Sequence [ str ] = BioMasstersNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , drop_last : bool = True , sensors : Sequence [ str ] = [ \"S1\" , \"S2\" ], as_time_series : bool = False , metadata_filename : str = default_metadata_filename , max_cloud_percentage : float | None = None , max_red_mean : float | None = None , include_corrupt : bool = True , subset : float = 1 , seed : int = 42 , use_four_frames : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the DataModule for the non-geospatial BioMassters datamodule. Args: data_root (str): Root directory containing the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (dict[str, Sequence[str]] | Sequence[str], optional): Band configuration; either a dict mapping sensors to bands or a list for the first sensor. Defaults to BioMasstersNonGeo.all_band_names train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation or normalization to apply. Defaults to normalization if not provided. drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True. sensors (Sequence[str], optional): List of sensors to use (e.g., [\"S1\", \"S2\"]). Defaults to [\"S1\", \"S2\"]. as_time_series (bool, optional): Whether to treat data as a time series. Defaults to False. metadata_filename (str, optional): Metadata filename. Defaults to \"The_BioMassters_-_features_metadata.csv.csv\". max_cloud_percentage (float | None, optional): Maximum allowed cloud percentage. Defaults to None. max_red_mean (float | None, optional): Maximum allowed red band mean. Defaults to None. include_corrupt (bool, optional): Whether to include corrupt data. Defaults to True. subset (float, optional): Fraction of the dataset to use. Defaults to 1. seed (int, optional): Random seed for reproducibility. Defaults to 42. use_four_frames (bool, optional): Whether to use a four frames configuration. Defaults to False. **kwargs: Additional keyword arguments. Returns: None. \"\"\" super () . __init__ ( BioMasstersNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . sensors = sensors if isinstance ( bands , dict ): self . bands = bands else : sens = sensors [ 0 ] self . bands = { sens : bands } self . means = {} self . stds = {} for sensor in self . sensors : self . means [ sensor ] = [ MEANS [ sensor ][ band ] for band in self . bands [ sensor ]] self . stds [ sensor ] = [ STDS [ sensor ][ band ] for band in self . bands [ sensor ]] self . mask_mean = MEANS [ \"AGBM\" ] self . mask_std = STDS [ \"AGBM\" ] self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) if len ( sensors ) == 1 : self . aug = Normalize ( self . means [ sensors [ 0 ]], self . stds [ sensors [ 0 ]]) if aug is None else aug else : MultimodalNormalize ( self . means , self . stds ) if aug is None else aug self . drop_last = drop_last self . as_time_series = as_time_series self . metadata_filename = metadata_filename self . max_cloud_percentage = max_cloud_percentage self . max_red_mean = max_red_mean self . include_corrupt = include_corrupt self . subset = subset self . seed = seed self . use_four_frames = use_four_frames def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , root = self . data_root , transform = self . train_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"test\" , root = self . data_root , transform = self . val_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , root = self . data_root , transform = self . test_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , root = self . data_root , transform = self . predict_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) def _dataloader_factory ( self , split : str ): dataset = self . _valid_attribute ( f \" { split } _dataset\" , \"dataset\" ) batch_size = self . _valid_attribute ( f \" { split } _batch_size\" , \"batch_size\" ) return DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = split == \"train\" , num_workers = self . num_workers , collate_fn = self . collate_fn , drop_last = split == \"train\" and self . drop_last , ) __init__ ( data_root , batch_size = 4 , num_workers = 0 , bands = BioMasstersNonGeo . all_band_names , train_transform = None , val_transform = None , test_transform = None , predict_transform = None , aug = None , drop_last = True , sensors = [ 'S1' , 'S2' ], as_time_series = False , metadata_filename = default_metadata_filename , max_cloud_percentage = None , max_red_mean = None , include_corrupt = True , subset = 1 , seed = 42 , use_four_frames = False , ** kwargs ) # Initializes the DataModule for the non-geospatial BioMassters datamodule. Parameters: data_root ( str ) \u2013 Root directory containing the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for DataLoaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. bands ( dict [ str , Sequence [ str ]] | Sequence [ str ] , default: all_band_names ) \u2013 Band configuration; either a dict mapping sensors to bands or a list for the first sensor. Defaults to BioMasstersNonGeo.all_band_names train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. aug ( AugmentationSequential , default: None ) \u2013 Augmentation or normalization to apply. Defaults to normalization if not provided. drop_last ( bool , default: True ) \u2013 Whether to drop the last incomplete batch. Defaults to True. sensors ( Sequence [ str ] , default: ['S1', 'S2'] ) \u2013 List of sensors to use (e.g., [\"S1\", \"S2\"]). Defaults to [\"S1\", \"S2\"]. as_time_series ( bool , default: False ) \u2013 Whether to treat data as a time series. Defaults to False. metadata_filename ( str , default: default_metadata_filename ) \u2013 Metadata filename. Defaults to \"The_BioMassters_-_features_metadata.csv.csv\". max_cloud_percentage ( float | None , default: None ) \u2013 Maximum allowed cloud percentage. Defaults to None. max_red_mean ( float | None , default: None ) \u2013 Maximum allowed red band mean. Defaults to None. include_corrupt ( bool , default: True ) \u2013 Whether to include corrupt data. Defaults to True. subset ( float , default: 1 ) \u2013 Fraction of the dataset to use. Defaults to 1. seed ( int , default: 42 ) \u2013 Random seed for reproducibility. Defaults to 42. use_four_frames ( bool , default: False ) \u2013 Whether to use a four frames configuration. Defaults to False. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Returns: None \u2013 None. Source code in terratorch/datamodules/biomassters.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : dict [ str , Sequence [ str ]] | Sequence [ str ] = BioMasstersNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , drop_last : bool = True , sensors : Sequence [ str ] = [ \"S1\" , \"S2\" ], as_time_series : bool = False , metadata_filename : str = default_metadata_filename , max_cloud_percentage : float | None = None , max_red_mean : float | None = None , include_corrupt : bool = True , subset : float = 1 , seed : int = 42 , use_four_frames : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the DataModule for the non-geospatial BioMassters datamodule. Args: data_root (str): Root directory containing the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (dict[str, Sequence[str]] | Sequence[str], optional): Band configuration; either a dict mapping sensors to bands or a list for the first sensor. Defaults to BioMasstersNonGeo.all_band_names train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation or normalization to apply. Defaults to normalization if not provided. drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True. sensors (Sequence[str], optional): List of sensors to use (e.g., [\"S1\", \"S2\"]). Defaults to [\"S1\", \"S2\"]. as_time_series (bool, optional): Whether to treat data as a time series. Defaults to False. metadata_filename (str, optional): Metadata filename. Defaults to \"The_BioMassters_-_features_metadata.csv.csv\". max_cloud_percentage (float | None, optional): Maximum allowed cloud percentage. Defaults to None. max_red_mean (float | None, optional): Maximum allowed red band mean. Defaults to None. include_corrupt (bool, optional): Whether to include corrupt data. Defaults to True. subset (float, optional): Fraction of the dataset to use. Defaults to 1. seed (int, optional): Random seed for reproducibility. Defaults to 42. use_four_frames (bool, optional): Whether to use a four frames configuration. Defaults to False. **kwargs: Additional keyword arguments. Returns: None. \"\"\" super () . __init__ ( BioMasstersNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . sensors = sensors if isinstance ( bands , dict ): self . bands = bands else : sens = sensors [ 0 ] self . bands = { sens : bands } self . means = {} self . stds = {} for sensor in self . sensors : self . means [ sensor ] = [ MEANS [ sensor ][ band ] for band in self . bands [ sensor ]] self . stds [ sensor ] = [ STDS [ sensor ][ band ] for band in self . bands [ sensor ]] self . mask_mean = MEANS [ \"AGBM\" ] self . mask_std = STDS [ \"AGBM\" ] self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) if len ( sensors ) == 1 : self . aug = Normalize ( self . means [ sensors [ 0 ]], self . stds [ sensors [ 0 ]]) if aug is None else aug else : MultimodalNormalize ( self . means , self . stds ) if aug is None else aug self . drop_last = drop_last self . as_time_series = as_time_series self . metadata_filename = metadata_filename self . max_cloud_percentage = max_cloud_percentage self . max_red_mean = max_red_mean self . include_corrupt = include_corrupt self . subset = subset self . seed = seed self . use_four_frames = use_four_frames setup ( stage ) # Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/biomassters.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , root = self . data_root , transform = self . train_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"test\" , root = self . data_root , transform = self . val_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , root = self . data_root , transform = self . test_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , root = self . data_root , transform = self . predict_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) terratorch.datamodules.burn_intensity # BurnIntensityNonGeoDataModule # Bases: NonGeoDataModule NonGeo LightningDataModule implementation for BurnIntensity datamodule. Source code in terratorch/datamodules/burn_intensity.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class BurnIntensityNonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for BurnIntensity datamodule.\"\"\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = BurnIntensityNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , use_full_data : bool = True , no_data_replace : float | None = 0.0001 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the DataModule for the BurnIntensity non-geospatial datamodule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to BurnIntensityNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction. use_full_data (bool, optional): Whether to use the full dataset or data with less than 25 percent zeros. Defaults to True. no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001. no_label_replace (int | None, optional): Value to replace missing labels. Defaults to -1. use_metadata (bool): Whether to return metadata info (time and location). **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( BurnIntensityNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = [ MEANS [ b ] for b in bands ] stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = NormalizeWithTimesteps ( means , stds ) self . use_full_data = use_full_data self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) __init__ ( data_root , batch_size = 4 , num_workers = 0 , bands = BurnIntensityNonGeo . all_band_names , train_transform = None , val_transform = None , test_transform = None , predict_transform = None , use_full_data = True , no_data_replace = 0.0001 , no_label_replace =- 1 , use_metadata = False , ** kwargs ) # Initializes the DataModule for the BurnIntensity non-geospatial datamodule. Parameters: data_root ( str ) \u2013 Root directory of the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for DataLoaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. bands ( Sequence [ str ] , default: all_band_names ) \u2013 List of bands to use. Defaults to BurnIntensityNonGeo.all_band_names. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction. use_full_data ( bool , default: True ) \u2013 Whether to use the full dataset or data with less than 25 percent zeros. Defaults to True. no_data_replace ( float | None , default: 0.0001 ) \u2013 Value to replace missing data. Defaults to 0.0001. no_label_replace ( int | None , default: -1 ) \u2013 Value to replace missing labels. Defaults to -1. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (time and location). **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/burn_intensity.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = BurnIntensityNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , use_full_data : bool = True , no_data_replace : float | None = 0.0001 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the DataModule for the BurnIntensity non-geospatial datamodule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to BurnIntensityNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction. use_full_data (bool, optional): Whether to use the full dataset or data with less than 25 percent zeros. Defaults to True. no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001. no_label_replace (int | None, optional): Value to replace missing labels. Defaults to -1. use_metadata (bool): Whether to return metadata info (time and location). **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( BurnIntensityNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = [ MEANS [ b ] for b in bands ] stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = NormalizeWithTimesteps ( means , stds ) self . use_full_data = use_full_data self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata setup ( stage ) # Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/burn_intensity.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) terratorch.datamodules.carbonflux # CarbonFluxNonGeoDataModule # Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Carbon FLux dataset. Source code in terratorch/datamodules/carbonflux.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class CarbonFluxNonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Carbon FLux dataset.\"\"\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = CarbonFluxNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , no_data_replace : float | None = 0.0001 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the CarbonFluxNonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to CarbonFluxNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation sequence; if None, applies multimodal normalization. no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001. use_metadata (bool): Whether to return metadata info. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( CarbonFluxNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = { m : ([ MEANS [ m ][ band ] for band in bands ] if m == \"image\" else MEANS [ m ]) for m in MEANS . keys () } stds = { m : ([ STDS [ m ][ band ] for band in bands ] if m == \"image\" else STDS [ m ]) for m in STDS . keys () } self . mask_means = MEANS [ \"mask\" ] self . mask_std = STDS [ \"mask\" ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = MultimodalNormalize ( means , stds ) if aug is None else aug self . no_data_replace = no_data_replace self . use_metadata = use_metadata def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , ) __init__ ( data_root , batch_size = 4 , num_workers = 0 , bands = CarbonFluxNonGeo . all_band_names , train_transform = None , val_transform = None , test_transform = None , predict_transform = None , aug = None , no_data_replace = 0.0001 , use_metadata = False , ** kwargs ) # Initializes the CarbonFluxNonGeoDataModule. Parameters: data_root ( str ) \u2013 Root directory of the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for DataLoaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. bands ( Sequence [ str ] , default: all_band_names ) \u2013 List of bands to use. Defaults to CarbonFluxNonGeo.all_band_names. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. aug ( AugmentationSequential , default: None ) \u2013 Augmentation sequence; if None, applies multimodal normalization. no_data_replace ( float | None , default: 0.0001 ) \u2013 Value to replace missing data. Defaults to 0.0001. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/carbonflux.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = CarbonFluxNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , no_data_replace : float | None = 0.0001 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the CarbonFluxNonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to CarbonFluxNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation sequence; if None, applies multimodal normalization. no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001. use_metadata (bool): Whether to return metadata info. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( CarbonFluxNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = { m : ([ MEANS [ m ][ band ] for band in bands ] if m == \"image\" else MEANS [ m ]) for m in MEANS . keys () } stds = { m : ([ STDS [ m ][ band ] for band in bands ] if m == \"image\" else STDS [ m ]) for m in STDS . keys () } self . mask_means = MEANS [ \"mask\" ] self . mask_std = STDS [ \"mask\" ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = MultimodalNormalize ( means , stds ) if aug is None else aug self . no_data_replace = no_data_replace self . use_metadata = use_metadata setup ( stage ) # Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/carbonflux.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , ) terratorch.datamodules.forestnet # ForestNetNonGeoDataModule # Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Landslide4Sense dataset. Source code in terratorch/datamodules/forestnet.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class ForestNetNonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Landslide4Sense dataset.\"\"\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , label_map : dict [ str , int ] = ForestNetNonGeo . default_label_map , bands : Sequence [ str ] = ForestNetNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , fraction : float = 1.0 , aug : AugmentationSequential = None , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the ForestNetNonGeoDataModule. Args: data_root (str): Directory containing the dataset. batch_size (int, optional): Batch size for data loaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. label_map (dict[str, int], optional): Mapping of labels to integers. Defaults to ForestNetNonGeo.default_label_map. bands (Sequence[str], optional): List of band names to use. Defaults to ForestNetNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction. fraction (float, optional): Fraction of data to use. Defaults to 1.0. aug (AugmentationSequential, optional): Augmentation/normalization pipeline; if None, uses Normalize. use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( ForestNetNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . means = [ MEANS [ b ] for b in bands ] self . stds = [ STDS [ b ] for b in bands ] self . label_map = label_map self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = Normalize ( self . means , self . stds ) if aug is None else aug self . fraction = fraction self . use_metadata = use_metadata def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , label_map = self . label_map , transform = self . train_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , label_map = self . label_map , transform = self . val_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , label_map = self . label_map , transform = self . test_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , label_map = self . label_map , transform = self . predict_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , ) __init__ ( data_root , batch_size = 4 , num_workers = 0 , label_map = ForestNetNonGeo . default_label_map , bands = ForestNetNonGeo . all_band_names , train_transform = None , val_transform = None , test_transform = None , predict_transform = None , fraction = 1.0 , aug = None , use_metadata = False , ** kwargs ) # Initializes the ForestNetNonGeoDataModule. Parameters: data_root ( str ) \u2013 Directory containing the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for data loaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. label_map ( dict [ str , int ] , default: default_label_map ) \u2013 Mapping of labels to integers. Defaults to ForestNetNonGeo.default_label_map. bands ( Sequence [ str ] , default: all_band_names ) \u2013 List of band names to use. Defaults to ForestNetNonGeo.all_band_names. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction. fraction ( float , default: 1.0 ) \u2013 Fraction of data to use. Defaults to 1.0. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline; if None, uses Normalize. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/forestnet.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , label_map : dict [ str , int ] = ForestNetNonGeo . default_label_map , bands : Sequence [ str ] = ForestNetNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , fraction : float = 1.0 , aug : AugmentationSequential = None , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the ForestNetNonGeoDataModule. Args: data_root (str): Directory containing the dataset. batch_size (int, optional): Batch size for data loaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. label_map (dict[str, int], optional): Mapping of labels to integers. Defaults to ForestNetNonGeo.default_label_map. bands (Sequence[str], optional): List of band names to use. Defaults to ForestNetNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction. fraction (float, optional): Fraction of data to use. Defaults to 1.0. aug (AugmentationSequential, optional): Augmentation/normalization pipeline; if None, uses Normalize. use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( ForestNetNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . means = [ MEANS [ b ] for b in bands ] self . stds = [ STDS [ b ] for b in bands ] self . label_map = label_map self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = Normalize ( self . means , self . stds ) if aug is None else aug self . fraction = fraction self . use_metadata = use_metadata setup ( stage ) # Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/forestnet.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , label_map = self . label_map , transform = self . train_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , label_map = self . label_map , transform = self . val_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , label_map = self . label_map , transform = self . test_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , label_map = self . label_map , transform = self . predict_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , ) terratorch.datamodules.fire_scars # FireScarsDataModule # Bases: GeoDataModule Geo Fire Scars data module implementation that merges input data with ground truth segmentation masks. Source code in terratorch/datamodules/fire_scars.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 class FireScarsDataModule ( GeoDataModule ): \"\"\"Geo Fire Scars data module implementation that merges input data with ground truth segmentation masks.\"\"\" def __init__ ( self , data_root : str , ** kwargs : Any ) -> None : super () . __init__ ( FireScarsSegmentationMask , 4 , 224 , 100 , 0 , ** kwargs ) means = list ( MEANS . values ()) stds = list ( STDS . values ()) self . train_aug = AugmentationSequential ( K . RandomCrop ( 224 , 224 ), K . Normalize ( means , stds )) self . aug = AugmentationSequential ( K . Normalize ( means , stds )) self . data_root = data_root def setup ( self , stage : str ) -> None : self . images = FireScarsHLS ( os . path . join ( self . data_root , \"training/\" ) ) self . labels = FireScarsSegmentationMask ( os . path . join ( self . data_root , \"training/\" ) ) self . dataset = self . images & self . labels self . train_aug = AugmentationSequential ( K . RandomCrop ( 224 , 224 ), K . normalize ()) self . images_test = FireScarsHLS ( os . path . join ( self . data_root , \"validation/\" ) ) self . labels_test = FireScarsSegmentationMask ( os . path . join ( self . data_root , \"validation/\" ) ) self . val_dataset = self . images_test & self . labels_test if stage in [ \"fit\" ]: self . train_batch_sampler = RandomBatchGeoSampler ( self . dataset , self . patch_size , self . batch_size , None ) if stage in [ \"fit\" , \"validate\" ]: self . val_sampler = GridGeoSampler ( self . val_dataset , self . patch_size , self . patch_size ) if stage in [ \"test\" ]: self . test_sampler = GridGeoSampler ( self . val_dataset , self . patch_size , self . patch_size ) FireScarsNonGeoDataModule # Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Fire Scars dataset. Source code in terratorch/datamodules/fire_scars.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class FireScarsNonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Fire Scars dataset.\"\"\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = FireScarsNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , drop_last : bool = True , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the FireScarsNonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of band names. Defaults to FireScarsNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction. drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True. no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0. no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1. use_metadata (bool): Whether to return metadata info. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( FireScarsNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = [ MEANS [ b ] for b in bands ] stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = AugmentationSequential ( K . Normalize ( means , stds ), data_keys = [ \"image\" ]) self . drop_last = drop_last self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) def _dataloader_factory ( self , split : str ) -> DataLoader [ dict [ str , Tensor ]]: \"\"\"Implement one or more PyTorch DataLoaders. Args: split: Either 'train', 'val', 'test', or 'predict'. Returns: A collection of data loaders specifying samples. Raises: MisconfigurationException: If :meth:`setup` does not define a dataset or sampler, or if the dataset or sampler has length 0. \"\"\" dataset = self . _valid_attribute ( f \" { split } _dataset\" , \"dataset\" ) batch_size = self . _valid_attribute ( f \" { split } _batch_size\" , \"batch_size\" ) return DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = split == \"train\" , num_workers = self . num_workers , collate_fn = self . collate_fn , drop_last = split == \"train\" and self . drop_last , ) __init__ ( data_root , batch_size = 4 , num_workers = 0 , bands = FireScarsNonGeo . all_band_names , train_transform = None , val_transform = None , test_transform = None , predict_transform = None , drop_last = True , no_data_replace = 0 , no_label_replace =- 1 , use_metadata = False , ** kwargs ) # Initializes the FireScarsNonGeoDataModule. Parameters: data_root ( str ) \u2013 Root directory of the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for DataLoaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. bands ( Sequence [ str ] , default: all_band_names ) \u2013 List of band names. Defaults to FireScarsNonGeo.all_band_names. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction. drop_last ( bool , default: True ) \u2013 Whether to drop the last incomplete batch. Defaults to True. no_data_replace ( float | None , default: 0 ) \u2013 Replacement value for missing data. Defaults to 0. no_label_replace ( int | None , default: -1 ) \u2013 Replacement value for missing labels. Defaults to -1. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/fire_scars.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = FireScarsNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , drop_last : bool = True , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the FireScarsNonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of band names. Defaults to FireScarsNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction. drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True. no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0. no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1. use_metadata (bool): Whether to return metadata info. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( FireScarsNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = [ MEANS [ b ] for b in bands ] stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = AugmentationSequential ( K . Normalize ( means , stds ), data_keys = [ \"image\" ]) self . drop_last = drop_last self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata setup ( stage ) # Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/fire_scars.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) terratorch.datamodules.landslide4sense # Landslide4SenseNonGeoDataModule # Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Landslide4Sense dataset. Source code in terratorch/datamodules/landslide4sense.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class Landslide4SenseNonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Landslide4Sense dataset.\"\"\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = Landslide4SenseNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the Landslide4SenseNonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for data loaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of band names to use. Defaults to Landslide4SenseNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation pipeline; if None, applies normalization using computed means and stds. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( Landslide4SenseNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . means = [ MEANS [ b ] for b in bands ] self . stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = ( AugmentationSequential ( K . Normalize ( self . means , self . stds ), data_keys = [ \"image\" ]) if aug is None else aug ) def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands ) __init__ ( data_root , batch_size = 4 , num_workers = 0 , bands = Landslide4SenseNonGeo . all_band_names , train_transform = None , val_transform = None , test_transform = None , predict_transform = None , aug = None , ** kwargs ) # Initializes the Landslide4SenseNonGeoDataModule. Parameters: data_root ( str ) \u2013 Root directory of the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for data loaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. bands ( Sequence [ str ] , default: all_band_names ) \u2013 List of band names to use. Defaults to Landslide4SenseNonGeo.all_band_names. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. aug ( AugmentationSequential , default: None ) \u2013 Augmentation pipeline; if None, applies normalization using computed means and stds. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/landslide4sense.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = Landslide4SenseNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the Landslide4SenseNonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for data loaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of band names to use. Defaults to Landslide4SenseNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation pipeline; if None, applies normalization using computed means and stds. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( Landslide4SenseNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . means = [ MEANS [ b ] for b in bands ] self . stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = ( AugmentationSequential ( K . Normalize ( self . means , self . stds ), data_keys = [ \"image\" ]) if aug is None else aug ) setup ( stage ) # Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/landslide4sense.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands ) terratorch.datamodules.m_eurosat # MEuroSATNonGeoDataModule # Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-EuroSAT dataset. Source code in terratorch/datamodules/m_eurosat.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class MEuroSATNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-EuroSAT dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MEuroSATNonGeoDataModule for the MEuroSATNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MEuroSATNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , ) __init__ ( batch_size = 8 , num_workers = 0 , data_root = './' , bands = None , train_transform = None , val_transform = None , test_transform = None , aug = None , partition = 'default' , ** kwargs ) # Initializes the MEuroSATNonGeoDataModule for the MEuroSATNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_eurosat.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MEuroSATNonGeoDataModule for the MEuroSATNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MEuroSATNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , ) terratorch.datamodules.m_bigearthnet # MBigEarthNonGeoDataModule # Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-BigEarthNet dataset. Source code in terratorch/datamodules/m_bigearthnet.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class MBigEarthNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-BigEarthNet dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MBigEarthNonGeoDataModule for the M-BigEarthNet dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MBigEarthNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , ) __init__ ( batch_size = 8 , num_workers = 0 , data_root = './' , bands = None , train_transform = None , val_transform = None , test_transform = None , aug = None , partition = 'default' , ** kwargs ) # Initializes the MBigEarthNonGeoDataModule for the M-BigEarthNet dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_bigearthnet.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MBigEarthNonGeoDataModule for the M-BigEarthNet dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MBigEarthNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , ) terratorch.datamodules.m_brick_kiln # MBrickKilnNonGeoDataModule # Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-BrickKiln dataset. Source code in terratorch/datamodules/m_brick_kiln.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class MBrickKilnNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-BrickKiln dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MBrickKilnNonGeoDataModule for the M-BrickKilnNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MBrickKilnNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , ) __init__ ( batch_size = 8 , num_workers = 0 , data_root = './' , bands = None , train_transform = None , val_transform = None , test_transform = None , aug = None , partition = 'default' , ** kwargs ) # Initializes the MBrickKilnNonGeoDataModule for the M-BrickKilnNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_brick_kiln.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MBrickKilnNonGeoDataModule for the M-BrickKilnNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MBrickKilnNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , ) terratorch.datamodules.m_forestnet # MForestNetNonGeoDataModule # Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-ForestNet dataset. Source code in terratorch/datamodules/m_forestnet.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 class MForestNetNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-ForestNet dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MForestNetNonGeoDataModule for the MForestNetNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MForestNetNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , ) __init__ ( batch_size = 8 , num_workers = 0 , data_root = './' , bands = None , train_transform = None , val_transform = None , test_transform = None , aug = None , partition = 'default' , use_metadata = False , ** kwargs ) # Initializes the MForestNetNonGeoDataModule for the MForestNetNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_forestnet.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MForestNetNonGeoDataModule for the MForestNetNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MForestNetNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , ) terratorch.datamodules.m_so2sat # MSo2SatNonGeoDataModule # Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-So2Sat dataset. Source code in terratorch/datamodules/m_so2sat.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 class MSo2SatNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-So2Sat dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MSo2SatNonGeoDataModule for the MSo2SatNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MSo2SatNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , ) __init__ ( batch_size = 8 , num_workers = 0 , data_root = './' , bands = None , train_transform = None , val_transform = None , test_transform = None , aug = None , partition = 'default' , ** kwargs ) # Initializes the MSo2SatNonGeoDataModule for the MSo2SatNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_so2sat.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MSo2SatNonGeoDataModule for the MSo2SatNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MSo2SatNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , ) terratorch.datamodules.m_pv4ger # MPv4gerNonGeoDataModule # Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-Pv4ger dataset. Source code in terratorch/datamodules/m_pv4ger.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class MPv4gerNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-Pv4ger dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MPv4gerNonGeoDataModule for the MPv4gerNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MPv4gerNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , ) __init__ ( batch_size = 8 , num_workers = 0 , data_root = './' , bands = None , train_transform = None , val_transform = None , test_transform = None , aug = None , partition = 'default' , use_metadata = False , ** kwargs ) # Initializes the MPv4gerNonGeoDataModule for the MPv4gerNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_pv4ger.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MPv4gerNonGeoDataModule for the MPv4gerNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MPv4gerNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , ) terratorch.datamodules.m_cashew_plantation # MBeninSmallHolderCashewsNonGeoDataModule # Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-Cashew Plantation dataset. Source code in terratorch/datamodules/m_cashew_plantation.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class MBeninSmallHolderCashewsNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-Cashew Plantation dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MBeninSmallHolderCashewsNonGeoDataModule for the M-BeninSmallHolderCashewsNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MBeninSmallHolderCashewsNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , ) __init__ ( batch_size = 8 , num_workers = 0 , data_root = './' , bands = None , train_transform = None , val_transform = None , test_transform = None , aug = None , partition = 'default' , use_metadata = False , ** kwargs ) # Initializes the MBeninSmallHolderCashewsNonGeoDataModule for the M-BeninSmallHolderCashewsNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_cashew_plantation.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MBeninSmallHolderCashewsNonGeoDataModule for the M-BeninSmallHolderCashewsNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MBeninSmallHolderCashewsNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , ) terratorch.datamodules.m_nz_cattle # MNzCattleNonGeoDataModule # Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-NZCattle dataset. Source code in terratorch/datamodules/m_nz_cattle.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class MNzCattleNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-NZCattle dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MNzCattleNonGeoDataModule for the MNzCattleNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MNzCattleNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , ) __init__ ( batch_size = 8 , num_workers = 0 , data_root = './' , bands = None , train_transform = None , val_transform = None , test_transform = None , aug = None , partition = 'default' , use_metadata = False , ** kwargs ) # Initializes the MNzCattleNonGeoDataModule for the MNzCattleNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_nz_cattle.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MNzCattleNonGeoDataModule for the MNzCattleNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MNzCattleNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , ) terratorch.datamodules.m_chesapeake_landcover # MChesapeakeLandcoverNonGeoDataModule # Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-ChesapeakeLandcover dataset. Source code in terratorch/datamodules/m_chesapeake_landcover.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class MChesapeakeLandcoverNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-ChesapeakeLandcover dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MChesapeakeLandcoverNonGeoDataModule for the M-BigEarthNet dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MChesapeakeLandcoverNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , ) __init__ ( batch_size = 8 , num_workers = 0 , data_root = './' , bands = None , train_transform = None , val_transform = None , test_transform = None , aug = None , partition = 'default' , ** kwargs ) # Initializes the MChesapeakeLandcoverNonGeoDataModule for the M-BigEarthNet dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_chesapeake_landcover.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MChesapeakeLandcoverNonGeoDataModule for the M-BigEarthNet dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MChesapeakeLandcoverNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , ) terratorch.datamodules.m_pv4ger_seg # MPv4gerSegNonGeoDataModule # Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-Pv4gerSeg dataset. Source code in terratorch/datamodules/m_pv4ger_seg.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class MPv4gerSegNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-Pv4gerSeg dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MPv4gerNonGeoDataModule for the MPv4gerSegNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MPv4gerSegNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , ) __init__ ( batch_size = 8 , num_workers = 0 , data_root = './' , bands = None , train_transform = None , val_transform = None , test_transform = None , aug = None , partition = 'default' , use_metadata = False , ** kwargs ) # Initializes the MPv4gerNonGeoDataModule for the MPv4gerSegNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_pv4ger_seg.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MPv4gerNonGeoDataModule for the MPv4gerSegNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MPv4gerSegNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , ) terratorch.datamodules.m_SA_crop_type # MSACropTypeNonGeoDataModule # Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-SA-CropType dataset. Source code in terratorch/datamodules/m_SA_crop_type.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class MSACropTypeNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-SA-CropType dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MSACropTypeNonGeoDataModule for the MSACropTypeNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MSACropTypeNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , ) __init__ ( batch_size = 8 , num_workers = 0 , data_root = './' , bands = None , train_transform = None , val_transform = None , test_transform = None , aug = None , partition = 'default' , ** kwargs ) # Initializes the MSACropTypeNonGeoDataModule for the MSACropTypeNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_SA_crop_type.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MSACropTypeNonGeoDataModule for the MSACropTypeNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MSACropTypeNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , ) terratorch.datamodules.m_neontree # MNeonTreeNonGeoDataModule # Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-NeonTree dataset. Source code in terratorch/datamodules/m_neontree.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class MNeonTreeNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-NeonTree dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MNeonTreeNonGeoDataModule for the MNeonTreeNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MNeonTreeNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , ) __init__ ( batch_size = 8 , num_workers = 0 , data_root = './' , bands = None , train_transform = None , val_transform = None , test_transform = None , aug = None , partition = 'default' , ** kwargs ) # Initializes the MNeonTreeNonGeoDataModule for the MNeonTreeNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_neontree.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MNeonTreeNonGeoDataModule for the MNeonTreeNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MNeonTreeNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , ) terratorch.datamodules.multi_temporal_crop_classification # MultiTemporalCropClassificationDataModule # Bases: NonGeoDataModule NonGeo LightningDataModule implementation for multi-temporal crop classification. Source code in terratorch/datamodules/multi_temporal_crop_classification.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 class MultiTemporalCropClassificationDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for multi-temporal crop classification.\"\"\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = MultiTemporalCropClassification . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , drop_last : bool = True , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , expand_temporal_dimension : bool = True , reduce_zero_label : bool = True , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the MultiTemporalCropClassificationDataModule for multi-temporal crop classification. Args: data_root (str): Directory containing the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to MultiTemporalCropClassification.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. drop_last (bool, optional): Whether to drop the last incomplete batch during training. Defaults to True. no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0. no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1. expand_temporal_dimension (bool, optional): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True. reduce_zero_label (bool, optional): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True. use_metadata (bool): Whether to return metadata info (time and location). **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( MultiTemporalCropClassification , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . means = [ MEANS [ b ] for b in bands ] self . stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = Normalize ( self . means , self . stds ) self . drop_last = drop_last self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . expand_temporal_dimension = expand_temporal_dimension self . reduce_zero_label = reduce_zero_label self . use_metadata = use_metadata def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) def _dataloader_factory ( self , split : str ) -> DataLoader [ dict [ str , Tensor ]]: \"\"\"Implement one or more PyTorch DataLoaders. Args: split: Either 'train', 'val', 'test', or 'predict'. Returns: A collection of data loaders specifying samples. Raises: MisconfigurationException: If :meth:`setup` does not define a dataset or sampler, or if the dataset or sampler has length 0. \"\"\" dataset = self . _valid_attribute ( f \" { split } _dataset\" , \"dataset\" ) batch_size = self . _valid_attribute ( f \" { split } _batch_size\" , \"batch_size\" ) return DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = split == \"train\" , num_workers = self . num_workers , collate_fn = self . collate_fn , drop_last = split == \"train\" and self . drop_last , ) __init__ ( data_root , batch_size = 4 , num_workers = 0 , bands = MultiTemporalCropClassification . all_band_names , train_transform = None , val_transform = None , test_transform = None , predict_transform = None , drop_last = True , no_data_replace = 0 , no_label_replace =- 1 , expand_temporal_dimension = True , reduce_zero_label = True , use_metadata = False , ** kwargs ) # Initializes the MultiTemporalCropClassificationDataModule for multi-temporal crop classification. Parameters: data_root ( str ) \u2013 Directory containing the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for DataLoaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. bands ( Sequence [ str ] , default: all_band_names ) \u2013 List of bands to use. Defaults to MultiTemporalCropClassification.all_band_names. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. drop_last ( bool , default: True ) \u2013 Whether to drop the last incomplete batch during training. Defaults to True. no_data_replace ( float | None , default: 0 ) \u2013 Replacement value for missing data. Defaults to 0. no_label_replace ( int | None , default: -1 ) \u2013 Replacement value for missing labels. Defaults to -1. expand_temporal_dimension ( bool , default: True ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True. reduce_zero_label ( bool , default: True ) \u2013 Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (time and location). **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/multi_temporal_crop_classification.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = MultiTemporalCropClassification . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , drop_last : bool = True , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , expand_temporal_dimension : bool = True , reduce_zero_label : bool = True , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the MultiTemporalCropClassificationDataModule for multi-temporal crop classification. Args: data_root (str): Directory containing the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to MultiTemporalCropClassification.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. drop_last (bool, optional): Whether to drop the last incomplete batch during training. Defaults to True. no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0. no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1. expand_temporal_dimension (bool, optional): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True. reduce_zero_label (bool, optional): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True. use_metadata (bool): Whether to return metadata info (time and location). **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( MultiTemporalCropClassification , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . means = [ MEANS [ b ] for b in bands ] self . stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = Normalize ( self . means , self . stds ) self . drop_last = drop_last self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . expand_temporal_dimension = expand_temporal_dimension self . reduce_zero_label = reduce_zero_label self . use_metadata = use_metadata setup ( stage ) # Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/multi_temporal_crop_classification.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) terratorch.datamodules.open_sentinel_map # OpenSentinelMapDataModule # Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Open Sentinel Map. Source code in terratorch/datamodules/open_sentinel_map.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 class OpenSentinelMapDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Open Sentinel Map.\"\"\" def __init__ ( self , bands : list [ str ] | None = None , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , spatial_interpolate_and_stack_temporally : bool = True , # noqa: FBT001, FBT002 pad_image : int | None = None , truncate_image : int | None = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the OpenSentinelMapDataModule for the Open Sentinel Map dataset. Args: bands (list[str] | None, optional): List of bands to use. Defaults to None. batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. spatial_interpolate_and_stack_temporally (bool, optional): If True, the bands are interpolated and concatenated over time. Default is True. pad_image (int | None, optional): Number of timesteps to pad the time dimension of the image. If None, no padding is applied. truncate_image (int | None, optional): Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( OpenSentinelMap , batch_size = batch_size , num_workers = num_workers , ** kwargs , ) self . bands = bands self . spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally self . pad_image = pad_image self . truncate_image = truncate_image self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . kwargs = kwargs def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = OpenSentinelMap ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = OpenSentinelMap ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , ) if stage in [ \"test\" ]: self . test_dataset = OpenSentinelMap ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , ) if stage in [ \"predict\" ]: self . predict_dataset = OpenSentinelMap ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , ) __init__ ( bands = None , batch_size = 8 , num_workers = 0 , data_root = './' , train_transform = None , val_transform = None , test_transform = None , predict_transform = None , spatial_interpolate_and_stack_temporally = True , pad_image = None , truncate_image = None , ** kwargs ) # Initializes the OpenSentinelMapDataModule for the Open Sentinel Map dataset. Parameters: bands ( list [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. spatial_interpolate_and_stack_temporally ( bool , default: True ) \u2013 If True, the bands are interpolated and concatenated over time. Default is True. pad_image ( int | None , default: None ) \u2013 Number of timesteps to pad the time dimension of the image. If None, no padding is applied. truncate_image ( int | None , default: None ) \u2013 Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/open_sentinel_map.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , bands : list [ str ] | None = None , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , spatial_interpolate_and_stack_temporally : bool = True , # noqa: FBT001, FBT002 pad_image : int | None = None , truncate_image : int | None = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the OpenSentinelMapDataModule for the Open Sentinel Map dataset. Args: bands (list[str] | None, optional): List of bands to use. Defaults to None. batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. spatial_interpolate_and_stack_temporally (bool, optional): If True, the bands are interpolated and concatenated over time. Default is True. pad_image (int | None, optional): Number of timesteps to pad the time dimension of the image. If None, no padding is applied. truncate_image (int | None, optional): Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( OpenSentinelMap , batch_size = batch_size , num_workers = num_workers , ** kwargs , ) self . bands = bands self . spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally self . pad_image = pad_image self . truncate_image = truncate_image self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . kwargs = kwargs setup ( stage ) # Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/open_sentinel_map.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = OpenSentinelMap ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = OpenSentinelMap ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , ) if stage in [ \"test\" ]: self . test_dataset = OpenSentinelMap ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , ) if stage in [ \"predict\" ]: self . predict_dataset = OpenSentinelMap ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , ) terratorch.datamodules.openearthmap # OpenEarthMapNonGeoDataModule # Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Open Earth Map. Source code in terratorch/datamodules/openearthmap.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class OpenEarthMapNonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Open Earth Map.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , ** kwargs : Any ) -> None : \"\"\" Initializes the OpenEarthMapNonGeoDataModule for the Open Earth Map dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation pipeline; if None, defaults to normalization using computed means and stds. **kwargs: Additional keyword arguments. Can include 'bands' (list[str]) to specify the bands; defaults to OpenEarthMapNonGeo.all_band_names if not provided. \"\"\" super () . __init__ ( OpenEarthMapNonGeo , batch_size , num_workers , ** kwargs ) bands = kwargs . get ( \"bands\" , OpenEarthMapNonGeo . all_band_names ) self . means = torch . tensor ([ MEANS [ b ] for b in bands ]) self . stds = torch . tensor ([ STDS [ b ] for b in bands ]) self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . aug = AugmentationSequential ( K . Normalize ( self . means , self . stds ), data_keys = [ \"image\" ]) if aug is None else aug def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , ** self . kwargs ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , ** self . kwargs ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , ** self . kwargs ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , ** self . kwargs ) __init__ ( batch_size = 8 , num_workers = 0 , data_root = './' , train_transform = None , val_transform = None , test_transform = None , predict_transform = None , aug = None , ** kwargs ) # Initializes the OpenEarthMapNonGeoDataModule for the Open Earth Map dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for test data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. aug ( AugmentationSequential , default: None ) \u2013 Augmentation pipeline; if None, defaults to normalization using computed means and stds. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Can include 'bands' (list[str]) to specify the bands; defaults to OpenEarthMapNonGeo.all_band_names if not provided. Source code in terratorch/datamodules/openearthmap.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , ** kwargs : Any ) -> None : \"\"\" Initializes the OpenEarthMapNonGeoDataModule for the Open Earth Map dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation pipeline; if None, defaults to normalization using computed means and stds. **kwargs: Additional keyword arguments. Can include 'bands' (list[str]) to specify the bands; defaults to OpenEarthMapNonGeo.all_band_names if not provided. \"\"\" super () . __init__ ( OpenEarthMapNonGeo , batch_size , num_workers , ** kwargs ) bands = kwargs . get ( \"bands\" , OpenEarthMapNonGeo . all_band_names ) self . means = torch . tensor ([ MEANS [ b ] for b in bands ]) self . stds = torch . tensor ([ STDS [ b ] for b in bands ]) self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . aug = AugmentationSequential ( K . Normalize ( self . means , self . stds ), data_keys = [ \"image\" ]) if aug is None else aug setup ( stage ) # Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/openearthmap.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , ** self . kwargs ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , ** self . kwargs ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , ** self . kwargs ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , ** self . kwargs ) terratorch.datamodules.pastis # PASTISDataModule # Bases: NonGeoDataModule NonGeo LightningDataModule implementation for PASTIS. Source code in terratorch/datamodules/pastis.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class PASTISDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for PASTIS.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , truncate_image : int | None = None , pad_image : int | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the PASTISDataModule for the PASTIS dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Directory containing the dataset. Defaults to \"./\". truncate_image (int, optional): Truncate the time dimension of the image to a specified number of timesteps. If None, no truncation is performed. pad_image (int, optional): Pad the time dimension of the image to a specified number of timesteps. If None, no padding is applied. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( PASTIS , batch_size = batch_size , num_workers = num_workers , ** kwargs , ) self . truncate_image = truncate_image self . pad_image = pad_image self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . kwargs = kwargs def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = PASTIS ( folds = [ 1 , 2 , 3 ], data_root = self . data_root , transform = self . train_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = PASTIS ( folds = [ 4 ], data_root = self . data_root , transform = self . val_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , ) if stage in [ \"test\" ]: self . test_dataset = PASTIS ( folds = [ 5 ], data_root = self . data_root , transform = self . test_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , ) if stage in [ \"predict\" ]: self . predict_dataset = PASTIS ( folds = [ 5 ], data_root = self . data_root , transform = self . predict_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , ) __init__ ( batch_size = 8 , num_workers = 0 , data_root = './' , truncate_image = None , pad_image = None , train_transform = None , val_transform = None , test_transform = None , predict_transform = None , ** kwargs ) # Initializes the PASTISDataModule for the PASTIS dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Directory containing the dataset. Defaults to \"./\". truncate_image ( int , default: None ) \u2013 Truncate the time dimension of the image to a specified number of timesteps. If None, no truncation is performed. pad_image ( int , default: None ) \u2013 Pad the time dimension of the image to a specified number of timesteps. If None, no padding is applied. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/pastis.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , truncate_image : int | None = None , pad_image : int | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the PASTISDataModule for the PASTIS dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Directory containing the dataset. Defaults to \"./\". truncate_image (int, optional): Truncate the time dimension of the image to a specified number of timesteps. If None, no truncation is performed. pad_image (int, optional): Pad the time dimension of the image to a specified number of timesteps. If None, no padding is applied. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( PASTIS , batch_size = batch_size , num_workers = num_workers , ** kwargs , ) self . truncate_image = truncate_image self . pad_image = pad_image self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . kwargs = kwargs setup ( stage ) # Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/pastis.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = PASTIS ( folds = [ 1 , 2 , 3 ], data_root = self . data_root , transform = self . train_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = PASTIS ( folds = [ 4 ], data_root = self . data_root , transform = self . val_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , ) if stage in [ \"test\" ]: self . test_dataset = PASTIS ( folds = [ 5 ], data_root = self . data_root , transform = self . test_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , ) if stage in [ \"predict\" ]: self . predict_dataset = PASTIS ( folds = [ 5 ], data_root = self . data_root , transform = self . predict_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , ) terratorch.datamodules.sen1floods11 # Sen1Floods11NonGeoDataModule # Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Fire Scars. Source code in terratorch/datamodules/sen1floods11.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 class Sen1Floods11NonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Fire Scars.\"\"\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = Sen1Floods11NonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , drop_last : bool = True , constant_scale : float = 0.0001 , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the Sen1Floods11NonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to Sen1Floods11NonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True. constant_scale (float, optional): Scale constant applied to the dataset. Defaults to 0.0001. no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0. no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1. use_metadata (bool): Whether to return metadata info (time and location). **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( Sen1Floods11NonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = [ MEANS [ b ] for b in bands ] stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = AugmentationSequential ( K . Normalize ( means , stds ), data_keys = [ \"image\" ]) self . drop_last = drop_last self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) def _dataloader_factory ( self , split : str ) -> DataLoader [ dict [ str , Tensor ]]: \"\"\"Implement one or more PyTorch DataLoaders. Args: split: Either 'train', 'val', 'test', or 'predict'. Returns: A collection of data loaders specifying samples. Raises: MisconfigurationException: If :meth:`setup` does not define a dataset or sampler, or if the dataset or sampler has length 0. \"\"\" dataset = self . _valid_attribute ( f \" { split } _dataset\" , \"dataset\" ) batch_size = self . _valid_attribute ( f \" { split } _batch_size\" , \"batch_size\" ) return DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = split == \"train\" , num_workers = self . num_workers , collate_fn = self . collate_fn , drop_last = split == \"train\" and self . drop_last , ) __init__ ( data_root , batch_size = 4 , num_workers = 0 , bands = Sen1Floods11NonGeo . all_band_names , train_transform = None , val_transform = None , test_transform = None , predict_transform = None , drop_last = True , constant_scale = 0.0001 , no_data_replace = 0 , no_label_replace =- 1 , use_metadata = False , ** kwargs ) # Initializes the Sen1Floods11NonGeoDataModule. Parameters: data_root ( str ) \u2013 Root directory of the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for DataLoaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. bands ( Sequence [ str ] , default: all_band_names ) \u2013 List of bands to use. Defaults to Sen1Floods11NonGeo.all_band_names. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for test data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. drop_last ( bool , default: True ) \u2013 Whether to drop the last incomplete batch. Defaults to True. constant_scale ( float , default: 0.0001 ) \u2013 Scale constant applied to the dataset. Defaults to 0.0001. no_data_replace ( float | None , default: 0 ) \u2013 Replacement value for missing data. Defaults to 0. no_label_replace ( int | None , default: -1 ) \u2013 Replacement value for missing labels. Defaults to -1. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (time and location). **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/sen1floods11.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = Sen1Floods11NonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , drop_last : bool = True , constant_scale : float = 0.0001 , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the Sen1Floods11NonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to Sen1Floods11NonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True. constant_scale (float, optional): Scale constant applied to the dataset. Defaults to 0.0001. no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0. no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1. use_metadata (bool): Whether to return metadata info (time and location). **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( Sen1Floods11NonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = [ MEANS [ b ] for b in bands ] stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = AugmentationSequential ( K . Normalize ( means , stds ), data_keys = [ \"image\" ]) self . drop_last = drop_last self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata setup ( stage ) # Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/sen1floods11.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) terratorch.datamodules.sen4agrinet # Sen4AgriNetDataModule # Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Sen4AgriNet. Source code in terratorch/datamodules/sen4agrinet.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class Sen4AgriNetDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Sen4AgriNet.\"\"\" def __init__ ( self , bands : list [ str ] | None = None , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , seed : int = 42 , scenario : str = \"random\" , requires_norm : bool = True , binary_labels : bool = False , linear_encoder : dict = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the Sen4AgriNetDataModule for the Sen4AgriNet dataset. Args: bands (list[str] | None, optional): List of bands to use. Defaults to None. batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. seed (int, optional): Random seed for reproducibility. Defaults to 42. scenario (str): Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020). requires_norm (bool, optional): Whether normalization is required. Defaults to True. binary_labels (bool, optional): Whether to use binary labels. Defaults to False. linear_encoder (dict, optional): Mapping for label encoding. Defaults to None. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( Sen4AgriNet , batch_size = batch_size , num_workers = num_workers , ** kwargs , ) self . bands = bands self . seed = seed self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . scenario = scenario self . requires_norm = requires_norm self . binary_labels = binary_labels self . linear_encoder = linear_encoder self . kwargs = kwargs def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = Sen4AgriNet ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = Sen4AgriNet ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , ) if stage in [ \"test\" ]: self . test_dataset = Sen4AgriNet ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , ) if stage in [ \"predict\" ]: self . predict_dataset = Sen4AgriNet ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , ) __init__ ( bands = None , batch_size = 8 , num_workers = 0 , data_root = './' , train_transform = None , val_transform = None , test_transform = None , predict_transform = None , seed = 42 , scenario = 'random' , requires_norm = True , binary_labels = False , linear_encoder = None , ** kwargs ) # Initializes the Sen4AgriNetDataModule for the Sen4AgriNet dataset. Parameters: bands ( list [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for test data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. seed ( int , default: 42 ) \u2013 Random seed for reproducibility. Defaults to 42. scenario ( str , default: 'random' ) \u2013 Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020). requires_norm ( bool , default: True ) \u2013 Whether normalization is required. Defaults to True. binary_labels ( bool , default: False ) \u2013 Whether to use binary labels. Defaults to False. linear_encoder ( dict , default: None ) \u2013 Mapping for label encoding. Defaults to None. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/sen4agrinet.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , bands : list [ str ] | None = None , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , seed : int = 42 , scenario : str = \"random\" , requires_norm : bool = True , binary_labels : bool = False , linear_encoder : dict = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the Sen4AgriNetDataModule for the Sen4AgriNet dataset. Args: bands (list[str] | None, optional): List of bands to use. Defaults to None. batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. seed (int, optional): Random seed for reproducibility. Defaults to 42. scenario (str): Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020). requires_norm (bool, optional): Whether normalization is required. Defaults to True. binary_labels (bool, optional): Whether to use binary labels. Defaults to False. linear_encoder (dict, optional): Mapping for label encoding. Defaults to None. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( Sen4AgriNet , batch_size = batch_size , num_workers = num_workers , ** kwargs , ) self . bands = bands self . seed = seed self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . scenario = scenario self . requires_norm = requires_norm self . binary_labels = binary_labels self . linear_encoder = linear_encoder self . kwargs = kwargs setup ( stage ) # Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/sen4agrinet.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = Sen4AgriNet ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = Sen4AgriNet ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , ) if stage in [ \"test\" ]: self . test_dataset = Sen4AgriNet ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , ) if stage in [ \"predict\" ]: self . predict_dataset = Sen4AgriNet ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , ) terratorch.datamodules.sen4map # Sen4MapLucasDataModule # Bases: LightningDataModule NonGeo LightningDataModule implementation for Sen4map. Source code in terratorch/datamodules/sen4map.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class Sen4MapLucasDataModule ( pl . LightningDataModule ): \"\"\"NonGeo LightningDataModule implementation for Sen4map.\"\"\" def __init__ ( self , batch_size , num_workers , prefetch_factor = 0 , # dataset_bands:list[HLSBands|int] = None, # input_bands:list[HLSBands|int] = None, train_hdf5_path = None , train_hdf5_keys_path = None , test_hdf5_path = None , test_hdf5_keys_path = None , val_hdf5_path = None , val_hdf5_keys_path = None , ** kwargs ): \"\"\" Initializes the Sen4MapLucasDataModule for handling Sen4Map monthly composites. Args: batch_size (int): Batch size for DataLoaders. num_workers (int): Number of worker processes for data loading. prefetch_factor (int, optional): Number of samples to prefetch per worker. Defaults to 0. train_hdf5_path (str, optional): Path to the training HDF5 file. train_hdf5_keys_path (str, optional): Path to the training HDF5 keys file. test_hdf5_path (str, optional): Path to the testing HDF5 file. test_hdf5_keys_path (str, optional): Path to the testing HDF5 keys file. val_hdf5_path (str, optional): Path to the validation HDF5 file. val_hdf5_keys_path (str, optional): Path to the validation HDF5 keys file. train_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated train keys. test_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated test keys. val_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated validation keys. shuffle (bool, optional): Global shuffle flag. train_shuffle (bool, optional): Shuffle flag for training data; defaults to global shuffle if unset. val_shuffle (bool, optional): Shuffle flag for validation data. test_shuffle (bool, optional): Shuffle flag for test data. train_data_fraction (float, optional): Fraction of training data to use. Defaults to 1.0. val_data_fraction (float, optional): Fraction of validation data to use. Defaults to 1.0. test_data_fraction (float, optional): Fraction of test data to use. Defaults to 1.0. all_hdf5_data_path (str, optional): General HDF5 data path for all splits. If provided, overrides specific paths. resize (bool, optional): Whether to resize images. Defaults to False. resize_to (int or tuple, optional): Target size for resizing images. resize_interpolation (str, optional): Interpolation mode for resizing ('bilinear', 'bicubic', etc.). resize_antialiasing (bool, optional): Whether to apply antialiasing during resizing. Defaults to True. **kwargs: Additional keyword arguments. \"\"\" self . prepare_data_per_node = False self . _log_hyperparams = None self . allow_zero_length_dataloader_with_multiple_devices = False self . batch_size = batch_size self . num_workers = num_workers self . prefetch_factor = prefetch_factor self . train_hdf5_path = train_hdf5_path self . test_hdf5_path = test_hdf5_path self . val_hdf5_path = val_hdf5_path self . train_hdf5_keys_path = train_hdf5_keys_path self . test_hdf5_keys_path = test_hdf5_keys_path self . val_hdf5_keys_path = val_hdf5_keys_path if train_hdf5_path and not train_hdf5_keys_path : print ( f \"Train dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\" ) if test_hdf5_path and not test_hdf5_keys_path : print ( f \"Test dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\" ) if val_hdf5_path and not val_hdf5_keys_path : print ( f \"Val dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\" ) self . train_hdf5_keys_save_path = kwargs . pop ( \"train_hdf5_keys_save_path\" , None ) self . test_hdf5_keys_save_path = kwargs . pop ( \"test_hdf5_keys_save_path\" , None ) self . val_hdf5_keys_save_path = kwargs . pop ( \"val_hdf5_keys_save_path\" , None ) self . shuffle = kwargs . pop ( \"shuffle\" , None ) self . train_shuffle = kwargs . pop ( \"train_shuffle\" , None ) or self . shuffle self . val_shuffle = kwargs . pop ( \"val_shuffle\" , None ) self . test_shuffle = kwargs . pop ( \"test_shuffle\" , None ) self . train_data_fraction = kwargs . pop ( \"train_data_fraction\" , 1.0 ) self . val_data_fraction = kwargs . pop ( \"val_data_fraction\" , 1.0 ) self . test_data_fraction = kwargs . pop ( \"test_data_fraction\" , 1.0 ) if self . train_data_fraction != 1.0 and not train_hdf5_keys_path : raise ValueError ( f \"train_data_fraction provided as non-unity but train_hdf5_keys_path is unset.\" ) if self . val_data_fraction != 1.0 and not val_hdf5_keys_path : raise ValueError ( f \"val_data_fraction provided as non-unity but val_hdf5_keys_path is unset.\" ) if self . test_data_fraction != 1.0 and not test_hdf5_keys_path : raise ValueError ( f \"test_data_fraction provided as non-unity but test_hdf5_keys_path is unset.\" ) all_hdf5_data_path = kwargs . pop ( \"all_hdf5_data_path\" , None ) if all_hdf5_data_path is not None : print ( f \"all_hdf5_data_path provided, will be interpreted as the general data path for all splits. \\n Keys in provided train_hdf5_keys_path assumed to encompass all keys for entire data. Validation and Test keys will be subtracted from Train keys.\" ) if self . train_hdf5_path : raise ValueError ( f \"Both general all_hdf5_data_path provided and a specific train_hdf5_path, remove the train_hdf5_path\" ) if self . val_hdf5_path : raise ValueError ( f \"Both general all_hdf5_data_path provided and a specific val_hdf5_path, remove the val_hdf5_path\" ) if self . test_hdf5_path : raise ValueError ( f \"Both general all_hdf5_data_path provided and a specific test_hdf5_path, remove the test_hdf5_path\" ) self . train_hdf5_path = all_hdf5_data_path self . val_hdf5_path = all_hdf5_data_path self . test_hdf5_path = all_hdf5_data_path self . reduce_train_keys = True else : self . reduce_train_keys = False self . resize = kwargs . pop ( \"resize\" , False ) self . resize_to = kwargs . pop ( \"resize_to\" , None ) if self . resize and self . resize_to is None : raise ValueError ( f \"Config provided resize as True, but resize_to parameter not given\" ) self . resize_interpolation = kwargs . pop ( \"resize_interpolation\" , None ) if self . resize and self . resize_interpolation is None : print ( f \"Config provided resize as True, but resize_interpolation mode not given. Will assume default bilinear\" ) self . resize_interpolation = \"bilinear\" interpolation_dict = { \"bilinear\" : InterpolationMode . BILINEAR , \"bicubic\" : InterpolationMode . BICUBIC , \"nearest\" : InterpolationMode . NEAREST , \"nearest_exact\" : InterpolationMode . NEAREST_EXACT } if self . resize : if self . resize_interpolation not in interpolation_dict . keys (): raise ValueError ( f \"resize_interpolation provided as { self . resize_interpolation } , but valid options are: { interpolation_dict . keys () } \" ) self . resize_interpolation = interpolation_dict [ self . resize_interpolation ] self . resize_antialiasing = kwargs . pop ( \"resize_antialiasing\" , True ) self . kwargs = kwargs def _load_hdf5_keys_from_path ( self , path , fraction = 1.0 ): if path is None : return None with open ( path , \"rb\" ) as f : keys = pickle . load ( f ) return keys [: int ( fraction * len ( keys ))] def setup ( self , stage : str ): \"\"\"Set up datasets. Args: stage: Either fit, test. \"\"\" if stage == \"fit\" : train_keys = self . _load_hdf5_keys_from_path ( self . train_hdf5_keys_path , fraction = self . train_data_fraction ) val_keys = self . _load_hdf5_keys_from_path ( self . val_hdf5_keys_path , fraction = self . val_data_fraction ) if self . reduce_train_keys : test_keys = self . _load_hdf5_keys_from_path ( self . test_hdf5_keys_path , fraction = self . test_data_fraction ) train_keys = list ( set ( train_keys ) - set ( val_keys ) - set ( test_keys )) train_file = h5py . File ( self . train_hdf5_path , 'r' ) self . lucasS2_train = Sen4MapDatasetMonthlyComposites ( train_file , h5data_keys = train_keys , resize = self . resize , resize_to = self . resize_to , resize_interpolation = self . resize_interpolation , resize_antialiasing = self . resize_antialiasing , save_keys_path = self . train_hdf5_keys_save_path , ** self . kwargs ) val_file = h5py . File ( self . val_hdf5_path , 'r' ) self . lucasS2_val = Sen4MapDatasetMonthlyComposites ( val_file , h5data_keys = val_keys , resize = self . resize , resize_to = self . resize_to , resize_interpolation = self . resize_interpolation , resize_antialiasing = self . resize_antialiasing , save_keys_path = self . val_hdf5_keys_save_path , ** self . kwargs ) if stage == \"test\" : test_file = h5py . File ( self . test_hdf5_path , 'r' ) test_keys = self . _load_hdf5_keys_from_path ( self . test_hdf5_keys_path , fraction = self . test_data_fraction ) self . lucasS2_test = Sen4MapDatasetMonthlyComposites ( test_file , h5data_keys = test_keys , resize = self . resize , resize_to = self . resize_to , resize_interpolation = self . resize_interpolation , resize_antialiasing = self . resize_antialiasing , save_keys_path = self . test_hdf5_keys_save_path , ** self . kwargs ) def train_dataloader ( self ): return DataLoader ( self . lucasS2_train , batch_size = self . batch_size , num_workers = self . num_workers , prefetch_factor = self . prefetch_factor , shuffle = self . train_shuffle ) def val_dataloader ( self ): return DataLoader ( self . lucasS2_val , batch_size = self . batch_size , num_workers = self . num_workers , prefetch_factor = self . prefetch_factor , shuffle = self . val_shuffle ) def test_dataloader ( self ): return DataLoader ( self . lucasS2_test , batch_size = self . batch_size , num_workers = self . num_workers , prefetch_factor = self . prefetch_factor , shuffle = self . test_shuffle ) __init__ ( batch_size , num_workers , prefetch_factor = 0 , train_hdf5_path = None , train_hdf5_keys_path = None , test_hdf5_path = None , test_hdf5_keys_path = None , val_hdf5_path = None , val_hdf5_keys_path = None , ** kwargs ) # Initializes the Sen4MapLucasDataModule for handling Sen4Map monthly composites. Parameters: batch_size ( int ) \u2013 Batch size for DataLoaders. num_workers ( int ) \u2013 Number of worker processes for data loading. prefetch_factor ( int , default: 0 ) \u2013 Number of samples to prefetch per worker. Defaults to 0. train_hdf5_path ( str , default: None ) \u2013 Path to the training HDF5 file. train_hdf5_keys_path ( str , default: None ) \u2013 Path to the training HDF5 keys file. test_hdf5_path ( str , default: None ) \u2013 Path to the testing HDF5 file. test_hdf5_keys_path ( str , default: None ) \u2013 Path to the testing HDF5 keys file. val_hdf5_path ( str , default: None ) \u2013 Path to the validation HDF5 file. val_hdf5_keys_path ( str , default: None ) \u2013 Path to the validation HDF5 keys file. train_hdf5_keys_save_path ( str ) \u2013 (from kwargs) Path to save generated train keys. test_hdf5_keys_save_path ( str ) \u2013 (from kwargs) Path to save generated test keys. val_hdf5_keys_save_path ( str ) \u2013 (from kwargs) Path to save generated validation keys. shuffle ( bool ) \u2013 Global shuffle flag. train_shuffle ( bool ) \u2013 Shuffle flag for training data; defaults to global shuffle if unset. val_shuffle ( bool ) \u2013 Shuffle flag for validation data. test_shuffle ( bool ) \u2013 Shuffle flag for test data. train_data_fraction ( float ) \u2013 Fraction of training data to use. Defaults to 1.0. val_data_fraction ( float ) \u2013 Fraction of validation data to use. Defaults to 1.0. test_data_fraction ( float ) \u2013 Fraction of test data to use. Defaults to 1.0. all_hdf5_data_path ( str ) \u2013 General HDF5 data path for all splits. If provided, overrides specific paths. resize ( bool ) \u2013 Whether to resize images. Defaults to False. resize_to ( int or tuple ) \u2013 Target size for resizing images. resize_interpolation ( str ) \u2013 Interpolation mode for resizing ('bilinear', 'bicubic', etc.). resize_antialiasing ( bool ) \u2013 Whether to apply antialiasing during resizing. Defaults to True. **kwargs \u2013 Additional keyword arguments. Source code in terratorch/datamodules/sen4map.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def __init__ ( self , batch_size , num_workers , prefetch_factor = 0 , # dataset_bands:list[HLSBands|int] = None, # input_bands:list[HLSBands|int] = None, train_hdf5_path = None , train_hdf5_keys_path = None , test_hdf5_path = None , test_hdf5_keys_path = None , val_hdf5_path = None , val_hdf5_keys_path = None , ** kwargs ): \"\"\" Initializes the Sen4MapLucasDataModule for handling Sen4Map monthly composites. Args: batch_size (int): Batch size for DataLoaders. num_workers (int): Number of worker processes for data loading. prefetch_factor (int, optional): Number of samples to prefetch per worker. Defaults to 0. train_hdf5_path (str, optional): Path to the training HDF5 file. train_hdf5_keys_path (str, optional): Path to the training HDF5 keys file. test_hdf5_path (str, optional): Path to the testing HDF5 file. test_hdf5_keys_path (str, optional): Path to the testing HDF5 keys file. val_hdf5_path (str, optional): Path to the validation HDF5 file. val_hdf5_keys_path (str, optional): Path to the validation HDF5 keys file. train_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated train keys. test_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated test keys. val_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated validation keys. shuffle (bool, optional): Global shuffle flag. train_shuffle (bool, optional): Shuffle flag for training data; defaults to global shuffle if unset. val_shuffle (bool, optional): Shuffle flag for validation data. test_shuffle (bool, optional): Shuffle flag for test data. train_data_fraction (float, optional): Fraction of training data to use. Defaults to 1.0. val_data_fraction (float, optional): Fraction of validation data to use. Defaults to 1.0. test_data_fraction (float, optional): Fraction of test data to use. Defaults to 1.0. all_hdf5_data_path (str, optional): General HDF5 data path for all splits. If provided, overrides specific paths. resize (bool, optional): Whether to resize images. Defaults to False. resize_to (int or tuple, optional): Target size for resizing images. resize_interpolation (str, optional): Interpolation mode for resizing ('bilinear', 'bicubic', etc.). resize_antialiasing (bool, optional): Whether to apply antialiasing during resizing. Defaults to True. **kwargs: Additional keyword arguments. \"\"\" self . prepare_data_per_node = False self . _log_hyperparams = None self . allow_zero_length_dataloader_with_multiple_devices = False self . batch_size = batch_size self . num_workers = num_workers self . prefetch_factor = prefetch_factor self . train_hdf5_path = train_hdf5_path self . test_hdf5_path = test_hdf5_path self . val_hdf5_path = val_hdf5_path self . train_hdf5_keys_path = train_hdf5_keys_path self . test_hdf5_keys_path = test_hdf5_keys_path self . val_hdf5_keys_path = val_hdf5_keys_path if train_hdf5_path and not train_hdf5_keys_path : print ( f \"Train dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\" ) if test_hdf5_path and not test_hdf5_keys_path : print ( f \"Test dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\" ) if val_hdf5_path and not val_hdf5_keys_path : print ( f \"Val dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\" ) self . train_hdf5_keys_save_path = kwargs . pop ( \"train_hdf5_keys_save_path\" , None ) self . test_hdf5_keys_save_path = kwargs . pop ( \"test_hdf5_keys_save_path\" , None ) self . val_hdf5_keys_save_path = kwargs . pop ( \"val_hdf5_keys_save_path\" , None ) self . shuffle = kwargs . pop ( \"shuffle\" , None ) self . train_shuffle = kwargs . pop ( \"train_shuffle\" , None ) or self . shuffle self . val_shuffle = kwargs . pop ( \"val_shuffle\" , None ) self . test_shuffle = kwargs . pop ( \"test_shuffle\" , None ) self . train_data_fraction = kwargs . pop ( \"train_data_fraction\" , 1.0 ) self . val_data_fraction = kwargs . pop ( \"val_data_fraction\" , 1.0 ) self . test_data_fraction = kwargs . pop ( \"test_data_fraction\" , 1.0 ) if self . train_data_fraction != 1.0 and not train_hdf5_keys_path : raise ValueError ( f \"train_data_fraction provided as non-unity but train_hdf5_keys_path is unset.\" ) if self . val_data_fraction != 1.0 and not val_hdf5_keys_path : raise ValueError ( f \"val_data_fraction provided as non-unity but val_hdf5_keys_path is unset.\" ) if self . test_data_fraction != 1.0 and not test_hdf5_keys_path : raise ValueError ( f \"test_data_fraction provided as non-unity but test_hdf5_keys_path is unset.\" ) all_hdf5_data_path = kwargs . pop ( \"all_hdf5_data_path\" , None ) if all_hdf5_data_path is not None : print ( f \"all_hdf5_data_path provided, will be interpreted as the general data path for all splits. \\n Keys in provided train_hdf5_keys_path assumed to encompass all keys for entire data. Validation and Test keys will be subtracted from Train keys.\" ) if self . train_hdf5_path : raise ValueError ( f \"Both general all_hdf5_data_path provided and a specific train_hdf5_path, remove the train_hdf5_path\" ) if self . val_hdf5_path : raise ValueError ( f \"Both general all_hdf5_data_path provided and a specific val_hdf5_path, remove the val_hdf5_path\" ) if self . test_hdf5_path : raise ValueError ( f \"Both general all_hdf5_data_path provided and a specific test_hdf5_path, remove the test_hdf5_path\" ) self . train_hdf5_path = all_hdf5_data_path self . val_hdf5_path = all_hdf5_data_path self . test_hdf5_path = all_hdf5_data_path self . reduce_train_keys = True else : self . reduce_train_keys = False self . resize = kwargs . pop ( \"resize\" , False ) self . resize_to = kwargs . pop ( \"resize_to\" , None ) if self . resize and self . resize_to is None : raise ValueError ( f \"Config provided resize as True, but resize_to parameter not given\" ) self . resize_interpolation = kwargs . pop ( \"resize_interpolation\" , None ) if self . resize and self . resize_interpolation is None : print ( f \"Config provided resize as True, but resize_interpolation mode not given. Will assume default bilinear\" ) self . resize_interpolation = \"bilinear\" interpolation_dict = { \"bilinear\" : InterpolationMode . BILINEAR , \"bicubic\" : InterpolationMode . BICUBIC , \"nearest\" : InterpolationMode . NEAREST , \"nearest_exact\" : InterpolationMode . NEAREST_EXACT } if self . resize : if self . resize_interpolation not in interpolation_dict . keys (): raise ValueError ( f \"resize_interpolation provided as { self . resize_interpolation } , but valid options are: { interpolation_dict . keys () } \" ) self . resize_interpolation = interpolation_dict [ self . resize_interpolation ] self . resize_antialiasing = kwargs . pop ( \"resize_antialiasing\" , True ) self . kwargs = kwargs setup ( stage ) # Set up datasets. Parameters: stage ( str ) \u2013 Either fit, test. Source code in terratorch/datamodules/sen4map.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def setup ( self , stage : str ): \"\"\"Set up datasets. Args: stage: Either fit, test. \"\"\" if stage == \"fit\" : train_keys = self . _load_hdf5_keys_from_path ( self . train_hdf5_keys_path , fraction = self . train_data_fraction ) val_keys = self . _load_hdf5_keys_from_path ( self . val_hdf5_keys_path , fraction = self . val_data_fraction ) if self . reduce_train_keys : test_keys = self . _load_hdf5_keys_from_path ( self . test_hdf5_keys_path , fraction = self . test_data_fraction ) train_keys = list ( set ( train_keys ) - set ( val_keys ) - set ( test_keys )) train_file = h5py . File ( self . train_hdf5_path , 'r' ) self . lucasS2_train = Sen4MapDatasetMonthlyComposites ( train_file , h5data_keys = train_keys , resize = self . resize , resize_to = self . resize_to , resize_interpolation = self . resize_interpolation , resize_antialiasing = self . resize_antialiasing , save_keys_path = self . train_hdf5_keys_save_path , ** self . kwargs ) val_file = h5py . File ( self . val_hdf5_path , 'r' ) self . lucasS2_val = Sen4MapDatasetMonthlyComposites ( val_file , h5data_keys = val_keys , resize = self . resize , resize_to = self . resize_to , resize_interpolation = self . resize_interpolation , resize_antialiasing = self . resize_antialiasing , save_keys_path = self . val_hdf5_keys_save_path , ** self . kwargs ) if stage == \"test\" : test_file = h5py . File ( self . test_hdf5_path , 'r' ) test_keys = self . _load_hdf5_keys_from_path ( self . test_hdf5_keys_path , fraction = self . test_data_fraction ) self . lucasS2_test = Sen4MapDatasetMonthlyComposites ( test_file , h5data_keys = test_keys , resize = self . resize , resize_to = self . resize_to , resize_interpolation = self . resize_interpolation , resize_antialiasing = self . resize_antialiasing , save_keys_path = self . test_hdf5_keys_save_path , ** self . kwargs ) Transforms # The transforms module provides a set of specialized image transformations designed to manipulate spatial, temporal, and multimodal data efficiently. These transformations allow for greater flexibility when working with multi-temporal, multi-channel, and multi-modal datasets, ensuring that data can be formatted appropriately for different model architectures. terratorch.datasets.transforms # FlattenSamplesIntoChannels # Bases: ImageOnlyTransform FlattenSamplesIntoChannels is an image transformation that merges the sample (and optionally temporal) dimensions into the channel dimension. This transform rearranges an input tensor by flattening the sample dimension, and if specified, also the temporal dimension, thereby concatenating these dimensions into a single channel dimension. Source code in terratorch/datasets/transforms.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class FlattenSamplesIntoChannels ( ImageOnlyTransform ): \"\"\" FlattenSamplesIntoChannels is an image transformation that merges the sample (and optionally temporal) dimensions into the channel dimension. This transform rearranges an input tensor by flattening the sample dimension, and if specified, also the temporal dimension, thereby concatenating these dimensions into a single channel dimension. \"\"\" def __init__ ( self , time_dim : bool = True ): \"\"\" Initialize the FlattenSamplesIntoChannels transform. Args: time_dim (bool): If True, the temporal dimension is included in the flattening process. Default is True. \"\"\" super () . __init__ ( True , 1 ) self . time_dim = time_dim def apply ( self , img , ** params ): if self . time_dim : rearranged = rearrange ( img , \"samples time height width channels -> height width (samples time channels)\" ) else : rearranged = rearrange ( img , \"samples height width channels -> height width (samples channels)\" ) return rearranged def get_transform_init_args_names ( self ): return () __init__ ( time_dim = True ) # Initialize the FlattenSamplesIntoChannels transform. Parameters: time_dim ( bool , default: True ) \u2013 If True, the temporal dimension is included in the flattening process. Default is True. Source code in terratorch/datasets/transforms.py 121 122 123 124 125 126 127 128 129 def __init__ ( self , time_dim : bool = True ): \"\"\" Initialize the FlattenSamplesIntoChannels transform. Args: time_dim (bool): If True, the temporal dimension is included in the flattening process. Default is True. \"\"\" super () . __init__ ( True , 1 ) self . time_dim = time_dim FlattenTemporalIntoChannels # Bases: ImageOnlyTransform FlattenTemporalIntoChannels is an image transformation that flattens the temporal dimension into the channel dimension. This transform rearranges an input tensor with a temporal dimension into one where the time and channel dimensions are merged. It expects the input to have a fixed number of dimensions defined by N_DIMS_FOR_TEMPORAL. Source code in terratorch/datasets/transforms.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class FlattenTemporalIntoChannels ( ImageOnlyTransform ): \"\"\" FlattenTemporalIntoChannels is an image transformation that flattens the temporal dimension into the channel dimension. This transform rearranges an input tensor with a temporal dimension into one where the time and channel dimensions are merged. It expects the input to have a fixed number of dimensions defined by N_DIMS_FOR_TEMPORAL. \"\"\" def __init__ ( self ): \"\"\" Initialize the FlattenTemporalIntoChannels transform. \"\"\" super () . __init__ ( True , 1 ) def apply ( self , img , ** params ): if len ( img . shape ) != N_DIMS_FOR_TEMPORAL : msg = f \"Expected input temporal sequence to have { N_DIMS_FOR_TEMPORAL } dimensions, but got { len ( img . shape ) } \" raise Exception ( msg ) rearranged = rearrange ( img , \"time height width channels -> height width (time channels)\" ) return rearranged def get_transform_init_args_names ( self ): return () __init__ () # Initialize the FlattenTemporalIntoChannels transform. Source code in terratorch/datasets/transforms.py 58 59 60 61 62 def __init__ ( self ): \"\"\" Initialize the FlattenTemporalIntoChannels transform. \"\"\" super () . __init__ ( True , 1 ) MultimodalTransforms # MultimodalTransforms applies albumentations transforms to multiple image modalities. This class supports both shared transformations across modalities and separate transformations for each modality. It also handles non-image modalities by applying a specified non-image transform. Source code in terratorch/datasets/transforms.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 class MultimodalTransforms : \"\"\" MultimodalTransforms applies albumentations transforms to multiple image modalities. This class supports both shared transformations across modalities and separate transformations for each modality. It also handles non-image modalities by applying a specified non-image transform. \"\"\" def __init__ ( self , transforms : dict | A . Compose , shared : bool = True , non_image_modalities : list [ str ] | None = None , non_image_transform : object | None = None , ): \"\"\" Initialize the MultimodalTransforms. Args: transforms (dict or A.Compose): The transformation(s) to apply to the data. shared (bool): If True, the same transform is applied to all modalities; if False, separate transforms are used. non_image_modalities (list[str] | None): List of keys corresponding to non-image modalities. non_image_transform (object | None): A transform to apply to non-image modalities. If None, a default transform is used. \"\"\" self . transforms = transforms self . shared = shared self . non_image_modalities = non_image_modalities self . non_image_transform = non_image_transform or default_non_image_transform def __call__ ( self , data : dict ): if self . shared : # albumentations requires a key 'image' and treats all other keys as additional targets image_modality = list ( set ( data . keys ()) - set ( self . non_image_modalities ))[ 0 ] data [ 'image' ] = data . pop ( image_modality ) data = self . transforms ( ** data ) data [ image_modality ] = data . pop ( 'image' ) # Process sequence data which is ignored by albumentations as 'global_label' for modality in self . non_image_modalities : data [ modality ] = self . non_image_transform ( data [ modality ]) else : # Applies transformations for each modality separate for key , value in data . items (): data [ key ] = self . transforms [ key ]( image = value )[ 'image' ] # Only works with image modalities return data __init__ ( transforms , shared = True , non_image_modalities = None , non_image_transform = None ) # Initialize the MultimodalTransforms. Parameters: transforms ( dict or Compose ) \u2013 The transformation(s) to apply to the data. shared ( bool , default: True ) \u2013 If True, the same transform is applied to all modalities; if False, separate transforms are used. non_image_modalities ( list [ str ] | None , default: None ) \u2013 List of keys corresponding to non-image modalities. non_image_transform ( object | None , default: None ) \u2013 A transform to apply to non-image modalities. If None, a default transform is used. Source code in terratorch/datasets/transforms.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def __init__ ( self , transforms : dict | A . Compose , shared : bool = True , non_image_modalities : list [ str ] | None = None , non_image_transform : object | None = None , ): \"\"\" Initialize the MultimodalTransforms. Args: transforms (dict or A.Compose): The transformation(s) to apply to the data. shared (bool): If True, the same transform is applied to all modalities; if False, separate transforms are used. non_image_modalities (list[str] | None): List of keys corresponding to non-image modalities. non_image_transform (object | None): A transform to apply to non-image modalities. If None, a default transform is used. \"\"\" self . transforms = transforms self . shared = shared self . non_image_modalities = non_image_modalities self . non_image_transform = non_image_transform or default_non_image_transform Padding # Bases: ImageOnlyTransform Padding to adjust (slight) discrepancies between input images Source code in terratorch/datasets/transforms.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class Padding ( ImageOnlyTransform ): \"\"\"Padding to adjust (slight) discrepancies between input images\"\"\" def __init__ ( self , input_shape : list = None ): super () . __init__ ( True , 1 ) self . input_shape = input_shape def apply ( self , img , ** params ): shape = img . shape [ - 2 :] pad_values_ = [ j - i for i , j in zip ( shape , self . input_shape )] if all ([ i % 2 == 0 for i in pad_values_ ]): pad_values = sum ([[ int ( j / 2 ), int ( j / 2 )] for j in pad_values_ ], []) else : pad_values = sum ([[ 0 , j ] for j in pad_values_ ], []) return F . pad ( img , pad_values ) def get_transform_init_args_names ( self ): return () Rearrange # Bases: ImageOnlyTransform Rearrange is a generic image transformation that reshapes an input tensor using a custom einops pattern. This transform allows flexible reordering of tensor dimensions based on the provided pattern and arguments. Source code in terratorch/datasets/transforms.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 class Rearrange ( ImageOnlyTransform ): \"\"\" Rearrange is a generic image transformation that reshapes an input tensor using a custom einops pattern. This transform allows flexible reordering of tensor dimensions based on the provided pattern and arguments. \"\"\" def __init__ ( self , rearrange : str , rearrange_args : dict [ str , int ] | None = None , always_apply : bool = True , p : float = 1 ): \"\"\" Initialize the Rearrange transform. Args: rearrange (str): The einops rearrangement pattern to apply. rearrange_args (dict[str, int] | None): Additional arguments for the rearrangement pattern. always_apply (bool): Whether to always apply this transform. Default is True. p (float): The probability of applying the transform. Default is 1. \"\"\" super () . __init__ ( always_apply , p ) self . rearrange = rearrange self . vars = rearrange_args if rearrange_args else {} def apply ( self , img , ** params ): return rearrange ( img , self . rearrange , ** self . vars ) def get_transform_init_args_names ( self ): return \"rearrange\" __init__ ( rearrange , rearrange_args = None , always_apply = True , p = 1 ) # Initialize the Rearrange transform. Parameters: rearrange ( str ) \u2013 The einops rearrangement pattern to apply. rearrange_args ( dict [ str , int ] | None , default: None ) \u2013 Additional arguments for the rearrangement pattern. always_apply ( bool , default: True ) \u2013 Whether to always apply this transform. Default is True. p ( float , default: 1 ) \u2013 The probability of applying the transform. Default is 1. Source code in terratorch/datasets/transforms.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def __init__ ( self , rearrange : str , rearrange_args : dict [ str , int ] | None = None , always_apply : bool = True , p : float = 1 ): \"\"\" Initialize the Rearrange transform. Args: rearrange (str): The einops rearrangement pattern to apply. rearrange_args (dict[str, int] | None): Additional arguments for the rearrangement pattern. always_apply (bool): Whether to always apply this transform. Default is True. p (float): The probability of applying the transform. Default is 1. \"\"\" super () . __init__ ( always_apply , p ) self . rearrange = rearrange self . vars = rearrange_args if rearrange_args else {} SelectBands # Bases: ImageOnlyTransform SelectBands is an image transformation that selects a subset of bands (channels) from an input image. This transform uses specified band indices to filter and output only the desired channels from the image tensor. Source code in terratorch/datasets/transforms.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 class SelectBands ( ImageOnlyTransform ): \"\"\" SelectBands is an image transformation that selects a subset of bands (channels) from an input image. This transform uses specified band indices to filter and output only the desired channels from the image tensor. \"\"\" def __init__ ( self , band_indices : list [ int ]): \"\"\" Initialize the SelectBands transform. Args: band_indices (list[int]): A list of indices specifying which bands to select. \"\"\" super () . __init__ ( True , 1 ) self . band_indices = band_indices def apply ( self , img , ** params ): return img [ ... , self . band_indices ] def get_transform_init_args_names ( self ): return \"band_indices\" __init__ ( band_indices ) # Initialize the SelectBands transform. Parameters: band_indices ( list [ int ] ) \u2013 A list of indices specifying which bands to select. Source code in terratorch/datasets/transforms.py 242 243 244 245 246 247 248 249 250 def __init__ ( self , band_indices : list [ int ]): \"\"\" Initialize the SelectBands transform. Args: band_indices (list[int]): A list of indices specifying which bands to select. \"\"\" super () . __init__ ( True , 1 ) self . band_indices = band_indices UnflattenSamplesFromChannels # Bases: ImageOnlyTransform UnflattenSamplesFromChannels is an image transformation that restores the sample (and optionally temporal) dimensions from the channel dimension. This transform is designed to reverse the flattening performed by FlattenSamplesIntoChannels and is typically applied after converting images to a channels-first format. Source code in terratorch/datasets/transforms.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 class UnflattenSamplesFromChannels ( ImageOnlyTransform ): \"\"\" UnflattenSamplesFromChannels is an image transformation that restores the sample (and optionally temporal) dimensions from the channel dimension. This transform is designed to reverse the flattening performed by FlattenSamplesIntoChannels and is typically applied after converting images to a channels-first format. \"\"\" def __init__ ( self , time_dim : bool = True , n_samples : int | None = None , n_timesteps : int | None = None , n_channels : int | None = None ): \"\"\" Initialize the UnflattenSamplesFromChannels transform. Args: time_dim (bool): If True, the temporal dimension is considered during unflattening. n_samples (int | None): The number of samples. n_timesteps (int | None): The number of time steps. n_channels (int | None): The number of channels per time step. Raises: Exception: If time_dim is True and fewer than two of n_channels, n_timesteps, and n_samples are provided. Exception: If time_dim is False and neither n_channels nor n_samples is provided. \"\"\" super () . __init__ ( True , 1 ) self . time_dim = time_dim if self . time_dim : if bool ( n_channels ) + bool ( n_timesteps ) + bool ( n_samples ) < 2 : msg = \"Two of n_channels, n_timesteps, and n_channels must be provided\" raise Exception ( msg ) if n_timesteps and n_channels : self . additional_info = { \"channels\" : n_channels , \"time\" : n_timesteps } elif n_timesteps and n_samples : self . additional_info = { \"time\" : n_timesteps , \"samples\" : n_samples } else : self . additional_info = { \"channels\" : n_channels , \"samples\" : n_samples } else : if n_channels is None and n_samples is None : msg = \"One of n_channels or n_samples must be provided\" raise Exception ( msg ) self . additional_info = { \"channels\" : n_channels } if n_channels else { \"samples\" : n_samples } def apply ( self , img , ** params ): if self . time_dim : rearranged = rearrange ( img , \"(samples time channels) height width -> samples channels time height width\" , ** self . additional_info ) else : rearranged = rearrange ( img , \"(samples channels) height width -> samples channels height width\" , ** self . additional_info ) return rearranged def get_transform_init_args_names ( self ): return ( \"n_timesteps\" , \"n_channels\" ) __init__ ( time_dim = True , n_samples = None , n_timesteps = None , n_channels = None ) # Initialize the UnflattenSamplesFromChannels transform. Parameters: time_dim ( bool , default: True ) \u2013 If True, the temporal dimension is considered during unflattening. n_samples ( int | None , default: None ) \u2013 The number of samples. n_timesteps ( int | None , default: None ) \u2013 The number of time steps. n_channels ( int | None , default: None ) \u2013 The number of channels per time step. Raises: Exception \u2013 If time_dim is True and fewer than two of n_channels, n_timesteps, and n_samples are provided. Exception \u2013 If time_dim is False and neither n_channels nor n_samples is provided. Source code in terratorch/datasets/transforms.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def __init__ ( self , time_dim : bool = True , n_samples : int | None = None , n_timesteps : int | None = None , n_channels : int | None = None ): \"\"\" Initialize the UnflattenSamplesFromChannels transform. Args: time_dim (bool): If True, the temporal dimension is considered during unflattening. n_samples (int | None): The number of samples. n_timesteps (int | None): The number of time steps. n_channels (int | None): The number of channels per time step. Raises: Exception: If time_dim is True and fewer than two of n_channels, n_timesteps, and n_samples are provided. Exception: If time_dim is False and neither n_channels nor n_samples is provided. \"\"\" super () . __init__ ( True , 1 ) self . time_dim = time_dim if self . time_dim : if bool ( n_channels ) + bool ( n_timesteps ) + bool ( n_samples ) < 2 : msg = \"Two of n_channels, n_timesteps, and n_channels must be provided\" raise Exception ( msg ) if n_timesteps and n_channels : self . additional_info = { \"channels\" : n_channels , \"time\" : n_timesteps } elif n_timesteps and n_samples : self . additional_info = { \"time\" : n_timesteps , \"samples\" : n_samples } else : self . additional_info = { \"channels\" : n_channels , \"samples\" : n_samples } else : if n_channels is None and n_samples is None : msg = \"One of n_channels or n_samples must be provided\" raise Exception ( msg ) self . additional_info = { \"channels\" : n_channels } if n_channels else { \"samples\" : n_samples } UnflattenTemporalFromChannels # Bases: ImageOnlyTransform UnflattenTemporalFromChannels is an image transformation that restores the temporal dimension from the channel dimension. This transform is typically applied after converting images to a channels-first format (e.g., after ToTensorV2) and rearranges the flattened temporal information back into separate time and channel dimensions. Source code in terratorch/datasets/transforms.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class UnflattenTemporalFromChannels ( ImageOnlyTransform ): \"\"\" UnflattenTemporalFromChannels is an image transformation that restores the temporal dimension from the channel dimension. This transform is typically applied after converting images to a channels-first format (e.g., after ToTensorV2) and rearranges the flattened temporal information back into separate time and channel dimensions. \"\"\" def __init__ ( self , n_timesteps : int | None = None , n_channels : int | None = None ): super () . __init__ ( True , 1 ) \"\"\" Initialize the UnflattenTemporalFromChannels transform. Args: n_timesteps (int | None): The number of time steps. Must be provided if n_channels is not provided. n_channels (int | None): The number of channels per time step. Must be provided if n_timesteps is not provided. Raises: Exception: If neither n_timesteps nor n_channels is provided. \"\"\" if n_timesteps is None and n_channels is None : msg = \"One of n_timesteps or n_channels must be provided\" raise Exception ( msg ) self . additional_info = { \"channels\" : n_channels } if n_channels else { \"time\" : n_timesteps } def apply ( self , img , ** params ): if len ( img . shape ) != N_DIMS_FLATTENED_TEMPORAL : msg = f \"Expected input temporal sequence to have { N_DIMS_FLATTENED_TEMPORAL } dimensions \\ , but got { len ( img . shape ) } \" raise Exception ( msg ) rearranged = rearrange ( img , \"(time channels) height width -> channels time height width\" , ** self . additional_info ) return rearranged def get_transform_init_args_names ( self ): return ( \"n_timesteps\" , \"n_channels\" )","title":"Data"},{"location":"data/#data-processing","text":"In our workflow, we leverage TorchGeo to implement datasets and data modules, ensuring robust and flexible data handling. For a deeper dive into working with datasets using TorchGeo, please refer to the TorchGeo tutorials on datasets . In most cases, it\u2019s best to create a custom TorchGeo dataset tailored to your specific data. Doing so gives you complete control over: - Data Loading: Customize how your data is read and organized. - Transforms: Decide which preprocessing or augmentation steps to apply. - Visualization: Define custom plotting methods (for example, when logging with TensorBoard). TorchGeo offers two primary classes to suit different data formats: - NonGeoDataset : Use this if your dataset is already split into neatly tiled pieces ready for neural network consumption. Essentially, NonGeoDataset is a wrapper around a standard PyTorch dataset, making it straightforward to integrate into your pipeline. - GeoDataset : Opt for this class if your data comes in the form of large GeoTiff files from which you need to sample during training. GeoDataset automatically aligns your input data with corresponding labels and supports a range of geo-aware sampling techniques. In addition to these specialized TorchGeo datasets, TerraTorch offers generic datasets and data modules designed to work with directory-based data structures, similar to those used in MMLab libraries. These generic tools simplify data loading when your data is organized in conventional file directories: - The Generic Pixel-wise Dataset is ideal for tasks where each pixel represents a sample (e.g., segmentation or dense prediction problems). - The Generic Scalar Label Dataset is best suited for classification tasks where each sample is associated with a single label. TerraTorch also provides corresponding generic data modules that bundle the dataset with training, validation, and testing splits, integrating seamlessly with PyTorch Lightning. This arrangement makes it easy to manage data loading, batching, and preprocessing with minimal configuration. While generic datasets offer a quick start for common data structures, many projects require more tailored solutions. Custom datasets and data modules give you complete control over the entire data handling process\u2014from fine-tuned data loading and specific transformations to enhanced visualization. By developing your own dataset and data module classes, you ensure that every step\u2014from data ingestion to final model input\u2014is optimized for your particular use case. TerraTorch\u2019s examples provide an excellent starting point to build these custom components and integrate them seamlessly into your training pipeline. For additional examples on fine-tuning a TerraTorch model using these components, please refer to the Prithvi EO Examples repository.","title":"Data Processing"},{"location":"data/#data-curation","text":"Generally speaking, all the datamodules work by collecting sets of files and concatenating them into batches with a size determined by the user. TerraTorch automatically checks the dimensionality of the files in order to guarantee that they are stackable, otherwise a stackability error will be raised. If you are sure that your data files are in the proper format and do not want to check for stackability, define check_stackability: false in the field data of your yaml file. If you are using the script interface, you just need to pass it as argument to your dataloader class. Alternatively, if you want to fix discrepancies related to dimensionality in your input files at the data loading stage, you can add a pad correction pipeline, as seen in the example tests/resources/configs/manufactured-finetune_prithvi_eo_v2_300_pad_transform.yaml .","title":"Data curation"},{"location":"data/#using-datasets-already-implemented-in-torchgeo","text":"Using existing TorchGeo DataModules is very easy! Just plug them in! For instance, to use the EuroSATDataModule , in your config file, set the data as: data : class_path : torchgeo.datamodules.EuroSATDataModule init_args : batch_size : 32 num_workers : 8 dict_kwargs : root : /dccstor/geofm-pre/EuroSat download : True bands : - B02 - B03 - B04 - B08A - B09 - B10 Modifying each parameter as you see fit. You can also do this outside of config files! Simply instantiate the data module as normal and plug it in. Warning To define transforms to be passed to DataModules from TorchGeo from config files, you must use the following format: data : class_path : terratorch.datamodules.TorchNonGeoDataModule init_args : cls : torchgeo.datamodules.EuroSATDataModule transforms : - class_path : albumentations.augmentations.geometric.resize.Resize init_args : height : 224 width : 224 - class_path : ToTensorV2 Note the class_path is TorchNonGeoDataModule and the class to be used is passed through cls (there is also a TorchGeoDataModule for geo modules). This has to be done as the transforms argument is passed through **kwargs in TorchGeo, making it difficult to instantiate with LightningCLI. See more details below.","title":"Using Datasets already implemented in TorchGeo"},{"location":"data/#terratorch.datamodules.torchgeo_data_module","text":"Ugly proxy objects so parsing config file works with transforms. These are necessary since, for LightningCLI to instantiate arguments as objects from the config, they must have type annotations In TorchGeo, transforms is passed in **kwargs, so it has no type annotations! To get around that, we create these wrappers that have transforms type annotated. They create the transforms and forward all method and attribute calls to the original TorchGeo datamodule. Additionally, TorchGeo datasets pass the data to the transforms callable as a dict, and as a tensor. Albumentations expects this data not as a dict but as different key-value arguments, and as numpy. We handle that conversion here.","title":"torchgeo_data_module"},{"location":"data/#terratorch.datamodules.torchgeo_data_module.TorchGeoDataModule","text":"Bases: GeoDataModule Proxy object for using Geo data modules defined by TorchGeo. Allows for transforms to be defined and passed using config files. The only reason this class exists is so that we can annotate the transforms argument with a type. This is required for lightningcli and config files. As such, all getattr and setattr will be redirected to the underlying class. Source code in terratorch/datamodules/torchgeo_data_module.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 class TorchGeoDataModule ( GeoDataModule ): \"\"\"Proxy object for using Geo data modules defined by TorchGeo. Allows for transforms to be defined and passed using config files. The only reason this class exists is so that we can annotate the transforms argument with a type. This is required for lightningcli and config files. As such, all getattr and setattr will be redirected to the underlying class. \"\"\" def __init__ ( self , cls : type [ GeoDataModule ], batch_size : int | None = None , num_workers : int = 0 , transforms : None | list [ BasicTransform ] = None , ** kwargs : Any , ): \"\"\"Constructor Args: cls (type[GeoDataModule]): TorchGeo DataModule class to be instantiated batch_size (int | None, optional): batch_size. Defaults to None. num_workers (int, optional): num_workers. Defaults to 0. transforms (None | list[BasicTransform], optional): List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None. **kwargs (Any): Arguments passed to instantiate `cls`. \"\"\" if batch_size is not None : kwargs [ \"batch_size\" ] = batch_size if transforms is not None : transforms_as_callable = albumentations_to_callable_with_dict ( transforms ) kwargs [ \"transforms\" ] = build_callable_transform_from_torch_tensor ( transforms_as_callable ) # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs) self . _proxy = cls ( num_workers = num_workers , ** kwargs ) super () . __init__ ( self . _proxy . dataset_class ) # dummy arg @property def collate_fn ( self ): return self . _proxy . collate_fn @collate_fn . setter def collate_fn ( self , value ): self . _proxy . collate_fn = value @property def patch_size ( self ): return self . _proxy . patch_size @property def length ( self ): return self . _proxy . length def setup ( self , stage : str ): return self . _proxy . setup ( stage ) def train_dataloader ( self ): return self . _proxy . train_dataloader () def val_dataloader ( self ): return self . _proxy . val_dataloader () def test_dataloader ( self ): return self . _proxy . test_dataloader () def predict_dataloader ( self ): return self . _proxy . predict_dataloader () def transfer_batch_to_device ( self , batch , device , dataloader_idx ): return self . _proxy . predict_dataloader ( batch , device , dataloader_idx )","title":"TorchGeoDataModule"},{"location":"data/#terratorch.datamodules.torchgeo_data_module.TorchGeoDataModule.__init__","text":"Constructor Parameters: cls ( type [ GeoDataModule ] ) \u2013 TorchGeo DataModule class to be instantiated batch_size ( int | None , default: None ) \u2013 batch_size. Defaults to None. num_workers ( int , default: 0 ) \u2013 num_workers. Defaults to 0. transforms ( None | list [ BasicTransform ] , default: None ) \u2013 List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None. **kwargs ( Any , default: {} ) \u2013 Arguments passed to instantiate cls . Source code in terratorch/datamodules/torchgeo_data_module.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def __init__ ( self , cls : type [ GeoDataModule ], batch_size : int | None = None , num_workers : int = 0 , transforms : None | list [ BasicTransform ] = None , ** kwargs : Any , ): \"\"\"Constructor Args: cls (type[GeoDataModule]): TorchGeo DataModule class to be instantiated batch_size (int | None, optional): batch_size. Defaults to None. num_workers (int, optional): num_workers. Defaults to 0. transforms (None | list[BasicTransform], optional): List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None. **kwargs (Any): Arguments passed to instantiate `cls`. \"\"\" if batch_size is not None : kwargs [ \"batch_size\" ] = batch_size if transforms is not None : transforms_as_callable = albumentations_to_callable_with_dict ( transforms ) kwargs [ \"transforms\" ] = build_callable_transform_from_torch_tensor ( transforms_as_callable ) # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs) self . _proxy = cls ( num_workers = num_workers , ** kwargs ) super () . __init__ ( self . _proxy . dataset_class ) # dummy arg","title":"__init__"},{"location":"data/#terratorch.datamodules.torchgeo_data_module.TorchNonGeoDataModule","text":"Bases: NonGeoDataModule Proxy object for using NonGeo data modules defined by TorchGeo. Allows for transforms to be defined and passed using config files. The only reason this class exists is so that we can annotate the transforms argument with a type. This is required for lightningcli and config files. As such, all getattr and setattr will be redirected to the underlying class. Source code in terratorch/datamodules/torchgeo_data_module.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class TorchNonGeoDataModule ( NonGeoDataModule ): \"\"\"Proxy object for using NonGeo data modules defined by TorchGeo. Allows for transforms to be defined and passed using config files. The only reason this class exists is so that we can annotate the transforms argument with a type. This is required for lightningcli and config files. As such, all getattr and setattr will be redirected to the underlying class. \"\"\" def __init__ ( self , cls : type [ NonGeoDataModule ], batch_size : int | None = None , num_workers : int = 0 , transforms : None | list [ BasicTransform ] = None , ** kwargs : Any , ): \"\"\"Constructor Args: cls (type[NonGeoDataModule]): TorchGeo DataModule class to be instantiated batch_size (int | None, optional): batch_size. Defaults to None. num_workers (int, optional): num_workers. Defaults to 0. transforms (None | list[BasicTransform], optional): List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None. **kwargs (Any): Arguments passed to instantiate `cls`. \"\"\" if batch_size is not None : kwargs [ \"batch_size\" ] = batch_size if transforms is not None : transforms_as_callable = albumentations_to_callable_with_dict ( transforms ) kwargs [ \"transforms\" ] = build_callable_transform_from_torch_tensor ( transforms_as_callable ) # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs) self . _proxy = cls ( num_workers = num_workers , ** kwargs ) super () . __init__ ( self . _proxy . dataset_class ) # dummy arg @property def collate_fn ( self ): return self . _proxy . collate_fn @collate_fn . setter def collate_fn ( self , value ): self . _proxy . collate_fn = value def setup ( self , stage : str ): return self . _proxy . setup ( stage ) def train_dataloader ( self ): return self . _proxy . train_dataloader () def val_dataloader ( self ): return self . _proxy . val_dataloader () def test_dataloader ( self ): return self . _proxy . test_dataloader () def predict_dataloader ( self ): return self . _proxy . predict_dataloader ()","title":"TorchNonGeoDataModule"},{"location":"data/#terratorch.datamodules.torchgeo_data_module.TorchNonGeoDataModule.__init__","text":"Constructor Parameters: cls ( type [ NonGeoDataModule ] ) \u2013 TorchGeo DataModule class to be instantiated batch_size ( int | None , default: None ) \u2013 batch_size. Defaults to None. num_workers ( int , default: 0 ) \u2013 num_workers. Defaults to 0. transforms ( None | list [ BasicTransform ] , default: None ) \u2013 List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None. **kwargs ( Any , default: {} ) \u2013 Arguments passed to instantiate cls . Source code in terratorch/datamodules/torchgeo_data_module.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , cls : type [ NonGeoDataModule ], batch_size : int | None = None , num_workers : int = 0 , transforms : None | list [ BasicTransform ] = None , ** kwargs : Any , ): \"\"\"Constructor Args: cls (type[NonGeoDataModule]): TorchGeo DataModule class to be instantiated batch_size (int | None, optional): batch_size. Defaults to None. num_workers (int, optional): num_workers. Defaults to 0. transforms (None | list[BasicTransform], optional): List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None. **kwargs (Any): Arguments passed to instantiate `cls`. \"\"\" if batch_size is not None : kwargs [ \"batch_size\" ] = batch_size if transforms is not None : transforms_as_callable = albumentations_to_callable_with_dict ( transforms ) kwargs [ \"transforms\" ] = build_callable_transform_from_torch_tensor ( transforms_as_callable ) # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs) self . _proxy = cls ( num_workers = num_workers , ** kwargs ) super () . __init__ ( self . _proxy . dataset_class ) # dummy arg","title":"__init__"},{"location":"data/#generic-datasets-and-data-modules","text":"For the NonGeoDataset case, we also provide \"generic\" datasets and datamodules. These can be used when you would like to load data from given directories, in a style similar to the MMLab libraries.","title":"Generic datasets and data modules"},{"location":"data/#generic-datasets","text":"","title":"Generic Datasets"},{"location":"data/#terratorch.datasets.generic_pixel_wise_dataset","text":"Module containing generic dataset classes","title":"generic_pixel_wise_dataset"},{"location":"data/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoPixelwiseRegressionDataset","text":"Bases: GenericPixelWiseDataset GenericNonGeoPixelwiseRegressionDataset Source code in terratorch/datasets/generic_pixel_wise_dataset.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 class GenericNonGeoPixelwiseRegressionDataset ( GenericPixelWiseDataset ): \"\"\"GenericNonGeoPixelwiseRegressionDataset\"\"\" def __init__ ( self , data_root : Path , label_data_root : Path | None = None , image_grep : str | None = \"*\" , label_grep : str | None = \"*\" , split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , rgb_indices : list [ int ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory label_data_root (Path, optional): Path to data root directory with labels. If not specified, will use the same as for images. image_grep (str, optional): Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep (str, optional): Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. \"\"\" super () . __init__ ( data_root , label_data_root = label_data_root , image_grep = image_grep , label_grep = label_grep , split = split , ignore_split_file_extensions = ignore_split_file_extensions , allow_substring_split_file = allow_substring_split_file , rgb_indices = rgb_indices , dataset_bands = dataset_bands , output_bands = output_bands , constant_scale = constant_scale , transform = transform , no_data_replace = no_data_replace , no_label_replace = no_label_replace , expand_temporal_dimension = expand_temporal_dimension , reduce_zero_label = reduce_zero_label , ) def __getitem__ ( self , index : int ) -> dict [ str , Any ]: item = super () . __getitem__ ( index ) item [ \"mask\" ] = item [ \"mask\" ] . float () return item def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__` suptitle (str|None): optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample .. versionadded:: 0.2 \"\"\" image = sample [ \"image\" ] if len ( image . shape ) == 5 : return if isinstance ( image , Tensor ): image = image . numpy () image = image . take ( self . rgb_indices , axis = 0 ) image = np . transpose ( image , ( 1 , 2 , 0 )) image = ( image - image . min ( axis = ( 0 , 1 ))) * ( 1 / image . max ( axis = ( 0 , 1 ))) image = np . clip ( image , 0 , 1 ) label_mask = sample [ \"mask\" ] if isinstance ( label_mask , Tensor ): label_mask = label_mask . numpy () showing_predictions = \"prediction\" in sample if showing_predictions : prediction_mask = sample [ \"prediction\" ] if isinstance ( prediction_mask , Tensor ): prediction_mask = prediction_mask . numpy () return self . _plot_sample ( image , label_mask , prediction = prediction_mask if showing_predictions else None , suptitle = suptitle , ) @staticmethod def _plot_sample ( image , label , prediction = None , suptitle = None ): num_images = 4 if prediction is not None else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 10 ), layout = \"compressed\" ) norm = mpl . colors . Normalize ( vmin = label . min (), vmax = label . max ()) ax [ 0 ] . axis ( \"off\" ) ax [ 0 ] . title . set_text ( \"Image\" ) ax [ 0 ] . imshow ( image ) ax [ 1 ] . axis ( \"off\" ) ax [ 1 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ 1 ] . imshow ( label , cmap = \"Greens\" , norm = norm ) ax [ 2 ] . axis ( \"off\" ) ax [ 2 ] . title . set_text ( \"GT Mask on Image\" ) ax [ 2 ] . imshow ( image ) ax [ 2 ] . imshow ( label , cmap = \"Greens\" , alpha = 0.3 , norm = norm ) # ax[2].legend() if prediction is not None : ax [ 3 ] . title . set_text ( \"Predicted Mask\" ) ax [ 3 ] . imshow ( prediction , cmap = \"Greens\" , norm = norm ) if suptitle is not None : plt . suptitle ( suptitle ) return fig","title":"GenericNonGeoPixelwiseRegressionDataset"},{"location":"data/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoPixelwiseRegressionDataset.__init__","text":"Constructor Parameters: data_root ( Path ) \u2013 Path to data root directory label_data_root ( Path , default: None ) \u2013 Path to data root directory with labels. If not specified, will use the same as for images. image_grep ( str , default: '*' ) \u2013 Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep ( str , default: '*' ) \u2013 Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split ( Path , default: None ) \u2013 Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices ( list [ str ] , default: None ) \u2013 Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands present in the dataset. output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands that should be output by the dataset. constant_scale ( float , default: 1 ) \u2013 Factor to multiply image values by. Defaults to 1. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float | None , default: None ) \u2013 Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace ( int | None , default: None ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label ( bool , default: False ) \u2013 Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. Source code in terratorch/datasets/generic_pixel_wise_dataset.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 def __init__ ( self , data_root : Path , label_data_root : Path | None = None , image_grep : str | None = \"*\" , label_grep : str | None = \"*\" , split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , rgb_indices : list [ int ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory label_data_root (Path, optional): Path to data root directory with labels. If not specified, will use the same as for images. image_grep (str, optional): Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep (str, optional): Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. \"\"\" super () . __init__ ( data_root , label_data_root = label_data_root , image_grep = image_grep , label_grep = label_grep , split = split , ignore_split_file_extensions = ignore_split_file_extensions , allow_substring_split_file = allow_substring_split_file , rgb_indices = rgb_indices , dataset_bands = dataset_bands , output_bands = output_bands , constant_scale = constant_scale , transform = transform , no_data_replace = no_data_replace , no_label_replace = no_label_replace , expand_temporal_dimension = expand_temporal_dimension , reduce_zero_label = reduce_zero_label , )","title":"__init__"},{"location":"data/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoPixelwiseRegressionDataset.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 a sample returned by :meth: __getitem__ suptitle ( str | None , default: None ) \u2013 optional string to use as a suptitle Returns: Figure \u2013 a matplotlib Figure with the rendered sample .. versionadded:: 0.2 Source code in terratorch/datasets/generic_pixel_wise_dataset.py 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__` suptitle (str|None): optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample .. versionadded:: 0.2 \"\"\" image = sample [ \"image\" ] if len ( image . shape ) == 5 : return if isinstance ( image , Tensor ): image = image . numpy () image = image . take ( self . rgb_indices , axis = 0 ) image = np . transpose ( image , ( 1 , 2 , 0 )) image = ( image - image . min ( axis = ( 0 , 1 ))) * ( 1 / image . max ( axis = ( 0 , 1 ))) image = np . clip ( image , 0 , 1 ) label_mask = sample [ \"mask\" ] if isinstance ( label_mask , Tensor ): label_mask = label_mask . numpy () showing_predictions = \"prediction\" in sample if showing_predictions : prediction_mask = sample [ \"prediction\" ] if isinstance ( prediction_mask , Tensor ): prediction_mask = prediction_mask . numpy () return self . _plot_sample ( image , label_mask , prediction = prediction_mask if showing_predictions else None , suptitle = suptitle , )","title":"plot"},{"location":"data/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoSegmentationDataset","text":"Bases: GenericPixelWiseDataset GenericNonGeoSegmentationDataset Source code in terratorch/datasets/generic_pixel_wise_dataset.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 class GenericNonGeoSegmentationDataset ( GenericPixelWiseDataset ): \"\"\"GenericNonGeoSegmentationDataset\"\"\" def __init__ ( self , data_root : Path , num_classes : int , label_data_root : Path | None = None , image_grep : str | None = \"*\" , label_grep : str | None = \"*\" , split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , rgb_indices : list [ str ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , class_names : list [ str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory num_classes (int): Number of classes in the dataset label_data_root (Path, optional): Path to data root directory with labels. If not specified, will use the same as for images. image_grep (str, optional): Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep (str, optional): Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. class_names (list[str], optional): Class names. Defaults to None. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. \"\"\" super () . __init__ ( data_root , label_data_root = label_data_root , image_grep = image_grep , label_grep = label_grep , split = split , ignore_split_file_extensions = ignore_split_file_extensions , allow_substring_split_file = allow_substring_split_file , rgb_indices = rgb_indices , dataset_bands = dataset_bands , output_bands = output_bands , constant_scale = constant_scale , transform = transform , no_data_replace = no_data_replace , no_label_replace = no_label_replace , expand_temporal_dimension = expand_temporal_dimension , reduce_zero_label = reduce_zero_label , ) self . num_classes = num_classes self . class_names = class_names def __getitem__ ( self , index : int ) -> dict [ str , Any ]: item = super () . __getitem__ ( index ) item [ \"mask\" ] = item [ \"mask\" ] . long () return item def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample .. versionadded:: 0.2 \"\"\" image = sample [ \"image\" ] if len ( image . shape ) == 5 : return if isinstance ( image , Tensor ): image = image . numpy () image = image . take ( self . rgb_indices , axis = 0 ) image = np . transpose ( image , ( 1 , 2 , 0 )) image = ( image - image . min ( axis = ( 0 , 1 ))) * ( 1 / image . max ( axis = ( 0 , 1 ))) image = np . clip ( image , 0 , 1 ) label_mask = sample [ \"mask\" ] if isinstance ( label_mask , Tensor ): label_mask = label_mask . numpy () showing_predictions = \"prediction\" in sample if showing_predictions : prediction_mask = sample [ \"prediction\" ] if isinstance ( prediction_mask , Tensor ): prediction_mask = prediction_mask . numpy () return self . _plot_sample ( image , label_mask , self . num_classes , prediction = prediction_mask if showing_predictions else None , suptitle = suptitle , class_names = self . class_names , ) @staticmethod def _plot_sample ( image , label , num_classes , prediction = None , suptitle = None , class_names = None ): num_images = 5 if prediction is not None else 4 fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 10 ), layout = \"compressed\" ) # for legend ax [ 0 ] . axis ( \"off\" ) norm = mpl . colors . Normalize ( vmin = 0 , vmax = num_classes - 1 ) ax [ 1 ] . axis ( \"off\" ) ax [ 1 ] . title . set_text ( \"Image\" ) ax [ 1 ] . imshow ( image ) ax [ 2 ] . axis ( \"off\" ) ax [ 2 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ 2 ] . imshow ( label , cmap = \"jet\" , norm = norm ) ax [ 3 ] . axis ( \"off\" ) ax [ 3 ] . title . set_text ( \"GT Mask on Image\" ) ax [ 3 ] . imshow ( image ) ax [ 3 ] . imshow ( label , cmap = \"jet\" , alpha = 0.3 , norm = norm ) if prediction is not None : ax [ 4 ] . title . set_text ( \"Predicted Mask\" ) ax [ 4 ] . imshow ( prediction , cmap = \"jet\" , norm = norm ) cmap = plt . get_cmap ( \"jet\" ) legend_data = [] for i , _ in enumerate ( range ( num_classes )): class_name = class_names [ i ] if class_names else str ( i ) data = [ i , cmap ( norm ( i )), class_name ] legend_data . append ( data ) handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = tuple ( v for v in c )) for k , c , n in legend_data ] labels = [ n for k , c , n in legend_data ] ax [ 0 ] . legend ( handles , labels , loc = \"center\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig","title":"GenericNonGeoSegmentationDataset"},{"location":"data/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoSegmentationDataset.__init__","text":"Constructor Parameters: data_root ( Path ) \u2013 Path to data root directory num_classes ( int ) \u2013 Number of classes in the dataset label_data_root ( Path , default: None ) \u2013 Path to data root directory with labels. If not specified, will use the same as for images. image_grep ( str , default: '*' ) \u2013 Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep ( str , default: '*' ) \u2013 Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split ( Path , default: None ) \u2013 Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices ( list [ str ] , default: None ) \u2013 Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands present in the dataset. output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands that should be output by the dataset. class_names ( list [ str ] , default: None ) \u2013 Class names. Defaults to None. constant_scale ( float , default: 1 ) \u2013 Factor to multiply image values by. Defaults to 1. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float | None , default: None ) \u2013 Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace ( int | None , default: None ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label ( bool , default: False ) \u2013 Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. Source code in terratorch/datasets/generic_pixel_wise_dataset.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def __init__ ( self , data_root : Path , num_classes : int , label_data_root : Path | None = None , image_grep : str | None = \"*\" , label_grep : str | None = \"*\" , split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , rgb_indices : list [ str ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , class_names : list [ str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory num_classes (int): Number of classes in the dataset label_data_root (Path, optional): Path to data root directory with labels. If not specified, will use the same as for images. image_grep (str, optional): Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep (str, optional): Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. class_names (list[str], optional): Class names. Defaults to None. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. \"\"\" super () . __init__ ( data_root , label_data_root = label_data_root , image_grep = image_grep , label_grep = label_grep , split = split , ignore_split_file_extensions = ignore_split_file_extensions , allow_substring_split_file = allow_substring_split_file , rgb_indices = rgb_indices , dataset_bands = dataset_bands , output_bands = output_bands , constant_scale = constant_scale , transform = transform , no_data_replace = no_data_replace , no_label_replace = no_label_replace , expand_temporal_dimension = expand_temporal_dimension , reduce_zero_label = reduce_zero_label , ) self . num_classes = num_classes self . class_names = class_names","title":"__init__"},{"location":"data/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoSegmentationDataset.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 a sample returned by :meth: __getitem__ suptitle ( str | None , default: None ) \u2013 optional string to use as a suptitle Returns: Figure \u2013 a matplotlib Figure with the rendered sample .. versionadded:: 0.2 Source code in terratorch/datasets/generic_pixel_wise_dataset.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample .. versionadded:: 0.2 \"\"\" image = sample [ \"image\" ] if len ( image . shape ) == 5 : return if isinstance ( image , Tensor ): image = image . numpy () image = image . take ( self . rgb_indices , axis = 0 ) image = np . transpose ( image , ( 1 , 2 , 0 )) image = ( image - image . min ( axis = ( 0 , 1 ))) * ( 1 / image . max ( axis = ( 0 , 1 ))) image = np . clip ( image , 0 , 1 ) label_mask = sample [ \"mask\" ] if isinstance ( label_mask , Tensor ): label_mask = label_mask . numpy () showing_predictions = \"prediction\" in sample if showing_predictions : prediction_mask = sample [ \"prediction\" ] if isinstance ( prediction_mask , Tensor ): prediction_mask = prediction_mask . numpy () return self . _plot_sample ( image , label_mask , self . num_classes , prediction = prediction_mask if showing_predictions else None , suptitle = suptitle , class_names = self . class_names , )","title":"plot"},{"location":"data/#terratorch.datasets.generic_pixel_wise_dataset.GenericPixelWiseDataset","text":"Bases: NonGeoDataset , ABC This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset. Source code in terratorch/datasets/generic_pixel_wise_dataset.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 class GenericPixelWiseDataset ( NonGeoDataset , ABC ): \"\"\" This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset. \"\"\" def __init__ ( self , data_root : Path , label_data_root : Path | None = None , image_grep : str | None = \"*\" , label_grep : str | None = \"*\" , split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , rgb_indices : list [ int ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory label_data_root (Path, optional): Path to data root directory with labels. If not specified, will use the same as for images. image_grep (str, optional): Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep (str, optional): Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be refered to by output_bands. Defaults to None. output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the dataset as named by dataset_bands. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. \"\"\" super () . __init__ () self . split_file = split label_data_root = label_data_root if label_data_root is not None else data_root self . image_files = sorted ( glob . glob ( os . path . join ( data_root , image_grep ))) self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( label_data_root , label_grep ))) self . reduce_zero_label = reduce_zero_label self . expand_temporal_dimension = expand_temporal_dimension if self . expand_temporal_dimension and output_bands is None : msg = \"Please provide output_bands when expand_temporal_dimension is True\" raise Exception ( msg ) if self . split_file is not None : with open ( self . split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = ignore_split_file_extensions , allow_substring = allow_substring_split_file , ) self . segmentation_mask_files = filter_valid_files ( self . segmentation_mask_files , valid_files = valid_files , ignore_extensions = ignore_split_file_extensions , allow_substring = allow_substring_split_file , ) self . rgb_indices = [ 0 , 1 , 2 ] if rgb_indices is None else rgb_indices self . dataset_bands = generate_bands_intervals ( dataset_bands ) self . output_bands = generate_bands_intervals ( output_bands ) if self . output_bands and not self . dataset_bands : msg = \"If output bands provided, dataset_bands must also be provided\" return Exception ( msg ) # noqa: PLE0101 # There is a special condition if the bands are defined as simple strings. if self . output_bands : if len ( set ( self . output_bands ) & set ( self . dataset_bands )) != len ( self . output_bands ): msg = \"Output bands must be a subset of dataset bands\" raise Exception ( msg ) self . filter_indices = [ self . dataset_bands . index ( band ) for band in self . output_bands ] else : self . filter_indices = None # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform # self.transform = transform if transform else ToTensorV2() import warnings import rasterio warnings . filterwarnings ( \"ignore\" , category = rasterio . errors . NotGeoreferencedWarning ) def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , Any ]: image = self . _load_file ( self . image_files [ index ], nan_replace = self . no_data_replace ) . to_numpy () # to channels last if self . expand_temporal_dimension : image = rearrange ( image , \"(channels time) h w -> channels time h w\" , channels = len ( self . output_bands )) image = np . moveaxis ( image , 0 , - 1 ) if self . filter_indices : image = image [ ... , self . filter_indices ] output = { \"image\" : image . astype ( np . float32 ) * self . constant_scale , \"mask\" : self . _load_file ( self . segmentation_mask_files [ index ], nan_replace = self . no_label_replace ) . to_numpy ()[ 0 ] } if self . reduce_zero_label : output [ \"mask\" ] -= 1 if self . transform : output = self . transform ( ** output ) output [ \"filename\" ] = self . image_files [ index ] return output def _load_file ( self , path , nan_replace : int | float | None = None ) -> xr . DataArray : data = rioxarray . open_rasterio ( path , masked = True ) if nan_replace is not None : data = data . fillna ( nan_replace ) return data","title":"GenericPixelWiseDataset"},{"location":"data/#terratorch.datasets.generic_pixel_wise_dataset.GenericPixelWiseDataset.__init__","text":"Constructor Parameters: data_root ( Path ) \u2013 Path to data root directory label_data_root ( Path , default: None ) \u2013 Path to data root directory with labels. If not specified, will use the same as for images. image_grep ( str , default: '*' ) \u2013 Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep ( str , default: '*' ) \u2013 Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split ( Path , default: None ) \u2013 Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices ( list [ str ] , default: None ) \u2013 Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands ( list [ HLSBands | int | tuple [ int , int ] | str ] | None , default: None ) \u2013 Bands present in the dataset. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be refered to by output_bands. Defaults to None. output_bands ( list [ HLSBands | int | tuple [ int , int ] | str ] | None , default: None ) \u2013 Bands that should be output by the dataset as named by dataset_bands. constant_scale ( float , default: 1 ) \u2013 Factor to multiply image values by. Defaults to 1. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float | None , default: None ) \u2013 Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace ( int | None , default: None ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to -1. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label ( bool , default: False ) \u2013 Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. Source code in terratorch/datasets/generic_pixel_wise_dataset.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def __init__ ( self , data_root : Path , label_data_root : Path | None = None , image_grep : str | None = \"*\" , label_grep : str | None = \"*\" , split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , rgb_indices : list [ int ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory label_data_root (Path, optional): Path to data root directory with labels. If not specified, will use the same as for images. image_grep (str, optional): Regular expression appended to data_root to find input images. Defaults to \"*\". label_grep (str, optional): Regular expression appended to data_root to find ground truth masks. Defaults to \"*\". split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be refered to by output_bands. Defaults to None. output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the dataset as named by dataset_bands. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. \"\"\" super () . __init__ () self . split_file = split label_data_root = label_data_root if label_data_root is not None else data_root self . image_files = sorted ( glob . glob ( os . path . join ( data_root , image_grep ))) self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( label_data_root , label_grep ))) self . reduce_zero_label = reduce_zero_label self . expand_temporal_dimension = expand_temporal_dimension if self . expand_temporal_dimension and output_bands is None : msg = \"Please provide output_bands when expand_temporal_dimension is True\" raise Exception ( msg ) if self . split_file is not None : with open ( self . split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = ignore_split_file_extensions , allow_substring = allow_substring_split_file , ) self . segmentation_mask_files = filter_valid_files ( self . segmentation_mask_files , valid_files = valid_files , ignore_extensions = ignore_split_file_extensions , allow_substring = allow_substring_split_file , ) self . rgb_indices = [ 0 , 1 , 2 ] if rgb_indices is None else rgb_indices self . dataset_bands = generate_bands_intervals ( dataset_bands ) self . output_bands = generate_bands_intervals ( output_bands ) if self . output_bands and not self . dataset_bands : msg = \"If output bands provided, dataset_bands must also be provided\" return Exception ( msg ) # noqa: PLE0101 # There is a special condition if the bands are defined as simple strings. if self . output_bands : if len ( set ( self . output_bands ) & set ( self . dataset_bands )) != len ( self . output_bands ): msg = \"Output bands must be a subset of dataset bands\" raise Exception ( msg ) self . filter_indices = [ self . dataset_bands . index ( band ) for band in self . output_bands ] else : self . filter_indices = None # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform # self.transform = transform if transform else ToTensorV2() import warnings import rasterio warnings . filterwarnings ( \"ignore\" , category = rasterio . errors . NotGeoreferencedWarning )","title":"__init__"},{"location":"data/#terratorch.datasets.generic_scalar_label_dataset","text":"Module containing generic dataset classes","title":"generic_scalar_label_dataset"},{"location":"data/#terratorch.datasets.generic_scalar_label_dataset.GenericNonGeoClassificationDataset","text":"Bases: GenericScalarLabelDataset GenericNonGeoClassificationDataset Source code in terratorch/datasets/generic_scalar_label_dataset.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 class GenericNonGeoClassificationDataset ( GenericScalarLabelDataset ): \"\"\"GenericNonGeoClassificationDataset\"\"\" def __init__ ( self , data_root : Path , num_classes : int , split : Path | None = None , ignore_split_file_extensions : bool = True , # noqa: FBT001, FBT002 allow_substring_split_file : bool = True , # noqa: FBT001, FBT002 rgb_indices : list [ str ] | None = None , dataset_bands : list [ HLSBands | int ] | None = None , output_bands : list [ HLSBands | int ] | None = None , class_names : list [ str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float = 0 , expand_temporal_dimension : bool = False , # noqa: FBT001, FBT002 ) -> None : \"\"\"A generic Non-Geo dataset for classification. Args: data_root (Path): Path to data root directory num_classes (int): Number of classes in the dataset split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. class_names (list[str], optional): Class names. Defaults to None. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float): Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. \"\"\" super () . __init__ ( data_root , split = split , ignore_split_file_extensions = ignore_split_file_extensions , allow_substring_split_file = allow_substring_split_file , rgb_indices = rgb_indices , dataset_bands = dataset_bands , output_bands = output_bands , constant_scale = constant_scale , transform = transform , no_data_replace = no_data_replace , expand_temporal_dimension = expand_temporal_dimension , ) self . num_classes = num_classes self . class_names = class_names def __getitem__ ( self , index : int ) -> dict [ str , Any ]: item = super () . __getitem__ ( index ) item [ \"label\" ] = torch . tensor ( item [ \"label\" ]) . long () return item def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : pass","title":"GenericNonGeoClassificationDataset"},{"location":"data/#terratorch.datasets.generic_scalar_label_dataset.GenericNonGeoClassificationDataset.__init__","text":"A generic Non-Geo dataset for classification. Parameters: data_root ( Path ) \u2013 Path to data root directory num_classes ( int ) \u2013 Number of classes in the dataset split ( Path , default: None ) \u2013 Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices ( list [ str ] , default: None ) \u2013 Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands present in the dataset. output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands that should be output by the dataset. class_names ( list [ str ] , default: None ) \u2013 Class names. Defaults to None. constant_scale ( float , default: 1 ) \u2013 Factor to multiply image values by. Defaults to 1. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float , default: 0 ) \u2013 Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. Source code in terratorch/datasets/generic_scalar_label_dataset.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def __init__ ( self , data_root : Path , num_classes : int , split : Path | None = None , ignore_split_file_extensions : bool = True , # noqa: FBT001, FBT002 allow_substring_split_file : bool = True , # noqa: FBT001, FBT002 rgb_indices : list [ str ] | None = None , dataset_bands : list [ HLSBands | int ] | None = None , output_bands : list [ HLSBands | int ] | None = None , class_names : list [ str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float = 0 , expand_temporal_dimension : bool = False , # noqa: FBT001, FBT002 ) -> None : \"\"\"A generic Non-Geo dataset for classification. Args: data_root (Path): Path to data root directory num_classes (int): Number of classes in the dataset split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. class_names (list[str], optional): Class names. Defaults to None. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float): Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. \"\"\" super () . __init__ ( data_root , split = split , ignore_split_file_extensions = ignore_split_file_extensions , allow_substring_split_file = allow_substring_split_file , rgb_indices = rgb_indices , dataset_bands = dataset_bands , output_bands = output_bands , constant_scale = constant_scale , transform = transform , no_data_replace = no_data_replace , expand_temporal_dimension = expand_temporal_dimension , ) self . num_classes = num_classes self . class_names = class_names","title":"__init__"},{"location":"data/#terratorch.datasets.generic_scalar_label_dataset.GenericScalarLabelDataset","text":"Bases: NonGeoDataset , ImageFolder , ABC This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset. Source code in terratorch/datasets/generic_scalar_label_dataset.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 class GenericScalarLabelDataset ( NonGeoDataset , ImageFolder , ABC ): \"\"\" This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset. \"\"\" def __init__ ( self , data_root : Path , split : Path | None = None , ignore_split_file_extensions : bool = True , # noqa: FBT001, FBT002 allow_substring_split_file : bool = True , # noqa: FBT001, FBT002 rgb_indices : list [ int ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float = 0 , expand_temporal_dimension : bool = False , # noqa: FBT001, FBT002 ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This parameter gives identifiers to input channels (bands) so that they can then be refered to by output_bands. Can use the HLSBands enum, ints, int ranges, or strings. Defaults to None. output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the dataset as named by dataset_bands. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float): Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. \"\"\" self . split_file = split self . image_files = sorted ( glob . glob ( os . path . join ( data_root , \"**\" ), recursive = True )) self . image_files = [ f for f in self . image_files if not os . path . isdir ( f )] self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . expand_temporal_dimension = expand_temporal_dimension if self . expand_temporal_dimension and output_bands is None : msg = \"Please provide output_bands when expand_temporal_dimension is True\" raise Exception ( msg ) if self . split_file is not None : with open ( self . split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = ignore_split_file_extensions , allow_substring = allow_substring_split_file , ) def is_valid_file ( x ): return x in self . image_files else : def is_valid_file ( x ): return True super () . __init__ ( root = data_root , transform = None , target_transform = None , loader = rasterio_loader , is_valid_file = is_valid_file ) self . rgb_indices = [ 0 , 1 , 2 ] if rgb_indices is None else rgb_indices self . dataset_bands = generate_bands_intervals ( dataset_bands ) self . output_bands = generate_bands_intervals ( output_bands ) if self . output_bands and not self . dataset_bands : msg = \"If output bands provided, dataset_bands must also be provided\" return Exception ( msg ) # noqa: PLE0101 # There is a special condition if the bands are defined as simple strings. if self . output_bands : if len ( set ( self . output_bands ) & set ( self . dataset_bands )) != len ( self . output_bands ): msg = \"Output bands must be a subset of dataset bands\" raise Exception ( msg ) self . filter_indices = [ self . dataset_bands . index ( band ) for band in self . output_bands ] else : self . filter_indices = None # If no transform is given, apply only to transform to torch tensor self . transforms = transform if transform else default_transform # self.transform = transform if transform else ToTensorV2() import warnings import rasterio warnings . filterwarnings ( \"ignore\" , category = rasterio . errors . NotGeoreferencedWarning ) def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , Any ]: image , label = ImageFolder . __getitem__ ( self , index ) if self . expand_temporal_dimension : image = rearrange ( image , \"h w (channels time) -> time h w channels\" , channels = len ( self . output_bands )) if self . filter_indices : image = image [ ... , self . filter_indices ] image = image . astype ( np . float32 ) * self . constant_scale if self . transforms : image = self . transforms ( image = image )[ \"image\" ] # albumentations returns dict output = { \"image\" : image , \"label\" : label , # samples is an attribute of ImageFolder. Contains a tuple of (Path, Target) \"filename\" : self . image_files [ index ] } return output def _generate_bands_intervals ( self , bands_intervals : list [ int | str | HLSBands | tuple [ int ]] | None = None ): if bands_intervals is None : return None bands = [] for element in bands_intervals : # if its an interval if isinstance ( element , tuple ): if len ( element ) != 2 : # noqa: PLR2004 msg = \"When defining an interval, a tuple of two integers should be passed, \\ defining start and end indices inclusive\" raise Exception ( msg ) expanded_element = list ( range ( element [ 0 ], element [ 1 ] + 1 )) bands . extend ( expanded_element ) else : bands . append ( element ) return bands def _load_file ( self , path ) -> xr . DataArray : data = rioxarray . open_rasterio ( path , masked = True ) data = data . fillna ( self . no_data_replace ) return data","title":"GenericScalarLabelDataset"},{"location":"data/#terratorch.datasets.generic_scalar_label_dataset.GenericScalarLabelDataset.__init__","text":"Constructor Parameters: data_root ( Path ) \u2013 Path to data root directory split ( Path , default: None ) \u2013 Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices ( list [ str ] , default: None ) \u2013 Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands ( list [ HLSBands | int | tuple [ int , int ] | str ] | None , default: None ) \u2013 Bands present in the dataset. This parameter gives identifiers to input channels (bands) so that they can then be refered to by output_bands. Can use the HLSBands enum, ints, int ranges, or strings. Defaults to None. output_bands ( list [ HLSBands | int | tuple [ int , int ] | str ] | None , default: None ) \u2013 Bands that should be output by the dataset as named by dataset_bands. constant_scale ( float , default: 1 ) \u2013 Factor to multiply image values by. Defaults to 1. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float , default: 0 ) \u2013 Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. Source code in terratorch/datasets/generic_scalar_label_dataset.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def __init__ ( self , data_root : Path , split : Path | None = None , ignore_split_file_extensions : bool = True , # noqa: FBT001, FBT002 allow_substring_split_file : bool = True , # noqa: FBT001, FBT002 rgb_indices : list [ int ] | None = None , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , transform : A . Compose | None = None , no_data_replace : float = 0 , expand_temporal_dimension : bool = False , # noqa: FBT001, FBT002 ) -> None : \"\"\"Constructor Args: data_root (Path): Path to data root directory split (Path, optional): Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep]) ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2]. dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This parameter gives identifiers to input channels (bands) so that they can then be refered to by output_bands. Can use the HLSBands enum, ints, int ranges, or strings. Defaults to None. output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the dataset as named by dataset_bands. constant_scale (float): Factor to multiply image values by. Defaults to 1. transform (Albumentations.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float): Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. \"\"\" self . split_file = split self . image_files = sorted ( glob . glob ( os . path . join ( data_root , \"**\" ), recursive = True )) self . image_files = [ f for f in self . image_files if not os . path . isdir ( f )] self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . expand_temporal_dimension = expand_temporal_dimension if self . expand_temporal_dimension and output_bands is None : msg = \"Please provide output_bands when expand_temporal_dimension is True\" raise Exception ( msg ) if self . split_file is not None : with open ( self . split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = ignore_split_file_extensions , allow_substring = allow_substring_split_file , ) def is_valid_file ( x ): return x in self . image_files else : def is_valid_file ( x ): return True super () . __init__ ( root = data_root , transform = None , target_transform = None , loader = rasterio_loader , is_valid_file = is_valid_file ) self . rgb_indices = [ 0 , 1 , 2 ] if rgb_indices is None else rgb_indices self . dataset_bands = generate_bands_intervals ( dataset_bands ) self . output_bands = generate_bands_intervals ( output_bands ) if self . output_bands and not self . dataset_bands : msg = \"If output bands provided, dataset_bands must also be provided\" return Exception ( msg ) # noqa: PLE0101 # There is a special condition if the bands are defined as simple strings. if self . output_bands : if len ( set ( self . output_bands ) & set ( self . dataset_bands )) != len ( self . output_bands ): msg = \"Output bands must be a subset of dataset bands\" raise Exception ( msg ) self . filter_indices = [ self . dataset_bands . index ( band ) for band in self . output_bands ] else : self . filter_indices = None # If no transform is given, apply only to transform to torch tensor self . transforms = transform if transform else default_transform # self.transform = transform if transform else ToTensorV2() import warnings import rasterio warnings . filterwarnings ( \"ignore\" , category = rasterio . errors . NotGeoreferencedWarning )","title":"__init__"},{"location":"data/#generic-data-modules","text":"","title":"Generic Data Modules"},{"location":"data/#terratorch.datamodules.generic_pixel_wise_data_module","text":"This module contains generic data modules for instantiation at runtime.","title":"generic_pixel_wise_data_module"},{"location":"data/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoPixelwiseRegressionDataModule","text":"Bases: NonGeoDataModule This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoPixelwiseRegressionDataset Source code in terratorch/datamodules/generic_pixel_wise_data_module.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 class GenericNonGeoPixelwiseRegressionDataModule ( NonGeoDataModule ): \"\"\"This is a generic datamodule class for instantiating data modules at runtime. Composes several [GenericNonGeoPixelwiseRegressionDataset][terratorch.datasets.GenericNonGeoPixelwiseRegressionDataset] \"\"\" def __init__ ( self , batch_size : int , num_workers : int , train_data_root : Path , val_data_root : Path , test_data_root : Path , means : list [ float ] | str , stds : list [ float ] | str , predict_data_root : Path | None = None , img_grep : str | None = \"*\" , label_grep : str | None = \"*\" , train_label_data_root : Path | None = None , val_label_data_root : Path | None = None , test_label_data_root : Path | None = None , train_split : Path | None = None , val_split : Path | None = None , test_split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , rgb_indices : list [ int ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , no_data_replace : float | None = None , no_label_replace : int | None = None , drop_last : bool = True , pin_memory : bool = False , check_stackability : bool = True , ** kwargs : Any , ) -> None : \"\"\"Constructor Args: batch_size (int): _description_ num_workers (int): _description_ train_data_root (Path): _description_ val_data_root (Path): _description_ test_data_root (Path): _description_ predict_data_root (Path): _description_ img_grep (str): _description_ label_grep (str): _description_ means (list[float]): _description_ stds (list[float]): _description_ train_label_data_root (Path | None, optional): _description_. Defaults to None. val_label_data_root (Path | None, optional): _description_. Defaults to None. test_label_data_root (Path | None, optional): _description_. Defaults to None. train_split (Path | None, optional): _description_. Defaults to None. val_split (Path | None, optional): _description_. Defaults to None. test_split (Path | None, optional): _description_. Defaults to None. ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None. predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite. predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite. constant_scale (float, optional): _description_. Defaults to 1. rgb_indices (list[int] | None, optional): _description_. Defaults to None. train_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. drop_last (bool): Drop the last batch if it is not complete. Defaults to True. pin_memory (bool): If ``True``, the data loader will copy Tensors into device/CUDA pinned memory before returning them. Defaults to False. check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked. \"\"\" super () . __init__ ( GenericNonGeoPixelwiseRegressionDataset , batch_size , num_workers , ** kwargs ) self . img_grep = img_grep self . label_grep = label_grep self . train_root = train_data_root self . val_root = val_data_root self . test_root = test_data_root self . predict_root = predict_data_root self . train_split = train_split self . val_split = val_split self . test_split = test_split self . ignore_split_file_extensions = ignore_split_file_extensions self . allow_substring_split_file = allow_substring_split_file self . drop_last = drop_last self . pin_memory = pin_memory self . expand_temporal_dimension = expand_temporal_dimension self . reduce_zero_label = reduce_zero_label self . train_label_data_root = train_label_data_root self . val_label_data_root = val_label_data_root self . test_label_data_root = test_label_data_root self . constant_scale = constant_scale self . dataset_bands = dataset_bands self . predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands self . predict_output_bands = predict_output_bands if predict_output_bands else output_bands self . output_bands = output_bands self . rgb_indices = rgb_indices # self.aug = AugmentationSequential( # K.Normalize(means, stds), # data_keys=[\"image\"], # ) means = load_from_file_or_attribute ( means ) stds = load_from_file_or_attribute ( stds ) self . aug = Normalize ( means , stds ) self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . check_stackability = check_stackability def setup ( self , stage : str ) -> None : if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( self . train_root , image_grep = self . img_grep , label_grep = self . label_grep , label_data_root = self . train_label_data_root , split = self . train_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . train_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( self . val_root , image_grep = self . img_grep , label_grep = self . label_grep , label_data_root = self . val_label_data_root , split = self . val_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . val_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( self . test_root , image_grep = self . img_grep , label_grep = self . label_grep , label_data_root = self . test_label_data_root , split = self . test_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . test_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) if stage in [ \"predict\" ] and self . predict_root : self . predict_dataset = self . dataset_class ( self . predict_root , image_grep = self . img_grep , dataset_bands = self . predict_dataset_bands , output_bands = self . predict_output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . test_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) def _dataloader_factory ( self , split : str ) -> DataLoader [ dict [ str , Tensor ]]: \"\"\"Implement one or more PyTorch DataLoaders. Args: split: Either 'train', 'val', 'test', or 'predict'. Returns: A collection of data loaders specifying samples. Raises: MisconfigurationException: If :meth:`setup` does not define a dataset or sampler, or if the dataset or sampler has length 0. \"\"\" dataset = self . _valid_attribute ( f \" { split } _dataset\" , \"dataset\" ) batch_size = self . _valid_attribute ( f \" { split } _batch_size\" , \"batch_size\" ) if self . check_stackability : print ( \"Checking stackability.\" ) batch_size = check_dataset_stackability ( dataset , batch_size ) return DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = split == \"train\" , num_workers = self . num_workers , collate_fn = self . collate_fn , drop_last = split == \"train\" and self . drop_last , pin_memory = self . pin_memory , )","title":"GenericNonGeoPixelwiseRegressionDataModule"},{"location":"data/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoPixelwiseRegressionDataModule.__init__","text":"Constructor Parameters: batch_size ( int ) \u2013 description num_workers ( int ) \u2013 description train_data_root ( Path ) \u2013 description val_data_root ( Path ) \u2013 description test_data_root ( Path ) \u2013 description predict_data_root ( Path , default: None ) \u2013 description img_grep ( str , default: '*' ) \u2013 description label_grep ( str , default: '*' ) \u2013 description means ( list [ float ] ) \u2013 description stds ( list [ float ] ) \u2013 description train_label_data_root ( Path | None , default: None ) \u2013 description . Defaults to None. val_label_data_root ( Path | None , default: None ) \u2013 description . Defaults to None. test_label_data_root ( Path | None , default: None ) \u2013 description . Defaults to None. train_split ( Path | None , default: None ) \u2013 description . Defaults to None. val_split ( Path | None , default: None ) \u2013 description . Defaults to None. test_split ( Path | None , default: None ) \u2013 description . Defaults to None. ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands present in the dataset. Defaults to None. output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None. predict_dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite. predict_output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite. constant_scale ( float , default: 1 ) \u2013 description . Defaults to 1. rgb_indices ( list [ int ] | None , default: None ) \u2013 description . Defaults to None. train_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float | None , default: None ) \u2013 Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace ( int | None , default: None ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label ( bool , default: False ) \u2013 Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. drop_last ( bool , default: True ) \u2013 Drop the last batch if it is not complete. Defaults to True. pin_memory ( bool , default: False ) \u2013 If True , the data loader will copy Tensors check_stackability ( bool , default: True ) \u2013 Check if all the files in the dataset has the same size and can be stacked. Source code in terratorch/datamodules/generic_pixel_wise_data_module.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 def __init__ ( self , batch_size : int , num_workers : int , train_data_root : Path , val_data_root : Path , test_data_root : Path , means : list [ float ] | str , stds : list [ float ] | str , predict_data_root : Path | None = None , img_grep : str | None = \"*\" , label_grep : str | None = \"*\" , train_label_data_root : Path | None = None , val_label_data_root : Path | None = None , test_label_data_root : Path | None = None , train_split : Path | None = None , val_split : Path | None = None , test_split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , rgb_indices : list [ int ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , no_data_replace : float | None = None , no_label_replace : int | None = None , drop_last : bool = True , pin_memory : bool = False , check_stackability : bool = True , ** kwargs : Any , ) -> None : \"\"\"Constructor Args: batch_size (int): _description_ num_workers (int): _description_ train_data_root (Path): _description_ val_data_root (Path): _description_ test_data_root (Path): _description_ predict_data_root (Path): _description_ img_grep (str): _description_ label_grep (str): _description_ means (list[float]): _description_ stds (list[float]): _description_ train_label_data_root (Path | None, optional): _description_. Defaults to None. val_label_data_root (Path | None, optional): _description_. Defaults to None. test_label_data_root (Path | None, optional): _description_. Defaults to None. train_split (Path | None, optional): _description_. Defaults to None. val_split (Path | None, optional): _description_. Defaults to None. test_split (Path | None, optional): _description_. Defaults to None. ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None. predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite. predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite. constant_scale (float, optional): _description_. Defaults to 1. rgb_indices (list[int] | None, optional): _description_. Defaults to None. train_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. drop_last (bool): Drop the last batch if it is not complete. Defaults to True. pin_memory (bool): If ``True``, the data loader will copy Tensors into device/CUDA pinned memory before returning them. Defaults to False. check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked. \"\"\" super () . __init__ ( GenericNonGeoPixelwiseRegressionDataset , batch_size , num_workers , ** kwargs ) self . img_grep = img_grep self . label_grep = label_grep self . train_root = train_data_root self . val_root = val_data_root self . test_root = test_data_root self . predict_root = predict_data_root self . train_split = train_split self . val_split = val_split self . test_split = test_split self . ignore_split_file_extensions = ignore_split_file_extensions self . allow_substring_split_file = allow_substring_split_file self . drop_last = drop_last self . pin_memory = pin_memory self . expand_temporal_dimension = expand_temporal_dimension self . reduce_zero_label = reduce_zero_label self . train_label_data_root = train_label_data_root self . val_label_data_root = val_label_data_root self . test_label_data_root = test_label_data_root self . constant_scale = constant_scale self . dataset_bands = dataset_bands self . predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands self . predict_output_bands = predict_output_bands if predict_output_bands else output_bands self . output_bands = output_bands self . rgb_indices = rgb_indices # self.aug = AugmentationSequential( # K.Normalize(means, stds), # data_keys=[\"image\"], # ) means = load_from_file_or_attribute ( means ) stds = load_from_file_or_attribute ( stds ) self . aug = Normalize ( means , stds ) self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . check_stackability = check_stackability","title":"__init__"},{"location":"data/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoSegmentationDataModule","text":"Bases: NonGeoDataModule This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoSegmentationDatasets Source code in terratorch/datamodules/generic_pixel_wise_data_module.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 class GenericNonGeoSegmentationDataModule ( NonGeoDataModule ): \"\"\" This is a generic datamodule class for instantiating data modules at runtime. Composes several [GenericNonGeoSegmentationDatasets][terratorch.datasets.GenericNonGeoSegmentationDataset] \"\"\" def __init__ ( self , batch_size : int , num_workers : int , train_data_root : Path , val_data_root : Path , test_data_root : Path , img_grep : str , label_grep : str , means : list [ float ] | str , stds : list [ float ] | str , num_classes : int , predict_data_root : Path | None = None , train_label_data_root : Path | None = None , val_label_data_root : Path | None = None , test_label_data_root : Path | None = None , train_split : Path | None = None , val_split : Path | None = None , test_split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , rgb_indices : list [ int ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , no_data_replace : float | None = None , no_label_replace : int | None = None , drop_last : bool = True , pin_memory : bool = False , ** kwargs : Any , ) -> None : \"\"\"Constructor Args: batch_size (int): _description_ num_workers (int): _description_ train_data_root (Path): _description_ val_data_root (Path): _description_ test_data_root (Path): _description_ predict_data_root (Path): _description_ img_grep (str): _description_ label_grep (str): _description_ means (list[float]): _description_ stds (list[float]): _description_ num_classes (int): _description_ train_label_data_root (Path | None, optional): _description_. Defaults to None. val_label_data_root (Path | None, optional): _description_. Defaults to None. test_label_data_root (Path | None, optional): _description_. Defaults to None. train_split (Path | None, optional): _description_. Defaults to None. val_split (Path | None, optional): _description_. Defaults to None. test_split (Path | None, optional): _description_. Defaults to None. ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None. predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite. predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite. constant_scale (float, optional): _description_. Defaults to 1. rgb_indices (list[int] | None, optional): _description_. Defaults to None. train_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. drop_last (bool): Drop the last batch if it is not complete. Defaults to True. pin_memory (bool): If ``True``, the data loader will copy Tensors into device/CUDA pinned memory before returning them. Defaults to False. \"\"\" super () . __init__ ( GenericNonGeoSegmentationDataset , batch_size , num_workers , ** kwargs ) self . num_classes = num_classes self . img_grep = img_grep self . label_grep = label_grep self . train_root = train_data_root self . val_root = val_data_root self . test_root = test_data_root self . predict_root = predict_data_root self . train_split = train_split self . val_split = val_split self . test_split = test_split self . ignore_split_file_extensions = ignore_split_file_extensions self . allow_substring_split_file = allow_substring_split_file self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . drop_last = drop_last self . pin_memory = pin_memory self . train_label_data_root = train_label_data_root self . val_label_data_root = val_label_data_root self . test_label_data_root = test_label_data_root self . dataset_bands = dataset_bands self . predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands self . predict_output_bands = predict_output_bands if predict_output_bands else output_bands self . output_bands = output_bands self . rgb_indices = rgb_indices self . expand_temporal_dimension = expand_temporal_dimension self . reduce_zero_label = reduce_zero_label self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) # self.aug = AugmentationSequential( # K.Normalize(means, stds), # data_keys=[\"image\"], # ) means = load_from_file_or_attribute ( means ) stds = load_from_file_or_attribute ( stds ) self . aug = Normalize ( means , stds ) # self.aug = Normalize(means, stds) # self.collate_fn = collate_fn_list_dicts def setup ( self , stage : str ) -> None : if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( self . train_root , self . num_classes , image_grep = self . img_grep , label_grep = self . label_grep , label_data_root = self . train_label_data_root , split = self . train_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . train_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( self . val_root , self . num_classes , image_grep = self . img_grep , label_grep = self . label_grep , label_data_root = self . val_label_data_root , split = self . val_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . val_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( self . test_root , self . num_classes , image_grep = self . img_grep , label_grep = self . label_grep , label_data_root = self . test_label_data_root , split = self . test_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . test_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) if stage in [ \"predict\" ] and self . predict_root : self . predict_dataset = self . dataset_class ( self . predict_root , self . num_classes , dataset_bands = self . predict_dataset_bands , output_bands = self . predict_output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . test_transform , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , ) def _dataloader_factory ( self , split : str ) -> DataLoader [ dict [ str , Tensor ]]: \"\"\"Implement one or more PyTorch DataLoaders. Args: split: Either 'train', 'val', 'test', or 'predict'. Returns: A collection of data loaders specifying samples. Raises: MisconfigurationException: If :meth:`setup` does not define a dataset or sampler, or if the dataset or sampler has length 0. \"\"\" dataset = self . _valid_attribute ( f \" { split } _dataset\" , \"dataset\" ) batch_size = self . _valid_attribute ( f \" { split } _batch_size\" , \"batch_size\" ) batch_size = check_dataset_stackability ( dataset , batch_size ) return DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = split == \"train\" , num_workers = self . num_workers , collate_fn = self . collate_fn , drop_last = split == \"train\" and self . drop_last , pin_memory = self . pin_memory , )","title":"GenericNonGeoSegmentationDataModule"},{"location":"data/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoSegmentationDataModule.__init__","text":"Constructor Parameters: batch_size ( int ) \u2013 description num_workers ( int ) \u2013 description train_data_root ( Path ) \u2013 description val_data_root ( Path ) \u2013 description test_data_root ( Path ) \u2013 description predict_data_root ( Path , default: None ) \u2013 description img_grep ( str ) \u2013 description label_grep ( str ) \u2013 description means ( list [ float ] ) \u2013 description stds ( list [ float ] ) \u2013 description num_classes ( int ) \u2013 description train_label_data_root ( Path | None , default: None ) \u2013 description . Defaults to None. val_label_data_root ( Path | None , default: None ) \u2013 description . Defaults to None. test_label_data_root ( Path | None , default: None ) \u2013 description . Defaults to None. train_split ( Path | None , default: None ) \u2013 description . Defaults to None. val_split ( Path | None , default: None ) \u2013 description . Defaults to None. test_split ( Path | None , default: None ) \u2013 description . Defaults to None. ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands present in the dataset. Defaults to None. output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None. predict_dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite. predict_output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite. constant_scale ( float , default: 1 ) \u2013 description . Defaults to 1. rgb_indices ( list [ int ] | None , default: None ) \u2013 description . Defaults to None. train_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float | None , default: None ) \u2013 Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace ( int | None , default: None ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label ( bool , default: False ) \u2013 Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. drop_last ( bool , default: True ) \u2013 Drop the last batch if it is not complete. Defaults to True. pin_memory ( bool , default: False ) \u2013 If True , the data loader will copy Tensors Source code in terratorch/datamodules/generic_pixel_wise_data_module.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 def __init__ ( self , batch_size : int , num_workers : int , train_data_root : Path , val_data_root : Path , test_data_root : Path , img_grep : str , label_grep : str , means : list [ float ] | str , stds : list [ float ] | str , num_classes : int , predict_data_root : Path | None = None , train_label_data_root : Path | None = None , val_label_data_root : Path | None = None , test_label_data_root : Path | None = None , train_split : Path | None = None , val_split : Path | None = None , test_split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_dataset_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , predict_output_bands : list [ HLSBands | int | tuple [ int , int ] | str ] | None = None , constant_scale : float = 1 , rgb_indices : list [ int ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , expand_temporal_dimension : bool = False , reduce_zero_label : bool = False , no_data_replace : float | None = None , no_label_replace : int | None = None , drop_last : bool = True , pin_memory : bool = False , ** kwargs : Any , ) -> None : \"\"\"Constructor Args: batch_size (int): _description_ num_workers (int): _description_ train_data_root (Path): _description_ val_data_root (Path): _description_ test_data_root (Path): _description_ predict_data_root (Path): _description_ img_grep (str): _description_ label_grep (str): _description_ means (list[float]): _description_ stds (list[float]): _description_ num_classes (int): _description_ train_label_data_root (Path | None, optional): _description_. Defaults to None. val_label_data_root (Path | None, optional): _description_. Defaults to None. test_label_data_root (Path | None, optional): _description_. Defaults to None. train_split (Path | None, optional): _description_. Defaults to None. val_split (Path | None, optional): _description_. Defaults to None. test_split (Path | None, optional): _description_. Defaults to None. ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True. allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None. output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None. predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite. predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite. constant_scale (float, optional): _description_. Defaults to 1. rgb_indices (list[int] | None, optional): _description_. Defaults to None. train_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False. drop_last (bool): Drop the last batch if it is not complete. Defaults to True. pin_memory (bool): If ``True``, the data loader will copy Tensors into device/CUDA pinned memory before returning them. Defaults to False. \"\"\" super () . __init__ ( GenericNonGeoSegmentationDataset , batch_size , num_workers , ** kwargs ) self . num_classes = num_classes self . img_grep = img_grep self . label_grep = label_grep self . train_root = train_data_root self . val_root = val_data_root self . test_root = test_data_root self . predict_root = predict_data_root self . train_split = train_split self . val_split = val_split self . test_split = test_split self . ignore_split_file_extensions = ignore_split_file_extensions self . allow_substring_split_file = allow_substring_split_file self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . drop_last = drop_last self . pin_memory = pin_memory self . train_label_data_root = train_label_data_root self . val_label_data_root = val_label_data_root self . test_label_data_root = test_label_data_root self . dataset_bands = dataset_bands self . predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands self . predict_output_bands = predict_output_bands if predict_output_bands else output_bands self . output_bands = output_bands self . rgb_indices = rgb_indices self . expand_temporal_dimension = expand_temporal_dimension self . reduce_zero_label = reduce_zero_label self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) # self.aug = AugmentationSequential( # K.Normalize(means, stds), # data_keys=[\"image\"], # ) means = load_from_file_or_attribute ( means ) stds = load_from_file_or_attribute ( stds ) self . aug = Normalize ( means , stds )","title":"__init__"},{"location":"data/#terratorch.datamodules.generic_scalar_label_data_module","text":"This module contains generic data modules for instantiation at runtime.","title":"generic_scalar_label_data_module"},{"location":"data/#terratorch.datamodules.generic_scalar_label_data_module.GenericNonGeoClassificationDataModule","text":"Bases: NonGeoDataModule This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoClassificationDatasets Source code in terratorch/datamodules/generic_scalar_label_data_module.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 class GenericNonGeoClassificationDataModule ( NonGeoDataModule ): \"\"\" This is a generic datamodule class for instantiating data modules at runtime. Composes several [GenericNonGeoClassificationDatasets][terratorch.datasets.GenericNonGeoClassificationDataset] \"\"\" def __init__ ( self , batch_size : int , num_workers : int , train_data_root : Path , val_data_root : Path , test_data_root : Path , means : list [ float ] | str , stds : list [ float ] | str , num_classes : int , predict_data_root : Path | None = None , train_split : Path | None = None , val_split : Path | None = None , test_split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , dataset_bands : list [ HLSBands | int ] | None = None , predict_dataset_bands : list [ HLSBands | int ] | None = None , output_bands : list [ HLSBands | int ] | None = None , constant_scale : float = 1 , rgb_indices : list [ int ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , expand_temporal_dimension : bool = False , no_data_replace : float = 0 , drop_last : bool = True , check_stackability : bool = True , ** kwargs : Any , ) -> None : \"\"\"Constructor Args: batch_size (int): _description_ num_workers (int): _description_ train_data_root (Path): _description_ val_data_root (Path): _description_ test_data_root (Path): _description_ means (list[float]): _description_ stds (list[float]): _description_ num_classes (int): _description_ predict_data_root (Path): _description_ train_split (Path | None, optional): _description_. Defaults to None. val_split (Path | None, optional): _description_. Defaults to None. test_split (Path | None, optional): _description_. Defaults to None. ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None. predict_dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None. output_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None. constant_scale (float, optional): _description_. Defaults to 1. rgb_indices (list[int] | None, optional): _description_. Defaults to None. train_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float): Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. drop_last (bool): Drop the last batch if it is not complete. Defaults to True. check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked. \"\"\" super () . __init__ ( GenericNonGeoClassificationDataset , batch_size , num_workers , ** kwargs ) self . num_classes = num_classes self . train_root = train_data_root self . val_root = val_data_root self . test_root = test_data_root self . predict_root = predict_data_root self . train_split = train_split self . val_split = val_split self . test_split = test_split self . ignore_split_file_extensions = ignore_split_file_extensions self . allow_substring_split_file = allow_substring_split_file self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . drop_last = drop_last self . dataset_bands = dataset_bands self . predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands self . output_bands = output_bands self . rgb_indices = rgb_indices self . expand_temporal_dimension = expand_temporal_dimension self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) # self.aug = AugmentationSequential( # K.Normalize(means, stds), # data_keys=[\"image\"], # ) means = load_from_file_or_attribute ( means ) stds = load_from_file_or_attribute ( stds ) self . aug = Normalize ( means , stds ) # self.aug = Normalize(means, stds) # self.collate_fn = collate_fn_list_dicts self . check_stackability = check_stackability def setup ( self , stage : str ) -> None : if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( self . train_root , self . num_classes , split = self . train_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . train_transform , no_data_replace = self . no_data_replace , expand_temporal_dimension = self . expand_temporal_dimension , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( self . val_root , self . num_classes , split = self . val_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . val_transform , no_data_replace = self . no_data_replace , expand_temporal_dimension = self . expand_temporal_dimension , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( self . test_root , self . num_classes , split = self . test_split , ignore_split_file_extensions = self . ignore_split_file_extensions , allow_substring_split_file = self . allow_substring_split_file , dataset_bands = self . dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . test_transform , no_data_replace = self . no_data_replace , expand_temporal_dimension = self . expand_temporal_dimension , ) if stage in [ \"predict\" ] and self . predict_root : self . predict_dataset = self . dataset_class ( self . predict_root , self . num_classes , dataset_bands = self . predict_dataset_bands , output_bands = self . output_bands , constant_scale = self . constant_scale , rgb_indices = self . rgb_indices , transform = self . test_transform , no_data_replace = self . no_data_replace , expand_temporal_dimension = self . expand_temporal_dimension , ) def _dataloader_factory ( self , split : str ) -> DataLoader [ dict [ str , Tensor ]]: \"\"\"Implement one or more PyTorch DataLoaders. Args: split: Either 'train', 'val', 'test', or 'predict'. Returns: A collection of data loaders specifying samples. Raises: MisconfigurationException: If :meth:`setup` does not define a dataset or sampler, or if the dataset or sampler has length 0. \"\"\" dataset = self . _valid_attribute ( f \" { split } _dataset\" , \"dataset\" ) batch_size = self . _valid_attribute ( f \" { split } _batch_size\" , \"batch_size\" ) if self . check_stackability : print ( \"Checking stackability.\" ) batch_size = check_dataset_stackability ( dataset , batch_size ) return DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = split == \"train\" , num_workers = self . num_workers , collate_fn = self . collate_fn , drop_last = split == \"train\" and self . drop_last , )","title":"GenericNonGeoClassificationDataModule"},{"location":"data/#terratorch.datamodules.generic_scalar_label_data_module.GenericNonGeoClassificationDataModule.__init__","text":"Constructor Parameters: batch_size ( int ) \u2013 description num_workers ( int ) \u2013 description train_data_root ( Path ) \u2013 description val_data_root ( Path ) \u2013 description test_data_root ( Path ) \u2013 description means ( list [ float ] ) \u2013 description stds ( list [ float ] ) \u2013 description num_classes ( int ) \u2013 description predict_data_root ( Path , default: None ) \u2013 description train_split ( Path | None , default: None ) \u2013 description . Defaults to None. val_split ( Path | None , default: None ) \u2013 description . Defaults to None. test_split ( Path | None , default: None ) \u2013 description . Defaults to None. ignore_split_file_extensions ( bool , default: True ) \u2013 Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". allow_substring_split_file ( bool , default: True ) \u2013 Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 description . Defaults to None. predict_dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 description . Defaults to None. output_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 description . Defaults to None. constant_scale ( float , default: 1 ) \u2013 description . Defaults to 1. rgb_indices ( list [ int ] | None , default: None ) \u2013 description . Defaults to None. train_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace ( float , default: 0 ) \u2013 Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension ( bool , default: False ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. drop_last ( bool , default: True ) \u2013 Drop the last batch if it is not complete. Defaults to True. check_stackability ( bool , default: True ) \u2013 Check if all the files in the dataset has the same size and can be stacked. Source code in terratorch/datamodules/generic_scalar_label_data_module.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def __init__ ( self , batch_size : int , num_workers : int , train_data_root : Path , val_data_root : Path , test_data_root : Path , means : list [ float ] | str , stds : list [ float ] | str , num_classes : int , predict_data_root : Path | None = None , train_split : Path | None = None , val_split : Path | None = None , test_split : Path | None = None , ignore_split_file_extensions : bool = True , allow_substring_split_file : bool = True , dataset_bands : list [ HLSBands | int ] | None = None , predict_dataset_bands : list [ HLSBands | int ] | None = None , output_bands : list [ HLSBands | int ] | None = None , constant_scale : float = 1 , rgb_indices : list [ int ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , expand_temporal_dimension : bool = False , no_data_replace : float = 0 , drop_last : bool = True , check_stackability : bool = True , ** kwargs : Any , ) -> None : \"\"\"Constructor Args: batch_size (int): _description_ num_workers (int): _description_ train_data_root (Path): _description_ val_data_root (Path): _description_ test_data_root (Path): _description_ means (list[float]): _description_ stds (list[float]): _description_ num_classes (int): _description_ predict_data_root (Path): _description_ train_split (Path | None, optional): _description_. Defaults to None. val_split (Path | None, optional): _description_. Defaults to None. test_split (Path | None, optional): _description_. Defaults to None. ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". allow_substring_split_file (bool, optional): Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True. dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None. predict_dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None. output_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None. constant_scale (float, optional): _description_. Defaults to 1. rgb_indices (list[int] | None, optional): _description_. Defaults to None. train_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). val_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). test_transform (Albumentations.Compose | None): Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2(). no_data_replace (float): Replace nan values in input images with this value. Defaults to 0. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False. drop_last (bool): Drop the last batch if it is not complete. Defaults to True. check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked. \"\"\" super () . __init__ ( GenericNonGeoClassificationDataset , batch_size , num_workers , ** kwargs ) self . num_classes = num_classes self . train_root = train_data_root self . val_root = val_data_root self . test_root = test_data_root self . predict_root = predict_data_root self . train_split = train_split self . val_split = val_split self . test_split = test_split self . ignore_split_file_extensions = ignore_split_file_extensions self . allow_substring_split_file = allow_substring_split_file self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . drop_last = drop_last self . dataset_bands = dataset_bands self . predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands self . output_bands = output_bands self . rgb_indices = rgb_indices self . expand_temporal_dimension = expand_temporal_dimension self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) # self.aug = AugmentationSequential( # K.Normalize(means, stds), # data_keys=[\"image\"], # ) means = load_from_file_or_attribute ( means ) stds = load_from_file_or_attribute ( stds ) self . aug = Normalize ( means , stds ) # self.aug = Normalize(means, stds) # self.collate_fn = collate_fn_list_dicts self . check_stackability = check_stackability","title":"__init__"},{"location":"data/#custom-datasets-and-data-modules","text":"Our custom datasets and data modules are crafted to handle specific data, offering enhanced control and flexibility throughout the workflow. In case you want to use TerraTorch on your specific data, we invite you to develop your own dataset and data module classes by following the examples below.","title":"Custom datasets and data modules"},{"location":"data/#datasets","text":"","title":"Datasets"},{"location":"data/#terratorch.datasets.biomassters","text":"","title":"biomassters"},{"location":"data/#terratorch.datasets.biomassters.BioMasstersNonGeo","text":"Bases: BioMassters BioMassters Dataset for Aboveground Biomass prediction. Dataset intended for Aboveground Biomass (AGB) prediction over Finnish forests based on Sentinel 1 and 2 data with corresponding target AGB mask values generated by Light Detection and Ranging (LiDAR). Dataset Format: .tif files for Sentinel 1 and 2 data .tif file for pixel wise AGB target mask .csv files for metadata regarding features and targets Dataset Features: 13,000 target AGB masks of size (256x256px) 12 months of data per target mask Sentinel 1 and Sentinel 2 data for each location Sentinel 1 available for every month Sentinel 2 available for almost every month (not available for every month due to ESA acquisition halt over the region during particular periods) If you use this dataset in your research, please cite the following paper: https://nascetti-a.github.io/BioMasster/ .. versionadded:: 0.5 Source code in terratorch/datasets/biomassters.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 class BioMasstersNonGeo ( BioMassters ): \"\"\"[BioMassters Dataset](https://huggingface.co/datasets/ibm-nasa-geospatial/BioMassters) for Aboveground Biomass prediction. Dataset intended for Aboveground Biomass (AGB) prediction over Finnish forests based on Sentinel 1 and 2 data with corresponding target AGB mask values generated by Light Detection and Ranging (LiDAR). Dataset Format: * .tif files for Sentinel 1 and 2 data * .tif file for pixel wise AGB target mask * .csv files for metadata regarding features and targets Dataset Features: * 13,000 target AGB masks of size (256x256px) * 12 months of data per target mask * Sentinel 1 and Sentinel 2 data for each location * Sentinel 1 available for every month * Sentinel 2 available for almost every month (not available for every month due to ESA acquisition halt over the region during particular periods) If you use this dataset in your research, please cite the following paper: * https://nascetti-a.github.io/BioMasster/ .. versionadded:: 0.5 \"\"\" S1_BAND_NAMES = [ \"VV_Asc\" , \"VH_Asc\" , \"VV_Desc\" , \"VH_Desc\" , \"RVI_Asc\" , \"RVI_Desc\" ] S2_BAND_NAMES = [ \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"NIR_NARROW\" , \"SWIR_1\" , \"SWIR_2\" , \"CLOUD_PROBABILITY\" , ] all_band_names = { \"S1\" : S1_BAND_NAMES , \"S2\" : S2_BAND_NAMES , } rgb_bands = { \"S1\" : [], \"S2\" : [ \"RED\" , \"GREEN\" , \"BLUE\" ], } valid_splits = ( \"train\" , \"test\" ) valid_sensors = ( \"S1\" , \"S2\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } default_metadata_filename = \"The_BioMassters_-_features_metadata.csv.csv\" def __init__ ( self , root = \"data\" , split : str = \"train\" , bands : dict [ str , Sequence [ str ]] | Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , mask_mean : float | None = 63.4584 , mask_std : float | None = 72.21242 , sensors : Sequence [ str ] = [ \"S1\" , \"S2\" ], as_time_series : bool = False , metadata_filename : str = default_metadata_filename , max_cloud_percentage : float | None = None , max_red_mean : float | None = None , include_corrupt : bool = True , subset : float = 1 , seed : int = 42 , use_four_frames : bool = False ) -> None : \"\"\"Initialize a new instance of BioMassters dataset. If ``as_time_series=False`` (the default), each time step becomes its own sample with the target being shared across multiple samples. Args: root: root directory where dataset can be found split: train or test split sensors: which sensors to consider for the sample, Sentinel 1 and/or Sentinel 2 ('S1', 'S2') as_time_series: whether or not to return all available time-steps or just a single one for a given target location metadata_filename: metadata file to be used max_cloud_percentage: maximum allowed cloud percentage for images max_red_mean: maximum allowed red_mean value for images include_corrupt: whether to include images marked as corrupted Raises: AssertionError: if ``split`` or ``sensors`` is invalid DatasetNotFoundError: If dataset is not found. \"\"\" self . root = root self . sensors = sensors self . bands = bands assert ( split in self . valid_splits ), f \"Please choose one of the valid splits: { self . valid_splits } .\" self . split = split assert set ( sensors ) . issubset ( set ( self . valid_sensors ) ), f \"Please choose a subset of valid sensors: { self . valid_sensors } .\" if len ( self . sensors ) == 1 : sens = self . sensors [ 0 ] self . band_indices = [ self . all_band_names [ sens ] . index ( band ) for band in self . bands [ sens ] ] else : self . band_indices = { sens : [ self . all_band_names [ sens ] . index ( band ) for band in self . bands [ sens ]] for sens in self . sensors } self . mask_mean = mask_mean self . mask_std = mask_std self . as_time_series = as_time_series self . metadata_filename = metadata_filename self . max_cloud_percentage = max_cloud_percentage self . max_red_mean = max_red_mean self . include_corrupt = include_corrupt self . subset = subset self . seed = seed self . use_four_frames = use_four_frames self . _verify () # open metadata csv files self . df = pd . read_csv ( os . path . join ( self . root , self . metadata_filename )) # Filter sensors self . df = self . df [ self . df [ \"satellite\" ] . isin ( self . sensors )] # Filter split self . df = self . df [ self . df [ \"split\" ] == self . split ] # Optional filtering self . _filter_and_select_data () # Optional subsampling self . _random_subsample () # generate numerical month from filename since first month is September # and has numerical index of 0 self . df [ \"num_month\" ] = ( self . df [ \"filename\" ] . str . split ( \"_\" , expand = True )[ 2 ] . str . split ( \".\" , expand = True )[ 0 ] . astype ( int ) ) # Set dataframe index depending on the task for easier indexing if self . as_time_series : self . df [ \"num_index\" ] = self . df . groupby ([ \"chip_id\" ]) . ngroup () else : filter_df = ( self . df . groupby ([ \"chip_id\" , \"month\" ])[ \"satellite\" ] . count () . reset_index () ) filter_df = filter_df [ filter_df [ \"satellite\" ] == len ( self . sensors ) ] . drop ( \"satellite\" , axis = 1 ) # Guarantee that each sample has corresponding number of images available self . df = self . df . merge ( filter_df , on = [ \"chip_id\" , \"month\" ], how = \"inner\" ) self . df [ \"num_index\" ] = self . df . groupby ([ \"chip_id\" , \"month\" ]) . ngroup () # Adjust transforms based on the number of sensors if len ( self . sensors ) == 1 : self . transform = transform if transform else default_transform elif transform is None : self . transform = MultimodalToTensor ( self . sensors ) else : transform = { s : transform [ s ] if s in transform else default_transform for s in self . sensors } self . transform = MultimodalTransforms ( transform , shared = False ) if self . use_four_frames : self . _select_4_frames () def __len__ ( self ) -> int : return len ( self . df [ \"num_index\" ] . unique ()) def _load_input ( self , filenames : list [ Path ]) -> Tensor : \"\"\"Load the input imagery at the index. Args: filenames: list of filenames corresponding to input Returns: input image \"\"\" filepaths = [ os . path . join ( self . root , f \" { self . split } _features\" , f ) for f in filenames ] arr_list = [ rasterio . open ( fp ) . read () for fp in filepaths ] if self . as_time_series : arr = np . stack ( arr_list , axis = 0 ) # (T, C, H, W) else : arr = np . concatenate ( arr_list , axis = 0 ) return arr . astype ( np . int32 ) def _load_target ( self , filename : Path ) -> Tensor : \"\"\"Load the target mask at the index. Args: filename: filename of target to index Returns: target mask \"\"\" with rasterio . open ( os . path . join ( self . root , f \" { self . split } _agbm\" , filename ), \"r\" ) as src : arr : np . typing . NDArray [ np . float64 ] = src . read () return arr def _compute_rvi ( self , img : np . ndarray , linear : np . ndarray , sens : str ) -> np . ndarray : \"\"\"Compute the RVI indices for S1 data.\"\"\" rvi_channels = [] if self . as_time_series : if \"RVI_Asc\" in self . bands [ sens ]: try : vv_asc_index = self . all_band_names [ \"S1\" ] . index ( \"VV_Asc\" ) vh_asc_index = self . all_band_names [ \"S1\" ] . index ( \"VH_Asc\" ) except ValueError as e : msg = f \"RVI_Asc needs band: { e } \" raise ValueError ( msg ) from e VV = linear [:, vv_asc_index , :, :] VH = linear [:, vh_asc_index , :, :] rvi_asc = 4 * VH / ( VV + VH + 1e-6 ) rvi_asc = np . expand_dims ( rvi_asc , axis = 1 ) rvi_channels . append ( rvi_asc ) if \"RVI_Desc\" in self . bands [ sens ]: try : vv_desc_index = self . all_band_names [ \"S1\" ] . index ( \"VV_Desc\" ) vh_desc_index = self . all_band_names [ \"S1\" ] . index ( \"VH_Desc\" ) except ValueError as e : msg = f \"RVI_Desc needs band: { e } \" raise ValueError ( msg ) from e VV_desc = linear [:, vv_desc_index , :, :] VH_desc = linear [:, vh_desc_index , :, :] rvi_desc = 4 * VH_desc / ( VV_desc + VH_desc + 1e-6 ) rvi_desc = np . expand_dims ( rvi_desc , axis = 1 ) rvi_channels . append ( rvi_desc ) if rvi_channels : rvi_concat = np . concatenate ( rvi_channels , axis = 1 ) img = np . concatenate ([ img , rvi_concat ], axis = 1 ) else : if \"RVI_Asc\" in self . bands [ sens ]: if linear . shape [ 0 ] < 2 : msg = f \"Not enough bands to calculate RVI_Asc. Available bands: { linear . shape [ 0 ] } \" raise ValueError ( msg ) VV = linear [ 0 ] VH = linear [ 1 ] rvi_asc = 4 * VH / ( VV + VH + 1e-6 ) rvi_asc = np . expand_dims ( rvi_asc , axis = 0 ) rvi_channels . append ( rvi_asc ) if \"RVI_Desc\" in self . bands [ sens ]: if linear . shape [ 0 ] < 4 : msg = f \"Not enough bands to calculate RVI_Desc. Available bands: { linear . shape [ 0 ] } \" raise ValueError ( msg ) VV_desc = linear [ 2 ] VH_desc = linear [ 3 ] rvi_desc = 4 * VH_desc / ( VV_desc + VH_desc + 1e-6 ) rvi_desc = np . expand_dims ( rvi_desc , axis = 0 ) rvi_channels . append ( rvi_desc ) if rvi_channels : rvi_concat = np . concatenate ( rvi_channels , axis = 0 ) img = np . concatenate ([ linear , rvi_concat ], axis = 0 ) return img def _select_4_frames ( self ): \"\"\"Filter the dataset to select only 4 frames per sample.\"\"\" if \"cloud_percentage\" in self . df . columns : self . df = self . df . sort_values ( by = [ \"chip_id\" , \"cloud_percentage\" ]) else : self . df = self . df . sort_values ( by = [ \"chip_id\" , \"num_month\" ]) self . df = ( self . df . groupby ( \"chip_id\" ) . head ( 4 ) # Select the first 4 frames per chip . reset_index ( drop = True ) ) def _process_sensor_images ( self , sens : str , sens_filepaths : list [ str ]) -> np . ndarray : \"\"\"Process images for a given sensor.\"\"\" img = self . _load_input ( sens_filepaths ) if sens == \"S1\" : img = img . astype ( np . float32 ) linear = 10 ** ( img / 10 ) img = self . _compute_rvi ( img , linear , sens ) if self . as_time_series : img = img . transpose ( 0 , 2 , 3 , 1 ) # (T, H, W, C) else : img = img . transpose ( 1 , 2 , 0 ) # (H, W, C) if len ( self . sensors ) == 1 : img = img [ ... , self . band_indices ] else : img = img [ ... , self . band_indices [ sens ]] return img def __getitem__ ( self , index : int ) -> dict : sample_df = self . df [ self . df [ \"num_index\" ] == index ] . copy () # Sort by satellite and month sample_df . sort_values ( by = [ \"satellite\" , \"num_month\" ], inplace = True , ascending = True ) filepaths = sample_df [ \"filename\" ] . tolist () output = {} if len ( self . sensors ) == 1 : sens = self . sensors [ 0 ] sens_filepaths = [ fp for fp in filepaths if sens in fp ] img = self . _process_sensor_images ( sens , sens_filepaths ) output [ \"image\" ] = img . astype ( np . float32 ) else : for sens in self . sensors : sens_filepaths = [ fp for fp in filepaths if sens in fp ] img = self . _process_sensor_images ( sens , sens_filepaths ) output [ sens ] = img . astype ( np . float32 ) # Load target target_filename = sample_df [ \"corresponding_agbm\" ] . unique ()[ 0 ] target = np . array ( self . _load_target ( Path ( target_filename ))) target = target . transpose ( 1 , 2 , 0 ) output [ \"mask\" ] = target if self . transform : if len ( self . sensors ) == 1 : output = self . transform ( ** output ) else : output = self . transform ( output ) output [ \"mask\" ] = output [ \"mask\" ] . squeeze () . float () return output def _filter_and_select_data ( self ): if ( self . max_cloud_percentage is not None and \"cloud_percentage\" in self . df . columns ): self . df = self . df [ self . df [ \"cloud_percentage\" ] <= self . max_cloud_percentage ] if self . max_red_mean is not None and \"red_mean\" in self . df . columns : self . df = self . df [ self . df [ \"red_mean\" ] <= self . max_red_mean ] if not self . include_corrupt and \"corrupt_values\" in self . df . columns : self . df = self . df [ self . df [ \"corrupt_values\" ] is False ] def _random_subsample ( self ): if self . split == \"train\" and self . subset < 1.0 : num_samples = int ( len ( self . df [ \"num_index\" ] . unique ()) * self . subset ) if self . seed is not None : random . seed ( self . seed ) selected_indices = random . sample ( list ( self . df [ \"num_index\" ] . unique ()), num_samples ) self . df = self . df [ self . df [ \"num_index\" ] . isin ( selected_indices )] self . df . reset_index ( drop = True , inplace = True ) def plot ( self , sample : dict [ str , Tensor ], show_titles : bool = True , suptitle : str | None = None , ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` show_titles: flag indicating whether to show titles above each panel suptitle: optional suptitle to use for figure Returns: a matplotlib Figure with the rendered sample \"\"\" # Determine if the sample contains multiple sensors or a single sensor if isinstance ( sample [ \"image\" ], dict ): ncols = len ( self . sensors ) + 1 else : ncols = 2 # One for the image and one for the mask showing_predictions = \"prediction\" in sample if showing_predictions : ncols += 1 fig , axs = plt . subplots ( 1 , ncols = ncols , figsize = ( 5 * ncols , 10 )) if isinstance ( sample [ \"image\" ], dict ): # Multiple sensors case for idx , sens in enumerate ( self . sensors ): img = sample [ \"image\" ][ sens ] . numpy () if self . as_time_series : # Plot last time step img = img [:, - 1 , ... ] if sens == \"S2\" : img = img [[ 2 , 1 , 0 ], ... ] . transpose ( 1 , 2 , 0 ) img = percentile_normalization ( img ) else : co_polarization = img [ 0 ] # transmit == receive cross_polarization = img [ 1 ] # transmit != receive ratio = co_polarization / ( cross_polarization + 1e-6 ) co_polarization = np . clip ( co_polarization / 0.3 , 0 , 1 ) cross_polarization = np . clip ( cross_polarization / 0.05 , 0 , 1 ) ratio = np . clip ( ratio / 25 , 0 , 1 ) img = np . stack ( ( co_polarization , cross_polarization , ratio ), axis = 0 ) img = img . transpose ( 1 , 2 , 0 ) # Convert to (H, W, 3) axs [ idx ] . imshow ( img ) axs [ idx ] . axis ( \"off\" ) if show_titles : axs [ idx ] . set_title ( sens ) mask_idx = len ( self . sensors ) else : # Single sensor case sens = self . sensors [ 0 ] img = sample [ \"image\" ] . numpy () if self . as_time_series : # Plot last time step img = img [:, - 1 , ... ] if sens == \"S2\" : img = img [[ 2 , 1 , 0 ], ... ] . transpose ( 1 , 2 , 0 ) img = percentile_normalization ( img ) else : co_polarization = img [ 0 ] # transmit == receive cross_polarization = img [ 1 ] # transmit != receive ratio = co_polarization / ( cross_polarization + 1e-6 ) co_polarization = np . clip ( co_polarization / 0.3 , 0 , 1 ) cross_polarization = np . clip ( cross_polarization / 0.05 , 0 , 1 ) ratio = np . clip ( ratio / 25 , 0 , 1 ) img = np . stack ( ( co_polarization , cross_polarization , ratio ), axis = 0 ) img = img . transpose ( 1 , 2 , 0 ) # Convert to (H, W, 3) axs [ 0 ] . imshow ( img ) axs [ 0 ] . axis ( \"off\" ) if show_titles : axs [ 0 ] . set_title ( sens ) mask_idx = 1 # Plot target mask if \"mask\" in sample : target = sample [ \"mask\" ] . squeeze () target_im = axs [ mask_idx ] . imshow ( target , cmap = \"YlGn\" ) plt . colorbar ( target_im , ax = axs [ mask_idx ], fraction = 0.046 , pad = 0.04 ) axs [ mask_idx ] . axis ( \"off\" ) if show_titles : axs [ mask_idx ] . set_title ( \"Target\" ) # Plot prediction if available if showing_predictions : pred_idx = mask_idx + 1 prediction = sample [ \"prediction\" ] . squeeze () pred_im = axs [ pred_idx ] . imshow ( prediction , cmap = \"YlGn\" ) plt . colorbar ( pred_im , ax = axs [ pred_idx ], fraction = 0.046 , pad = 0.04 ) axs [ pred_idx ] . axis ( \"off\" ) if show_titles : axs [ pred_idx ] . set_title ( \"Prediction\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig","title":"BioMasstersNonGeo"},{"location":"data/#terratorch.datasets.biomassters.BioMasstersNonGeo.__init__","text":"Initialize a new instance of BioMassters dataset. If as_time_series=False (the default), each time step becomes its own sample with the target being shared across multiple samples. Parameters: root \u2013 root directory where dataset can be found split ( str , default: 'train' ) \u2013 train or test split sensors ( Sequence [ str ] , default: ['S1', 'S2'] ) \u2013 which sensors to consider for the sample, Sentinel 1 and/or Sentinel 2 ('S1', 'S2') as_time_series ( bool , default: False ) \u2013 whether or not to return all available time-steps or just a single one for a given target location metadata_filename ( str , default: default_metadata_filename ) \u2013 metadata file to be used max_cloud_percentage ( float | None , default: None ) \u2013 maximum allowed cloud percentage for images max_red_mean ( float | None , default: None ) \u2013 maximum allowed red_mean value for images include_corrupt ( bool , default: True ) \u2013 whether to include images marked as corrupted Raises: AssertionError \u2013 if split or sensors is invalid DatasetNotFoundError \u2013 If dataset is not found. Source code in terratorch/datasets/biomassters.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def __init__ ( self , root = \"data\" , split : str = \"train\" , bands : dict [ str , Sequence [ str ]] | Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , mask_mean : float | None = 63.4584 , mask_std : float | None = 72.21242 , sensors : Sequence [ str ] = [ \"S1\" , \"S2\" ], as_time_series : bool = False , metadata_filename : str = default_metadata_filename , max_cloud_percentage : float | None = None , max_red_mean : float | None = None , include_corrupt : bool = True , subset : float = 1 , seed : int = 42 , use_four_frames : bool = False ) -> None : \"\"\"Initialize a new instance of BioMassters dataset. If ``as_time_series=False`` (the default), each time step becomes its own sample with the target being shared across multiple samples. Args: root: root directory where dataset can be found split: train or test split sensors: which sensors to consider for the sample, Sentinel 1 and/or Sentinel 2 ('S1', 'S2') as_time_series: whether or not to return all available time-steps or just a single one for a given target location metadata_filename: metadata file to be used max_cloud_percentage: maximum allowed cloud percentage for images max_red_mean: maximum allowed red_mean value for images include_corrupt: whether to include images marked as corrupted Raises: AssertionError: if ``split`` or ``sensors`` is invalid DatasetNotFoundError: If dataset is not found. \"\"\" self . root = root self . sensors = sensors self . bands = bands assert ( split in self . valid_splits ), f \"Please choose one of the valid splits: { self . valid_splits } .\" self . split = split assert set ( sensors ) . issubset ( set ( self . valid_sensors ) ), f \"Please choose a subset of valid sensors: { self . valid_sensors } .\" if len ( self . sensors ) == 1 : sens = self . sensors [ 0 ] self . band_indices = [ self . all_band_names [ sens ] . index ( band ) for band in self . bands [ sens ] ] else : self . band_indices = { sens : [ self . all_band_names [ sens ] . index ( band ) for band in self . bands [ sens ]] for sens in self . sensors } self . mask_mean = mask_mean self . mask_std = mask_std self . as_time_series = as_time_series self . metadata_filename = metadata_filename self . max_cloud_percentage = max_cloud_percentage self . max_red_mean = max_red_mean self . include_corrupt = include_corrupt self . subset = subset self . seed = seed self . use_four_frames = use_four_frames self . _verify () # open metadata csv files self . df = pd . read_csv ( os . path . join ( self . root , self . metadata_filename )) # Filter sensors self . df = self . df [ self . df [ \"satellite\" ] . isin ( self . sensors )] # Filter split self . df = self . df [ self . df [ \"split\" ] == self . split ] # Optional filtering self . _filter_and_select_data () # Optional subsampling self . _random_subsample () # generate numerical month from filename since first month is September # and has numerical index of 0 self . df [ \"num_month\" ] = ( self . df [ \"filename\" ] . str . split ( \"_\" , expand = True )[ 2 ] . str . split ( \".\" , expand = True )[ 0 ] . astype ( int ) ) # Set dataframe index depending on the task for easier indexing if self . as_time_series : self . df [ \"num_index\" ] = self . df . groupby ([ \"chip_id\" ]) . ngroup () else : filter_df = ( self . df . groupby ([ \"chip_id\" , \"month\" ])[ \"satellite\" ] . count () . reset_index () ) filter_df = filter_df [ filter_df [ \"satellite\" ] == len ( self . sensors ) ] . drop ( \"satellite\" , axis = 1 ) # Guarantee that each sample has corresponding number of images available self . df = self . df . merge ( filter_df , on = [ \"chip_id\" , \"month\" ], how = \"inner\" ) self . df [ \"num_index\" ] = self . df . groupby ([ \"chip_id\" , \"month\" ]) . ngroup () # Adjust transforms based on the number of sensors if len ( self . sensors ) == 1 : self . transform = transform if transform else default_transform elif transform is None : self . transform = MultimodalToTensor ( self . sensors ) else : transform = { s : transform [ s ] if s in transform else default_transform for s in self . sensors } self . transform = MultimodalTransforms ( transform , shared = False ) if self . use_four_frames : self . _select_4_frames ()","title":"__init__"},{"location":"data/#terratorch.datasets.biomassters.BioMasstersNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 a sample returned by :meth: __getitem__ show_titles ( bool , default: True ) \u2013 flag indicating whether to show titles above each panel suptitle ( str | None , default: None ) \u2013 optional suptitle to use for figure Returns: Figure \u2013 a matplotlib Figure with the rendered sample Source code in terratorch/datasets/biomassters.py 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 def plot ( self , sample : dict [ str , Tensor ], show_titles : bool = True , suptitle : str | None = None , ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` show_titles: flag indicating whether to show titles above each panel suptitle: optional suptitle to use for figure Returns: a matplotlib Figure with the rendered sample \"\"\" # Determine if the sample contains multiple sensors or a single sensor if isinstance ( sample [ \"image\" ], dict ): ncols = len ( self . sensors ) + 1 else : ncols = 2 # One for the image and one for the mask showing_predictions = \"prediction\" in sample if showing_predictions : ncols += 1 fig , axs = plt . subplots ( 1 , ncols = ncols , figsize = ( 5 * ncols , 10 )) if isinstance ( sample [ \"image\" ], dict ): # Multiple sensors case for idx , sens in enumerate ( self . sensors ): img = sample [ \"image\" ][ sens ] . numpy () if self . as_time_series : # Plot last time step img = img [:, - 1 , ... ] if sens == \"S2\" : img = img [[ 2 , 1 , 0 ], ... ] . transpose ( 1 , 2 , 0 ) img = percentile_normalization ( img ) else : co_polarization = img [ 0 ] # transmit == receive cross_polarization = img [ 1 ] # transmit != receive ratio = co_polarization / ( cross_polarization + 1e-6 ) co_polarization = np . clip ( co_polarization / 0.3 , 0 , 1 ) cross_polarization = np . clip ( cross_polarization / 0.05 , 0 , 1 ) ratio = np . clip ( ratio / 25 , 0 , 1 ) img = np . stack ( ( co_polarization , cross_polarization , ratio ), axis = 0 ) img = img . transpose ( 1 , 2 , 0 ) # Convert to (H, W, 3) axs [ idx ] . imshow ( img ) axs [ idx ] . axis ( \"off\" ) if show_titles : axs [ idx ] . set_title ( sens ) mask_idx = len ( self . sensors ) else : # Single sensor case sens = self . sensors [ 0 ] img = sample [ \"image\" ] . numpy () if self . as_time_series : # Plot last time step img = img [:, - 1 , ... ] if sens == \"S2\" : img = img [[ 2 , 1 , 0 ], ... ] . transpose ( 1 , 2 , 0 ) img = percentile_normalization ( img ) else : co_polarization = img [ 0 ] # transmit == receive cross_polarization = img [ 1 ] # transmit != receive ratio = co_polarization / ( cross_polarization + 1e-6 ) co_polarization = np . clip ( co_polarization / 0.3 , 0 , 1 ) cross_polarization = np . clip ( cross_polarization / 0.05 , 0 , 1 ) ratio = np . clip ( ratio / 25 , 0 , 1 ) img = np . stack ( ( co_polarization , cross_polarization , ratio ), axis = 0 ) img = img . transpose ( 1 , 2 , 0 ) # Convert to (H, W, 3) axs [ 0 ] . imshow ( img ) axs [ 0 ] . axis ( \"off\" ) if show_titles : axs [ 0 ] . set_title ( sens ) mask_idx = 1 # Plot target mask if \"mask\" in sample : target = sample [ \"mask\" ] . squeeze () target_im = axs [ mask_idx ] . imshow ( target , cmap = \"YlGn\" ) plt . colorbar ( target_im , ax = axs [ mask_idx ], fraction = 0.046 , pad = 0.04 ) axs [ mask_idx ] . axis ( \"off\" ) if show_titles : axs [ mask_idx ] . set_title ( \"Target\" ) # Plot prediction if available if showing_predictions : pred_idx = mask_idx + 1 prediction = sample [ \"prediction\" ] . squeeze () pred_im = axs [ pred_idx ] . imshow ( prediction , cmap = \"YlGn\" ) plt . colorbar ( pred_im , ax = axs [ pred_idx ], fraction = 0.046 , pad = 0.04 ) axs [ pred_idx ] . axis ( \"off\" ) if show_titles : axs [ pred_idx ] . set_title ( \"Prediction\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.burn_intensity","text":"","title":"burn_intensity"},{"location":"data/#terratorch.datasets.burn_intensity.BurnIntensityNonGeo","text":"Bases: NonGeoDataset Dataset implementation for Burn Intensity classification . Source code in terratorch/datasets/burn_intensity.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 class BurnIntensityNonGeo ( NonGeoDataset ): \"\"\"Dataset implementation for [Burn Intensity classification](https://huggingface.co/datasets/ibm-nasa-geospatial/burn_intensity).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" , \"NIR\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } class_names = ( \"No burn\" , \"Unburned to Very Low\" , \"Low Severity\" , \"Moderate Severity\" , \"High Severity\" ) CSV_FILES = { \"limited\" : \"BS_files_with_less_than_25_percent_zeros.csv\" , \"full\" : \"BS_files_raw.csv\" , } num_classes = 5 splits = { \"train\" : \"train\" , \"val\" : \"val\" } time_steps = [ \"pre\" , \"during\" , \"post\" ] def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , use_full_data : bool = True , no_data_replace : float | None = 0.0001 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ) -> None : \"\"\"Initialize the BurnIntensity dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train' or 'val'. bands (Sequence[str]): Bands to output. Defaults to all bands. transform (Optional[A.Compose]): Albumentations transform to be applied. use_metadata (bool): Whether to return metadata info (location). use_full_data (bool): Wheter to use full data or data with less than 25 percent zeros. no_data_replace (Optional[float]): Value to replace NaNs in images. no_label_replace (Optional[int]): Value to replace NaNs in labels. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) # Read the CSV file to get the list of cases to include csv_file_key = \"full\" if use_full_data else \"limited\" csv_path = self . data_root / self . CSV_FILES [ csv_file_key ] df = pd . read_csv ( csv_path ) casenames = df [ \"Case_Name\" ] . tolist () split_file = self . data_root / f \" { split } .txt\" with open ( split_file ) as f : split_images = [ line . strip () for line in f . readlines ()] split_images = [ img for img in split_images if self . _extract_casename ( img ) in casenames ] # Build the samples list self . samples = [] for image_filename in split_images : image_files = [] for time_step in self . time_steps : image_file = self . data_root / time_step / image_filename image_files . append ( str ( image_file )) mask_filename = image_filename . replace ( \"HLS_\" , \"BS_\" ) mask_file = self . data_root / \"pre\" / mask_filename self . samples . append ({ \"image_files\" : image_files , \"mask_file\" : str ( mask_file ), \"casename\" : self . _extract_casename ( image_filename ), }) self . use_metadata = use_metadata self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . transform = transform if transform else default_transform def _extract_basename ( self , filepath : str ) -> str : \"\"\"Extract the base filename without extension.\"\"\" return os . path . splitext ( os . path . basename ( filepath ))[ 0 ] def _extract_casename ( self , filename : str ) -> str : \"\"\"Extract the casename from the filename.\"\"\" basename = self . _extract_basename ( filename ) # Remove 'HLS_' or 'BS_' prefix casename = basename . replace ( \"HLS_\" , \"\" ) . replace ( \"BS_\" , \"\" ) return casename def __len__ ( self ) -> int : return len ( self . samples ) def _get_coords ( self , image : DataArray ) -> torch . Tensor : pixel_scale = image . rio . resolution () width , height = image . rio . width , image . rio . height left , bottom , right , top = image . rio . bounds () tie_point_x , tie_point_y = left , top center_col = width / 2 center_row = height / 2 center_lon = tie_point_x + ( center_col * pixel_scale [ 0 ]) center_lat = tie_point_y - ( center_row * pixel_scale [ 1 ]) lat_lon = np . asarray ([ center_lat , center_lon ]) return torch . tensor ( lat_lon , dtype = torch . float32 ) def __getitem__ ( self , index : int ) -> dict [ str , Any ]: sample = self . samples [ index ] image_files = sample [ \"image_files\" ] mask_file = sample [ \"mask_file\" ] images = [] for idx , image_file in enumerate ( image_files ): image = self . _load_file ( Path ( image_file ), nan_replace = self . no_data_replace ) if idx == 0 and self . use_metadata : location_coords = self . _get_coords ( image ) image = image . to_numpy () image = np . moveaxis ( image , 0 , - 1 ) image = image [ ... , self . band_indices ] images . append ( image ) images = np . stack ( images , axis = 0 ) # (T, H, W, C) output = { \"image\" : images . astype ( np . float32 ), \"mask\" : self . _load_file ( Path ( mask_file ), nan_replace = self . no_label_replace ) . to_numpy ()[ 0 ] } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () if self . use_metadata : output [ \"location_coords\" ] = location_coords return output def _load_file ( self , path : Path , nan_replace : float | int | None = None ) -> DataArray : data = rioxarray . open_rasterio ( path , masked = True ) if nan_replace is not None : data = data . fillna ( nan_replace ) return data def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Any : \"\"\"Plot a sample from the dataset. Args: sample: A sample returned by `__getitem__`. suptitle: Optional string to use as a suptitle. Returns: A matplotlib Figure with the rendered sample. \"\"\" num_images = len ( self . time_steps ) + 2 if \"prediction\" in sample : num_images += 1 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) images = sample [ \"image\" ] # (C, T, H, W) mask = sample [ \"mask\" ] . numpy () num_classes = len ( np . unique ( mask )) fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 5 , 5 )) for i in range ( len ( self . time_steps )): image = images [:, i , :, :] # (C, H, W) image = np . transpose ( image , ( 1 , 2 , 0 )) # (H, W, C) rgb_image = image [ ... , rgb_indices ] rgb_image = ( rgb_image - rgb_image . min ()) / ( rgb_image . max () - rgb_image . min () + 1e-8 ) rgb_image = np . clip ( rgb_image , 0 , 1 ) ax [ i ] . imshow ( rgb_image ) ax [ i ] . axis ( \"off\" ) ax [ i ] . set_title ( f \" { self . time_steps [ i ] . capitalize () } Image\" ) cmap = plt . get_cmap ( \"jet\" , num_classes ) norm = Normalize ( vmin = 0 , vmax = num_classes - 1 ) mask_ax_index = len ( self . time_steps ) ax [ mask_ax_index ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ mask_ax_index ] . axis ( \"off\" ) ax [ mask_ax_index ] . set_title ( \"Ground Truth Mask\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () pred_ax_index = mask_ax_index + 1 ax [ pred_ax_index ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ pred_ax_index ] . axis ( \"off\" ) ax [ pred_ax_index ] . set_title ( \"Predicted Mask\" ) legend_ax_index = - 1 class_names = sample . get ( \"class_names\" , self . class_names ) positions = np . linspace ( 0 , 1 , num_classes ) if num_classes > 1 else [ 0.5 ] legend_handles = [ mpatches . Patch ( color = cmap ( pos ), label = class_names [ i ]) for i , pos in enumerate ( positions ) ] ax [ legend_ax_index ] . legend ( handles = legend_handles , loc = \"center\" ) ax [ legend_ax_index ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) plt . tight_layout () return fig","title":"BurnIntensityNonGeo"},{"location":"data/#terratorch.datasets.burn_intensity.BurnIntensityNonGeo.__init__","text":"Initialize the BurnIntensity dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train' or 'val'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to output. Defaults to all bands. transform ( Optional [ Compose ] , default: None ) \u2013 Albumentations transform to be applied. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (location). use_full_data ( bool , default: True ) \u2013 Wheter to use full data or data with less than 25 percent zeros. no_data_replace ( Optional [ float ] , default: 0.0001 ) \u2013 Value to replace NaNs in images. no_label_replace ( Optional [ int ] , default: -1 ) \u2013 Value to replace NaNs in labels. Source code in terratorch/datasets/burn_intensity.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , use_full_data : bool = True , no_data_replace : float | None = 0.0001 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ) -> None : \"\"\"Initialize the BurnIntensity dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train' or 'val'. bands (Sequence[str]): Bands to output. Defaults to all bands. transform (Optional[A.Compose]): Albumentations transform to be applied. use_metadata (bool): Whether to return metadata info (location). use_full_data (bool): Wheter to use full data or data with less than 25 percent zeros. no_data_replace (Optional[float]): Value to replace NaNs in images. no_label_replace (Optional[int]): Value to replace NaNs in labels. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) # Read the CSV file to get the list of cases to include csv_file_key = \"full\" if use_full_data else \"limited\" csv_path = self . data_root / self . CSV_FILES [ csv_file_key ] df = pd . read_csv ( csv_path ) casenames = df [ \"Case_Name\" ] . tolist () split_file = self . data_root / f \" { split } .txt\" with open ( split_file ) as f : split_images = [ line . strip () for line in f . readlines ()] split_images = [ img for img in split_images if self . _extract_casename ( img ) in casenames ] # Build the samples list self . samples = [] for image_filename in split_images : image_files = [] for time_step in self . time_steps : image_file = self . data_root / time_step / image_filename image_files . append ( str ( image_file )) mask_filename = image_filename . replace ( \"HLS_\" , \"BS_\" ) mask_file = self . data_root / \"pre\" / mask_filename self . samples . append ({ \"image_files\" : image_files , \"mask_file\" : str ( mask_file ), \"casename\" : self . _extract_casename ( image_filename ), }) self . use_metadata = use_metadata self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.burn_intensity.BurnIntensityNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Any \u2013 A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/burn_intensity.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Any : \"\"\"Plot a sample from the dataset. Args: sample: A sample returned by `__getitem__`. suptitle: Optional string to use as a suptitle. Returns: A matplotlib Figure with the rendered sample. \"\"\" num_images = len ( self . time_steps ) + 2 if \"prediction\" in sample : num_images += 1 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) images = sample [ \"image\" ] # (C, T, H, W) mask = sample [ \"mask\" ] . numpy () num_classes = len ( np . unique ( mask )) fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 5 , 5 )) for i in range ( len ( self . time_steps )): image = images [:, i , :, :] # (C, H, W) image = np . transpose ( image , ( 1 , 2 , 0 )) # (H, W, C) rgb_image = image [ ... , rgb_indices ] rgb_image = ( rgb_image - rgb_image . min ()) / ( rgb_image . max () - rgb_image . min () + 1e-8 ) rgb_image = np . clip ( rgb_image , 0 , 1 ) ax [ i ] . imshow ( rgb_image ) ax [ i ] . axis ( \"off\" ) ax [ i ] . set_title ( f \" { self . time_steps [ i ] . capitalize () } Image\" ) cmap = plt . get_cmap ( \"jet\" , num_classes ) norm = Normalize ( vmin = 0 , vmax = num_classes - 1 ) mask_ax_index = len ( self . time_steps ) ax [ mask_ax_index ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ mask_ax_index ] . axis ( \"off\" ) ax [ mask_ax_index ] . set_title ( \"Ground Truth Mask\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () pred_ax_index = mask_ax_index + 1 ax [ pred_ax_index ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ pred_ax_index ] . axis ( \"off\" ) ax [ pred_ax_index ] . set_title ( \"Predicted Mask\" ) legend_ax_index = - 1 class_names = sample . get ( \"class_names\" , self . class_names ) positions = np . linspace ( 0 , 1 , num_classes ) if num_classes > 1 else [ 0.5 ] legend_handles = [ mpatches . Patch ( color = cmap ( pos ), label = class_names [ i ]) for i , pos in enumerate ( positions ) ] ax [ legend_ax_index ] . legend ( handles = legend_handles , loc = \"center\" ) ax [ legend_ax_index ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) plt . tight_layout () return fig","title":"plot"},{"location":"data/#terratorch.datasets.carbonflux","text":"","title":"carbonflux"},{"location":"data/#terratorch.datasets.carbonflux.CarbonFluxNonGeo","text":"Bases: NonGeoDataset Dataset for Carbon Flux regression from HLS images and MERRA data. Source code in terratorch/datasets/carbonflux.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 class CarbonFluxNonGeo ( NonGeoDataset ): \"\"\"Dataset for [Carbon Flux](https://huggingface.co/datasets/ibm-nasa-geospatial/hls_merra2_gppFlux) regression from HLS images and MERRA data.\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" , \"NIR\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" , ) merra_var_names = ( \"T2MIN\" , \"T2MAX\" , \"T2MEAN\" , \"TSMDEWMEAN\" , \"GWETROOT\" , \"LHLAND\" , \"SHLAND\" , \"SWLAND\" , \"PARDFLAND\" , \"PRECTOTLAND\" ) splits = { \"train\" : \"train\" , \"test\" : \"test\" } BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } metadata_file = \"data_train_hls_37sites_v0_1.csv\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , gpp_mean : float | None = None , gpp_std : float | None = None , no_data_replace : float | None = 0.0001 , use_metadata : bool = False , modalities : Sequence [ str ] = ( \"image\" , \"merra_vars\" ) ) -> None : \"\"\"Initialize the CarbonFluxNonGeo dataset. Args: data_root (str): Path to the data root directory. split (str): 'train' or 'test'. bands (Sequence[str]): Bands to use. Defaults to all bands. transform (Optional[A.Compose]): Albumentations transform to be applied. use_metadata (bool): Whether to return metadata (coordinates and date). merra_means (Sequence[float]): Means for MERRA data normalization. merra_stds (Sequence[float]): Standard deviations for MERRA data normalization. gpp_mean (float): Mean for GPP normalization. gpp_std (float): Standard deviation for GPP normalization. no_data_replace (Optional[float]): Value to replace NO_DATA values in images. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( band ) for band in bands ] self . data_root = Path ( data_root ) # Load the CSV file with metadata csv_file = self . data_root / self . metadata_file df = pd . read_csv ( csv_file ) # Get list of image filenames in the split directory image_dir = self . data_root / self . split image_files = [ f . name for f in image_dir . glob ( \"*.tiff\" )] df [ \"Chip\" ] = df [ \"Chip\" ] . str . replace ( \".tif$\" , \".tiff\" , regex = True ) # Filter the DataFrame to include only rows with 'Chip' in image_files df = df [ df [ \"Chip\" ] . isin ( image_files )] # Build the samples list self . samples = [] for _ , row in df . iterrows (): image_filename = row [ \"Chip\" ] image_path = image_dir / image_filename # MERRA vectors merra_vars = row [ list ( self . merra_var_names )] . values . astype ( np . float32 ) # GPP target gpp = row [ \"GPP\" ] image_path = image_dir / row [ \"Chip\" ] merra_vars = row [ list ( self . merra_var_names )] . values . astype ( np . float32 ) gpp = row [ \"GPP\" ] self . samples . append ({ \"image_path\" : str ( image_path ), \"merra_vars\" : merra_vars , \"gpp\" : gpp , }) if gpp_mean is None or gpp_std is None : msg = \"Mean and standard deviation for GPP must be provided.\" raise ValueError ( msg ) self . gpp_mean = gpp_mean self . gpp_std = gpp_std self . use_metadata = use_metadata self . modalities = modalities self . no_data_replace = no_data_replace if transform is None : self . transform = MultimodalToTensor ( self . modalities ) else : transform = { m : transform [ m ] if m in transform else default_transform for m in self . modalities } self . transform = MultimodalTransforms ( transform , shared = False ) def __len__ ( self ) -> int : return len ( self . samples ) def _load_file ( self , path : str , nan_replace : float | int | None = None ): data = rioxarray . open_rasterio ( path , masked = True ) if nan_replace is not None : data = data . fillna ( nan_replace ) return data def _get_coords ( self , image ) -> torch . Tensor : \"\"\"Extract the center coordinates from the image geospatial metadata.\"\"\" pixel_scale = image . rio . resolution () width , height = image . rio . width , image . rio . height left , bottom , right , top = image . rio . bounds () tie_point_x , tie_point_y = left , top center_col = width / 2 center_row = height / 2 center_lon = tie_point_x + ( center_col * pixel_scale [ 0 ]) center_lat = tie_point_y - ( center_row * pixel_scale [ 1 ]) src_crs = image . rio . crs dst_crs = \"EPSG:4326\" transformer = pyproj . Transformer . from_crs ( src_crs , dst_crs , always_xy = True ) lon , lat = transformer . transform ( center_lon , center_lat ) coords = np . array ([ lat , lon ], dtype = np . float32 ) return torch . from_numpy ( coords ) def _get_date ( self , filename : str ) -> torch . Tensor : \"\"\"Extract the date from the filename.\"\"\" base_filename = os . path . basename ( filename ) pattern = r \"HLS\\.. {3} \\.[A-Z0-9] {6} \\.(?P<date>\\d {7} T\\d {6} )\\..*\\.tiff$\" match = re . match ( pattern , base_filename ) if not match : msg = f \"Filename { filename } does not match expected pattern.\" raise ValueError ( msg ) date_str = match . group ( \"date\" ) year = int ( date_str [: 4 ]) julian_day = int ( date_str [ 4 : 7 ]) date_tensor = torch . tensor ([ year , julian_day ], dtype = torch . int32 ) return date_tensor def __getitem__ ( self , idx : int ) -> dict [ str , Any ]: sample = self . samples [ idx ] image_path = sample [ \"image_path\" ] image = self . _load_file ( image_path , nan_replace = self . no_data_replace ) if self . use_metadata : location_coords = self . _get_coords ( image ) temporal_coords = self . _get_date ( os . path . basename ( image_path )) image = image . to_numpy () # (C, H, W) image = image [ self . band_indices , ... ] image = np . moveaxis ( image , 0 , - 1 ) # (H, W, C) merra_vars = np . array ( sample [ \"merra_vars\" ]) target = np . array ( sample [ \"gpp\" ]) target_norm = ( target - self . gpp_mean ) / self . gpp_std target_norm = torch . tensor ( target_norm , dtype = torch . float32 ) output = { \"image\" : image . astype ( np . float32 ), \"merra_vars\" : merra_vars , } if self . transform : output = self . transform ( output ) output = { \"image\" : { m : output [ m ] for m in self . modalities if m in output }, \"mask\" : target_norm } if self . use_metadata : output [ \"location_coords\" ] = location_coords output [ \"temporal_coords\" ] = temporal_coords return output def plot ( self , sample : dict [ str , Any ], suptitle : str | None = None ) -> Any : \"\"\"Plot a sample from the dataset. Args: sample: A sample returned by `__getitem__`. suptitle: Optional title for the figure. Returns: A matplotlib figure with the rendered sample. \"\"\" image = sample [ \"image\" ] . numpy () image = np . transpose ( image , ( 1 , 2 , 0 )) # (H, W, C) rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) rgb_image = image [ ... , rgb_indices ] rgb_image = ( rgb_image - rgb_image . min ()) / ( rgb_image . max () - rgb_image . min () + 1e-8 ) rgb_image = np . clip ( rgb_image , 0 , 1 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( \"Image\" ) if suptitle : plt . suptitle ( suptitle ) plt . tight_layout () return fig","title":"CarbonFluxNonGeo"},{"location":"data/#terratorch.datasets.carbonflux.CarbonFluxNonGeo.__init__","text":"Initialize the CarbonFluxNonGeo dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 'train' or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to use. Defaults to all bands. transform ( Optional [ Compose ] , default: None ) \u2013 Albumentations transform to be applied. use_metadata ( bool , default: False ) \u2013 Whether to return metadata (coordinates and date). merra_means ( Sequence [ float ] ) \u2013 Means for MERRA data normalization. merra_stds ( Sequence [ float ] ) \u2013 Standard deviations for MERRA data normalization. gpp_mean ( float , default: None ) \u2013 Mean for GPP normalization. gpp_std ( float , default: None ) \u2013 Standard deviation for GPP normalization. no_data_replace ( Optional [ float ] , default: 0.0001 ) \u2013 Value to replace NO_DATA values in images. Source code in terratorch/datasets/carbonflux.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , gpp_mean : float | None = None , gpp_std : float | None = None , no_data_replace : float | None = 0.0001 , use_metadata : bool = False , modalities : Sequence [ str ] = ( \"image\" , \"merra_vars\" ) ) -> None : \"\"\"Initialize the CarbonFluxNonGeo dataset. Args: data_root (str): Path to the data root directory. split (str): 'train' or 'test'. bands (Sequence[str]): Bands to use. Defaults to all bands. transform (Optional[A.Compose]): Albumentations transform to be applied. use_metadata (bool): Whether to return metadata (coordinates and date). merra_means (Sequence[float]): Means for MERRA data normalization. merra_stds (Sequence[float]): Standard deviations for MERRA data normalization. gpp_mean (float): Mean for GPP normalization. gpp_std (float): Standard deviation for GPP normalization. no_data_replace (Optional[float]): Value to replace NO_DATA values in images. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( band ) for band in bands ] self . data_root = Path ( data_root ) # Load the CSV file with metadata csv_file = self . data_root / self . metadata_file df = pd . read_csv ( csv_file ) # Get list of image filenames in the split directory image_dir = self . data_root / self . split image_files = [ f . name for f in image_dir . glob ( \"*.tiff\" )] df [ \"Chip\" ] = df [ \"Chip\" ] . str . replace ( \".tif$\" , \".tiff\" , regex = True ) # Filter the DataFrame to include only rows with 'Chip' in image_files df = df [ df [ \"Chip\" ] . isin ( image_files )] # Build the samples list self . samples = [] for _ , row in df . iterrows (): image_filename = row [ \"Chip\" ] image_path = image_dir / image_filename # MERRA vectors merra_vars = row [ list ( self . merra_var_names )] . values . astype ( np . float32 ) # GPP target gpp = row [ \"GPP\" ] image_path = image_dir / row [ \"Chip\" ] merra_vars = row [ list ( self . merra_var_names )] . values . astype ( np . float32 ) gpp = row [ \"GPP\" ] self . samples . append ({ \"image_path\" : str ( image_path ), \"merra_vars\" : merra_vars , \"gpp\" : gpp , }) if gpp_mean is None or gpp_std is None : msg = \"Mean and standard deviation for GPP must be provided.\" raise ValueError ( msg ) self . gpp_mean = gpp_mean self . gpp_std = gpp_std self . use_metadata = use_metadata self . modalities = modalities self . no_data_replace = no_data_replace if transform is None : self . transform = MultimodalToTensor ( self . modalities ) else : transform = { m : transform [ m ] if m in transform else default_transform for m in self . modalities } self . transform = MultimodalTransforms ( transform , shared = False )","title":"__init__"},{"location":"data/#terratorch.datasets.carbonflux.CarbonFluxNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Any ] ) \u2013 A sample returned by __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional title for the figure. Returns: Any \u2013 A matplotlib figure with the rendered sample. Source code in terratorch/datasets/carbonflux.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def plot ( self , sample : dict [ str , Any ], suptitle : str | None = None ) -> Any : \"\"\"Plot a sample from the dataset. Args: sample: A sample returned by `__getitem__`. suptitle: Optional title for the figure. Returns: A matplotlib figure with the rendered sample. \"\"\" image = sample [ \"image\" ] . numpy () image = np . transpose ( image , ( 1 , 2 , 0 )) # (H, W, C) rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) rgb_image = image [ ... , rgb_indices ] rgb_image = ( rgb_image - rgb_image . min ()) / ( rgb_image . max () - rgb_image . min () + 1e-8 ) rgb_image = np . clip ( rgb_image , 0 , 1 ) fig , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( \"Image\" ) if suptitle : plt . suptitle ( suptitle ) plt . tight_layout () return fig","title":"plot"},{"location":"data/#terratorch.datasets.forestnet","text":"","title":"forestnet"},{"location":"data/#terratorch.datasets.forestnet.ForestNetNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for ForestNet . Source code in terratorch/datasets/forestnet.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 class ForestNetNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [ForestNet](https://huggingface.co/datasets/ibm-nasa-geospatial/ForestNet).\"\"\" all_band_names = ( \"RED\" , \"GREEN\" , \"BLUE\" , \"NIR\" , \"SWIR_1\" , \"SWIR_2\" ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" , ) splits = ( \"train\" , \"test\" , \"val\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } default_label_map = { # noqa: RUF012 \"Plantation\" : 0 , \"Smallholder agriculture\" : 1 , \"Grassland shrubland\" : 2 , \"Other\" : 3 , } def __init__ ( self , data_root : str , split : str = \"train\" , label_map : dict [ str , int ] = default_label_map , transform : A . Compose | None = None , fraction : float = 1.0 , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], use_metadata : bool = False , ) -> None : \"\"\" Initialize the ForestNetNonGeo dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. label_map (Dict[str, int]): Mapping from label names to integer labels. transform: Transformations to be applied to the images. fraction (float): Fraction of the dataset to use. Defaults to 1.0 (use all data). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits ) } .\" raise ValueError ( msg ) self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . label_map = label_map # Load the CSV file corresponding to the split csv_file = self . data_root / f \" { split } _filtered.csv\" original_df = pd . read_csv ( csv_file ) # Apply stratified sampling if fraction < 1.0 if fraction < 1.0 : sss = StratifiedShuffleSplit ( n_splits = 1 , test_size = 1 - fraction , random_state = 47 ) stratified_indices , _ = next ( sss . split ( original_df , original_df [ \"merged_label\" ])) self . dataset = original_df . iloc [ stratified_indices ] . reset_index ( drop = True ) else : self . dataset = original_df self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . dataset ) def _get_coords ( self , event_path : Path ) -> torch . Tensor : auxiliary_path = event_path / \"auxiliary\" osm_json_path = auxiliary_path / \"osm.json\" with open ( osm_json_path ) as f : osm_data = json . load ( f ) lat = float ( osm_data [ \"closest_city\" ][ \"lat\" ]) lon = float ( osm_data [ \"closest_city\" ][ \"lon\" ]) lat_lon = np . asarray ([ lat , lon ]) return torch . tensor ( lat_lon , dtype = torch . float32 ) def _get_dates ( self , image_files : list ) -> list : dates = [] pattern = re . compile ( r \"(\\d {4} )_(\\d {2} )_(\\d {2} )_cloud_\\d+\\.(png|npy)\" ) for img_path in image_files : match = pattern . search ( img_path ) year , month , day = int ( match . group ( 1 )), int ( match . group ( 2 )), int ( match . group ( 3 )) date_obj = datetime . datetime ( year , month , day ) # noqa: DTZ001 julian_day = date_obj . timetuple () . tm_yday date_tensor = torch . tensor ([ year , julian_day ], dtype = torch . int32 ) dates . append ( date_tensor ) return torch . stack ( dates , dim = 0 ) def __getitem__ ( self , index : int ): path = self . data_root / self . dataset [ \"example_path\" ][ index ] label = self . map_label ( index ) visible_images , infrared_images , temporal_coords = self . _load_images ( path ) visible_images = np . stack ( visible_images , axis = 0 ) infrared_images = np . stack ( infrared_images , axis = 0 ) merged_images = np . concatenate ([ visible_images , infrared_images ], axis =- 1 ) merged_images = merged_images [ ... , self . band_indices ] # (T, H, W, 2C) output = { \"image\" : merged_images . astype ( np . float32 ) } if self . transform : output = self . transform ( ** output ) if self . use_metadata : location_coords = self . _get_coords ( path ) output [ \"location_coords\" ] = location_coords output [ \"temporal_coords\" ] = temporal_coords output [ \"label\" ] = label return output def _load_images ( self , path : str ): \"\"\"Load visible and infrared images from the given event path\"\"\" visible_image_files = glob . glob ( os . path . join ( path , \"images/visible/*_cloud_*.png\" )) infra_image_files = glob . glob ( os . path . join ( path , \"images/infrared/*_cloud_*.npy\" )) selected_visible_images = self . select_images ( visible_image_files ) selected_infra_images = self . select_images ( infra_image_files ) dates = None if self . use_metadata : dates = self . _get_dates ( selected_visible_images ) vis_images = [ np . array ( Image . open ( img )) for img in selected_visible_images ] # (T, H, W, C) inf_images = [ np . load ( img , allow_pickle = True ) for img in selected_infra_images ] # (T, H, W, C) return vis_images , inf_images , dates def least_cloudy_image ( self , image_files ): pattern = re . compile ( r \"(\\d {4} )_\\d {2} _\\d {2} _cloud_(\\d+)\\.(png|npy)\" ) lowest_cloud_images = defaultdict ( lambda : { \"path\" : None , \"cloud_value\" : float ( \"inf\" )}) for path in image_files : match = pattern . search ( path ) if match : year , cloud_value = match . group ( 1 ), int ( match . group ( 2 )) if cloud_value < lowest_cloud_images [ year ][ \"cloud_value\" ]: lowest_cloud_images [ year ] = { \"path\" : path , \"cloud_value\" : cloud_value } return [ info [ \"path\" ] for info in lowest_cloud_images . values ()] def match_timesteps ( self , image_files , selected_images ): if len ( selected_images ) < 3 : extra_imgs = [ img for img in image_files if img not in selected_images ] selected_images += extra_imgs [: 3 - len ( selected_images )] while len ( selected_images ) < 3 : selected_images . append ( selected_images [ - 1 ]) return selected_images [: 3 ] def select_images ( self , image_files ): selected = self . least_cloudy_image ( image_files ) return self . match_timesteps ( image_files , selected ) def map_label ( self , index : int ) -> torch . Tensor : \"\"\"Map the label name to an integer label.\"\"\" label_name = self . dataset [ \"merged_label\" ][ index ] label = self . label_map [ label_name ] return label def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ): num_images = sample [ \"image\" ] . shape [ 1 ] + 1 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) fig , ax = plt . subplots ( 1 , num_images , figsize = ( 15 , 5 )) for i in range ( sample [ \"image\" ] . shape [ 1 ]): image = sample [ \"image\" ][:, i , :, :] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [ ... , rgb_indices ] rgb_image = ( rgb_image - rgb_image . min ()) / ( rgb_image . max () - rgb_image . min () + 1e-8 ) rgb_image = np . clip ( rgb_image , 0 , 1 ) ax [ i ] . imshow ( rgb_image ) ax [ i ] . axis ( \"off\" ) ax [ i ] . set_title ( f \"Timestep { i + 1 } \" ) legend_handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = \"blue\" )] legend_label = [ self . label_map . get ( sample [ \"label\" ], \"Unknown Label\" )] ax [ - 1 ] . legend ( legend_handles , legend_label , loc = \"center\" ) ax [ - 1 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) plt . tight_layout () return fig","title":"ForestNetNonGeo"},{"location":"data/#terratorch.datasets.forestnet.ForestNetNonGeo.__init__","text":"Initialize the ForestNetNonGeo dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. label_map ( Dict [ str , int ] , default: default_label_map ) \u2013 Mapping from label names to integer labels. transform ( Compose | None , default: None ) \u2013 Transformations to be applied to the images. fraction ( float , default: 1.0 ) \u2013 Fraction of the dataset to use. Defaults to 1.0 (use all data). Source code in terratorch/datasets/forestnet.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def __init__ ( self , data_root : str , split : str = \"train\" , label_map : dict [ str , int ] = default_label_map , transform : A . Compose | None = None , fraction : float = 1.0 , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], use_metadata : bool = False , ) -> None : \"\"\" Initialize the ForestNetNonGeo dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. label_map (Dict[str, int]): Mapping from label names to integer labels. transform: Transformations to be applied to the images. fraction (float): Fraction of the dataset to use. Defaults to 1.0 (use all data). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits ) } .\" raise ValueError ( msg ) self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . label_map = label_map # Load the CSV file corresponding to the split csv_file = self . data_root / f \" { split } _filtered.csv\" original_df = pd . read_csv ( csv_file ) # Apply stratified sampling if fraction < 1.0 if fraction < 1.0 : sss = StratifiedShuffleSplit ( n_splits = 1 , test_size = 1 - fraction , random_state = 47 ) stratified_indices , _ = next ( sss . split ( original_df , original_df [ \"merged_label\" ])) self . dataset = original_df . iloc [ stratified_indices ] . reset_index ( drop = True ) else : self . dataset = original_df self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.forestnet.ForestNetNonGeo.map_label","text":"Map the label name to an integer label. Source code in terratorch/datasets/forestnet.py 189 190 191 192 193 def map_label ( self , index : int ) -> torch . Tensor : \"\"\"Map the label name to an integer label.\"\"\" label_name = self . dataset [ \"merged_label\" ][ index ] label = self . label_map [ label_name ] return label","title":"map_label"},{"location":"data/#terratorch.datasets.fire_scars","text":"","title":"fire_scars"},{"location":"data/#terratorch.datasets.fire_scars.FireScarsHLS","text":"Bases: RasterDataset RasterDataset implementation for fire scars input images. Source code in terratorch/datasets/fire_scars.py 216 217 218 219 220 221 222 223 224 225 class FireScarsHLS ( RasterDataset ): \"\"\"RasterDataset implementation for fire scars input images.\"\"\" filename_glob = \"subsetted*_merged.tif\" filename_regex = r \"subsetted_512x512_HLS\\..30\\.. {6} \\.(?P<date>[0-9]*)\\.v1.4_merged.tif\" date_format = \"%Y%j\" is_image = True separate_files = False all_bands = dataclasses . field ( default_factory = [ \"B02\" , \"B03\" , \"B04\" , \"B8A\" , \"B11\" , \"B12\" ]) rgb_bands = dataclasses . field ( default_factory = [ \"B04\" , \"B03\" , \"B02\" ])","title":"FireScarsHLS"},{"location":"data/#terratorch.datasets.fire_scars.FireScarsNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for fire scars . Source code in terratorch/datasets/fire_scars.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 class FireScarsNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [fire scars](https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" , \"NIR_NARROW\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } num_classes = 2 splits = { \"train\" : \"training\" , \"val\" : \"validation\" } # Only train and val splits available def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ) -> None : \"\"\"Constructor Args: data_root (str): Path to the data root directory. bands (list[str]): Bands that should be output by the dataset. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If None, does no replacement. Defaults to 0. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1. use_metadata (bool): whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { self . splits } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) input_dir = self . data_root / split_name self . image_files = sorted ( glob . glob ( os . path . join ( input_dir , \"*_merged.tif\" ))) self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( input_dir , \"*.mask.tif\" ))) self . use_metadata = use_metadata self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def _get_date ( self , index : int ) -> torch . Tensor : file_name = self . image_files [ index ] base_filename = os . path . basename ( file_name ) filename_regex = r \"subsetted_512x512_HLS\\.S30\\.T[0-9A-Z] {5} \\.(?P<date>[0-9]+)\\.v1\\.4_merged\\.tif\" match = re . match ( filename_regex , base_filename ) date_str = match . group ( \"date\" ) year = int ( date_str [: 4 ]) julian_day = int ( date_str [ 4 :]) return torch . tensor ([[ year , julian_day ]], dtype = torch . float32 ) def _get_coords ( self , image : DataArray ) -> torch . Tensor : px = image . x . shape [ 0 ] // 2 py = image . y . shape [ 0 ] // 2 # get center point to reproject to lat/lon point = image . isel ( band = 0 , x = slice ( px , px + 1 ), y = slice ( py , py + 1 )) point = point . rio . reproject ( \"epsg:4326\" ) lat_lon = np . asarray ([ point . y [ 0 ], point . x [ 0 ]]) return torch . tensor ( lat_lon , dtype = torch . float32 ) def __getitem__ ( self , index : int ) -> dict [ str , Any ]: image = self . _load_file ( self . image_files [ index ], nan_replace = self . no_data_replace ) location_coords , temporal_coords = None , None if self . use_metadata : location_coords = self . _get_coords ( image ) temporal_coords = self . _get_date ( index ) # to channels last image = image . to_numpy () image = np . moveaxis ( image , 0 , - 1 ) # filter bands image = image [ ... , self . band_indices ] output = { \"image\" : image . astype ( np . float32 ), \"mask\" : self . _load_file ( self . segmentation_mask_files [ index ], nan_replace = self . no_label_replace ) . to_numpy ()[ 0 ], } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () if self . use_metadata : output [ \"location_coords\" ] = location_coords output [ \"temporal_coords\" ] = temporal_coords return output def _load_file ( self , path : Path , nan_replace : int | float | None = None ) -> DataArray : data = rioxarray . open_rasterio ( path , masked = True ) if nan_replace is not None : data = data . fillna ( nan_replace ) return data def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample \"\"\" num_images = 4 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) # RGB -> channels-last image = sample [ \"image\" ][ rgb_indices , ... ] . permute ( 1 , 2 , 0 ) . numpy () mask = sample [ \"mask\" ] . numpy () image = clip_image_percentile ( image ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] num_images += 1 else : prediction = None fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 5 ), layout = \"compressed\" ) ax [ 0 ] . axis ( \"off\" ) norm = mpl . colors . Normalize ( vmin = 0 , vmax = self . num_classes - 1 ) ax [ 1 ] . axis ( \"off\" ) ax [ 1 ] . title . set_text ( \"Image\" ) ax [ 1 ] . imshow ( image ) ax [ 2 ] . axis ( \"off\" ) ax [ 2 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ 2 ] . imshow ( mask , cmap = \"jet\" , norm = norm ) ax [ 3 ] . axis ( \"off\" ) ax [ 3 ] . title . set_text ( \"GT Mask on Image\" ) ax [ 3 ] . imshow ( image ) ax [ 3 ] . imshow ( mask , cmap = \"jet\" , alpha = 0.3 , norm = norm ) if \"prediction\" in sample : ax [ 4 ] . title . set_text ( \"Predicted Mask\" ) ax [ 4 ] . imshow ( prediction , cmap = \"jet\" , norm = norm ) cmap = plt . get_cmap ( \"jet\" ) legend_data = [[ i , cmap ( norm ( i )), str ( i )] for i in range ( self . num_classes )] handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = tuple ( v for v in c )) for k , c , n in legend_data ] labels = [ n for k , c , n in legend_data ] ax [ 0 ] . legend ( handles , labels , loc = \"center\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig","title":"FireScarsNonGeo"},{"location":"data/#terratorch.datasets.fire_scars.FireScarsNonGeo.__init__","text":"Constructor Parameters: data_root ( str ) \u2013 Path to the data root directory. bands ( list [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands that should be output by the dataset. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2(). no_data_replace ( float | None , default: 0 ) \u2013 Replace nan values in input images with this value. If None, does no replacement. Defaults to 0. no_label_replace ( int | None , default: -1 ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to -1. use_metadata ( bool , default: False ) \u2013 whether to return metadata info (time and location). Source code in terratorch/datasets/fire_scars.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ) -> None : \"\"\"Constructor Args: data_root (str): Path to the data root directory. bands (list[str]): Bands that should be output by the dataset. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If None, does no replacement. Defaults to 0. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1. use_metadata (bool): whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { self . splits } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) input_dir = self . data_root / split_name self . image_files = sorted ( glob . glob ( os . path . join ( input_dir , \"*_merged.tif\" ))) self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( input_dir , \"*.mask.tif\" ))) self . use_metadata = use_metadata self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.fire_scars.FireScarsNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 a sample returned by :meth: __getitem__ suptitle ( str | None , default: None ) \u2013 optional string to use as a suptitle Returns: Figure \u2013 a matplotlib Figure with the rendered sample Source code in terratorch/datasets/fire_scars.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample \"\"\" num_images = 4 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) # RGB -> channels-last image = sample [ \"image\" ][ rgb_indices , ... ] . permute ( 1 , 2 , 0 ) . numpy () mask = sample [ \"mask\" ] . numpy () image = clip_image_percentile ( image ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] num_images += 1 else : prediction = None fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 5 ), layout = \"compressed\" ) ax [ 0 ] . axis ( \"off\" ) norm = mpl . colors . Normalize ( vmin = 0 , vmax = self . num_classes - 1 ) ax [ 1 ] . axis ( \"off\" ) ax [ 1 ] . title . set_text ( \"Image\" ) ax [ 1 ] . imshow ( image ) ax [ 2 ] . axis ( \"off\" ) ax [ 2 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ 2 ] . imshow ( mask , cmap = \"jet\" , norm = norm ) ax [ 3 ] . axis ( \"off\" ) ax [ 3 ] . title . set_text ( \"GT Mask on Image\" ) ax [ 3 ] . imshow ( image ) ax [ 3 ] . imshow ( mask , cmap = \"jet\" , alpha = 0.3 , norm = norm ) if \"prediction\" in sample : ax [ 4 ] . title . set_text ( \"Predicted Mask\" ) ax [ 4 ] . imshow ( prediction , cmap = \"jet\" , norm = norm ) cmap = plt . get_cmap ( \"jet\" ) legend_data = [[ i , cmap ( norm ( i )), str ( i )] for i in range ( self . num_classes )] handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = tuple ( v for v in c )) for k , c , n in legend_data ] labels = [ n for k , c , n in legend_data ] ax [ 0 ] . legend ( handles , labels , loc = \"center\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.fire_scars.FireScarsSegmentationMask","text":"Bases: RasterDataset RasterDataset implementation for fire scars segmentation mask. Can be easily merged with input images using the & operator. Source code in terratorch/datasets/fire_scars.py 228 229 230 231 232 233 234 235 236 237 class FireScarsSegmentationMask ( RasterDataset ): \"\"\"RasterDataset implementation for fire scars segmentation mask. Can be easily merged with input images using the & operator. \"\"\" filename_glob = \"subsetted*.mask.tif\" filename_regex = r \"subsetted_512x512_HLS\\..30\\.. {6} \\.(?P<date>[0-9]*)\\.v1.4.mask.tif\" date_format = \"%Y%j\" is_image = False separate_files = False","title":"FireScarsSegmentationMask"},{"location":"data/#terratorch.datasets.landslide4sense","text":"","title":"landslide4sense"},{"location":"data/#terratorch.datasets.landslide4sense.Landslide4SenseNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for Landslide4Sense . Source code in terratorch/datasets/landslide4sense.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 class Landslide4SenseNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [Landslide4Sense](https://huggingface.co/datasets/ibm-nasa-geospatial/Landslide4sense).\"\"\" all_band_names = ( \"COASTAL AEROSOL\" , \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"WATER_VAPOR\" , \"CIRRUS\" , \"SWIR_1\" , \"SWIR_2\" , \"SLOPE\" , \"DEM\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"validation\" , \"test\" : \"test\" } def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , ) -> None : \"\"\"Initialize the Landslide4Sense dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'validation', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . data_directory = Path ( data_root ) images_dir = self . data_directory / \"images\" / split_name annotations_dir = self . data_directory / \"annotations\" / split_name self . image_files = sorted ( images_dir . glob ( \"image_*.h5\" )) self . mask_files = sorted ( annotations_dir . glob ( \"mask_*.h5\" )) self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: image_file = self . image_files [ index ] mask_file = self . mask_files [ index ] with h5py . File ( image_file , \"r\" ) as h5file : image = np . array ( h5file [ \"img\" ])[ ... , self . band_indices ] with h5py . File ( mask_file , \"r\" ) as h5file : mask = np . array ( h5file [ \"mask\" ]) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = ( rgb_image - rgb_image . min ( axis = ( 0 , 1 ))) * ( 1 / rgb_image . max ( axis = ( 0 , 1 ))) rgb_image = np . clip ( rgb_image , 0 , 1 ) num_classes = len ( np . unique ( mask )) cmap = colormaps [ \"jet\" ] norm = Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if sample . get ( \"class_names\" ): class_names = sample [ \"class_names\" ] legend_handles = [ mpatches . Patch ( color = cmap ( i ), label = class_names [ i ]) for i in range ( num_classes ) ] ax [ 0 ] . legend ( handles = legend_handles , bbox_to_anchor = ( 1.05 , 1 ), loc = \"upper left\" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"Landslide4SenseNonGeo"},{"location":"data/#terratorch.datasets.landslide4sense.Landslide4SenseNonGeo.__init__","text":"Initialize the Landslide4Sense dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'validation', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). Source code in terratorch/datasets/landslide4sense.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , ) -> None : \"\"\"Initialize the Landslide4Sense dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'validation', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . data_directory = Path ( data_root ) images_dir = self . data_directory / \"images\" / split_name annotations_dir = self . data_directory / \"annotations\" / split_name self . image_files = sorted ( images_dir . glob ( \"image_*.h5\" )) self . mask_files = sorted ( annotations_dir . glob ( \"mask_*.h5\" )) self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.m_eurosat","text":"","title":"m_eurosat"},{"location":"data/#terratorch.datasets.m_eurosat.MEuroSATNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for M-EuroSAT . Source code in terratorch/datasets/m_eurosat.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class MEuroSATNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-EuroSAT](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"COASTAL_AEROSOL\" , \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"NIR_NARROW\" , \"WATER_VAPOR\" , \"CIRRUS\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-eurosat\" partition_file_template = \" {partition} _partition.json\" label_map_file = \"label_map.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] \\ self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir label_map_path = self . data_directory / self . label_map_file with open ( label_map_path ) as file : self . label_map = json . load ( file ) self . id_to_class = { img_id : cls for cls , ids in self . label_map . items () for img_id in ids } partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] image_id = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) label_class = self . id_to_class [ image_id ] label_index = list ( self . label_map . keys ()) . index ( label_class ) output = { \"image\" : image . astype ( np . float32 )} if self . transform : output = self . transform ( ** output ) output [ \"label\" ] = label_index return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label_index = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) class_names = list ( self . label_map . keys ()) class_name = class_names [ label_index ] fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { class_name } \" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"MEuroSATNonGeo"},{"location":"data/#terratorch.datasets.m_eurosat.MEuroSATNonGeo.__init__","text":"Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. Source code in terratorch/datasets/m_eurosat.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] \\ self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir label_map_path = self . data_directory / self . label_map_file with open ( label_map_path ) as file : self . label_map = json . load ( file ) self . id_to_class = { img_id : cls for cls , ids in self . label_map . items () for img_id in ids } partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.m_eurosat.MEuroSATNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_eurosat.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label_index = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) class_names = list ( self . label_map . keys ()) class_name = class_names [ label_index ] fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { class_name } \" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.m_bigearthnet","text":"","title":"m_bigearthnet"},{"location":"data/#terratorch.datasets.m_bigearthnet.MBigEarthNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for M-BigEarthNet . Source code in terratorch/datasets/m_bigearthnet.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class MBigEarthNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-BigEarthNet](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"COASTAL_AEROSOL\" , \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"NIR_NARROW\" , \"WATER_VAPOR\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-bigearthnet\" label_map_file = \"label_stats.json\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir label_map_path = self . data_directory / self . label_map_file with open ( label_map_path ) as file : self . label_map = json . load ( file ) self . num_classes = len ( next ( iter ( self . label_map . values ()))) partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] image_id = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) labels_vector = self . label_map [ image_id ] labels_tensor = torch . tensor ( labels_vector , dtype = torch . float ) output = { \"image\" : image } if self . transform : output = self . transform ( ** output ) output [ \"label\" ] = labels_tensor return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # Convert to (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) active_labels = [ i for i , lbl in enumerate ( label ) if lbl == 1 ] fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Active Labels: { active_labels } \" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig","title":"MBigEarthNonGeo"},{"location":"data/#terratorch.datasets.m_bigearthnet.MBigEarthNonGeo.__init__","text":"Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. Source code in terratorch/datasets/m_bigearthnet.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir label_map_path = self . data_directory / self . label_map_file with open ( label_map_path ) as file : self . label_map = json . load ( file ) self . num_classes = len ( next ( iter ( self . label_map . values ()))) partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.m_bigearthnet.MBigEarthNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_bigearthnet.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # Convert to (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) active_labels = [ i for i , lbl in enumerate ( label ) if lbl == 1 ] fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Active Labels: { active_labels } \" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.m_brick_kiln","text":"","title":"m_brick_kiln"},{"location":"data/#terratorch.datasets.m_brick_kiln.MBrickKilnNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for M-BrickKiln . Source code in terratorch/datasets/m_brick_kiln.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class MBrickKilnNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-BrickKiln](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"COASTAL_AEROSOL\" , \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"NIR_NARROW\" , \"WATER_VAPOR\" , \"CIRRUS\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-brick-kiln\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] image_id = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) attr_dict = pickle . loads ( ast . literal_eval ( h5file . attrs [ \"pickle\" ])) class_index = attr_dict [ \"label\" ] output = { \"image\" : image . astype ( np . float32 )} if self . transform : output = self . transform ( ** output ) output [ \"label\" ] = class_index return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # Convert to (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { label } \" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig","title":"MBrickKilnNonGeo"},{"location":"data/#terratorch.datasets.m_brick_kiln.MBrickKilnNonGeo.__init__","text":"Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. Source code in terratorch/datasets/m_brick_kiln.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.m_brick_kiln.MBrickKilnNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_brick_kiln.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # Convert to (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { label } \" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.m_forestnet","text":"","title":"m_forestnet"},{"location":"data/#terratorch.datasets.m_forestnet.MForestNetNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for M-ForestNet . Source code in terratorch/datasets/m_forestnet.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 class MForestNetNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-ForestNet](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" , \"NIR\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-forestnet\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] image_id = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) attr_dict = pickle . loads ( ast . literal_eval ( h5file . attrs [ \"pickle\" ])) # noqa: S301 class_index = attr_dict [ \"label\" ] output = { \"image\" : image . astype ( np . float32 )} if self . transform : output = self . transform ( ** output ) output [ \"label\" ] = class_index if self . use_metadata : temporal_coords = self . _get_date ( image_id ) location_coords = self . _get_coords ( image_id ) output [ \"temporal_coords\" ] = temporal_coords output [ \"location_coords\" ] = location_coords return output def _get_coords ( self , image_id : str ) -> torch . Tensor : \"\"\"Extract spatial coordinates from the image ID. Args: image_id (str): The ID of the image. Returns: torch.Tensor: Tensor containing latitude and longitude. \"\"\" lat_str , lon_str , _ = image_id . split ( \"_\" , 2 ) latitude = float ( lat_str ) longitude = float ( lon_str ) return torch . tensor ([ latitude , longitude ], dtype = torch . float32 ) def _get_date ( self , image_id : str ) -> torch . Tensor : _ , _ , date_str = image_id . split ( \"_\" , 2 ) date = pd . to_datetime ( date_str , format = \"%Y_%m_ %d \" ) return torch . tensor ([[ date . year , date . dayofyear - 1 ]], dtype = torch . float32 ) def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { label } \" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"MForestNetNonGeo"},{"location":"data/#terratorch.datasets.m_forestnet.MForestNetNonGeo.__init__","text":"Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (time and location). Source code in terratorch/datasets/m_forestnet.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.m_forestnet.MForestNetNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_forestnet.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { label } \" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.m_so2sat","text":"","title":"m_so2sat"},{"location":"data/#terratorch.datasets.m_so2sat.MSo2SatNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for M-So2Sat . Source code in terratorch/datasets/m_so2sat.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class MSo2SatNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-So2Sat](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"VH_REAL\" , \"BLUE\" , \"VH_IMAGINARY\" , \"GREEN\" , \"VV_REAL\" , \"RED\" , \"VV_IMAGINARY\" , \"VH_LEE_FILTERED\" , \"RED_EDGE_1\" , \"VV_LEE_FILTERED\" , \"RED_EDGE_2\" , \"VH_LEE_FILTERED_REAL\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"VV_LEE_FILTERED_IMAGINARY\" , \"NIR_NARROW\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-so2sat\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] image_id = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) attr_dict = pickle . loads ( ast . literal_eval ( h5file . attrs [ \"pickle\" ])) class_index = attr_dict [ \"label\" ] output = { \"image\" : image . astype ( np . float32 )} if self . transform : output = self . transform ( ** output ) output [ \"label\" ] = class_index return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label_index = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) class_name = str ( label_index ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { class_name } \" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"MSo2SatNonGeo"},{"location":"data/#terratorch.datasets.m_so2sat.MSo2SatNonGeo.__init__","text":"Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. Source code in terratorch/datasets/m_so2sat.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.m_so2sat.MSo2SatNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_so2sat.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label_index = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) class_name = str ( label_index ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { class_name } \" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.m_pv4ger","text":"","title":"m_pv4ger"},{"location":"data/#terratorch.datasets.m_pv4ger.MPv4gerNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for M-PV4GER . Source code in terratorch/datasets/m_pv4ger.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 class MPv4gerNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-PV4GER](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-pv4ger\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (location coordinates). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] image_id = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) attr_dict = pickle . loads ( ast . literal_eval ( h5file . attrs [ \"pickle\" ])) # noqa: S301 class_index = attr_dict [ \"label\" ] output = { \"image\" : image . astype ( np . float32 )} if self . transform : output = self . transform ( ** output ) output [ \"label\" ] = class_index if self . use_metadata : output [ \"location_coords\" ] = self . _get_coords ( image_id ) return output def _get_coords ( self , image_id : str ) -> torch . Tensor : \"\"\"Extract spatial coordinates from the image ID.\"\"\" lat_str , lon_str = image_id . split ( \",\" ) latitude = float ( lat_str ) longitude = float ( lon_str ) return torch . tensor ([ latitude , longitude ], dtype = torch . float32 ) def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { label } \" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"MPv4gerNonGeo"},{"location":"data/#terratorch.datasets.m_pv4ger.MPv4gerNonGeo.__init__","text":"Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (location coordinates). Source code in terratorch/datasets/m_pv4ger.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (location coordinates). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.m_pv4ger.MPv4gerNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_pv4ger.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] label = sample [ \"label\" ] if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) fig , ax = plt . subplots ( figsize = ( 6 , 6 )) ax . imshow ( rgb_image ) ax . axis ( \"off\" ) ax . set_title ( f \"Class: { label } \" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.m_cashew_plantation","text":"","title":"m_cashew_plantation"},{"location":"data/#terratorch.datasets.m_cashew_plantation.MBeninSmallHolderCashewsNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for M-BeninSmallHolderCashews . Source code in terratorch/datasets/m_cashew_plantation.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 class MBeninSmallHolderCashewsNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-BeninSmallHolderCashews](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"COASTAL_AEROSOL\" , \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"NIR_NARROW\" , \"WATER_VAPOR\" , \"SWIR_1\" , \"SWIR_2\" , \"CLOUD_PROBABILITY\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-cashew-plant\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (time). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def _get_date ( self , keys ) -> torch . Tensor : date_pattern = re . compile ( r \"\\d {4} -\\d {2} -\\d {2} \" ) date_str = None for key in keys : match = date_pattern . search ( key ) if match : date_str = match . group () break date = torch . zeros (( 1 , 2 ), dtype = torch . float32 ) if date_str : date = pd . to_datetime ( date_str , format = \"%Y-%m- %d \" ) date = torch . tensor ([[ date . year , date . dayofyear - 1 ]], dtype = torch . float32 ) return date def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) temporal_coords = self . _get_date ( h5file ) mask = np . array ( h5file [ \"label\" ]) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () if self . use_metadata : output [ \"temporal_coords\" ] = temporal_coords return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"MBeninSmallHolderCashewsNonGeo"},{"location":"data/#terratorch.datasets.m_cashew_plantation.MBeninSmallHolderCashewsNonGeo.__init__","text":"Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (time). Source code in terratorch/datasets/m_cashew_plantation.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (time). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.m_cashew_plantation.MBeninSmallHolderCashewsNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_cashew_plantation.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.m_nz_cattle","text":"","title":"m_nz_cattle"},{"location":"data/#terratorch.datasets.m_nz_cattle.MNzCattleNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for M-NZ-Cattle . Source code in terratorch/datasets/m_nz_cattle.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 class MNzCattleNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-NZ-Cattle](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-nz-cattle\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] file_name = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) data_keys = [ key for key in keys if \"label\" not in key ] label_keys = [ key for key in keys if \"label\" in key ] temporal_coords = self . _get_date ( data_keys [ 0 ]) bands = [ np . array ( h5file [ key ]) for key in data_keys ] image = np . stack ( bands , axis =- 1 ) mask = np . array ( h5file [ label_keys [ 0 ]]) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () if self . use_metadata : location_coords = self . _get_coords ( file_name ) output [ \"location_coords\" ] = location_coords output [ \"temporal_coords\" ] = temporal_coords return output def _get_coords ( self , file_name : str ) -> torch . Tensor : \"\"\"Extract spatial coordinates from the file name.\"\"\" match = re . search ( r \"_(\\-?\\d+\\.\\d+),(\\-?\\d+\\.\\d+)\" , file_name ) if match : longitude , latitude = map ( float , match . groups ()) return torch . tensor ([ latitude , longitude ], dtype = torch . float32 ) def _get_date ( self , band_name : str ) -> torch . Tensor : date_str = band_name . split ( \"_\" )[ - 1 ] date = pd . to_datetime ( date_str , format = \"%Y-%m- %d \" ) return torch . tensor ([[ date . year , date . dayofyear - 1 ]], dtype = torch . float32 ) def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"MNzCattleNonGeo"},{"location":"data/#terratorch.datasets.m_nz_cattle.MNzCattleNonGeo.__init__","text":"Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (time and location). Source code in terratorch/datasets/m_nz_cattle.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.m_nz_cattle.MNzCattleNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_nz_cattle.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.m_chesapeake_landcover","text":"","title":"m_chesapeake_landcover"},{"location":"data/#terratorch.datasets.m_chesapeake_landcover.MChesapeakeLandcoverNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for M-ChesapeakeLandcover . Source code in terratorch/datasets/m_chesapeake_landcover.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class MChesapeakeLandcoverNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-ChesapeakeLandcover](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"NIR\" , \"RED\" ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-chesapeake\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) mask = np . array ( h5file [ \"label\" ]) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"MChesapeakeLandcoverNonGeo"},{"location":"data/#terratorch.datasets.m_chesapeake_landcover.MChesapeakeLandcoverNonGeo.__init__","text":"Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. Source code in terratorch/datasets/m_chesapeake_landcover.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found in partition file.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.m_chesapeake_landcover.MChesapeakeLandcoverNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_chesapeake_landcover.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.m_pv4ger_seg","text":"","title":"m_pv4ger_seg"},{"location":"data/#terratorch.datasets.m_pv4ger_seg.MPv4gerSegNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for M-PV4GER-SEG . Source code in terratorch/datasets/m_pv4ger_seg.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 class MPv4gerSegNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-PV4GER-SEG](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-pv4ger-seg\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (location coordinates). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] image_id = file_path . stem with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) mask = np . array ( h5file [ \"label\" ]) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () if self . use_metadata : output [ \"location_coords\" ] = self . _get_coords ( image_id ) return output def _get_coords ( self , image_id : str ) -> torch . Tensor : \"\"\"Extract spatial coordinates from the image ID.\"\"\" lat_str , lon_str = image_id . split ( \",\" ) latitude = float ( lat_str ) longitude = float ( lon_str ) return torch . tensor ([ latitude , longitude ], dtype = torch . float32 ) def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"MPv4gerSegNonGeo"},{"location":"data/#terratorch.datasets.m_pv4ger_seg.MPv4gerSegNonGeo.__init__","text":"Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (location coordinates). Source code in terratorch/datasets/m_pv4ger_seg.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , use_metadata : bool = False , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. use_metadata (bool): Whether to return metadata info (location coordinates). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . use_metadata = use_metadata self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.m_pv4ger_seg.MPv4gerSegNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_pv4ger_seg.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.m_SA_crop_type","text":"","title":"m_SA_crop_type"},{"location":"data/#terratorch.datasets.m_SA_crop_type.MSACropTypeNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for M-SA-Crop-Type . Source code in terratorch/datasets/m_SA_crop_type.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class MSACropTypeNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-SA-Crop-Type](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"COASTAL_AEROSOL\" , \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"NIR_NARROW\" , \"WATER_VAPOR\" , \"SWIR_1\" , \"SWIR_2\" , \"CLOUD_PROBABILITY\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-SA-crop-type\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) mask = np . array ( h5file [ \"label\" ]) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"MSACropTypeNonGeo"},{"location":"data/#terratorch.datasets.m_SA_crop_type.MSACropTypeNonGeo.__init__","text":"Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands to be used. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. Source code in terratorch/datasets/m_SA_crop_type.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = [ self . all_band_names . index ( b ) for b in bands ] self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.m_SA_crop_type.MSACropTypeNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_SA_crop_type.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.m_neontree","text":"","title":"m_neontree"},{"location":"data/#terratorch.datasets.m_neontree.MNeonTreeNonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for M-NeonTree . Source code in terratorch/datasets/m_neontree.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 class MNeonTreeNonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [M-NeonTree](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\" all_band_names = ( \"BLUE\" , \"CANOPY_HEIGHT_MODEL\" , \"GREEN\" , \"NEON\" , \"RED\" ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"m-NeonTree\" partition_file_template = \" {partition} _partition.json\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = rgb_bands , transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to RGB bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: file_path = self . image_files [ index ] with h5py . File ( file_path , \"r\" ) as h5file : keys = sorted ( h5file . keys ()) keys = np . array ([ key for key in keys if key != \"label\" ])[ self . band_indices ] bands = [ np . array ( h5file [ key ]) for key in keys ] image = np . stack ( bands , axis =- 1 ) mask = np . array ( h5file [ \"label\" ]) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () return output def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"MNeonTreeNonGeo"},{"location":"data/#terratorch.datasets.m_neontree.MNeonTreeNonGeo.__init__","text":"Initialize the dataset. Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 One of 'train', 'val', or 'test'. bands ( Sequence [ str ] , default: rgb_bands ) \u2013 Bands to be used. Defaults to RGB bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition ( str , default: 'default' ) \u2013 Partition name for the dataset splits. Defaults to 'default'. Source code in terratorch/datasets/m_neontree.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = rgb_bands , transform : A . Compose | None = None , partition : str = \"default\" , ) -> None : \"\"\"Initialize the dataset. Args: data_root (str): Path to the data root directory. split (str): One of 'train', 'val', or 'test'. bands (Sequence[str]): Bands to be used. Defaults to RGB bands. transform (A.Compose | None): Albumentations transform to be applied. Defaults to None, which applies default_transform(). partition (str): Partition name for the dataset splits. Defaults to 'default'. \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { list ( self . splits . keys ()) } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . array ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) self . data_directory = self . data_root / self . data_dir partition_file = self . data_directory / self . partition_file_template . format ( partition = partition ) with open ( partition_file ) as file : partitions = json . load ( file ) if split_name not in partitions : msg = f \"Split ' { split_name } ' not found.\" raise ValueError ( msg ) self . image_files = [ self . data_directory / f \" { filename } .hdf5\" for filename in partitions [ split_name ]] self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.m_neontree.MNeonTreeNonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 A sample returned by :meth: __getitem__ . suptitle ( str | None , default: None ) \u2013 Optional string to use as a suptitle. Returns: Figure \u2013 matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. Source code in terratorch/datasets/m_neontree.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def plot ( self , sample : dict [ str , torch . Tensor ], suptitle : str | None = None ) -> plt . Figure : \"\"\"Plot a sample from the dataset. Args: sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`. suptitle (str | None): Optional string to use as a suptitle. Returns: matplotlib.figure.Figure: A matplotlib Figure with the rendered sample. \"\"\" rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands if band in self . bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) image = sample [ \"image\" ] mask = sample [ \"mask\" ] . numpy () if torch . is_tensor ( image ): image = image . permute ( 1 , 2 , 0 ) . numpy () # (H, W, C) rgb_image = image [:, :, rgb_indices ] rgb_image = clip_image ( rgb_image ) num_classes = len ( np . unique ( mask )) cmap = plt . get_cmap ( \"jet\" ) norm = plt . Normalize ( vmin = 0 , vmax = num_classes - 1 ) num_images = 4 if \"prediction\" in sample else 3 fig , ax = plt . subplots ( 1 , num_images , figsize = ( num_images * 4 , 4 ), tight_layout = True ) ax [ 0 ] . imshow ( rgb_image ) ax [ 0 ] . set_title ( \"Image\" ) ax [ 0 ] . axis ( \"off\" ) ax [ 1 ] . imshow ( mask , cmap = cmap , norm = norm ) ax [ 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ 1 ] . axis ( \"off\" ) ax [ 2 ] . imshow ( rgb_image ) ax [ 2 ] . imshow ( mask , cmap = cmap , alpha = 0.3 , norm = norm ) ax [ 2 ] . set_title ( \"GT Mask on Image\" ) ax [ 2 ] . axis ( \"off\" ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] . numpy () ax [ 3 ] . imshow ( prediction , cmap = cmap , norm = norm ) ax [ 3 ] . set_title ( \"Predicted Mask\" ) ax [ 3 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.multi_temporal_crop_classification","text":"","title":"multi_temporal_crop_classification"},{"location":"data/#terratorch.datasets.multi_temporal_crop_classification.MultiTemporalCropClassification","text":"Bases: NonGeoDataset NonGeo dataset implementation for multi-temporal crop classification . Source code in terratorch/datasets/multi_temporal_crop_classification.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 class MultiTemporalCropClassification ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [multi-temporal crop classification](https://huggingface.co/datasets/ibm-nasa-geospatial/multi-temporal-crop-classification).\"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" , \"NIR_NARROW\" , \"SWIR_1\" , \"SWIR_2\" , ) class_names = ( \"Natural Vegetation\" , \"Forest\" , \"Corn\" , \"Soybeans\" , \"Wetlands\" , \"Developed / Barren\" , \"Open Water\" , \"Winter Wheat\" , \"Alfalfa\" , \"Fallow / Idle Cropland\" , \"Cotton\" , \"Sorghum\" , \"Other\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } num_classes = 13 time_steps = 3 splits = { \"train\" : \"training\" , \"val\" : \"validation\" } # Only train and val splits available metadata_file_name = \"chips_df.csv\" col_name = \"chip_id\" date_columns = [ \"first_img_date\" , \"middle_img_date\" , \"last_img_date\" ] def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = True , reduce_zero_label : bool = True , use_metadata : bool = False , ) -> None : \"\"\"Constructor Args: data_root (str): Path to the data root directory. split (str): one of 'train' or 'val'. bands (list[str]): Bands that should be output by the dataset. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If None, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True. use_metadata (bool): whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { self . splits } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) data_dir = self . data_root / f \" { split_name } _chips\" self . image_files = sorted ( glob . glob ( os . path . join ( data_dir , \"*_merged.tif\" ))) self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( data_dir , \"*.mask.tif\" ))) split_file = self . data_root / f \" { split_name } _data.txt\" with open ( split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . segmentation_mask_files = filter_valid_files ( self . segmentation_mask_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . reduce_zero_label = reduce_zero_label self . expand_temporal_dimension = expand_temporal_dimension self . use_metadata = use_metadata self . metadata = None if self . use_metadata : metadata_file = self . data_root / self . metadata_file_name self . metadata = pd . read_csv ( metadata_file ) self . _build_image_metadata_mapping () # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform def _build_image_metadata_mapping ( self ): \"\"\"Build a mapping from image filenames to metadata indices.\"\"\" self . image_to_metadata_index = dict () for idx , image_file in enumerate ( self . image_files ): image_filename = Path ( image_file ) . name image_id = image_filename . replace ( \"_merged.tif\" , \"\" ) . replace ( \".tif\" , \"\" ) metadata_indices = self . metadata . index [ self . metadata [ self . col_name ] == image_id ] . tolist () self . image_to_metadata_index [ idx ] = metadata_indices [ 0 ] def __len__ ( self ) -> int : return len ( self . image_files ) def _get_date ( self , row : pd . Series ) -> torch . Tensor : \"\"\"Extract and format temporal coordinates (T, date) from metadata.\"\"\" temporal_coords = [] for col in self . date_columns : date_str = row [ col ] date = pd . to_datetime ( date_str , format = \"%Y-%m- %d \" ) temporal_coords . append ([ date . year , date . dayofyear - 1 ]) return torch . tensor ( temporal_coords , dtype = torch . float32 ) def _get_coords ( self , image : DataArray ) -> torch . Tensor : px = image . x . shape [ 0 ] // 2 py = image . y . shape [ 0 ] // 2 # get center point to reproject to lat/lon point = image . isel ( band = 0 , x = slice ( px , px + 1 ), y = slice ( py , py + 1 )) point = point . rio . reproject ( \"epsg:4326\" ) lat_lon = np . asarray ([ point . y [ 0 ], point . x [ 0 ]]) return torch . tensor ( lat_lon , dtype = torch . float32 ) def __getitem__ ( self , index : int ) -> dict [ str , Any ]: image = self . _load_file ( self . image_files [ index ], nan_replace = self . no_data_replace ) location_coords , temporal_coords = None , None if self . use_metadata : location_coords = self . _get_coords ( image ) metadata_idx = self . image_to_metadata_index . get ( index , None ) if metadata_idx is not None : row = self . metadata . iloc [ metadata_idx ] temporal_coords = self . _get_date ( row ) # to channels last image = image . to_numpy () if self . expand_temporal_dimension : image = rearrange ( image , \"(channels time) h w -> channels time h w\" , channels = len ( self . bands )) image = np . moveaxis ( image , 0 , - 1 ) # filter bands image = image [ ... , self . band_indices ] output = { \"image\" : image . astype ( np . float32 ), \"mask\" : self . _load_file ( self . segmentation_mask_files [ index ], nan_replace = self . no_label_replace ) . to_numpy ()[ 0 ], } if self . reduce_zero_label : output [ \"mask\" ] -= 1 if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () if self . use_metadata : output [ \"location_coords\" ] = location_coords output [ \"temporal_coords\" ] = temporal_coords return output def _load_file ( self , path : Path , nan_replace : int | float | None = None ) -> DataArray : data = rioxarray . open_rasterio ( path , masked = True ) if nan_replace is not None : data = data . fillna ( nan_replace ) return data def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample \"\"\" num_images = self . time_steps + 2 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) images = sample [ \"image\" ] images = images [ rgb_indices , ... ] # Shape: (T, 3, H, W) processed_images = [] for t in range ( self . time_steps ): img = images [ t ] img = img . permute ( 1 , 2 , 0 ) img = img . numpy () img = clip_image ( img ) processed_images . append ( img ) mask = sample [ \"mask\" ] . numpy () if \"prediction\" in sample : num_images += 1 fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 5 ), layout = \"compressed\" ) ax [ 0 ] . axis ( \"off\" ) norm = mpl . colors . Normalize ( vmin = 0 , vmax = self . num_classes - 1 ) for i , img in enumerate ( processed_images ): ax [ i + 1 ] . axis ( \"off\" ) ax [ i + 1 ] . title . set_text ( f \"T { i } \" ) ax [ i + 1 ] . imshow ( img ) ax [ self . time_steps + 1 ] . axis ( \"off\" ) ax [ self . time_steps + 1 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ self . time_steps + 1 ] . imshow ( mask , cmap = \"jet\" , norm = norm ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] ax [ self . time_steps + 2 ] . axis ( \"off\" ) ax [ self . time_steps + 2 ] . title . set_text ( \"Predicted Mask\" ) ax [ self . time_steps + 2 ] . imshow ( prediction , cmap = \"jet\" , norm = norm ) cmap = plt . get_cmap ( \"jet\" ) legend_data = [[ i , cmap ( norm ( i )), self . class_names [ i ]] for i in range ( self . num_classes )] handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = tuple ( v for v in c )) for k , c , n in legend_data ] labels = [ n for k , c , n in legend_data ] ax [ 0 ] . legend ( handles , labels , loc = \"center\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig","title":"MultiTemporalCropClassification"},{"location":"data/#terratorch.datasets.multi_temporal_crop_classification.MultiTemporalCropClassification.__init__","text":"Constructor Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 one of 'train' or 'val'. bands ( list [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands that should be output by the dataset. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2(). no_data_replace ( float | None , default: None ) \u2013 Replace nan values in input images with this value. If None, does no replacement. Defaults to None. no_label_replace ( int | None , default: None ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension ( bool , default: True ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True. reduce_zero_label ( bool , default: True ) \u2013 Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True. use_metadata ( bool , default: False ) \u2013 whether to return metadata info (time and location). Source code in terratorch/datasets/multi_temporal_crop_classification.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , no_data_replace : float | None = None , no_label_replace : int | None = None , expand_temporal_dimension : bool = True , reduce_zero_label : bool = True , use_metadata : bool = False , ) -> None : \"\"\"Constructor Args: data_root (str): Path to the data root directory. split (str): one of 'train' or 'val'. bands (list[str]): Bands that should be output by the dataset. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2(). no_data_replace (float | None): Replace nan values in input images with this value. If None, does no replacement. Defaults to None. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None. expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True. reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True. use_metadata (bool): whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { self . splits } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . data_root = Path ( data_root ) data_dir = self . data_root / f \" { split_name } _chips\" self . image_files = sorted ( glob . glob ( os . path . join ( data_dir , \"*_merged.tif\" ))) self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( data_dir , \"*.mask.tif\" ))) split_file = self . data_root / f \" { split_name } _data.txt\" with open ( split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . segmentation_mask_files = filter_valid_files ( self . segmentation_mask_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . reduce_zero_label = reduce_zero_label self . expand_temporal_dimension = expand_temporal_dimension self . use_metadata = use_metadata self . metadata = None if self . use_metadata : metadata_file = self . data_root / self . metadata_file_name self . metadata = pd . read_csv ( metadata_file ) self . _build_image_metadata_mapping () # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.multi_temporal_crop_classification.MultiTemporalCropClassification.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 a sample returned by :meth: __getitem__ suptitle ( str | None , default: None ) \u2013 optional string to use as a suptitle Returns: Figure \u2013 a matplotlib Figure with the rendered sample Source code in terratorch/datasets/multi_temporal_crop_classification.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample \"\"\" num_images = self . time_steps + 2 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) images = sample [ \"image\" ] images = images [ rgb_indices , ... ] # Shape: (T, 3, H, W) processed_images = [] for t in range ( self . time_steps ): img = images [ t ] img = img . permute ( 1 , 2 , 0 ) img = img . numpy () img = clip_image ( img ) processed_images . append ( img ) mask = sample [ \"mask\" ] . numpy () if \"prediction\" in sample : num_images += 1 fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 5 ), layout = \"compressed\" ) ax [ 0 ] . axis ( \"off\" ) norm = mpl . colors . Normalize ( vmin = 0 , vmax = self . num_classes - 1 ) for i , img in enumerate ( processed_images ): ax [ i + 1 ] . axis ( \"off\" ) ax [ i + 1 ] . title . set_text ( f \"T { i } \" ) ax [ i + 1 ] . imshow ( img ) ax [ self . time_steps + 1 ] . axis ( \"off\" ) ax [ self . time_steps + 1 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ self . time_steps + 1 ] . imshow ( mask , cmap = \"jet\" , norm = norm ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] ax [ self . time_steps + 2 ] . axis ( \"off\" ) ax [ self . time_steps + 2 ] . title . set_text ( \"Predicted Mask\" ) ax [ self . time_steps + 2 ] . imshow ( prediction , cmap = \"jet\" , norm = norm ) cmap = plt . get_cmap ( \"jet\" ) legend_data = [[ i , cmap ( norm ( i )), self . class_names [ i ]] for i in range ( self . num_classes )] handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = tuple ( v for v in c )) for k , c , n in legend_data ] labels = [ n for k , c , n in legend_data ] ax [ 0 ] . legend ( handles , labels , loc = \"center\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.open_sentinel_map","text":"","title":"open_sentinel_map"},{"location":"data/#terratorch.datasets.open_sentinel_map.OpenSentinelMap","text":"Bases: NonGeoDataset Pytorch Dataset class to load samples from the OpenSentinelMap dataset, supporting multiple bands and temporal sampling strategies. Source code in terratorch/datasets/open_sentinel_map.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 class OpenSentinelMap ( NonGeoDataset ): \"\"\" Pytorch Dataset class to load samples from the [OpenSentinelMap](https://visionsystemsinc.github.io/open-sentinel-map/) dataset, supporting multiple bands and temporal sampling strategies. \"\"\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : list [ str ] | None = None , transform : A . Compose | None = None , spatial_interpolate_and_stack_temporally : bool = True , # noqa: FBT001, FBT002 pad_image : int | None = None , truncate_image : int | None = None , target : int = 0 , pick_random_pair : bool = True , # noqa: FBT002, FBT001 ) -> None : \"\"\" Args: data_root (str): Path to the root directory of the dataset. split (str): Dataset split to load. Options are 'train', 'val', or 'test'. Defaults to 'train'. bands (list of str, optional): List of band names to load. Defaults to ['gsd_10', 'gsd_20', 'gsd_60']. transform (albumentations.Compose, optional): Albumentations transformations to apply to the data. spatial_interpolate_and_stack_temporally (bool): If True, the bands are interpolated and concatenated over time. Default is True. pad_image (int, optional): Number of timesteps to pad the time dimension of the image. If None, no padding is applied. truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed. target (int): Specifies which target class to use from the mask. Default is 0. pick_random_pair (bool): If True, selects two random images from the temporal sequence. Default is True. \"\"\" split = \"test\" if bands is None : bands = [ \"gsd_10\" , \"gsd_20\" , \"gsd_60\" ] allowed_bands = { \"gsd_10\" , \"gsd_20\" , \"gsd_60\" } for band in bands : if band not in allowed_bands : msg = f \"Band ' { band } ' is not recognized. Available values are: { ', ' . join ( allowed_bands ) } \" raise ValueError ( msg ) if split not in [ \"train\" , \"val\" , \"test\" ]: msg = f \"Split ' { split } ' not recognized. Use 'train', 'val', or 'test'.\" raise ValueError ( msg ) self . data_root = Path ( data_root ) split_mapping = { \"train\" : \"training\" , \"val\" : \"validation\" , \"test\" : \"testing\" } split = split_mapping [ split ] self . imagery_root = self . data_root / \"osm_sentinel_imagery\" self . label_root = self . data_root / \"osm_label_images_v10\" self . auxiliary_data = pd . read_csv ( self . data_root / \"spatial_cell_info.csv\" ) self . auxiliary_data = self . auxiliary_data [ self . auxiliary_data [ \"split\" ] == split ] self . bands = bands self . transform = transform if transform else lambda ** batch : to_tensor ( batch ) self . label_mappings = self . _load_label_mappings () self . split_data = self . auxiliary_data [ self . auxiliary_data [ \"split\" ] == split ] self . spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally self . pad_image = pad_image self . truncate_image = truncate_image self . target = target self . pick_random_pair = pick_random_pair self . image_files = [] self . label_files = [] for _ , row in self . split_data . iterrows (): mgrs_tile = row [ \"MGRS_tile\" ] spatial_cell = str ( row [ \"cell_id\" ]) label_file = self . label_root / mgrs_tile / f \" { spatial_cell } .png\" if label_file . exists (): self . image_files . append (( mgrs_tile , spatial_cell )) self . label_files . append ( label_file ) def _load_label_mappings ( self ): with open ( self . data_root / \"osm_categories.json\" ) as f : return json . load ( f ) def _extract_date_from_filename ( self , filename : str ) -> str : match = re . search ( r \"(\\d {8} )\" , filename ) if match : return match . group ( 1 ) else : msg = f \"Date not found in filename { filename } \" raise ValueError ( msg ) def __len__ ( self ) -> int : return len ( self . image_files ) def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : if \"gsd_10\" not in self . bands : return None num_images = len ([ key for key in sample if key . startswith ( \"image\" )]) images = [] for i in range ( 1 , num_images + 1 ): image_dict = sample [ f \"image { i } \" ] image = image_dict [ \"gsd_10\" ] if isinstance ( image , Tensor ): image = image . numpy () image = image . take ( range ( 3 ), axis = 2 ) image = image . squeeze () image = ( image - image . min ( axis = ( 0 , 1 ))) * ( 1 / image . max ( axis = ( 0 , 1 ))) image = np . clip ( image , 0 , 1 ) images . append ( image ) label_mask = sample [ \"mask\" ] if isinstance ( label_mask , Tensor ): label_mask = label_mask . numpy () return self . _plot_sample ( images , label_mask , suptitle = suptitle ) def _plot_sample ( self , images : list [ np . ndarray ], label : np . ndarray , suptitle : str | None = None , ) -> Figure : num_images = len ( images ) fig , ax = plt . subplots ( 1 , num_images + 1 , figsize = ( 15 , 5 )) for i , image in enumerate ( images ): ax [ i ] . imshow ( image ) ax [ i ] . set_title ( f \"Image { i + 1 } \" ) ax [ i ] . axis ( \"off\" ) ax [ - 1 ] . imshow ( label , cmap = \"gray\" ) ax [ - 1 ] . set_title ( \"Ground Truth Mask\" ) ax [ - 1 ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) return fig def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: mgrs_tile , spatial_cell = self . image_files [ index ] spatial_cell_path = self . imagery_root / mgrs_tile / spatial_cell npz_files = list ( spatial_cell_path . glob ( \"*.npz\" )) npz_files . sort ( key = lambda x : self . _extract_date_from_filename ( x . stem )) if self . pick_random_pair : npz_files = random . sample ( npz_files , 2 ) npz_files . sort ( key = lambda x : self . _extract_date_from_filename ( x . stem )) output = {} if self . spatial_interpolate_and_stack_temporally : images_over_time = [] for _ , npz_file in enumerate ( npz_files ): data = np . load ( npz_file ) interpolated_bands = [] for band in self . bands : band_frame = data [ band ] band_frame = torch . from_numpy ( band_frame ) . float () band_frame = band_frame . permute ( 2 , 0 , 1 ) interpolated = F . interpolate ( band_frame . unsqueeze ( 0 ), size = MAX_TEMPORAL_IMAGE_SIZE , mode = \"bilinear\" , align_corners = False ) . squeeze ( 0 ) interpolated_bands . append ( interpolated ) concatenated_bands = torch . cat ( interpolated_bands , dim = 0 ) images_over_time . append ( concatenated_bands ) images = torch . stack ( images_over_time , dim = 0 ) . numpy () if self . truncate_image : images = images [ - self . truncate_image :] if self . pad_image : images = pad_numpy ( images , self . pad_image ) output [ \"image\" ] = images . transpose ( 0 , 2 , 3 , 1 ) else : image_dict = { band : [] for band in self . bands } for _ , npz_file in enumerate ( npz_files ): data = np . load ( npz_file ) for band in self . bands : band_frames = data [ band ] band_frames = band_frames . astype ( np . float32 ) band_frames = np . transpose ( band_frames , ( 2 , 0 , 1 )) image_dict [ band ] . append ( band_frames ) final_image_dict = {} for band in self . bands : band_images = image_dict [ band ] if self . truncate_image : band_images = band_images [ - self . truncate_image :] if self . pad_image : band_images = [ pad_numpy ( img , self . pad_image ) for img in band_images ] band_images = np . stack ( band_images , axis = 0 ) final_image_dict [ band ] = band_images output [ \"image\" ] = final_image_dict label_file = self . label_files [ index ] mask = np . array ( Image . open ( label_file )) . astype ( int ) # Map 'unlabel' (254) and 'none' (255) to unused classes 15 and 16 for processing mask [ mask == 254 ] = 15 # noqa: PLR2004 mask [ mask == 255 ] = 16 # noqa: PLR2004 output [ \"mask\" ] = mask [:, :, self . target ] if self . transform : output = self . transform ( ** output ) return output","title":"OpenSentinelMap"},{"location":"data/#terratorch.datasets.open_sentinel_map.OpenSentinelMap.__init__","text":"Parameters: data_root ( str ) \u2013 Path to the root directory of the dataset. split ( str , default: 'train' ) \u2013 Dataset split to load. Options are 'train', 'val', or 'test'. Defaults to 'train'. bands ( list of str , default: None ) \u2013 List of band names to load. Defaults to ['gsd_10', 'gsd_20', 'gsd_60']. transform ( Compose , default: None ) \u2013 Albumentations transformations to apply to the data. spatial_interpolate_and_stack_temporally ( bool , default: True ) \u2013 If True, the bands are interpolated and concatenated over time. Default is True. pad_image ( int , default: None ) \u2013 Number of timesteps to pad the time dimension of the image. If None, no padding is applied. truncate_image ( int , default: None ) \u2013 Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed. target ( int , default: 0 ) \u2013 Specifies which target class to use from the mask. Default is 0. pick_random_pair ( bool , default: True ) \u2013 If True, selects two random images from the temporal sequence. Default is True. Source code in terratorch/datasets/open_sentinel_map.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , data_root : str , split : str = \"train\" , bands : list [ str ] | None = None , transform : A . Compose | None = None , spatial_interpolate_and_stack_temporally : bool = True , # noqa: FBT001, FBT002 pad_image : int | None = None , truncate_image : int | None = None , target : int = 0 , pick_random_pair : bool = True , # noqa: FBT002, FBT001 ) -> None : \"\"\" Args: data_root (str): Path to the root directory of the dataset. split (str): Dataset split to load. Options are 'train', 'val', or 'test'. Defaults to 'train'. bands (list of str, optional): List of band names to load. Defaults to ['gsd_10', 'gsd_20', 'gsd_60']. transform (albumentations.Compose, optional): Albumentations transformations to apply to the data. spatial_interpolate_and_stack_temporally (bool): If True, the bands are interpolated and concatenated over time. Default is True. pad_image (int, optional): Number of timesteps to pad the time dimension of the image. If None, no padding is applied. truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed. target (int): Specifies which target class to use from the mask. Default is 0. pick_random_pair (bool): If True, selects two random images from the temporal sequence. Default is True. \"\"\" split = \"test\" if bands is None : bands = [ \"gsd_10\" , \"gsd_20\" , \"gsd_60\" ] allowed_bands = { \"gsd_10\" , \"gsd_20\" , \"gsd_60\" } for band in bands : if band not in allowed_bands : msg = f \"Band ' { band } ' is not recognized. Available values are: { ', ' . join ( allowed_bands ) } \" raise ValueError ( msg ) if split not in [ \"train\" , \"val\" , \"test\" ]: msg = f \"Split ' { split } ' not recognized. Use 'train', 'val', or 'test'.\" raise ValueError ( msg ) self . data_root = Path ( data_root ) split_mapping = { \"train\" : \"training\" , \"val\" : \"validation\" , \"test\" : \"testing\" } split = split_mapping [ split ] self . imagery_root = self . data_root / \"osm_sentinel_imagery\" self . label_root = self . data_root / \"osm_label_images_v10\" self . auxiliary_data = pd . read_csv ( self . data_root / \"spatial_cell_info.csv\" ) self . auxiliary_data = self . auxiliary_data [ self . auxiliary_data [ \"split\" ] == split ] self . bands = bands self . transform = transform if transform else lambda ** batch : to_tensor ( batch ) self . label_mappings = self . _load_label_mappings () self . split_data = self . auxiliary_data [ self . auxiliary_data [ \"split\" ] == split ] self . spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally self . pad_image = pad_image self . truncate_image = truncate_image self . target = target self . pick_random_pair = pick_random_pair self . image_files = [] self . label_files = [] for _ , row in self . split_data . iterrows (): mgrs_tile = row [ \"MGRS_tile\" ] spatial_cell = str ( row [ \"cell_id\" ]) label_file = self . label_root / mgrs_tile / f \" { spatial_cell } .png\" if label_file . exists (): self . image_files . append (( mgrs_tile , spatial_cell )) self . label_files . append ( label_file )","title":"__init__"},{"location":"data/#terratorch.datasets.openearthmap","text":"","title":"openearthmap"},{"location":"data/#terratorch.datasets.openearthmap.OpenEarthMapNonGeo","text":"Bases: NonGeoDataset OpenEarthMapNonGeo Dataset for non-georeferenced imagery. This dataset class handles non-georeferenced image data from the OpenEarthMap dataset. It supports configurable band sets and transformations, and performs cropping operations to ensure that the images conform to the required input dimensions. The dataset is split into \"train\", \"test\", and \"val\" subsets based on the provided split parameter. Source code in terratorch/datasets/openearthmap.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 class OpenEarthMapNonGeo ( NonGeoDataset ): \"\"\" [OpenEarthMapNonGeo](https://open-earth-map.org/) Dataset for non-georeferenced imagery. This dataset class handles non-georeferenced image data from the OpenEarthMap dataset. It supports configurable band sets and transformations, and performs cropping operations to ensure that the images conform to the required input dimensions. The dataset is split into \"train\", \"test\", and \"val\" subsets based on the provided split parameter. \"\"\" all_band_names = ( \"BLUE\" , \"GREEN\" , \"RED\" ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } def __init__ ( self , data_root : str , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , split = \"train\" , crop_size : int = 256 , random_crop : bool = True ) -> None : \"\"\" Initialize a new instance of the OpenEarthMapNonGeo dataset. Args: data_root (str): The root directory containing the dataset files. bands (Sequence[str], optional): A list of band names to be used. Default is BAND_SETS[\"all\"]. transform (A.Compose or None, optional): A transformation pipeline to be applied to the data. If None, a default transform converting the data to a tensor is applied. split (str, optional): The dataset split to use (\"train\", \"test\", or \"val\"). Default is \"train\". crop_size (int, optional): The size (in pixels) of the crop to apply to images. Must be greater than 0. Default is 256. random_crop (bool, optional): If True, performs a random crop; otherwise, performs a center crop. Default is True. Raises: Exception: If the provided split is not one of \"train\", \"test\", or \"val\". AssertionError: If crop_size is not greater than 0. \"\"\" super () . __init__ () if split not in [ \"train\" , \"test\" , \"val\" ]: msg = \"Split must be one of train, test, val.\" raise Exception ( msg ) self . transform = transform if transform else lambda ** batch : to_tensor ( batch , transpose = False ) self . split = split self . data_root = data_root # images in openearthmap are not all 1024x1024 and must be cropped self . crop_size = crop_size self . random_crop = random_crop assert self . crop_size > 0 , \"Crop size must be greater than 0\" self . image_files = self . _get_file_paths ( Path ( self . data_root , f \" { split } .txt\" )) def __getitem__ ( self , index : int ) -> dict [ str , torch . Tensor ]: image_path , label_path = self . image_files [ index ] with rasterio . open ( image_path ) as src : image = src . read () with rasterio . open ( label_path ) as src : mask = src . read () # some images in the dataset are not perfect squares # cropping to fit to the prepare_features_for_image_model call if self . random_crop : image , mask = self . _random_crop ( image , mask ) else : image , mask = self . _center_crop ( image , mask ) output = { \"image\" : image . astype ( np . float32 ), \"mask\" : mask } output = self . transform ( ** output ) output [ 'mask' ] = output [ 'mask' ] . long () return output def _parse_file_name ( self , file_name : str ): underscore_pos = file_name . rfind ( '_' ) folder_name = file_name [: underscore_pos ] region_path = Path ( self . data_root , folder_name ) image_path = Path ( region_path , \"images\" , file_name ) label_path = Path ( region_path , \"labels\" , file_name ) return image_path , label_path def _get_file_paths ( self , text_file_path : str ): with open ( text_file_path , 'r' ) as file : lines = file . readlines () file_paths = [ self . _parse_file_name ( line . strip ()) for line in lines ] return file_paths def __len__ ( self ): return len ( self . image_files ) def _random_crop ( self , image , mask ): h , w = image . shape [ 1 :] top = np . random . randint ( 0 , h - self . crop_size ) left = np . random . randint ( 0 , w - self . crop_size ) image = image [:, top : top + self . crop_size , left : left + self . crop_size ] mask = mask [:, top : top + self . crop_size , left : left + self . crop_size ] return image , mask def _center_crop ( self , image , mask ): h , w = image . shape [ 1 :] top = ( h - self . crop_size ) // 2 left = ( w - self . crop_size ) // 2 image = image [:, top : top + self . crop_size , left : left + self . crop_size ] mask = mask [:, top : top + self . crop_size , left : left + self . crop_size ] return image , mask def plot ( self , arg , suptitle : str | None = None ) -> None : pass def plot_sample ( self , sample , prediction = None , suptitle : str | None = None , class_names = None ): pass","title":"OpenEarthMapNonGeo"},{"location":"data/#terratorch.datasets.openearthmap.OpenEarthMapNonGeo.__init__","text":"Initialize a new instance of the OpenEarthMapNonGeo dataset. Parameters: data_root ( str ) \u2013 The root directory containing the dataset files. bands ( Sequence [ str ] , default: BAND_SETS ['all'] ) \u2013 A list of band names to be used. Default is BAND_SETS[\"all\"]. transform ( Compose or None , default: None ) \u2013 A transformation pipeline to be applied to the data. If None, a default transform converting the data to a tensor is applied. split ( str , default: 'train' ) \u2013 The dataset split to use (\"train\", \"test\", or \"val\"). Default is \"train\". crop_size ( int , default: 256 ) \u2013 The size (in pixels) of the crop to apply to images. Must be greater than 0. Default is 256. random_crop ( bool , default: True ) \u2013 If True, performs a random crop; otherwise, performs a center crop. Default is True. Raises: Exception \u2013 If the provided split is not one of \"train\", \"test\", or \"val\". AssertionError \u2013 If crop_size is not greater than 0. Source code in terratorch/datasets/openearthmap.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , data_root : str , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , split = \"train\" , crop_size : int = 256 , random_crop : bool = True ) -> None : \"\"\" Initialize a new instance of the OpenEarthMapNonGeo dataset. Args: data_root (str): The root directory containing the dataset files. bands (Sequence[str], optional): A list of band names to be used. Default is BAND_SETS[\"all\"]. transform (A.Compose or None, optional): A transformation pipeline to be applied to the data. If None, a default transform converting the data to a tensor is applied. split (str, optional): The dataset split to use (\"train\", \"test\", or \"val\"). Default is \"train\". crop_size (int, optional): The size (in pixels) of the crop to apply to images. Must be greater than 0. Default is 256. random_crop (bool, optional): If True, performs a random crop; otherwise, performs a center crop. Default is True. Raises: Exception: If the provided split is not one of \"train\", \"test\", or \"val\". AssertionError: If crop_size is not greater than 0. \"\"\" super () . __init__ () if split not in [ \"train\" , \"test\" , \"val\" ]: msg = \"Split must be one of train, test, val.\" raise Exception ( msg ) self . transform = transform if transform else lambda ** batch : to_tensor ( batch , transpose = False ) self . split = split self . data_root = data_root # images in openearthmap are not all 1024x1024 and must be cropped self . crop_size = crop_size self . random_crop = random_crop assert self . crop_size > 0 , \"Crop size must be greater than 0\" self . image_files = self . _get_file_paths ( Path ( self . data_root , f \" { split } .txt\" ))","title":"__init__"},{"location":"data/#terratorch.datasets.pastis","text":"","title":"pastis"},{"location":"data/#terratorch.datasets.pastis.PASTIS","text":"Bases: NonGeoDataset \" Pytorch Dataset class to load samples from the PASTIS dataset, for semantic and panoptic segmentation. Source code in terratorch/datasets/pastis.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 class PASTIS ( NonGeoDataset ): \"\"\"\" Pytorch Dataset class to load samples from the [PASTIS](https://github.com/VSainteuf/pastis-benchmark) dataset, for semantic and panoptic segmentation. \"\"\" def __init__ ( self , data_root , norm = True , # noqa: FBT002 target = \"semantic\" , folds = None , reference_date = \"2018-09-01\" , date_interval = ( - 200 , 600 ), class_mapping = None , transform = None , truncate_image = None , pad_image = None , satellites = [ \"S2\" ], # noqa: B006 ): \"\"\" Args: data_root (str): Path to the dataset. norm (bool): If true, images are standardised using pre-computed channel-wise means and standard deviations. reference_date (str, Format : 'YYYY-MM-DD'): Defines the reference date based on which all observation dates are expressed. Along with the image time series and the target tensor, this dataloader yields the sequence of observation dates (in terms of number of days since the reference date). This sequence of dates is used for instance for the positional encoding in attention based approaches. target (str): 'semantic' or 'instance'. Defines which type of target is returned by the dataloader. * If 'semantic' the target tensor is a tensor containing the class of each pixel. * If 'instance' the target tensor is the concatenation of several signals, necessary to train the Parcel-as-Points module: - the centerness heatmap, - the instance ids, - the voronoi partitioning of the patch with regards to the parcels' centers, - the (height, width) size of each parcel, - the semantic label of each parcel, - the semantic label of each pixel. folds (list, optional): List of ints specifying which of the 5 official folds to load. By default (when None is specified), all folds are loaded. class_mapping (dict, optional): A dictionary to define a mapping between the default 18 class nomenclature and another class grouping. If not provided, the default class mapping is used. transform (callable, optional): A transform to apply to the loaded data (images, dates, and masks). By default, no transformation is applied. truncate_image (int, optional): Truncate the time dimension of the image to a specified number of timesteps. If None, no truncation is performed. pad_image (int, optional): Pad the time dimension of the image to a specified number of timesteps. If None, no padding is applied. satellites (list): Defines the satellites to use. If you are using PASTIS-R, you have access to Sentinel-2 imagery and Sentinel-1 observations in Ascending and Descending orbits, respectively S2, S1A, and S1D. For example, use satellites=['S2', 'S1A'] for Sentinel-2 + Sentinel-1 ascending time series, or satellites=['S2', 'S1A', 'S1D'] to retrieve all time series. If you are using PASTIS, only S2 observations are available. \"\"\" if target not in [ \"semantic\" , \"instance\" ]: msg = f \"Target ' { target } ' not recognized. Use 'semantic', or 'instance'.\" raise ValueError ( msg ) valid_satellites = { \"S2\" , \"S1A\" , \"S1D\" } for sat in satellites : if sat not in valid_satellites : msg = f \"Satellite ' { sat } ' not recognized. Valid options are { valid_satellites } .\" raise ValueError ( msg ) super () . __init__ () self . data_root = data_root self . norm = norm self . reference_date = datetime ( * map ( int , reference_date . split ( \"-\" )), tzinfo = timezone . utc ) self . class_mapping = ( np . vectorize ( lambda x : class_mapping [ x ]) if class_mapping is not None else class_mapping ) self . target = target self . satellites = satellites self . transform = transform self . truncate_image = truncate_image self . pad_image = pad_image # loads patches metadata self . meta_patch = gpd . read_file ( os . path . join ( data_root , \"metadata.geojson\" )) self . meta_patch . index = self . meta_patch [ \"ID_PATCH\" ] . astype ( int ) self . meta_patch . sort_index ( inplace = True ) # stores table for each satalite date self . date_tables = { s : None for s in satellites } # date interval used in the PASTIS benchmark paper. date_interval_begin , date_interval_end = date_interval self . date_range = np . array ( range ( date_interval_begin , date_interval_end )) for s in satellites : # maps patches to its observation dates dates = self . meta_patch [ f \"dates- { s } \" ] date_table = pd . DataFrame ( index = self . meta_patch . index , columns = self . date_range , dtype = int ) for pid , date_seq in dates . items (): if type ( date_seq ) is str : date_seq = json . loads ( date_seq ) # noqa: PLW2901 # convert date to days since obersavation format d = pd . DataFrame () . from_dict ( date_seq , orient = \"index\" ) d = d [ 0 ] . apply ( lambda x : ( datetime ( int ( str ( x )[: 4 ]), int ( str ( x )[ 4 : 6 ]), int ( str ( x )[ 6 :]), tzinfo = timezone . utc ) - self . reference_date ) . days ) date_table . loc [ pid , d . values ] = 1 date_table = date_table . fillna ( 0 ) self . date_tables [ s ] = { index : np . array ( list ( d . values ())) for index , d in date_table . to_dict ( orient = \"index\" ) . items () } # selects patches correspondig to selected folds if folds is not None : self . meta_patch = pd . concat ( [ self . meta_patch [ self . meta_patch [ \"Fold\" ] == f ] for f in folds ] ) self . len = self . meta_patch . shape [ 0 ] self . id_patches = self . meta_patch . index # loads normalization values if norm : self . norm = {} for s in self . satellites : with open ( os . path . join ( data_root , f \"NORM_ { s } _patch.json\" ) ) as file : normvals = json . loads ( file . read ()) selected_folds = folds if folds is not None else range ( 1 , 6 ) means = [ normvals [ f \"Fold_ { f } \" ][ \"mean\" ] for f in selected_folds ] stds = [ normvals [ f \"Fold_ { f } \" ][ \"std\" ] for f in selected_folds ] self . norm [ s ] = np . stack ( means ) . mean ( axis = 0 ), np . stack ( stds ) . mean ( axis = 0 ) self . norm [ s ] = ( self . norm [ s ][ 0 ], self . norm [ s ][ 1 ], ) else : self . norm = None def __len__ ( self ): return self . len def get_dates ( self , id_patch , sat ): return self . date_range [ np . where ( self . date_tables [ sat ][ id_patch ] == 1 )[ 0 ]] def __getitem__ ( self , item ): id_patch = self . id_patches [ item ] output = {} satellites = {} for satellite in self . satellites : data = np . load ( os . path . join ( self . data_root , f \"DATA_ { satellite } \" , f \" { satellite } _ { id_patch } .npy\" , ) ) . astype ( np . float32 ) if self . norm is not None : data = data - self . norm [ satellite ][ 0 ][ None , :, None , None ] data = data / self . norm [ satellite ][ 1 ][ None , :, None , None ] if self . truncate_image and data . shape [ 0 ] > self . truncate_image : data = data [ - self . truncate_image :] if self . pad_image and data . shape [ 0 ] < self . pad_image : data = pad_numpy ( data , self . pad_image ) satellites [ satellite ] = data . astype ( np . float32 ) if self . target == \"semantic\" : target = np . load ( os . path . join ( self . data_root , \"ANNOTATIONS\" , f \"TARGET_ { id_patch } .npy\" ) ) target = target [ 0 ] . astype ( int ) if self . class_mapping is not None : target = self . class_mapping ( target ) elif self . target == \"instance\" : heatmap = np . load ( os . path . join ( self . data_root , \"INSTANCE_ANNOTATIONS\" , f \"HEATMAP_ { id_patch } .npy\" )) instance_ids = np . load ( os . path . join ( self . data_root , \"INSTANCE_ANNOTATIONS\" , f \"INSTANCES_ { id_patch } .npy\" )) zones_path = os . path . join ( self . data_root , \"INSTANCE_ANNOTATIONS\" , f \"ZONES_ { id_patch } .npy\" ) pixel_to_object_mapping = np . load ( zones_path ) pixel_semantic_annotation = np . load ( os . path . join ( self . data_root , \"ANNOTATIONS\" , f \"TARGET_ { id_patch } .npy\" )) if self . class_mapping is not None : pixel_semantic_annotation = self . class_mapping ( pixel_semantic_annotation [ 0 ]) else : pixel_semantic_annotation = pixel_semantic_annotation [ 0 ] size = np . zeros (( * instance_ids . shape , 2 )) object_semantic_annotation = np . zeros ( instance_ids . shape ) for instance_id in np . unique ( instance_ids ): if instance_id != 0 : h = ( instance_ids == instance_id ) . any ( axis =- 1 ) . sum () w = ( instance_ids == instance_id ) . any ( axis =- 2 ) . sum () size [ pixel_to_object_mapping == instance_id ] = ( h , w ) semantic_value = pixel_semantic_annotation [ instance_ids == instance_id ][ 0 ] object_semantic_annotation [ pixel_to_object_mapping == instance_id ] = semantic_value target = np . concatenate ( [ heatmap [:, :, None ], instance_ids [:, :, None ], pixel_to_object_mapping [:, :, None ], size , object_semantic_annotation [:, :, None ], pixel_semantic_annotation [:, :, None ], ], axis =- 1 ) . astype ( np . float32 ) dates = {} for satellite in self . satellites : date = np . array ( self . get_dates ( id_patch , satellite )) if self . truncate_image and len ( date ) > self . truncate_image : date = date [ - self . truncate_image :] if self . pad_image and len ( date ) < self . pad_image : date = pad_dates_numpy ( date , self . pad_image ) dates [ satellite ] = torch . from_numpy ( date ) output [ \"image\" ] = satellites [ \"S2\" ] . transpose ( 0 , 2 , 3 , 1 ) output [ \"mask\" ] = target if self . transform : output = self . transform ( ** output ) output . update ( satellites ) output [ \"dates\" ] = dates return output def plot ( self , sample , suptitle = None ): dates = sample [ \"dates\" ] target = sample [ \"target\" ] if \"S2\" not in sample : warnings . warn ( \"No RGB image.\" , stacklevel = 2 ) return None image_data = sample [ \"S2\" ] date_data = dates [ \"S2\" ] rgb_images = [] for i in range ( image_data . shape [ 0 ]): rgb_image = image_data [ i , : 3 , :, :] . numpy () . transpose ( 1 , 2 , 0 ) rgb_min = rgb_image . min ( axis = ( 0 , 1 ), keepdims = True ) rgb_max = rgb_image . max ( axis = ( 0 , 1 ), keepdims = True ) denom = rgb_max - rgb_min denom [ denom == 0 ] = 1 rgb_image = ( rgb_image - rgb_min ) / denom rgb_images . append ( np . clip ( rgb_image , 0 , 1 )) return self . _plot_sample ( rgb_images , date_data , target , suptitle = suptitle ) def _plot_sample ( self , images : list [ np . ndarray ], dates : torch . Tensor , target : torch . Tensor | None , suptitle : str | None = None ): num_images = len ( images ) cols = 5 rows = ( num_images + cols ) // cols fig , ax = plt . subplots ( rows , cols , figsize = ( 20 , 4 * rows )) for i , image in enumerate ( images ): ax [ i // cols , i % cols ] . imshow ( image ) ax [ i // cols , i % cols ] . set_title ( f \"Image { i + 1 } - Day { dates [ i ] . item () } \" ) ax [ i // cols , i % cols ] . axis ( \"off\" ) if target is not None : if rows * cols > num_images : target_ax = ax [( num_images ) // cols , ( num_images ) % cols ] else : fig . add_subplot ( rows + 1 , 1 , 1 ) target_ax = fig . gca () target_ax . imshow ( target . numpy (), cmap = \"tab20\" ) target_ax . set_title ( \"Target\" ) target_ax . axis ( \"off\" ) for k in range ( num_images + 1 , rows * cols ): ax [ k // cols , k % cols ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) plt . tight_layout () return fig","title":"PASTIS"},{"location":"data/#terratorch.datasets.pastis.PASTIS.__init__","text":"Parameters: data_root ( str ) \u2013 Path to the dataset. norm ( bool , default: True ) \u2013 If true, images are standardised using pre-computed channel-wise means and standard deviations. reference_date ( ( str , Format ) , default: '2018-09-01' ) \u2013 'YYYY-MM-DD'): Defines the reference date based on which all observation dates are expressed. Along with the image time series and the target tensor, this dataloader yields the sequence of observation dates (in terms of number of days since the reference date). This sequence of dates is used for instance for the positional encoding in attention based approaches. target ( str , default: 'semantic' ) \u2013 'semantic' or 'instance'. Defines which type of target is returned by the dataloader. * If 'semantic' the target tensor is a tensor containing the class of each pixel. * If 'instance' the target tensor is the concatenation of several signals, necessary to train the Parcel-as-Points module: - the centerness heatmap, - the instance ids, - the voronoi partitioning of the patch with regards to the parcels' centers, - the (height, width) size of each parcel, - the semantic label of each parcel, - the semantic label of each pixel. folds ( list , default: None ) \u2013 List of ints specifying which of the 5 official folds to load. By default (when None is specified), all folds are loaded. class_mapping ( dict , default: None ) \u2013 A dictionary to define a mapping between the default 18 class nomenclature and another class grouping. If not provided, the default class mapping is used. transform ( callable , default: None ) \u2013 A transform to apply to the loaded data (images, dates, and masks). By default, no transformation is applied. truncate_image ( int , default: None ) \u2013 Truncate the time dimension of the image to a specified number of timesteps. If None, no truncation is performed. pad_image ( int , default: None ) \u2013 Pad the time dimension of the image to a specified number of timesteps. If None, no padding is applied. satellites ( list , default: ['S2'] ) \u2013 Defines the satellites to use. If you are using PASTIS-R, you have access to Sentinel-2 imagery and Sentinel-1 observations in Ascending and Descending orbits, respectively S2, S1A, and S1D. For example, use satellites=['S2', 'S1A'] for Sentinel-2 + Sentinel-1 ascending time series, or satellites=['S2', 'S1A', 'S1D'] to retrieve all time series. If you are using PASTIS, only S2 observations are available. Source code in terratorch/datasets/pastis.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def __init__ ( self , data_root , norm = True , # noqa: FBT002 target = \"semantic\" , folds = None , reference_date = \"2018-09-01\" , date_interval = ( - 200 , 600 ), class_mapping = None , transform = None , truncate_image = None , pad_image = None , satellites = [ \"S2\" ], # noqa: B006 ): \"\"\" Args: data_root (str): Path to the dataset. norm (bool): If true, images are standardised using pre-computed channel-wise means and standard deviations. reference_date (str, Format : 'YYYY-MM-DD'): Defines the reference date based on which all observation dates are expressed. Along with the image time series and the target tensor, this dataloader yields the sequence of observation dates (in terms of number of days since the reference date). This sequence of dates is used for instance for the positional encoding in attention based approaches. target (str): 'semantic' or 'instance'. Defines which type of target is returned by the dataloader. * If 'semantic' the target tensor is a tensor containing the class of each pixel. * If 'instance' the target tensor is the concatenation of several signals, necessary to train the Parcel-as-Points module: - the centerness heatmap, - the instance ids, - the voronoi partitioning of the patch with regards to the parcels' centers, - the (height, width) size of each parcel, - the semantic label of each parcel, - the semantic label of each pixel. folds (list, optional): List of ints specifying which of the 5 official folds to load. By default (when None is specified), all folds are loaded. class_mapping (dict, optional): A dictionary to define a mapping between the default 18 class nomenclature and another class grouping. If not provided, the default class mapping is used. transform (callable, optional): A transform to apply to the loaded data (images, dates, and masks). By default, no transformation is applied. truncate_image (int, optional): Truncate the time dimension of the image to a specified number of timesteps. If None, no truncation is performed. pad_image (int, optional): Pad the time dimension of the image to a specified number of timesteps. If None, no padding is applied. satellites (list): Defines the satellites to use. If you are using PASTIS-R, you have access to Sentinel-2 imagery and Sentinel-1 observations in Ascending and Descending orbits, respectively S2, S1A, and S1D. For example, use satellites=['S2', 'S1A'] for Sentinel-2 + Sentinel-1 ascending time series, or satellites=['S2', 'S1A', 'S1D'] to retrieve all time series. If you are using PASTIS, only S2 observations are available. \"\"\" if target not in [ \"semantic\" , \"instance\" ]: msg = f \"Target ' { target } ' not recognized. Use 'semantic', or 'instance'.\" raise ValueError ( msg ) valid_satellites = { \"S2\" , \"S1A\" , \"S1D\" } for sat in satellites : if sat not in valid_satellites : msg = f \"Satellite ' { sat } ' not recognized. Valid options are { valid_satellites } .\" raise ValueError ( msg ) super () . __init__ () self . data_root = data_root self . norm = norm self . reference_date = datetime ( * map ( int , reference_date . split ( \"-\" )), tzinfo = timezone . utc ) self . class_mapping = ( np . vectorize ( lambda x : class_mapping [ x ]) if class_mapping is not None else class_mapping ) self . target = target self . satellites = satellites self . transform = transform self . truncate_image = truncate_image self . pad_image = pad_image # loads patches metadata self . meta_patch = gpd . read_file ( os . path . join ( data_root , \"metadata.geojson\" )) self . meta_patch . index = self . meta_patch [ \"ID_PATCH\" ] . astype ( int ) self . meta_patch . sort_index ( inplace = True ) # stores table for each satalite date self . date_tables = { s : None for s in satellites } # date interval used in the PASTIS benchmark paper. date_interval_begin , date_interval_end = date_interval self . date_range = np . array ( range ( date_interval_begin , date_interval_end )) for s in satellites : # maps patches to its observation dates dates = self . meta_patch [ f \"dates- { s } \" ] date_table = pd . DataFrame ( index = self . meta_patch . index , columns = self . date_range , dtype = int ) for pid , date_seq in dates . items (): if type ( date_seq ) is str : date_seq = json . loads ( date_seq ) # noqa: PLW2901 # convert date to days since obersavation format d = pd . DataFrame () . from_dict ( date_seq , orient = \"index\" ) d = d [ 0 ] . apply ( lambda x : ( datetime ( int ( str ( x )[: 4 ]), int ( str ( x )[ 4 : 6 ]), int ( str ( x )[ 6 :]), tzinfo = timezone . utc ) - self . reference_date ) . days ) date_table . loc [ pid , d . values ] = 1 date_table = date_table . fillna ( 0 ) self . date_tables [ s ] = { index : np . array ( list ( d . values ())) for index , d in date_table . to_dict ( orient = \"index\" ) . items () } # selects patches correspondig to selected folds if folds is not None : self . meta_patch = pd . concat ( [ self . meta_patch [ self . meta_patch [ \"Fold\" ] == f ] for f in folds ] ) self . len = self . meta_patch . shape [ 0 ] self . id_patches = self . meta_patch . index # loads normalization values if norm : self . norm = {} for s in self . satellites : with open ( os . path . join ( data_root , f \"NORM_ { s } _patch.json\" ) ) as file : normvals = json . loads ( file . read ()) selected_folds = folds if folds is not None else range ( 1 , 6 ) means = [ normvals [ f \"Fold_ { f } \" ][ \"mean\" ] for f in selected_folds ] stds = [ normvals [ f \"Fold_ { f } \" ][ \"std\" ] for f in selected_folds ] self . norm [ s ] = np . stack ( means ) . mean ( axis = 0 ), np . stack ( stds ) . mean ( axis = 0 ) self . norm [ s ] = ( self . norm [ s ][ 0 ], self . norm [ s ][ 1 ], ) else : self . norm = None","title":"__init__"},{"location":"data/#terratorch.datasets.sen1floods11","text":"","title":"sen1floods11"},{"location":"data/#terratorch.datasets.sen1floods11.Sen1Floods11NonGeo","text":"Bases: NonGeoDataset NonGeo dataset implementation for sen1floods11 . Source code in terratorch/datasets/sen1floods11.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 class Sen1Floods11NonGeo ( NonGeoDataset ): \"\"\"NonGeo dataset implementation for [sen1floods11](https://github.com/cloudtostreet/Sen1Floods11).\"\"\" all_band_names = ( \"COASTAL_AEROSOL\" , \"BLUE\" , \"GREEN\" , \"RED\" , \"RED_EDGE_1\" , \"RED_EDGE_2\" , \"RED_EDGE_3\" , \"NIR_BROAD\" , \"NIR_NARROW\" , \"WATER_VAPOR\" , \"CIRRUS\" , \"SWIR_1\" , \"SWIR_2\" , ) rgb_bands = ( \"RED\" , \"GREEN\" , \"BLUE\" ) BAND_SETS = { \"all\" : all_band_names , \"rgb\" : rgb_bands } num_classes = 2 splits = { \"train\" : \"train\" , \"val\" : \"valid\" , \"test\" : \"test\" } data_dir = \"v1.1/data/flood_events/HandLabeled/S2Hand\" label_dir = \"v1.1/data/flood_events/HandLabeled/LabelHand\" split_dir = \"v1.1/splits/flood_handlabeled\" metadata_file = \"v1.1/Sen1Floods11_Metadata.geojson\" def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , constant_scale : float = 0.0001 , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , # noqa: FBT001, FBT002 ) -> None : \"\"\"Constructor Args: data_root (str): Path to the data root directory. split (str): one of 'train', 'val' or 'test'. bands (list[str]): Bands that should be output by the dataset. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). Defaults to None, which applies ToTensorV2(). constant_scale (float): Factor to multiply image values by. Defaults to 0.0001. no_data_replace (float | None): Replace nan values in input images with this value. If None, does no replacement. Defaults to 0. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1. use_metadata (bool): whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { self . splits } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . constant_scale = constant_scale self . data_root = Path ( data_root ) data_dir = self . data_root / self . data_dir label_dir = self . data_root / self . label_dir self . image_files = sorted ( glob . glob ( os . path . join ( data_dir , \"*_S2Hand.tif\" ))) self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( label_dir , \"*_LabelHand.tif\" ))) split_file = self . data_root / self . split_dir / f \"flood_ { split_name } _data.txt\" with open ( split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . segmentation_mask_files = filter_valid_files ( self . segmentation_mask_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata self . metadata = None if self . use_metadata : self . metadata = geopandas . read_file ( self . data_root / self . metadata_file ) # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform def __len__ ( self ) -> int : return len ( self . image_files ) def _get_date ( self , index : int ) -> torch . Tensor : file_name = self . image_files [ index ] location = os . path . basename ( file_name ) . split ( \"_\" )[ 0 ] if self . metadata [ self . metadata [ \"location\" ] == location ] . shape [ 0 ] != 1 : date = pd . to_datetime ( \"13-10-1998\" , dayfirst = True ) else : date = pd . to_datetime ( self . metadata [ self . metadata [ \"location\" ] == location ][ \"s2_date\" ] . item ()) return torch . tensor ([[ date . year , date . dayofyear - 1 ]], dtype = torch . float32 ) # (n_timesteps, coords) def _get_coords ( self , image : DataArray ) -> torch . Tensor : center_lat = image . y [ image . y . shape [ 0 ] // 2 ] center_lon = image . x [ image . x . shape [ 0 ] // 2 ] lat_lon = np . asarray ([ center_lat , center_lon ]) return torch . tensor ( lat_lon , dtype = torch . float32 ) def __getitem__ ( self , index : int ) -> dict [ str , Any ]: image = self . _load_file ( self . image_files [ index ], nan_replace = self . no_data_replace ) location_coords , temporal_coords = None , None if self . use_metadata : location_coords = self . _get_coords ( image ) temporal_coords = self . _get_date ( index ) # to channels last image = image . to_numpy () image = np . moveaxis ( image , 0 , - 1 ) # filter bands image = image [ ... , self . band_indices ] output = { \"image\" : image . astype ( np . float32 ) * self . constant_scale , \"mask\" : self . _load_file ( self . segmentation_mask_files [ index ], nan_replace = self . no_label_replace ) . to_numpy ()[ 0 ], } if self . transform : output = self . transform ( ** output ) output [ \"mask\" ] = output [ \"mask\" ] . long () if self . use_metadata : output [ \"location_coords\" ] = location_coords output [ \"temporal_coords\" ] = temporal_coords return output def _load_file ( self , path : Path , nan_replace : int | float | None = None ) -> DataArray : data = rioxarray . open_rasterio ( path , masked = True ) if nan_replace is not None : data = data . fillna ( nan_replace ) return data def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample \"\"\" num_images = 4 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) # RGB -> channels-last image = sample [ \"image\" ][ rgb_indices , ... ] . permute ( 1 , 2 , 0 ) . numpy () mask = sample [ \"mask\" ] . numpy () image = clip_image ( image ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] num_images += 1 else : prediction = None fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 5 ), layout = \"compressed\" ) ax [ 0 ] . axis ( \"off\" ) norm = mpl . colors . Normalize ( vmin = 0 , vmax = self . num_classes - 1 ) ax [ 1 ] . axis ( \"off\" ) ax [ 1 ] . title . set_text ( \"Image\" ) ax [ 1 ] . imshow ( image ) ax [ 2 ] . axis ( \"off\" ) ax [ 2 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ 2 ] . imshow ( mask , cmap = \"jet\" , norm = norm ) ax [ 3 ] . axis ( \"off\" ) ax [ 3 ] . title . set_text ( \"GT Mask on Image\" ) ax [ 3 ] . imshow ( image ) ax [ 3 ] . imshow ( mask , cmap = \"jet\" , alpha = 0.3 , norm = norm ) if \"prediction\" in sample : ax [ 4 ] . title . set_text ( \"Predicted Mask\" ) ax [ 4 ] . imshow ( prediction , cmap = \"jet\" , norm = norm ) cmap = plt . get_cmap ( \"jet\" ) legend_data = [[ i , cmap ( norm ( i )), str ( i )] for i in range ( self . num_classes )] handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = tuple ( v for v in c )) for k , c , n in legend_data ] labels = [ n for k , c , n in legend_data ] ax [ 0 ] . legend ( handles , labels , loc = \"center\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig","title":"Sen1Floods11NonGeo"},{"location":"data/#terratorch.datasets.sen1floods11.Sen1Floods11NonGeo.__init__","text":"Constructor Parameters: data_root ( str ) \u2013 Path to the data root directory. split ( str , default: 'train' ) \u2013 one of 'train', 'val' or 'test'. bands ( list [ str ] , default: BAND_SETS ['all'] ) \u2013 Bands that should be output by the dataset. Defaults to all bands. transform ( Compose | None , default: None ) \u2013 Albumentations transform to be applied. Should end with ToTensorV2(). Defaults to None, which applies ToTensorV2(). constant_scale ( float , default: 0.0001 ) \u2013 Factor to multiply image values by. Defaults to 0.0001. no_data_replace ( float | None , default: 0 ) \u2013 Replace nan values in input images with this value. If None, does no replacement. Defaults to 0. no_label_replace ( int | None , default: -1 ) \u2013 Replace nan values in label with this value. If none, does no replacement. Defaults to -1. use_metadata ( bool , default: False ) \u2013 whether to return metadata info (time and location). Source code in terratorch/datasets/sen1floods11.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def __init__ ( self , data_root : str , split : str = \"train\" , bands : Sequence [ str ] = BAND_SETS [ \"all\" ], transform : A . Compose | None = None , constant_scale : float = 0.0001 , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , # noqa: FBT001, FBT002 ) -> None : \"\"\"Constructor Args: data_root (str): Path to the data root directory. split (str): one of 'train', 'val' or 'test'. bands (list[str]): Bands that should be output by the dataset. Defaults to all bands. transform (A.Compose | None): Albumentations transform to be applied. Should end with ToTensorV2(). Defaults to None, which applies ToTensorV2(). constant_scale (float): Factor to multiply image values by. Defaults to 0.0001. no_data_replace (float | None): Replace nan values in input images with this value. If None, does no replacement. Defaults to 0. no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1. use_metadata (bool): whether to return metadata info (time and location). \"\"\" super () . __init__ () if split not in self . splits : msg = f \"Incorrect split ' { split } ', please choose one of { self . splits } .\" raise ValueError ( msg ) split_name = self . splits [ split ] self . split = split validate_bands ( bands , self . all_band_names ) self . bands = bands self . band_indices = np . asarray ([ self . all_band_names . index ( b ) for b in bands ]) self . constant_scale = constant_scale self . data_root = Path ( data_root ) data_dir = self . data_root / self . data_dir label_dir = self . data_root / self . label_dir self . image_files = sorted ( glob . glob ( os . path . join ( data_dir , \"*_S2Hand.tif\" ))) self . segmentation_mask_files = sorted ( glob . glob ( os . path . join ( label_dir , \"*_LabelHand.tif\" ))) split_file = self . data_root / self . split_dir / f \"flood_ { split_name } _data.txt\" with open ( split_file ) as f : split = f . readlines () valid_files = { rf \" { substring . strip () } \" for substring in split } self . image_files = filter_valid_files ( self . image_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . segmentation_mask_files = filter_valid_files ( self . segmentation_mask_files , valid_files = valid_files , ignore_extensions = True , allow_substring = True , ) self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata self . metadata = None if self . use_metadata : self . metadata = geopandas . read_file ( self . data_root / self . metadata_file ) # If no transform is given, apply only to transform to torch tensor self . transform = transform if transform else default_transform","title":"__init__"},{"location":"data/#terratorch.datasets.sen1floods11.Sen1Floods11NonGeo.plot","text":"Plot a sample from the dataset. Parameters: sample ( dict [ str , Tensor ] ) \u2013 a sample returned by :meth: __getitem__ suptitle ( str | None , default: None ) \u2013 optional string to use as a suptitle Returns: Figure \u2013 a matplotlib Figure with the rendered sample Source code in terratorch/datasets/sen1floods11.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def plot ( self , sample : dict [ str , Tensor ], suptitle : str | None = None ) -> Figure : \"\"\"Plot a sample from the dataset. Args: sample: a sample returned by :meth:`__getitem__` suptitle: optional string to use as a suptitle Returns: a matplotlib Figure with the rendered sample \"\"\" num_images = 4 rgb_indices = [ self . bands . index ( band ) for band in self . rgb_bands ] if len ( rgb_indices ) != 3 : msg = \"Dataset doesn't contain some of the RGB bands\" raise ValueError ( msg ) # RGB -> channels-last image = sample [ \"image\" ][ rgb_indices , ... ] . permute ( 1 , 2 , 0 ) . numpy () mask = sample [ \"mask\" ] . numpy () image = clip_image ( image ) if \"prediction\" in sample : prediction = sample [ \"prediction\" ] num_images += 1 else : prediction = None fig , ax = plt . subplots ( 1 , num_images , figsize = ( 12 , 5 ), layout = \"compressed\" ) ax [ 0 ] . axis ( \"off\" ) norm = mpl . colors . Normalize ( vmin = 0 , vmax = self . num_classes - 1 ) ax [ 1 ] . axis ( \"off\" ) ax [ 1 ] . title . set_text ( \"Image\" ) ax [ 1 ] . imshow ( image ) ax [ 2 ] . axis ( \"off\" ) ax [ 2 ] . title . set_text ( \"Ground Truth Mask\" ) ax [ 2 ] . imshow ( mask , cmap = \"jet\" , norm = norm ) ax [ 3 ] . axis ( \"off\" ) ax [ 3 ] . title . set_text ( \"GT Mask on Image\" ) ax [ 3 ] . imshow ( image ) ax [ 3 ] . imshow ( mask , cmap = \"jet\" , alpha = 0.3 , norm = norm ) if \"prediction\" in sample : ax [ 4 ] . title . set_text ( \"Predicted Mask\" ) ax [ 4 ] . imshow ( prediction , cmap = \"jet\" , norm = norm ) cmap = plt . get_cmap ( \"jet\" ) legend_data = [[ i , cmap ( norm ( i )), str ( i )] for i in range ( self . num_classes )] handles = [ Rectangle (( 0 , 0 ), 1 , 1 , color = tuple ( v for v in c )) for k , c , n in legend_data ] labels = [ n for k , c , n in legend_data ] ax [ 0 ] . legend ( handles , labels , loc = \"center\" ) if suptitle is not None : plt . suptitle ( suptitle ) return fig","title":"plot"},{"location":"data/#terratorch.datasets.sen4agrinet","text":"","title":"sen4agrinet"},{"location":"data/#terratorch.datasets.sen4agrinet.Sen4AgriNet","text":"Bases: NonGeoDataset Source code in terratorch/datasets/sen4agrinet.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 class Sen4AgriNet ( NonGeoDataset ): def __init__ ( self , data_root : str , bands : list [ str ] | None = None , scenario : str = \"random\" , split : str = \"train\" , transform : A . Compose = None , truncate_image : int | None = 4 , pad_image : int | None = 4 , spatial_interpolate_and_stack_temporally : bool = True , # noqa: FBT001, FBT002 seed : int = 42 , ): \"\"\" Pytorch Dataset class to load samples from the [Sen4AgriNet](https://github.com/Orion-AI-Lab/S4A) dataset, supporting multiple scenarios for splitting the data. Args: data_root (str): Root directory of the dataset. bands (list of str, optional): List of band names to load. Defaults to all available bands. scenario (str): Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020). split (str): Specifies the dataset split. Options are 'train', 'val', or 'test'. transform (albumentations.Compose, optional): Albumentations transformations to apply to the data. truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image. If None, no truncation is applied. Default is 4. pad_image (int, optional): Number of timesteps to pad the time dimension of the image. If None, no padding is applied. Default is 4. spatial_interpolate_and_stack_temporally (bool): Whether to interpolate bands and concatenate them over time seed (int): Random seed used for data splitting. \"\"\" self . data_root = Path ( data_root ) / \"data\" self . transform = transform if transform else lambda ** batch : to_tensor ( batch ) self . scenario = scenario self . seed = seed self . truncate_image = truncate_image self . pad_image = pad_image self . spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally if bands is None : bands = [ \"B01\" , \"B02\" , \"B03\" , \"B04\" , \"B05\" , \"B06\" , \"B07\" , \"B08\" , \"B09\" , \"B10\" , \"B11\" , \"B12\" , \"B8A\" ] self . bands = bands self . image_files = list ( self . data_root . glob ( \"**/*.nc\" )) self . train_files , self . val_files , self . test_files = self . split_data () if split == \"train\" : self . image_files = self . train_files elif split == \"val\" : self . image_files = self . val_files elif split == \"test\" : self . image_files = self . test_files def __len__ ( self ): return len ( self . image_files ) def split_data ( self ): random . seed ( self . seed ) if self . scenario == \"random\" : random . shuffle ( self . image_files ) total_files = len ( self . image_files ) train_split = int ( 0.6 * total_files ) val_split = int ( 0.8 * total_files ) train_files = self . image_files [: train_split ] val_files = self . image_files [ train_split : val_split ] test_files = self . image_files [ val_split :] elif self . scenario == \"spatial\" : catalonia_files = [ f for f in self . image_files if any ( tile in f . stem for tile in CAT_TILES )] france_files = [ f for f in self . image_files if any ( tile in f . stem for tile in FR_TILES )] val_split_cat = int ( 0.2 * len ( catalonia_files )) train_files = catalonia_files [ val_split_cat :] val_files = catalonia_files [: val_split_cat ] test_files = france_files elif self . scenario == \"spatio-temporal\" : france_files = [ f for f in self . image_files if any ( tile in f . stem for tile in FR_TILES )] catalonia_files = [ f for f in self . image_files if any ( tile in f . stem for tile in CAT_TILES )] france_2019_files = [ f for f in france_files if \"2019\" in f . stem ] catalonia_2020_files = [ f for f in catalonia_files if \"2020\" in f . stem ] val_split_france_2019 = int ( 0.2 * len ( france_2019_files )) train_files = france_2019_files [ val_split_france_2019 :] val_files = france_2019_files [: val_split_france_2019 ] test_files = catalonia_2020_files return train_files , val_files , test_files def __getitem__ ( self , index : int ): patch_file = self . image_files [ index ] with h5py . File ( patch_file , \"r\" ) as patch_data : output = {} images_over_time = [] for band in self . bands : band_group = patch_data [ band ] band_data = band_group [ f \" { band } \" ][:] time_vector = band_group [ \"time\" ][:] sorted_indices = np . argsort ( time_vector ) band_data = band_data [ sorted_indices ] . astype ( np . float32 ) if self . truncate_image : band_data = band_data [ - self . truncate_image :] if self . pad_image : band_data = pad_numpy ( band_data , self . pad_image ) if self . spatial_interpolate_and_stack_temporally : band_data = torch . from_numpy ( band_data ) band_data = band_data . clone () . detach () interpolated = F . interpolate ( band_data . unsqueeze ( 0 ), size = MAX_TEMPORAL_IMAGE_SIZE , mode = \"bilinear\" , align_corners = False ) . squeeze ( 0 ) images_over_time . append ( interpolated ) else : output [ band ] = band_data if self . spatial_interpolate_and_stack_temporally : images = torch . stack ( images_over_time , dim = 0 ) . numpy () output [ \"image\" ] = images labels = patch_data [ \"labels\" ][ \"labels\" ][:] . astype ( int ) parcels = patch_data [ \"parcels\" ][ \"parcels\" ][:] . astype ( int ) output [ \"mask\" ] = labels image_shape = output [ \"image\" ] . shape [ - 2 :] mask_shape = output [ \"mask\" ] . shape if image_shape != mask_shape : diff_h = mask_shape [ 0 ] - image_shape [ 0 ] diff_w = mask_shape [ 1 ] - image_shape [ 1 ] output [ \"image\" ] = np . pad ( output [ \"image\" ], [( 0 , 0 ), ( 0 , 0 ), ( diff_h // 2 , diff_h - diff_h // 2 ), ( diff_w // 2 , diff_w - diff_w // 2 )], mode = \"constant\" , constant_values = 0 ) linear_encoder = { val : i + 1 for i , val in enumerate ( sorted ( SELECTED_CLASSES ))} linear_encoder [ 0 ] = 0 output [ \"image\" ] = output [ \"image\" ] . transpose ( 0 , 2 , 3 , 1 ) output [ \"mask\" ] = self . map_mask_to_discrete_classes ( output [ \"mask\" ], linear_encoder ) if self . transform : output = self . transform ( ** output ) output [ \"parcels\" ] = parcels return output def plot ( self , sample , suptitle = None ): rgb_bands = [ \"B04\" , \"B03\" , \"B02\" ] if not all ( band in sample for band in rgb_bands ): warnings . warn ( \"No RGB image.\" ) # noqa: B028 return None rgb_images = [] for t in range ( sample [ \"B04\" ] . shape [ 0 ]): rgb_image = torch . stack ([ sample [ band ][ t ] for band in rgb_bands ]) # Normalization rgb_min = rgb_image . min ( dim = 1 , keepdim = True ) . values . min ( dim = 2 , keepdim = True ) . values rgb_max = rgb_image . max ( dim = 1 , keepdim = True ) . values . max ( dim = 2 , keepdim = True ) . values denom = rgb_max - rgb_min denom [ denom == 0 ] = 1 rgb_image = ( rgb_image - rgb_min ) / denom rgb_image = rgb_image . permute ( 1 , 2 , 0 ) . numpy () rgb_images . append ( np . clip ( rgb_image , 0 , 1 )) dates = torch . arange ( sample [ \"B04\" ] . shape [ 0 ]) return self . _plot_sample ( rgb_images , dates , sample . get ( \"labels\" ), suptitle = suptitle ) def _plot_sample ( self , images , dates , labels = None , suptitle = None ): num_images = len ( images ) cols = 5 rows = ( num_images + cols - 1 ) // cols fig , ax = plt . subplots ( rows , cols , figsize = ( 20 , 4 * rows )) for i , image in enumerate ( images ): ax [ i // cols , i % cols ] . imshow ( image ) ax [ i // cols , i % cols ] . set_title ( f \"T { i + 1 } - Day { dates [ i ] . item () } \" ) ax [ i // cols , i % cols ] . axis ( \"off\" ) if labels is not None : if rows * cols > num_images : target_ax = ax [( num_images ) // cols , ( num_images ) % cols ] else : fig . add_subplot ( rows + 1 , 1 , 1 ) target_ax = fig . gca () target_ax . imshow ( labels . numpy (), cmap = \"tab20\" ) target_ax . set_title ( \"Labels\" ) target_ax . axis ( \"off\" ) for k in range ( num_images , rows * cols ): ax [ k // cols , k % cols ] . axis ( \"off\" ) if suptitle : plt . suptitle ( suptitle ) plt . tight_layout () plt . show () def map_mask_to_discrete_classes ( self , mask , encoder ): map_func = np . vectorize ( lambda x : encoder . get ( x , 0 )) return map_func ( mask )","title":"Sen4AgriNet"},{"location":"data/#terratorch.datasets.sen4agrinet.Sen4AgriNet.__init__","text":"Pytorch Dataset class to load samples from the Sen4AgriNet dataset, supporting multiple scenarios for splitting the data. Parameters: data_root ( str ) \u2013 Root directory of the dataset. bands ( list of str , default: None ) \u2013 List of band names to load. Defaults to all available bands. scenario ( str , default: 'random' ) \u2013 Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020). split ( str , default: 'train' ) \u2013 Specifies the dataset split. Options are 'train', 'val', or 'test'. transform ( Compose , default: None ) \u2013 Albumentations transformations to apply to the data. truncate_image ( int , default: 4 ) \u2013 Number of timesteps to truncate the time dimension of the image. If None, no truncation is applied. Default is 4. pad_image ( int , default: 4 ) \u2013 Number of timesteps to pad the time dimension of the image. If None, no padding is applied. Default is 4. spatial_interpolate_and_stack_temporally ( bool , default: True ) \u2013 Whether to interpolate bands and concatenate them over time seed ( int , default: 42 ) \u2013 Random seed used for data splitting. Source code in terratorch/datasets/sen4agrinet.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , data_root : str , bands : list [ str ] | None = None , scenario : str = \"random\" , split : str = \"train\" , transform : A . Compose = None , truncate_image : int | None = 4 , pad_image : int | None = 4 , spatial_interpolate_and_stack_temporally : bool = True , # noqa: FBT001, FBT002 seed : int = 42 , ): \"\"\" Pytorch Dataset class to load samples from the [Sen4AgriNet](https://github.com/Orion-AI-Lab/S4A) dataset, supporting multiple scenarios for splitting the data. Args: data_root (str): Root directory of the dataset. bands (list of str, optional): List of band names to load. Defaults to all available bands. scenario (str): Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020). split (str): Specifies the dataset split. Options are 'train', 'val', or 'test'. transform (albumentations.Compose, optional): Albumentations transformations to apply to the data. truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image. If None, no truncation is applied. Default is 4. pad_image (int, optional): Number of timesteps to pad the time dimension of the image. If None, no padding is applied. Default is 4. spatial_interpolate_and_stack_temporally (bool): Whether to interpolate bands and concatenate them over time seed (int): Random seed used for data splitting. \"\"\" self . data_root = Path ( data_root ) / \"data\" self . transform = transform if transform else lambda ** batch : to_tensor ( batch ) self . scenario = scenario self . seed = seed self . truncate_image = truncate_image self . pad_image = pad_image self . spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally if bands is None : bands = [ \"B01\" , \"B02\" , \"B03\" , \"B04\" , \"B05\" , \"B06\" , \"B07\" , \"B08\" , \"B09\" , \"B10\" , \"B11\" , \"B12\" , \"B8A\" ] self . bands = bands self . image_files = list ( self . data_root . glob ( \"**/*.nc\" )) self . train_files , self . val_files , self . test_files = self . split_data () if split == \"train\" : self . image_files = self . train_files elif split == \"val\" : self . image_files = self . val_files elif split == \"test\" : self . image_files = self . test_files","title":"__init__"},{"location":"data/#terratorch.datasets.sen4map","text":"","title":"sen4map"},{"location":"data/#terratorch.datasets.sen4map.Sen4MapDatasetMonthlyComposites","text":"Bases: Dataset Sen4Map Dataset for Monthly Composites. Dataset intended for land-cover and crop classification tasks based on monthly composites derived from multi-temporal satellite data stored in HDF5 files. Dataset Format: HDF5 files containing multi-temporal acquisitions with spectral bands (e.g., B2, B3, \u2026, B12) Composite images computed as the median across available acquisitions for each month. Classification labels provided via HDF5 attributes (e.g., 'lc1') with mappings defined for: Land-cover: using land_cover_classification_map Crops: using crop_classification_map Dataset Features: Supports two classification tasks: \"land-cover\" (default) and \"crops\". Pre-processing options include center cropping, reverse tiling, and resizing. Option to save the keys HDF5 for later filtering. Input channel selection via a mapping between available bands and input bands. Source code in terratorch/datasets/sen4map.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 class Sen4MapDatasetMonthlyComposites ( Dataset ): \"\"\"[Sen4Map](https://gitlab.jsc.fz-juelich.de/sdlrs/sen4map-benchmark-dataset) Dataset for Monthly Composites. Dataset intended for land-cover and crop classification tasks based on monthly composites derived from multi-temporal satellite data stored in HDF5 files. Dataset Format: * HDF5 files containing multi-temporal acquisitions with spectral bands (e.g., B2, B3, \u2026, B12) * Composite images computed as the median across available acquisitions for each month. * Classification labels provided via HDF5 attributes (e.g., 'lc1') with mappings defined for: - Land-cover: using `land_cover_classification_map` - Crops: using `crop_classification_map` Dataset Features: * Supports two classification tasks: \"land-cover\" (default) and \"crops\". * Pre-processing options include center cropping, reverse tiling, and resizing. * Option to save the keys HDF5 for later filtering. * Input channel selection via a mapping between available bands and input bands. \"\"\" land_cover_classification_map = { 'A10' : 0 , 'A11' : 0 , 'A12' : 0 , 'A13' : 0 , 'A20' : 0 , 'A21' : 0 , 'A30' : 0 , 'A22' : 1 , 'F10' : 1 , 'F20' : 1 , 'F30' : 1 , 'F40' : 1 , 'E10' : 2 , 'E20' : 2 , 'E30' : 2 , 'B50' : 2 , 'B51' : 2 , 'B52' : 2 , 'B53' : 2 , 'B54' : 2 , 'B55' : 2 , 'B10' : 3 , 'B11' : 3 , 'B12' : 3 , 'B13' : 3 , 'B14' : 3 , 'B15' : 3 , 'B16' : 3 , 'B17' : 3 , 'B18' : 3 , 'B19' : 3 , 'B10' : 3 , 'B20' : 3 , 'B21' : 3 , 'B22' : 3 , 'B23' : 3 , 'B30' : 3 , 'B31' : 3 , 'B32' : 3 , 'B33' : 3 , 'B34' : 3 , 'B35' : 3 , 'B30' : 3 , 'B36' : 3 , 'B37' : 3 , 'B40' : 3 , 'B41' : 3 , 'B42' : 3 , 'B43' : 3 , 'B44' : 3 , 'B45' : 3 , 'B70' : 3 , 'B71' : 3 , 'B72' : 3 , 'B73' : 3 , 'B74' : 3 , 'B75' : 3 , 'B76' : 3 , 'B77' : 3 , 'B80' : 3 , 'B81' : 3 , 'B82' : 3 , 'B83' : 3 , 'B84' : 3 , 'BX1' : 3 , 'BX2' : 3 , 'C10' : 4 , 'C20' : 5 , 'C21' : 5 , 'C22' : 5 , 'C23' : 5 , 'C30' : 5 , 'C31' : 5 , 'C32' : 5 , 'C33' : 5 , 'CXX1' : 5 , 'CXX2' : 5 , 'CXX3' : 5 , 'CXX4' : 5 , 'CXX5' : 5 , 'CXX5' : 5 , 'CXX6' : 5 , 'CXX7' : 5 , 'CXX8' : 5 , 'CXX9' : 5 , 'CXXA' : 5 , 'CXXB' : 5 , 'CXXC' : 5 , 'CXXD' : 5 , 'CXXE' : 5 , 'D10' : 6 , 'D20' : 6 , 'D10' : 6 , 'G10' : 7 , 'G11' : 7 , 'G12' : 7 , 'G20' : 7 , 'G21' : 7 , 'G22' : 7 , 'G30' : 7 , 'G40' : 7 , 'G50' : 7 , 'H10' : 8 , 'H11' : 8 , 'H12' : 8 , 'H11' : 8 , 'H20' : 8 , 'H21' : 8 , 'H22' : 8 , 'H23' : 8 , '' : 9 } # This dictionary maps the LUCAS classes to crop classes. crop_classification_map = { \"B11\" : 0 , \"B12\" : 0 , \"B13\" : 0 , \"B14\" : 0 , \"B15\" : 0 , \"B16\" : 0 , \"B17\" : 0 , \"B18\" : 0 , \"B19\" : 0 , # Cereals \"B21\" : 1 , \"B22\" : 1 , \"B23\" : 1 , # Root Crops \"B31\" : 2 , \"B32\" : 2 , \"B33\" : 2 , \"B34\" : 2 , \"B35\" : 2 , \"B36\" : 2 , \"B37\" : 2 , # Nonpermanent Industrial Crops \"B41\" : 3 , \"B42\" : 3 , \"B43\" : 3 , \"B44\" : 3 , \"B45\" : 3 , # Dry Pulses, Vegetables and Flowers \"B51\" : 4 , \"B52\" : 4 , \"B53\" : 4 , \"B54\" : 4 , # Fodder Crops \"F10\" : 5 , \"F20\" : 5 , \"F30\" : 5 , \"F40\" : 5 , # Bareland \"B71\" : 6 , \"B72\" : 6 , \"B73\" : 6 , \"B74\" : 6 , \"B75\" : 6 , \"B76\" : 6 , \"B77\" : 6 , \"B81\" : 6 , \"B82\" : 6 , \"B83\" : 6 , \"B84\" : 6 , \"C10\" : 6 , \"C21\" : 6 , \"C22\" : 6 , \"C23\" : 6 , \"C31\" : 6 , \"C32\" : 6 , \"C33\" : 6 , \"D10\" : 6 , \"D20\" : 6 , # Woodland and Shrubland \"B55\" : 7 , \"E10\" : 7 , \"E20\" : 7 , \"E30\" : 7 , # Grassland } def __init__ ( self , h5py_file_object : h5py . File , h5data_keys = None , crop_size : None | int = None , dataset_bands : list [ HLSBands | int ] | None = None , input_bands : list [ HLSBands | int ] | None = None , resize = False , resize_to = [ 224 , 224 ], resize_interpolation = InterpolationMode . BILINEAR , resize_antialiasing = True , reverse_tile = False , reverse_tile_size = 3 , save_keys_path = None , classification_map = \"land-cover\" ): \"\"\"Initialize a new instance of Sen4MapDatasetMonthlyComposites. This dataset loads data from an HDF5 file object containing multi-temporal satellite data and computes monthly composite images by aggregating acquisitions (via median). Args: h5py_file_object: An open h5py.File object containing the dataset. h5data_keys: Optional list of keys to select a subset of data samples from the HDF5 file. If None, all keys are used. crop_size: Optional integer specifying the square crop size for the output image. dataset_bands: Optional list of bands available in the dataset. input_bands: Optional list of bands to be used as input channels. Must be provided along with `dataset_bands`. resize: Boolean flag indicating whether the image should be resized. Default is False. resize_to: Target dimensions [height, width] for resizing. Default is [224, 224]. resize_interpolation: Interpolation mode used for resizing. Default is InterpolationMode.BILINEAR. resize_antialiasing: Boolean flag to apply antialiasing during resizing. Default is True. reverse_tile: Boolean flag indicating whether to apply reverse tiling to the image. Default is False. reverse_tile_size: Kernel size for the reverse tiling operation. Must be an odd number >= 3. Default is 3. save_keys_path: Optional file path to save the list of dataset keys. classification_map: String specifying the classification mapping to use (\"land-cover\" or \"crops\"). Default is \"land-cover\". Raises: ValueError: If `input_bands` is provided without specifying `dataset_bands`. ValueError: If an invalid `classification_map` is provided. \"\"\" self . h5data = h5py_file_object if h5data_keys is None : if classification_map == \"crops\" : print ( f \"Crop classification task chosen but no keys supplied. Will fail unless dataset hdf5 files have been filtered. Either filter dataset files or create a filtered set of keys.\" ) self . h5data_keys = list ( self . h5data . keys ()) if save_keys_path is not None : with open ( save_keys_path , \"wb\" ) as file : pickle . dump ( self . h5data_keys , file ) else : self . h5data_keys = h5data_keys self . crop_size = crop_size if input_bands and not dataset_bands : raise ValueError ( f \"input_bands was provided without specifying the dataset_bands\" ) # self.dataset_bands = dataset_bands # self.input_bands = input_bands if input_bands and dataset_bands : self . input_channels = [ dataset_bands . index ( band_ind ) for band_ind in input_bands if band_ind in dataset_bands ] else : self . input_channels = None classification_maps = { \"land-cover\" : Sen4MapDatasetMonthlyComposites . land_cover_classification_map , \"crops\" : Sen4MapDatasetMonthlyComposites . crop_classification_map } if classification_map not in classification_maps . keys (): raise ValueError ( f \"Provided classification_map of: { classification_map } , is not from the list of valid ones: { classification_maps } \" ) self . classification_map = classification_maps [ classification_map ] self . resize = resize self . resize_to = resize_to self . resize_interpolation = resize_interpolation self . resize_antialiasing = resize_antialiasing self . reverse_tile = reverse_tile self . reverse_tile_size = reverse_tile_size def __getitem__ ( self , index ): # we can call dataset with an index, eg. dataset[0] im = self . h5data [ self . h5data_keys [ index ]] Image , Label = self . get_data ( im ) Image = self . min_max_normalize ( Image , [ 67.0 , 122.0 , 93.27 , 158.5 , 160.77 , 174.27 , 162.27 , 149.0 , 84.5 , 66.27 ], [ 2089.0 , 2598.45 , 3214.5 , 3620.45 , 4033.61 , 4613.0 , 4825.45 , 4945.72 , 5140.84 , 4414.45 ]) Image = Image . clip ( 0 , 1 ) Label = torch . LongTensor ( Label ) if self . input_channels : Image = Image [ self . input_channels , ... ] return { \"image\" : Image , \"label\" : Label } def __len__ ( self ): return len ( self . h5data_keys ) def get_data ( self , im ): mask = im [ 'SCL' ] < 9 B2 = np . where ( mask == 1 , im [ 'B2' ], 0 ) B3 = np . where ( mask == 1 , im [ 'B3' ], 0 ) B4 = np . where ( mask == 1 , im [ 'B4' ], 0 ) B5 = np . where ( mask == 1 , im [ 'B5' ], 0 ) B6 = np . where ( mask == 1 , im [ 'B6' ], 0 ) B7 = np . where ( mask == 1 , im [ 'B7' ], 0 ) B8 = np . where ( mask == 1 , im [ 'B8' ], 0 ) B8A = np . where ( mask == 1 , im [ 'B8A' ], 0 ) B11 = np . where ( mask == 1 , im [ 'B11' ], 0 ) B12 = np . where ( mask == 1 , im [ 'B12' ], 0 ) Image = np . stack (( B2 , B3 , B4 , B5 , B6 , B7 , B8 , B8A , B11 , B12 ), axis = 0 , dtype = \"float32\" ) Image = np . moveaxis ( Image , [ 0 ],[ 1 ]) Image = torch . from_numpy ( Image ) # Composites: n1 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201801' in s ] n2 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201802' in s ] n3 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201803' in s ] n4 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201804' in s ] n5 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201805' in s ] n6 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201806' in s ] n7 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201807' in s ] n8 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201808' in s ] n9 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201809' in s ] n10 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201810' in s ] n11 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201811' in s ] n12 = [ i for i , s in enumerate ( im . attrs [ 'Image_ID' ] . tolist ()) if '201812' in s ] Jan = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n1 else n1 Feb = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n2 else n2 Mar = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n3 else n3 Apr = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n4 else n4 May = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n5 else n5 Jun = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n6 else n6 Jul = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n7 else n7 Aug = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n8 else n8 Sep = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n9 else n9 Oct = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n10 else n10 Nov = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n11 else n11 Dec = n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n12 else n12 month_indices = [ Jan , Feb , Mar , Apr , May , Jun , Jul , Aug , Sep , Oct , Nov , Dec ] month_medians = [ torch . stack ([ Image [ month_indices [ i ][ j ]] for j in range ( len ( month_indices [ i ]))]) . median ( dim = 0 ) . values for i in range ( 12 )] Image = torch . stack ( month_medians , dim = 0 ) Image = torch . moveaxis ( Image , 0 , 1 ) if self . crop_size : Image = self . crop_center ( Image , self . crop_size , self . crop_size ) if self . reverse_tile : Image = self . reverse_tiling_pytorch ( Image , kernel_size = self . reverse_tile_size ) if self . resize : Image = resize ( Image , size = self . resize_to , interpolation = self . resize_interpolation , antialias = self . resize_antialiasing ) Label = im . attrs [ 'lc1' ] Label = self . classification_map [ Label ] Label = np . array ( Label ) Label = Label . astype ( 'float32' ) return Image , Label def crop_center ( self , img_b : torch . Tensor , cropx , cropy ) -> torch . Tensor : c , t , y , x = img_b . shape startx = x // 2 - ( cropx // 2 ) starty = y // 2 - ( cropy // 2 ) return img_b [ 0 : c , 0 : t , starty : starty + cropy , startx : startx + cropx ] def reverse_tiling_pytorch ( self , img_tensor : torch . Tensor , kernel_size : int = 3 ): \"\"\" Upscales an image where every pixel is expanded into `kernel_size`*`kernel_size` pixels. Used to test whether the benefit of resizing images to the pre-trained size comes from the bilnearly interpolated pixels, or if the same would be realized with no interpolated pixels. \"\"\" assert kernel_size % 2 == 1 assert kernel_size >= 3 padding = ( kernel_size - 1 ) // 2 # img_tensor shape: (batch_size, channels, H, W) batch_size , channels , H , W = img_tensor . shape # Unfold: Extract 3x3 patches with padding of 1 to cover borders img_tensor = F . pad ( img_tensor , pad = ( padding , padding , padding , padding ), mode = \"replicate\" ) patches = F . unfold ( img_tensor , kernel_size = kernel_size , padding = 0 ) # Shape: (batch_size, channels*9, H*W) # Reshape to organize the 9 values from each 3x3 neighborhood patches = patches . view ( batch_size , channels , kernel_size * kernel_size , H , W ) # Shape: (batch_size, channels, 9, H, W) # Rearrange the patches into (batch_size, channels, 3, 3, H, W) patches = patches . view ( batch_size , channels , kernel_size , kernel_size , H , W ) # Permute to have the spatial dimensions first and unfold them patches = patches . permute ( 0 , 1 , 4 , 2 , 5 , 3 ) # Shape: (batch_size, channels, H, 3, W, 3) # Reshape to get the final expanded image of shape (batch_size, channels, H*3, W*3) expanded_img = patches . reshape ( batch_size , channels , H * kernel_size , W * kernel_size ) return expanded_img def min_max_normalize ( self , tensor : torch . Tensor , q_low : list [ float ], q_hi : list [ float ]) -> torch . Tensor : dtype = tensor . dtype q_low = torch . as_tensor ( q_low , dtype = dtype , device = tensor . device ) q_hi = torch . as_tensor ( q_hi , dtype = dtype , device = tensor . device ) x = torch . tensor ( - 12.0 ) y = torch . exp ( x ) tensor . sub_ ( q_low [:, None , None , None ]) . div_ (( q_hi [:, None , None , None ] . sub_ ( q_low [:, None , None , None ])) . add ( y )) return tensor","title":"Sen4MapDatasetMonthlyComposites"},{"location":"data/#terratorch.datasets.sen4map.Sen4MapDatasetMonthlyComposites.__init__","text":"Initialize a new instance of Sen4MapDatasetMonthlyComposites. This dataset loads data from an HDF5 file object containing multi-temporal satellite data and computes monthly composite images by aggregating acquisitions (via median). Parameters: h5py_file_object ( File ) \u2013 An open h5py.File object containing the dataset. h5data_keys \u2013 Optional list of keys to select a subset of data samples from the HDF5 file. If None, all keys are used. crop_size ( None | int , default: None ) \u2013 Optional integer specifying the square crop size for the output image. dataset_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Optional list of bands available in the dataset. input_bands ( list [ HLSBands | int ] | None , default: None ) \u2013 Optional list of bands to be used as input channels. Must be provided along with dataset_bands . resize \u2013 Boolean flag indicating whether the image should be resized. Default is False. resize_to \u2013 Target dimensions [height, width] for resizing. Default is [224, 224]. resize_interpolation \u2013 Interpolation mode used for resizing. Default is InterpolationMode.BILINEAR. resize_antialiasing \u2013 Boolean flag to apply antialiasing during resizing. Default is True. reverse_tile \u2013 Boolean flag indicating whether to apply reverse tiling to the image. Default is False. reverse_tile_size \u2013 Kernel size for the reverse tiling operation. Must be an odd number >= 3. Default is 3. save_keys_path \u2013 Optional file path to save the list of dataset keys. classification_map \u2013 String specifying the classification mapping to use (\"land-cover\" or \"crops\"). Default is \"land-cover\". Raises: ValueError \u2013 If input_bands is provided without specifying dataset_bands . ValueError \u2013 If an invalid classification_map is provided. Source code in terratorch/datasets/sen4map.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def __init__ ( self , h5py_file_object : h5py . File , h5data_keys = None , crop_size : None | int = None , dataset_bands : list [ HLSBands | int ] | None = None , input_bands : list [ HLSBands | int ] | None = None , resize = False , resize_to = [ 224 , 224 ], resize_interpolation = InterpolationMode . BILINEAR , resize_antialiasing = True , reverse_tile = False , reverse_tile_size = 3 , save_keys_path = None , classification_map = \"land-cover\" ): \"\"\"Initialize a new instance of Sen4MapDatasetMonthlyComposites. This dataset loads data from an HDF5 file object containing multi-temporal satellite data and computes monthly composite images by aggregating acquisitions (via median). Args: h5py_file_object: An open h5py.File object containing the dataset. h5data_keys: Optional list of keys to select a subset of data samples from the HDF5 file. If None, all keys are used. crop_size: Optional integer specifying the square crop size for the output image. dataset_bands: Optional list of bands available in the dataset. input_bands: Optional list of bands to be used as input channels. Must be provided along with `dataset_bands`. resize: Boolean flag indicating whether the image should be resized. Default is False. resize_to: Target dimensions [height, width] for resizing. Default is [224, 224]. resize_interpolation: Interpolation mode used for resizing. Default is InterpolationMode.BILINEAR. resize_antialiasing: Boolean flag to apply antialiasing during resizing. Default is True. reverse_tile: Boolean flag indicating whether to apply reverse tiling to the image. Default is False. reverse_tile_size: Kernel size for the reverse tiling operation. Must be an odd number >= 3. Default is 3. save_keys_path: Optional file path to save the list of dataset keys. classification_map: String specifying the classification mapping to use (\"land-cover\" or \"crops\"). Default is \"land-cover\". Raises: ValueError: If `input_bands` is provided without specifying `dataset_bands`. ValueError: If an invalid `classification_map` is provided. \"\"\" self . h5data = h5py_file_object if h5data_keys is None : if classification_map == \"crops\" : print ( f \"Crop classification task chosen but no keys supplied. Will fail unless dataset hdf5 files have been filtered. Either filter dataset files or create a filtered set of keys.\" ) self . h5data_keys = list ( self . h5data . keys ()) if save_keys_path is not None : with open ( save_keys_path , \"wb\" ) as file : pickle . dump ( self . h5data_keys , file ) else : self . h5data_keys = h5data_keys self . crop_size = crop_size if input_bands and not dataset_bands : raise ValueError ( f \"input_bands was provided without specifying the dataset_bands\" ) # self.dataset_bands = dataset_bands # self.input_bands = input_bands if input_bands and dataset_bands : self . input_channels = [ dataset_bands . index ( band_ind ) for band_ind in input_bands if band_ind in dataset_bands ] else : self . input_channels = None classification_maps = { \"land-cover\" : Sen4MapDatasetMonthlyComposites . land_cover_classification_map , \"crops\" : Sen4MapDatasetMonthlyComposites . crop_classification_map } if classification_map not in classification_maps . keys (): raise ValueError ( f \"Provided classification_map of: { classification_map } , is not from the list of valid ones: { classification_maps } \" ) self . classification_map = classification_maps [ classification_map ] self . resize = resize self . resize_to = resize_to self . resize_interpolation = resize_interpolation self . resize_antialiasing = resize_antialiasing self . reverse_tile = reverse_tile self . reverse_tile_size = reverse_tile_size","title":"__init__"},{"location":"data/#terratorch.datasets.sen4map.Sen4MapDatasetMonthlyComposites.reverse_tiling_pytorch","text":"Upscales an image where every pixel is expanded into kernel_size * kernel_size pixels. Used to test whether the benefit of resizing images to the pre-trained size comes from the bilnearly interpolated pixels, or if the same would be realized with no interpolated pixels. Source code in terratorch/datasets/sen4map.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def reverse_tiling_pytorch ( self , img_tensor : torch . Tensor , kernel_size : int = 3 ): \"\"\" Upscales an image where every pixel is expanded into `kernel_size`*`kernel_size` pixels. Used to test whether the benefit of resizing images to the pre-trained size comes from the bilnearly interpolated pixels, or if the same would be realized with no interpolated pixels. \"\"\" assert kernel_size % 2 == 1 assert kernel_size >= 3 padding = ( kernel_size - 1 ) // 2 # img_tensor shape: (batch_size, channels, H, W) batch_size , channels , H , W = img_tensor . shape # Unfold: Extract 3x3 patches with padding of 1 to cover borders img_tensor = F . pad ( img_tensor , pad = ( padding , padding , padding , padding ), mode = \"replicate\" ) patches = F . unfold ( img_tensor , kernel_size = kernel_size , padding = 0 ) # Shape: (batch_size, channels*9, H*W) # Reshape to organize the 9 values from each 3x3 neighborhood patches = patches . view ( batch_size , channels , kernel_size * kernel_size , H , W ) # Shape: (batch_size, channels, 9, H, W) # Rearrange the patches into (batch_size, channels, 3, 3, H, W) patches = patches . view ( batch_size , channels , kernel_size , kernel_size , H , W ) # Permute to have the spatial dimensions first and unfold them patches = patches . permute ( 0 , 1 , 4 , 2 , 5 , 3 ) # Shape: (batch_size, channels, H, 3, W, 3) # Reshape to get the final expanded image of shape (batch_size, channels, H*3, W*3) expanded_img = patches . reshape ( batch_size , channels , H * kernel_size , W * kernel_size ) return expanded_img","title":"reverse_tiling_pytorch"},{"location":"data/#datamodules","text":"","title":"Datamodules"},{"location":"data/#terratorch.datamodules.biomassters","text":"","title":"biomassters"},{"location":"data/#terratorch.datamodules.biomassters.BioMasstersNonGeoDataModule","text":"Bases: NonGeoDataModule NonGeo LightningDataModule implementation for BioMassters datamodule. Source code in terratorch/datamodules/biomassters.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 class BioMasstersNonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for BioMassters datamodule.\"\"\" default_metadata_filename = \"The_BioMassters_-_features_metadata.csv.csv\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : dict [ str , Sequence [ str ]] | Sequence [ str ] = BioMasstersNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , drop_last : bool = True , sensors : Sequence [ str ] = [ \"S1\" , \"S2\" ], as_time_series : bool = False , metadata_filename : str = default_metadata_filename , max_cloud_percentage : float | None = None , max_red_mean : float | None = None , include_corrupt : bool = True , subset : float = 1 , seed : int = 42 , use_four_frames : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the DataModule for the non-geospatial BioMassters datamodule. Args: data_root (str): Root directory containing the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (dict[str, Sequence[str]] | Sequence[str], optional): Band configuration; either a dict mapping sensors to bands or a list for the first sensor. Defaults to BioMasstersNonGeo.all_band_names train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation or normalization to apply. Defaults to normalization if not provided. drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True. sensors (Sequence[str], optional): List of sensors to use (e.g., [\"S1\", \"S2\"]). Defaults to [\"S1\", \"S2\"]. as_time_series (bool, optional): Whether to treat data as a time series. Defaults to False. metadata_filename (str, optional): Metadata filename. Defaults to \"The_BioMassters_-_features_metadata.csv.csv\". max_cloud_percentage (float | None, optional): Maximum allowed cloud percentage. Defaults to None. max_red_mean (float | None, optional): Maximum allowed red band mean. Defaults to None. include_corrupt (bool, optional): Whether to include corrupt data. Defaults to True. subset (float, optional): Fraction of the dataset to use. Defaults to 1. seed (int, optional): Random seed for reproducibility. Defaults to 42. use_four_frames (bool, optional): Whether to use a four frames configuration. Defaults to False. **kwargs: Additional keyword arguments. Returns: None. \"\"\" super () . __init__ ( BioMasstersNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . sensors = sensors if isinstance ( bands , dict ): self . bands = bands else : sens = sensors [ 0 ] self . bands = { sens : bands } self . means = {} self . stds = {} for sensor in self . sensors : self . means [ sensor ] = [ MEANS [ sensor ][ band ] for band in self . bands [ sensor ]] self . stds [ sensor ] = [ STDS [ sensor ][ band ] for band in self . bands [ sensor ]] self . mask_mean = MEANS [ \"AGBM\" ] self . mask_std = STDS [ \"AGBM\" ] self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) if len ( sensors ) == 1 : self . aug = Normalize ( self . means [ sensors [ 0 ]], self . stds [ sensors [ 0 ]]) if aug is None else aug else : MultimodalNormalize ( self . means , self . stds ) if aug is None else aug self . drop_last = drop_last self . as_time_series = as_time_series self . metadata_filename = metadata_filename self . max_cloud_percentage = max_cloud_percentage self . max_red_mean = max_red_mean self . include_corrupt = include_corrupt self . subset = subset self . seed = seed self . use_four_frames = use_four_frames def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , root = self . data_root , transform = self . train_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"test\" , root = self . data_root , transform = self . val_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , root = self . data_root , transform = self . test_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , root = self . data_root , transform = self . predict_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) def _dataloader_factory ( self , split : str ): dataset = self . _valid_attribute ( f \" { split } _dataset\" , \"dataset\" ) batch_size = self . _valid_attribute ( f \" { split } _batch_size\" , \"batch_size\" ) return DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = split == \"train\" , num_workers = self . num_workers , collate_fn = self . collate_fn , drop_last = split == \"train\" and self . drop_last , )","title":"BioMasstersNonGeoDataModule"},{"location":"data/#terratorch.datamodules.biomassters.BioMasstersNonGeoDataModule.__init__","text":"Initializes the DataModule for the non-geospatial BioMassters datamodule. Parameters: data_root ( str ) \u2013 Root directory containing the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for DataLoaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. bands ( dict [ str , Sequence [ str ]] | Sequence [ str ] , default: all_band_names ) \u2013 Band configuration; either a dict mapping sensors to bands or a list for the first sensor. Defaults to BioMasstersNonGeo.all_band_names train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. aug ( AugmentationSequential , default: None ) \u2013 Augmentation or normalization to apply. Defaults to normalization if not provided. drop_last ( bool , default: True ) \u2013 Whether to drop the last incomplete batch. Defaults to True. sensors ( Sequence [ str ] , default: ['S1', 'S2'] ) \u2013 List of sensors to use (e.g., [\"S1\", \"S2\"]). Defaults to [\"S1\", \"S2\"]. as_time_series ( bool , default: False ) \u2013 Whether to treat data as a time series. Defaults to False. metadata_filename ( str , default: default_metadata_filename ) \u2013 Metadata filename. Defaults to \"The_BioMassters_-_features_metadata.csv.csv\". max_cloud_percentage ( float | None , default: None ) \u2013 Maximum allowed cloud percentage. Defaults to None. max_red_mean ( float | None , default: None ) \u2013 Maximum allowed red band mean. Defaults to None. include_corrupt ( bool , default: True ) \u2013 Whether to include corrupt data. Defaults to True. subset ( float , default: 1 ) \u2013 Fraction of the dataset to use. Defaults to 1. seed ( int , default: 42 ) \u2013 Random seed for reproducibility. Defaults to 42. use_four_frames ( bool , default: False ) \u2013 Whether to use a four frames configuration. Defaults to False. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Returns: None \u2013 None. Source code in terratorch/datamodules/biomassters.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : dict [ str , Sequence [ str ]] | Sequence [ str ] = BioMasstersNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , drop_last : bool = True , sensors : Sequence [ str ] = [ \"S1\" , \"S2\" ], as_time_series : bool = False , metadata_filename : str = default_metadata_filename , max_cloud_percentage : float | None = None , max_red_mean : float | None = None , include_corrupt : bool = True , subset : float = 1 , seed : int = 42 , use_four_frames : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the DataModule for the non-geospatial BioMassters datamodule. Args: data_root (str): Root directory containing the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (dict[str, Sequence[str]] | Sequence[str], optional): Band configuration; either a dict mapping sensors to bands or a list for the first sensor. Defaults to BioMasstersNonGeo.all_band_names train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation or normalization to apply. Defaults to normalization if not provided. drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True. sensors (Sequence[str], optional): List of sensors to use (e.g., [\"S1\", \"S2\"]). Defaults to [\"S1\", \"S2\"]. as_time_series (bool, optional): Whether to treat data as a time series. Defaults to False. metadata_filename (str, optional): Metadata filename. Defaults to \"The_BioMassters_-_features_metadata.csv.csv\". max_cloud_percentage (float | None, optional): Maximum allowed cloud percentage. Defaults to None. max_red_mean (float | None, optional): Maximum allowed red band mean. Defaults to None. include_corrupt (bool, optional): Whether to include corrupt data. Defaults to True. subset (float, optional): Fraction of the dataset to use. Defaults to 1. seed (int, optional): Random seed for reproducibility. Defaults to 42. use_four_frames (bool, optional): Whether to use a four frames configuration. Defaults to False. **kwargs: Additional keyword arguments. Returns: None. \"\"\" super () . __init__ ( BioMasstersNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . sensors = sensors if isinstance ( bands , dict ): self . bands = bands else : sens = sensors [ 0 ] self . bands = { sens : bands } self . means = {} self . stds = {} for sensor in self . sensors : self . means [ sensor ] = [ MEANS [ sensor ][ band ] for band in self . bands [ sensor ]] self . stds [ sensor ] = [ STDS [ sensor ][ band ] for band in self . bands [ sensor ]] self . mask_mean = MEANS [ \"AGBM\" ] self . mask_std = STDS [ \"AGBM\" ] self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) if len ( sensors ) == 1 : self . aug = Normalize ( self . means [ sensors [ 0 ]], self . stds [ sensors [ 0 ]]) if aug is None else aug else : MultimodalNormalize ( self . means , self . stds ) if aug is None else aug self . drop_last = drop_last self . as_time_series = as_time_series self . metadata_filename = metadata_filename self . max_cloud_percentage = max_cloud_percentage self . max_red_mean = max_red_mean self . include_corrupt = include_corrupt self . subset = subset self . seed = seed self . use_four_frames = use_four_frames","title":"__init__"},{"location":"data/#terratorch.datamodules.biomassters.BioMasstersNonGeoDataModule.setup","text":"Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/biomassters.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , root = self . data_root , transform = self . train_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"test\" , root = self . data_root , transform = self . val_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , root = self . data_root , transform = self . test_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , root = self . data_root , transform = self . predict_transform , bands = self . bands , mask_mean = self . mask_mean , mask_std = self . mask_std , sensors = self . sensors , as_time_series = self . as_time_series , metadata_filename = self . metadata_filename , max_cloud_percentage = self . max_cloud_percentage , max_red_mean = self . max_red_mean , include_corrupt = self . include_corrupt , subset = self . subset , seed = self . seed , use_four_frames = self . use_four_frames , )","title":"setup"},{"location":"data/#terratorch.datamodules.burn_intensity","text":"","title":"burn_intensity"},{"location":"data/#terratorch.datamodules.burn_intensity.BurnIntensityNonGeoDataModule","text":"Bases: NonGeoDataModule NonGeo LightningDataModule implementation for BurnIntensity datamodule. Source code in terratorch/datamodules/burn_intensity.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class BurnIntensityNonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for BurnIntensity datamodule.\"\"\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = BurnIntensityNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , use_full_data : bool = True , no_data_replace : float | None = 0.0001 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the DataModule for the BurnIntensity non-geospatial datamodule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to BurnIntensityNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction. use_full_data (bool, optional): Whether to use the full dataset or data with less than 25 percent zeros. Defaults to True. no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001. no_label_replace (int | None, optional): Value to replace missing labels. Defaults to -1. use_metadata (bool): Whether to return metadata info (time and location). **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( BurnIntensityNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = [ MEANS [ b ] for b in bands ] stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = NormalizeWithTimesteps ( means , stds ) self . use_full_data = use_full_data self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , )","title":"BurnIntensityNonGeoDataModule"},{"location":"data/#terratorch.datamodules.burn_intensity.BurnIntensityNonGeoDataModule.__init__","text":"Initializes the DataModule for the BurnIntensity non-geospatial datamodule. Parameters: data_root ( str ) \u2013 Root directory of the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for DataLoaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. bands ( Sequence [ str ] , default: all_band_names ) \u2013 List of bands to use. Defaults to BurnIntensityNonGeo.all_band_names. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction. use_full_data ( bool , default: True ) \u2013 Whether to use the full dataset or data with less than 25 percent zeros. Defaults to True. no_data_replace ( float | None , default: 0.0001 ) \u2013 Value to replace missing data. Defaults to 0.0001. no_label_replace ( int | None , default: -1 ) \u2013 Value to replace missing labels. Defaults to -1. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (time and location). **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/burn_intensity.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = BurnIntensityNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , use_full_data : bool = True , no_data_replace : float | None = 0.0001 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the DataModule for the BurnIntensity non-geospatial datamodule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to BurnIntensityNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction. use_full_data (bool, optional): Whether to use the full dataset or data with less than 25 percent zeros. Defaults to True. no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001. no_label_replace (int | None, optional): Value to replace missing labels. Defaults to -1. use_metadata (bool): Whether to return metadata info (time and location). **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( BurnIntensityNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = [ MEANS [ b ] for b in bands ] stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = NormalizeWithTimesteps ( means , stds ) self . use_full_data = use_full_data self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata","title":"__init__"},{"location":"data/#terratorch.datamodules.burn_intensity.BurnIntensityNonGeoDataModule.setup","text":"Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/burn_intensity.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , use_full_data = self . use_full_data , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , )","title":"setup"},{"location":"data/#terratorch.datamodules.carbonflux","text":"","title":"carbonflux"},{"location":"data/#terratorch.datamodules.carbonflux.CarbonFluxNonGeoDataModule","text":"Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Carbon FLux dataset. Source code in terratorch/datamodules/carbonflux.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class CarbonFluxNonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Carbon FLux dataset.\"\"\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = CarbonFluxNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , no_data_replace : float | None = 0.0001 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the CarbonFluxNonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to CarbonFluxNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation sequence; if None, applies multimodal normalization. no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001. use_metadata (bool): Whether to return metadata info. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( CarbonFluxNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = { m : ([ MEANS [ m ][ band ] for band in bands ] if m == \"image\" else MEANS [ m ]) for m in MEANS . keys () } stds = { m : ([ STDS [ m ][ band ] for band in bands ] if m == \"image\" else STDS [ m ]) for m in STDS . keys () } self . mask_means = MEANS [ \"mask\" ] self . mask_std = STDS [ \"mask\" ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = MultimodalNormalize ( means , stds ) if aug is None else aug self . no_data_replace = no_data_replace self . use_metadata = use_metadata def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , )","title":"CarbonFluxNonGeoDataModule"},{"location":"data/#terratorch.datamodules.carbonflux.CarbonFluxNonGeoDataModule.__init__","text":"Initializes the CarbonFluxNonGeoDataModule. Parameters: data_root ( str ) \u2013 Root directory of the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for DataLoaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. bands ( Sequence [ str ] , default: all_band_names ) \u2013 List of bands to use. Defaults to CarbonFluxNonGeo.all_band_names. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. aug ( AugmentationSequential , default: None ) \u2013 Augmentation sequence; if None, applies multimodal normalization. no_data_replace ( float | None , default: 0.0001 ) \u2013 Value to replace missing data. Defaults to 0.0001. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/carbonflux.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = CarbonFluxNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , no_data_replace : float | None = 0.0001 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the CarbonFluxNonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to CarbonFluxNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation sequence; if None, applies multimodal normalization. no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001. use_metadata (bool): Whether to return metadata info. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( CarbonFluxNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = { m : ([ MEANS [ m ][ band ] for band in bands ] if m == \"image\" else MEANS [ m ]) for m in MEANS . keys () } stds = { m : ([ STDS [ m ][ band ] for band in bands ] if m == \"image\" else STDS [ m ]) for m in STDS . keys () } self . mask_means = MEANS [ \"mask\" ] self . mask_std = STDS [ \"mask\" ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = MultimodalNormalize ( means , stds ) if aug is None else aug self . no_data_replace = no_data_replace self . use_metadata = use_metadata","title":"__init__"},{"location":"data/#terratorch.datamodules.carbonflux.CarbonFluxNonGeoDataModule.setup","text":"Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/carbonflux.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , gpp_mean = self . mask_means , gpp_std = self . mask_std , no_data_replace = self . no_data_replace , use_metadata = self . use_metadata , )","title":"setup"},{"location":"data/#terratorch.datamodules.forestnet","text":"","title":"forestnet"},{"location":"data/#terratorch.datamodules.forestnet.ForestNetNonGeoDataModule","text":"Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Landslide4Sense dataset. Source code in terratorch/datamodules/forestnet.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 class ForestNetNonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Landslide4Sense dataset.\"\"\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , label_map : dict [ str , int ] = ForestNetNonGeo . default_label_map , bands : Sequence [ str ] = ForestNetNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , fraction : float = 1.0 , aug : AugmentationSequential = None , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the ForestNetNonGeoDataModule. Args: data_root (str): Directory containing the dataset. batch_size (int, optional): Batch size for data loaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. label_map (dict[str, int], optional): Mapping of labels to integers. Defaults to ForestNetNonGeo.default_label_map. bands (Sequence[str], optional): List of band names to use. Defaults to ForestNetNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction. fraction (float, optional): Fraction of data to use. Defaults to 1.0. aug (AugmentationSequential, optional): Augmentation/normalization pipeline; if None, uses Normalize. use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( ForestNetNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . means = [ MEANS [ b ] for b in bands ] self . stds = [ STDS [ b ] for b in bands ] self . label_map = label_map self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = Normalize ( self . means , self . stds ) if aug is None else aug self . fraction = fraction self . use_metadata = use_metadata def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , label_map = self . label_map , transform = self . train_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , label_map = self . label_map , transform = self . val_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , label_map = self . label_map , transform = self . test_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , label_map = self . label_map , transform = self . predict_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , )","title":"ForestNetNonGeoDataModule"},{"location":"data/#terratorch.datamodules.forestnet.ForestNetNonGeoDataModule.__init__","text":"Initializes the ForestNetNonGeoDataModule. Parameters: data_root ( str ) \u2013 Directory containing the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for data loaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. label_map ( dict [ str , int ] , default: default_label_map ) \u2013 Mapping of labels to integers. Defaults to ForestNetNonGeo.default_label_map. bands ( Sequence [ str ] , default: all_band_names ) \u2013 List of band names to use. Defaults to ForestNetNonGeo.all_band_names. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction. fraction ( float , default: 1.0 ) \u2013 Fraction of data to use. Defaults to 1.0. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline; if None, uses Normalize. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/forestnet.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , label_map : dict [ str , int ] = ForestNetNonGeo . default_label_map , bands : Sequence [ str ] = ForestNetNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , fraction : float = 1.0 , aug : AugmentationSequential = None , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the ForestNetNonGeoDataModule. Args: data_root (str): Directory containing the dataset. batch_size (int, optional): Batch size for data loaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. label_map (dict[str, int], optional): Mapping of labels to integers. Defaults to ForestNetNonGeo.default_label_map. bands (Sequence[str], optional): List of band names to use. Defaults to ForestNetNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction. fraction (float, optional): Fraction of data to use. Defaults to 1.0. aug (AugmentationSequential, optional): Augmentation/normalization pipeline; if None, uses Normalize. use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( ForestNetNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . means = [ MEANS [ b ] for b in bands ] self . stds = [ STDS [ b ] for b in bands ] self . label_map = label_map self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = Normalize ( self . means , self . stds ) if aug is None else aug self . fraction = fraction self . use_metadata = use_metadata","title":"__init__"},{"location":"data/#terratorch.datamodules.forestnet.ForestNetNonGeoDataModule.setup","text":"Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/forestnet.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , label_map = self . label_map , transform = self . train_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , label_map = self . label_map , transform = self . val_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , label_map = self . label_map , transform = self . test_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , label_map = self . label_map , transform = self . predict_transform , bands = self . bands , fraction = self . fraction , use_metadata = self . use_metadata , )","title":"setup"},{"location":"data/#terratorch.datamodules.fire_scars","text":"","title":"fire_scars"},{"location":"data/#terratorch.datamodules.fire_scars.FireScarsDataModule","text":"Bases: GeoDataModule Geo Fire Scars data module implementation that merges input data with ground truth segmentation masks. Source code in terratorch/datamodules/fire_scars.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 class FireScarsDataModule ( GeoDataModule ): \"\"\"Geo Fire Scars data module implementation that merges input data with ground truth segmentation masks.\"\"\" def __init__ ( self , data_root : str , ** kwargs : Any ) -> None : super () . __init__ ( FireScarsSegmentationMask , 4 , 224 , 100 , 0 , ** kwargs ) means = list ( MEANS . values ()) stds = list ( STDS . values ()) self . train_aug = AugmentationSequential ( K . RandomCrop ( 224 , 224 ), K . Normalize ( means , stds )) self . aug = AugmentationSequential ( K . Normalize ( means , stds )) self . data_root = data_root def setup ( self , stage : str ) -> None : self . images = FireScarsHLS ( os . path . join ( self . data_root , \"training/\" ) ) self . labels = FireScarsSegmentationMask ( os . path . join ( self . data_root , \"training/\" ) ) self . dataset = self . images & self . labels self . train_aug = AugmentationSequential ( K . RandomCrop ( 224 , 224 ), K . normalize ()) self . images_test = FireScarsHLS ( os . path . join ( self . data_root , \"validation/\" ) ) self . labels_test = FireScarsSegmentationMask ( os . path . join ( self . data_root , \"validation/\" ) ) self . val_dataset = self . images_test & self . labels_test if stage in [ \"fit\" ]: self . train_batch_sampler = RandomBatchGeoSampler ( self . dataset , self . patch_size , self . batch_size , None ) if stage in [ \"fit\" , \"validate\" ]: self . val_sampler = GridGeoSampler ( self . val_dataset , self . patch_size , self . patch_size ) if stage in [ \"test\" ]: self . test_sampler = GridGeoSampler ( self . val_dataset , self . patch_size , self . patch_size )","title":"FireScarsDataModule"},{"location":"data/#terratorch.datamodules.fire_scars.FireScarsNonGeoDataModule","text":"Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Fire Scars dataset. Source code in terratorch/datamodules/fire_scars.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 class FireScarsNonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Fire Scars dataset.\"\"\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = FireScarsNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , drop_last : bool = True , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the FireScarsNonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of band names. Defaults to FireScarsNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction. drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True. no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0. no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1. use_metadata (bool): Whether to return metadata info. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( FireScarsNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = [ MEANS [ b ] for b in bands ] stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = AugmentationSequential ( K . Normalize ( means , stds ), data_keys = [ \"image\" ]) self . drop_last = drop_last self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) def _dataloader_factory ( self , split : str ) -> DataLoader [ dict [ str , Tensor ]]: \"\"\"Implement one or more PyTorch DataLoaders. Args: split: Either 'train', 'val', 'test', or 'predict'. Returns: A collection of data loaders specifying samples. Raises: MisconfigurationException: If :meth:`setup` does not define a dataset or sampler, or if the dataset or sampler has length 0. \"\"\" dataset = self . _valid_attribute ( f \" { split } _dataset\" , \"dataset\" ) batch_size = self . _valid_attribute ( f \" { split } _batch_size\" , \"batch_size\" ) return DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = split == \"train\" , num_workers = self . num_workers , collate_fn = self . collate_fn , drop_last = split == \"train\" and self . drop_last , )","title":"FireScarsNonGeoDataModule"},{"location":"data/#terratorch.datamodules.fire_scars.FireScarsNonGeoDataModule.__init__","text":"Initializes the FireScarsNonGeoDataModule. Parameters: data_root ( str ) \u2013 Root directory of the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for DataLoaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. bands ( Sequence [ str ] , default: all_band_names ) \u2013 List of band names. Defaults to FireScarsNonGeo.all_band_names. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction. drop_last ( bool , default: True ) \u2013 Whether to drop the last incomplete batch. Defaults to True. no_data_replace ( float | None , default: 0 ) \u2013 Replacement value for missing data. Defaults to 0. no_label_replace ( int | None , default: -1 ) \u2013 Replacement value for missing labels. Defaults to -1. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/fire_scars.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = FireScarsNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , drop_last : bool = True , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the FireScarsNonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of band names. Defaults to FireScarsNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction. drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True. no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0. no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1. use_metadata (bool): Whether to return metadata info. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( FireScarsNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = [ MEANS [ b ] for b in bands ] stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = AugmentationSequential ( K . Normalize ( means , stds ), data_keys = [ \"image\" ]) self . drop_last = drop_last self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata","title":"__init__"},{"location":"data/#terratorch.datamodules.fire_scars.FireScarsNonGeoDataModule.setup","text":"Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/fire_scars.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , )","title":"setup"},{"location":"data/#terratorch.datamodules.landslide4sense","text":"","title":"landslide4sense"},{"location":"data/#terratorch.datamodules.landslide4sense.Landslide4SenseNonGeoDataModule","text":"Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Landslide4Sense dataset. Source code in terratorch/datamodules/landslide4sense.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class Landslide4SenseNonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Landslide4Sense dataset.\"\"\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = Landslide4SenseNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the Landslide4SenseNonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for data loaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of band names to use. Defaults to Landslide4SenseNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation pipeline; if None, applies normalization using computed means and stds. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( Landslide4SenseNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . means = [ MEANS [ b ] for b in bands ] self . stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = ( AugmentationSequential ( K . Normalize ( self . means , self . stds ), data_keys = [ \"image\" ]) if aug is None else aug ) def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands )","title":"Landslide4SenseNonGeoDataModule"},{"location":"data/#terratorch.datamodules.landslide4sense.Landslide4SenseNonGeoDataModule.__init__","text":"Initializes the Landslide4SenseNonGeoDataModule. Parameters: data_root ( str ) \u2013 Root directory of the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for data loaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. bands ( Sequence [ str ] , default: all_band_names ) \u2013 List of band names to use. Defaults to Landslide4SenseNonGeo.all_band_names. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. aug ( AugmentationSequential , default: None ) \u2013 Augmentation pipeline; if None, applies normalization using computed means and stds. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/landslide4sense.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = Landslide4SenseNonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the Landslide4SenseNonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for data loaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of band names to use. Defaults to Landslide4SenseNonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation pipeline; if None, applies normalization using computed means and stds. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( Landslide4SenseNonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . means = [ MEANS [ b ] for b in bands ] self . stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = ( AugmentationSequential ( K . Normalize ( self . means , self . stds ), data_keys = [ \"image\" ]) if aug is None else aug )","title":"__init__"},{"location":"data/#terratorch.datamodules.landslide4sense.Landslide4SenseNonGeoDataModule.setup","text":"Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/landslide4sense.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands )","title":"setup"},{"location":"data/#terratorch.datamodules.m_eurosat","text":"","title":"m_eurosat"},{"location":"data/#terratorch.datamodules.m_eurosat.MEuroSATNonGeoDataModule","text":"Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-EuroSAT dataset. Source code in terratorch/datamodules/m_eurosat.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class MEuroSATNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-EuroSAT dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MEuroSATNonGeoDataModule for the MEuroSATNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MEuroSATNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , )","title":"MEuroSATNonGeoDataModule"},{"location":"data/#terratorch.datamodules.m_eurosat.MEuroSATNonGeoDataModule.__init__","text":"Initializes the MEuroSATNonGeoDataModule for the MEuroSATNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_eurosat.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MEuroSATNonGeoDataModule for the MEuroSATNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MEuroSATNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , )","title":"__init__"},{"location":"data/#terratorch.datamodules.m_bigearthnet","text":"","title":"m_bigearthnet"},{"location":"data/#terratorch.datamodules.m_bigearthnet.MBigEarthNonGeoDataModule","text":"Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-BigEarthNet dataset. Source code in terratorch/datamodules/m_bigearthnet.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class MBigEarthNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-BigEarthNet dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MBigEarthNonGeoDataModule for the M-BigEarthNet dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MBigEarthNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , )","title":"MBigEarthNonGeoDataModule"},{"location":"data/#terratorch.datamodules.m_bigearthnet.MBigEarthNonGeoDataModule.__init__","text":"Initializes the MBigEarthNonGeoDataModule for the M-BigEarthNet dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_bigearthnet.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MBigEarthNonGeoDataModule for the M-BigEarthNet dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MBigEarthNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , )","title":"__init__"},{"location":"data/#terratorch.datamodules.m_brick_kiln","text":"","title":"m_brick_kiln"},{"location":"data/#terratorch.datamodules.m_brick_kiln.MBrickKilnNonGeoDataModule","text":"Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-BrickKiln dataset. Source code in terratorch/datamodules/m_brick_kiln.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class MBrickKilnNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-BrickKiln dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MBrickKilnNonGeoDataModule for the M-BrickKilnNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MBrickKilnNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , )","title":"MBrickKilnNonGeoDataModule"},{"location":"data/#terratorch.datamodules.m_brick_kiln.MBrickKilnNonGeoDataModule.__init__","text":"Initializes the MBrickKilnNonGeoDataModule for the M-BrickKilnNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_brick_kiln.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MBrickKilnNonGeoDataModule for the M-BrickKilnNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MBrickKilnNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , )","title":"__init__"},{"location":"data/#terratorch.datamodules.m_forestnet","text":"","title":"m_forestnet"},{"location":"data/#terratorch.datamodules.m_forestnet.MForestNetNonGeoDataModule","text":"Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-ForestNet dataset. Source code in terratorch/datamodules/m_forestnet.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 class MForestNetNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-ForestNet dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MForestNetNonGeoDataModule for the MForestNetNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MForestNetNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , )","title":"MForestNetNonGeoDataModule"},{"location":"data/#terratorch.datamodules.m_forestnet.MForestNetNonGeoDataModule.__init__","text":"Initializes the MForestNetNonGeoDataModule for the MForestNetNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_forestnet.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MForestNetNonGeoDataModule for the MForestNetNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MForestNetNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , )","title":"__init__"},{"location":"data/#terratorch.datamodules.m_so2sat","text":"","title":"m_so2sat"},{"location":"data/#terratorch.datamodules.m_so2sat.MSo2SatNonGeoDataModule","text":"Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-So2Sat dataset. Source code in terratorch/datamodules/m_so2sat.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 class MSo2SatNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-So2Sat dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MSo2SatNonGeoDataModule for the MSo2SatNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MSo2SatNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , )","title":"MSo2SatNonGeoDataModule"},{"location":"data/#terratorch.datamodules.m_so2sat.MSo2SatNonGeoDataModule.__init__","text":"Initializes the MSo2SatNonGeoDataModule for the MSo2SatNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_so2sat.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MSo2SatNonGeoDataModule for the MSo2SatNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MSo2SatNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , )","title":"__init__"},{"location":"data/#terratorch.datamodules.m_pv4ger","text":"","title":"m_pv4ger"},{"location":"data/#terratorch.datamodules.m_pv4ger.MPv4gerNonGeoDataModule","text":"Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-Pv4ger dataset. Source code in terratorch/datamodules/m_pv4ger.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class MPv4gerNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-Pv4ger dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MPv4gerNonGeoDataModule for the MPv4gerNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MPv4gerNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , )","title":"MPv4gerNonGeoDataModule"},{"location":"data/#terratorch.datamodules.m_pv4ger.MPv4gerNonGeoDataModule.__init__","text":"Initializes the MPv4gerNonGeoDataModule for the MPv4gerNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_pv4ger.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MPv4gerNonGeoDataModule for the MPv4gerNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MPv4gerNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , )","title":"__init__"},{"location":"data/#terratorch.datamodules.m_cashew_plantation","text":"","title":"m_cashew_plantation"},{"location":"data/#terratorch.datamodules.m_cashew_plantation.MBeninSmallHolderCashewsNonGeoDataModule","text":"Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-Cashew Plantation dataset. Source code in terratorch/datamodules/m_cashew_plantation.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class MBeninSmallHolderCashewsNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-Cashew Plantation dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MBeninSmallHolderCashewsNonGeoDataModule for the M-BeninSmallHolderCashewsNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MBeninSmallHolderCashewsNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , )","title":"MBeninSmallHolderCashewsNonGeoDataModule"},{"location":"data/#terratorch.datamodules.m_cashew_plantation.MBeninSmallHolderCashewsNonGeoDataModule.__init__","text":"Initializes the MBeninSmallHolderCashewsNonGeoDataModule for the M-BeninSmallHolderCashewsNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_cashew_plantation.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MBeninSmallHolderCashewsNonGeoDataModule for the M-BeninSmallHolderCashewsNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MBeninSmallHolderCashewsNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , )","title":"__init__"},{"location":"data/#terratorch.datamodules.m_nz_cattle","text":"","title":"m_nz_cattle"},{"location":"data/#terratorch.datamodules.m_nz_cattle.MNzCattleNonGeoDataModule","text":"Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-NZCattle dataset. Source code in terratorch/datamodules/m_nz_cattle.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class MNzCattleNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-NZCattle dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MNzCattleNonGeoDataModule for the MNzCattleNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MNzCattleNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , )","title":"MNzCattleNonGeoDataModule"},{"location":"data/#terratorch.datamodules.m_nz_cattle.MNzCattleNonGeoDataModule.__init__","text":"Initializes the MNzCattleNonGeoDataModule for the MNzCattleNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_nz_cattle.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MNzCattleNonGeoDataModule for the MNzCattleNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MNzCattleNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , )","title":"__init__"},{"location":"data/#terratorch.datamodules.m_chesapeake_landcover","text":"","title":"m_chesapeake_landcover"},{"location":"data/#terratorch.datamodules.m_chesapeake_landcover.MChesapeakeLandcoverNonGeoDataModule","text":"Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-ChesapeakeLandcover dataset. Source code in terratorch/datamodules/m_chesapeake_landcover.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 class MChesapeakeLandcoverNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-ChesapeakeLandcover dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MChesapeakeLandcoverNonGeoDataModule for the M-BigEarthNet dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MChesapeakeLandcoverNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , )","title":"MChesapeakeLandcoverNonGeoDataModule"},{"location":"data/#terratorch.datamodules.m_chesapeake_landcover.MChesapeakeLandcoverNonGeoDataModule.__init__","text":"Initializes the MChesapeakeLandcoverNonGeoDataModule for the M-BigEarthNet dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_chesapeake_landcover.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MChesapeakeLandcoverNonGeoDataModule for the M-BigEarthNet dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MChesapeakeLandcoverNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , )","title":"__init__"},{"location":"data/#terratorch.datamodules.m_pv4ger_seg","text":"","title":"m_pv4ger_seg"},{"location":"data/#terratorch.datamodules.m_pv4ger_seg.MPv4gerSegNonGeoDataModule","text":"Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-Pv4gerSeg dataset. Source code in terratorch/datamodules/m_pv4ger_seg.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 class MPv4gerSegNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-Pv4gerSeg dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MPv4gerNonGeoDataModule for the MPv4gerSegNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MPv4gerSegNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , )","title":"MPv4gerSegNonGeoDataModule"},{"location":"data/#terratorch.datamodules.m_pv4ger_seg.MPv4gerSegNonGeoDataModule.__init__","text":"Initializes the MPv4gerNonGeoDataModule for the MPv4gerSegNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". use_metadata ( bool , default: False ) \u2013 Whether to return metadata info. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_pv4ger_seg.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , use_metadata : bool = False , # noqa: FBT002, FBT001 ** kwargs : Any , ) -> None : \"\"\" Initializes the MPv4gerNonGeoDataModule for the MPv4gerSegNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". use_metadata (bool): Whether to return metadata info. **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MPv4gerSegNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , use_metadata = use_metadata , ** kwargs , )","title":"__init__"},{"location":"data/#terratorch.datamodules.m_SA_crop_type","text":"","title":"m_SA_crop_type"},{"location":"data/#terratorch.datamodules.m_SA_crop_type.MSACropTypeNonGeoDataModule","text":"Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-SA-CropType dataset. Source code in terratorch/datamodules/m_SA_crop_type.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class MSACropTypeNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-SA-CropType dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MSACropTypeNonGeoDataModule for the MSACropTypeNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MSACropTypeNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , )","title":"MSACropTypeNonGeoDataModule"},{"location":"data/#terratorch.datamodules.m_SA_crop_type.MSACropTypeNonGeoDataModule.__init__","text":"Initializes the MSACropTypeNonGeoDataModule for the MSACropTypeNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_SA_crop_type.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MSACropTypeNonGeoDataModule for the MSACropTypeNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MSACropTypeNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , )","title":"__init__"},{"location":"data/#terratorch.datamodules.m_neontree","text":"","title":"m_neontree"},{"location":"data/#terratorch.datamodules.m_neontree.MNeonTreeNonGeoDataModule","text":"Bases: GeobenchDataModule NonGeo LightningDataModule implementation for M-NeonTree dataset. Source code in terratorch/datamodules/m_neontree.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class MNeonTreeNonGeoDataModule ( GeobenchDataModule ): \"\"\"NonGeo LightningDataModule implementation for M-NeonTree dataset.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MNeonTreeNonGeoDataModule for the MNeonTreeNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MNeonTreeNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , )","title":"MNeonTreeNonGeoDataModule"},{"location":"data/#terratorch.datamodules.m_neontree.MNeonTreeNonGeoDataModule.__init__","text":"Initializes the MNeonTreeNonGeoDataModule for the MNeonTreeNonGeo dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". bands ( Sequence [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing. aug ( AugmentationSequential , default: None ) \u2013 Augmentation/normalization pipeline. Defaults to None. partition ( str , default: 'default' ) \u2013 Partition size. Defaults to \"default\". **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/m_neontree.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , bands : Sequence [ str ] | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , partition : str = \"default\" , ** kwargs : Any , ) -> None : \"\"\" Initializes the MNeonTreeNonGeoDataModule for the MNeonTreeNonGeo dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". bands (Sequence[str] | None, optional): List of bands to use. Defaults to None. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing. aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None. partition (str, optional): Partition size. Defaults to \"default\". **kwargs (Any): Additional keyword arguments. \"\"\" super () . __init__ ( MNeonTreeNonGeo , MEANS , STDS , batch_size = batch_size , num_workers = num_workers , data_root = data_root , bands = bands , train_transform = train_transform , val_transform = val_transform , test_transform = test_transform , aug = aug , partition = partition , ** kwargs , )","title":"__init__"},{"location":"data/#terratorch.datamodules.multi_temporal_crop_classification","text":"","title":"multi_temporal_crop_classification"},{"location":"data/#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule","text":"Bases: NonGeoDataModule NonGeo LightningDataModule implementation for multi-temporal crop classification. Source code in terratorch/datamodules/multi_temporal_crop_classification.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 class MultiTemporalCropClassificationDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for multi-temporal crop classification.\"\"\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = MultiTemporalCropClassification . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , drop_last : bool = True , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , expand_temporal_dimension : bool = True , reduce_zero_label : bool = True , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the MultiTemporalCropClassificationDataModule for multi-temporal crop classification. Args: data_root (str): Directory containing the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to MultiTemporalCropClassification.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. drop_last (bool, optional): Whether to drop the last incomplete batch during training. Defaults to True. no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0. no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1. expand_temporal_dimension (bool, optional): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True. reduce_zero_label (bool, optional): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True. use_metadata (bool): Whether to return metadata info (time and location). **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( MultiTemporalCropClassification , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . means = [ MEANS [ b ] for b in bands ] self . stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = Normalize ( self . means , self . stds ) self . drop_last = drop_last self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . expand_temporal_dimension = expand_temporal_dimension self . reduce_zero_label = reduce_zero_label self . use_metadata = use_metadata def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) def _dataloader_factory ( self , split : str ) -> DataLoader [ dict [ str , Tensor ]]: \"\"\"Implement one or more PyTorch DataLoaders. Args: split: Either 'train', 'val', 'test', or 'predict'. Returns: A collection of data loaders specifying samples. Raises: MisconfigurationException: If :meth:`setup` does not define a dataset or sampler, or if the dataset or sampler has length 0. \"\"\" dataset = self . _valid_attribute ( f \" { split } _dataset\" , \"dataset\" ) batch_size = self . _valid_attribute ( f \" { split } _batch_size\" , \"batch_size\" ) return DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = split == \"train\" , num_workers = self . num_workers , collate_fn = self . collate_fn , drop_last = split == \"train\" and self . drop_last , )","title":"MultiTemporalCropClassificationDataModule"},{"location":"data/#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule.__init__","text":"Initializes the MultiTemporalCropClassificationDataModule for multi-temporal crop classification. Parameters: data_root ( str ) \u2013 Directory containing the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for DataLoaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. bands ( Sequence [ str ] , default: all_band_names ) \u2013 List of bands to use. Defaults to MultiTemporalCropClassification.all_band_names. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. drop_last ( bool , default: True ) \u2013 Whether to drop the last incomplete batch during training. Defaults to True. no_data_replace ( float | None , default: 0 ) \u2013 Replacement value for missing data. Defaults to 0. no_label_replace ( int | None , default: -1 ) \u2013 Replacement value for missing labels. Defaults to -1. expand_temporal_dimension ( bool , default: True ) \u2013 Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True. reduce_zero_label ( bool , default: True ) \u2013 Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (time and location). **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/multi_temporal_crop_classification.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = MultiTemporalCropClassification . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , drop_last : bool = True , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , expand_temporal_dimension : bool = True , reduce_zero_label : bool = True , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the MultiTemporalCropClassificationDataModule for multi-temporal crop classification. Args: data_root (str): Directory containing the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to MultiTemporalCropClassification.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. drop_last (bool, optional): Whether to drop the last incomplete batch during training. Defaults to True. no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0. no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1. expand_temporal_dimension (bool, optional): Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True. reduce_zero_label (bool, optional): Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True. use_metadata (bool): Whether to return metadata info (time and location). **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( MultiTemporalCropClassification , batch_size , num_workers , ** kwargs ) self . data_root = data_root self . means = [ MEANS [ b ] for b in bands ] self . stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = Normalize ( self . means , self . stds ) self . drop_last = drop_last self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . expand_temporal_dimension = expand_temporal_dimension self . reduce_zero_label = reduce_zero_label self . use_metadata = use_metadata","title":"__init__"},{"location":"data/#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule.setup","text":"Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/multi_temporal_crop_classification.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , expand_temporal_dimension = self . expand_temporal_dimension , reduce_zero_label = self . reduce_zero_label , use_metadata = self . use_metadata , )","title":"setup"},{"location":"data/#terratorch.datamodules.open_sentinel_map","text":"","title":"open_sentinel_map"},{"location":"data/#terratorch.datamodules.open_sentinel_map.OpenSentinelMapDataModule","text":"Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Open Sentinel Map. Source code in terratorch/datamodules/open_sentinel_map.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 class OpenSentinelMapDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Open Sentinel Map.\"\"\" def __init__ ( self , bands : list [ str ] | None = None , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , spatial_interpolate_and_stack_temporally : bool = True , # noqa: FBT001, FBT002 pad_image : int | None = None , truncate_image : int | None = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the OpenSentinelMapDataModule for the Open Sentinel Map dataset. Args: bands (list[str] | None, optional): List of bands to use. Defaults to None. batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. spatial_interpolate_and_stack_temporally (bool, optional): If True, the bands are interpolated and concatenated over time. Default is True. pad_image (int | None, optional): Number of timesteps to pad the time dimension of the image. If None, no padding is applied. truncate_image (int | None, optional): Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( OpenSentinelMap , batch_size = batch_size , num_workers = num_workers , ** kwargs , ) self . bands = bands self . spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally self . pad_image = pad_image self . truncate_image = truncate_image self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . kwargs = kwargs def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = OpenSentinelMap ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = OpenSentinelMap ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , ) if stage in [ \"test\" ]: self . test_dataset = OpenSentinelMap ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , ) if stage in [ \"predict\" ]: self . predict_dataset = OpenSentinelMap ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , )","title":"OpenSentinelMapDataModule"},{"location":"data/#terratorch.datamodules.open_sentinel_map.OpenSentinelMapDataModule.__init__","text":"Initializes the OpenSentinelMapDataModule for the Open Sentinel Map dataset. Parameters: bands ( list [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. spatial_interpolate_and_stack_temporally ( bool , default: True ) \u2013 If True, the bands are interpolated and concatenated over time. Default is True. pad_image ( int | None , default: None ) \u2013 Number of timesteps to pad the time dimension of the image. If None, no padding is applied. truncate_image ( int | None , default: None ) \u2013 Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/open_sentinel_map.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , bands : list [ str ] | None = None , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , spatial_interpolate_and_stack_temporally : bool = True , # noqa: FBT001, FBT002 pad_image : int | None = None , truncate_image : int | None = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the OpenSentinelMapDataModule for the Open Sentinel Map dataset. Args: bands (list[str] | None, optional): List of bands to use. Defaults to None. batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. spatial_interpolate_and_stack_temporally (bool, optional): If True, the bands are interpolated and concatenated over time. Default is True. pad_image (int | None, optional): Number of timesteps to pad the time dimension of the image. If None, no padding is applied. truncate_image (int | None, optional): Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( OpenSentinelMap , batch_size = batch_size , num_workers = num_workers , ** kwargs , ) self . bands = bands self . spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally self . pad_image = pad_image self . truncate_image = truncate_image self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . kwargs = kwargs","title":"__init__"},{"location":"data/#terratorch.datamodules.open_sentinel_map.OpenSentinelMapDataModule.setup","text":"Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/open_sentinel_map.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = OpenSentinelMap ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = OpenSentinelMap ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , ) if stage in [ \"test\" ]: self . test_dataset = OpenSentinelMap ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , ) if stage in [ \"predict\" ]: self . predict_dataset = OpenSentinelMap ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , spatial_interpolate_and_stack_temporally = self . spatial_interpolate_and_stack_temporally , pad_image = self . pad_image , truncate_image = self . truncate_image , ** self . kwargs , )","title":"setup"},{"location":"data/#terratorch.datamodules.openearthmap","text":"","title":"openearthmap"},{"location":"data/#terratorch.datamodules.openearthmap.OpenEarthMapNonGeoDataModule","text":"Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Open Earth Map. Source code in terratorch/datamodules/openearthmap.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class OpenEarthMapNonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Open Earth Map.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , ** kwargs : Any ) -> None : \"\"\" Initializes the OpenEarthMapNonGeoDataModule for the Open Earth Map dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation pipeline; if None, defaults to normalization using computed means and stds. **kwargs: Additional keyword arguments. Can include 'bands' (list[str]) to specify the bands; defaults to OpenEarthMapNonGeo.all_band_names if not provided. \"\"\" super () . __init__ ( OpenEarthMapNonGeo , batch_size , num_workers , ** kwargs ) bands = kwargs . get ( \"bands\" , OpenEarthMapNonGeo . all_band_names ) self . means = torch . tensor ([ MEANS [ b ] for b in bands ]) self . stds = torch . tensor ([ STDS [ b ] for b in bands ]) self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . aug = AugmentationSequential ( K . Normalize ( self . means , self . stds ), data_keys = [ \"image\" ]) if aug is None else aug def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , ** self . kwargs ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , ** self . kwargs ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , ** self . kwargs ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , ** self . kwargs )","title":"OpenEarthMapNonGeoDataModule"},{"location":"data/#terratorch.datamodules.openearthmap.OpenEarthMapNonGeoDataModule.__init__","text":"Initializes the OpenEarthMapNonGeoDataModule for the Open Earth Map dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for test data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. aug ( AugmentationSequential , default: None ) \u2013 Augmentation pipeline; if None, defaults to normalization using computed means and stds. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Can include 'bands' (list[str]) to specify the bands; defaults to OpenEarthMapNonGeo.all_band_names if not provided. Source code in terratorch/datamodules/openearthmap.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , aug : AugmentationSequential = None , ** kwargs : Any ) -> None : \"\"\" Initializes the OpenEarthMapNonGeoDataModule for the Open Earth Map dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. aug (AugmentationSequential, optional): Augmentation pipeline; if None, defaults to normalization using computed means and stds. **kwargs: Additional keyword arguments. Can include 'bands' (list[str]) to specify the bands; defaults to OpenEarthMapNonGeo.all_band_names if not provided. \"\"\" super () . __init__ ( OpenEarthMapNonGeo , batch_size , num_workers , ** kwargs ) bands = kwargs . get ( \"bands\" , OpenEarthMapNonGeo . all_band_names ) self . means = torch . tensor ([ MEANS [ b ] for b in bands ]) self . stds = torch . tensor ([ STDS [ b ] for b in bands ]) self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . aug = AugmentationSequential ( K . Normalize ( self . means , self . stds ), data_keys = [ \"image\" ]) if aug is None else aug","title":"__init__"},{"location":"data/#terratorch.datamodules.openearthmap.OpenEarthMapNonGeoDataModule.setup","text":"Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/openearthmap.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , ** self . kwargs ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , ** self . kwargs ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , ** self . kwargs ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , ** self . kwargs )","title":"setup"},{"location":"data/#terratorch.datamodules.pastis","text":"","title":"pastis"},{"location":"data/#terratorch.datamodules.pastis.PASTISDataModule","text":"Bases: NonGeoDataModule NonGeo LightningDataModule implementation for PASTIS. Source code in terratorch/datamodules/pastis.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class PASTISDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for PASTIS.\"\"\" def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , truncate_image : int | None = None , pad_image : int | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the PASTISDataModule for the PASTIS dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Directory containing the dataset. Defaults to \"./\". truncate_image (int, optional): Truncate the time dimension of the image to a specified number of timesteps. If None, no truncation is performed. pad_image (int, optional): Pad the time dimension of the image to a specified number of timesteps. If None, no padding is applied. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( PASTIS , batch_size = batch_size , num_workers = num_workers , ** kwargs , ) self . truncate_image = truncate_image self . pad_image = pad_image self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . kwargs = kwargs def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = PASTIS ( folds = [ 1 , 2 , 3 ], data_root = self . data_root , transform = self . train_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = PASTIS ( folds = [ 4 ], data_root = self . data_root , transform = self . val_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , ) if stage in [ \"test\" ]: self . test_dataset = PASTIS ( folds = [ 5 ], data_root = self . data_root , transform = self . test_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , ) if stage in [ \"predict\" ]: self . predict_dataset = PASTIS ( folds = [ 5 ], data_root = self . data_root , transform = self . predict_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , )","title":"PASTISDataModule"},{"location":"data/#terratorch.datamodules.pastis.PASTISDataModule.__init__","text":"Initializes the PASTISDataModule for the PASTIS dataset. Parameters: batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Directory containing the dataset. Defaults to \"./\". truncate_image ( int , default: None ) \u2013 Truncate the time dimension of the image to a specified number of timesteps. If None, no truncation is performed. pad_image ( int , default: None ) \u2013 Pad the time dimension of the image to a specified number of timesteps. If None, no padding is applied. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for testing data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/pastis.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def __init__ ( self , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , truncate_image : int | None = None , pad_image : int | None = None , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the PASTISDataModule for the PASTIS dataset. Args: batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Directory containing the dataset. Defaults to \"./\". truncate_image (int, optional): Truncate the time dimension of the image to a specified number of timesteps. If None, no truncation is performed. pad_image (int, optional): Pad the time dimension of the image to a specified number of timesteps. If None, no padding is applied. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( PASTIS , batch_size = batch_size , num_workers = num_workers , ** kwargs , ) self . truncate_image = truncate_image self . pad_image = pad_image self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . kwargs = kwargs","title":"__init__"},{"location":"data/#terratorch.datamodules.pastis.PASTISDataModule.setup","text":"Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/pastis.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = PASTIS ( folds = [ 1 , 2 , 3 ], data_root = self . data_root , transform = self . train_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = PASTIS ( folds = [ 4 ], data_root = self . data_root , transform = self . val_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , ) if stage in [ \"test\" ]: self . test_dataset = PASTIS ( folds = [ 5 ], data_root = self . data_root , transform = self . test_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , ) if stage in [ \"predict\" ]: self . predict_dataset = PASTIS ( folds = [ 5 ], data_root = self . data_root , transform = self . predict_transform , truncate_image = self . truncate_image , pad_image = self . pad_image , ** self . kwargs , )","title":"setup"},{"location":"data/#terratorch.datamodules.sen1floods11","text":"","title":"sen1floods11"},{"location":"data/#terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule","text":"Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Fire Scars. Source code in terratorch/datamodules/sen1floods11.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 class Sen1Floods11NonGeoDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Fire Scars.\"\"\" def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = Sen1Floods11NonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , drop_last : bool = True , constant_scale : float = 0.0001 , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the Sen1Floods11NonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to Sen1Floods11NonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True. constant_scale (float, optional): Scale constant applied to the dataset. Defaults to 0.0001. no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0. no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1. use_metadata (bool): Whether to return metadata info (time and location). **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( Sen1Floods11NonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = [ MEANS [ b ] for b in bands ] stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = AugmentationSequential ( K . Normalize ( means , stds ), data_keys = [ \"image\" ]) self . drop_last = drop_last self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) def _dataloader_factory ( self , split : str ) -> DataLoader [ dict [ str , Tensor ]]: \"\"\"Implement one or more PyTorch DataLoaders. Args: split: Either 'train', 'val', 'test', or 'predict'. Returns: A collection of data loaders specifying samples. Raises: MisconfigurationException: If :meth:`setup` does not define a dataset or sampler, or if the dataset or sampler has length 0. \"\"\" dataset = self . _valid_attribute ( f \" { split } _dataset\" , \"dataset\" ) batch_size = self . _valid_attribute ( f \" { split } _batch_size\" , \"batch_size\" ) return DataLoader ( dataset = dataset , batch_size = batch_size , shuffle = split == \"train\" , num_workers = self . num_workers , collate_fn = self . collate_fn , drop_last = split == \"train\" and self . drop_last , )","title":"Sen1Floods11NonGeoDataModule"},{"location":"data/#terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule.__init__","text":"Initializes the Sen1Floods11NonGeoDataModule. Parameters: data_root ( str ) \u2013 Root directory of the dataset. batch_size ( int , default: 4 ) \u2013 Batch size for DataLoaders. Defaults to 4. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. bands ( Sequence [ str ] , default: all_band_names ) \u2013 List of bands to use. Defaults to Sen1Floods11NonGeo.all_band_names. train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for test data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. drop_last ( bool , default: True ) \u2013 Whether to drop the last incomplete batch. Defaults to True. constant_scale ( float , default: 0.0001 ) \u2013 Scale constant applied to the dataset. Defaults to 0.0001. no_data_replace ( float | None , default: 0 ) \u2013 Replacement value for missing data. Defaults to 0. no_label_replace ( int | None , default: -1 ) \u2013 Replacement value for missing labels. Defaults to -1. use_metadata ( bool , default: False ) \u2013 Whether to return metadata info (time and location). **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/sen1floods11.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def __init__ ( self , data_root : str , batch_size : int = 4 , num_workers : int = 0 , bands : Sequence [ str ] = Sen1Floods11NonGeo . all_band_names , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , drop_last : bool = True , constant_scale : float = 0.0001 , no_data_replace : float | None = 0 , no_label_replace : int | None = - 1 , use_metadata : bool = False , ** kwargs : Any , ) -> None : \"\"\" Initializes the Sen1Floods11NonGeoDataModule. Args: data_root (str): Root directory of the dataset. batch_size (int, optional): Batch size for DataLoaders. Defaults to 4. num_workers (int, optional): Number of workers for data loading. Defaults to 0. bands (Sequence[str], optional): List of bands to use. Defaults to Sen1Floods11NonGeo.all_band_names. train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True. constant_scale (float, optional): Scale constant applied to the dataset. Defaults to 0.0001. no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0. no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1. use_metadata (bool): Whether to return metadata info (time and location). **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( Sen1Floods11NonGeo , batch_size , num_workers , ** kwargs ) self . data_root = data_root means = [ MEANS [ b ] for b in bands ] stds = [ STDS [ b ] for b in bands ] self . bands = bands self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . aug = AugmentationSequential ( K . Normalize ( means , stds ), data_keys = [ \"image\" ]) self . drop_last = drop_last self . constant_scale = constant_scale self . no_data_replace = no_data_replace self . no_label_replace = no_label_replace self . use_metadata = use_metadata","title":"__init__"},{"location":"data/#terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule.setup","text":"Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/sen1floods11.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = self . dataset_class ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = self . dataset_class ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"test\" ]: self . test_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , ) if stage in [ \"predict\" ]: self . predict_dataset = self . dataset_class ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , constant_scale = self . constant_scale , no_data_replace = self . no_data_replace , no_label_replace = self . no_label_replace , use_metadata = self . use_metadata , )","title":"setup"},{"location":"data/#terratorch.datamodules.sen4agrinet","text":"","title":"sen4agrinet"},{"location":"data/#terratorch.datamodules.sen4agrinet.Sen4AgriNetDataModule","text":"Bases: NonGeoDataModule NonGeo LightningDataModule implementation for Sen4AgriNet. Source code in terratorch/datamodules/sen4agrinet.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class Sen4AgriNetDataModule ( NonGeoDataModule ): \"\"\"NonGeo LightningDataModule implementation for Sen4AgriNet.\"\"\" def __init__ ( self , bands : list [ str ] | None = None , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , seed : int = 42 , scenario : str = \"random\" , requires_norm : bool = True , binary_labels : bool = False , linear_encoder : dict = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the Sen4AgriNetDataModule for the Sen4AgriNet dataset. Args: bands (list[str] | None, optional): List of bands to use. Defaults to None. batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. seed (int, optional): Random seed for reproducibility. Defaults to 42. scenario (str): Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020). requires_norm (bool, optional): Whether normalization is required. Defaults to True. binary_labels (bool, optional): Whether to use binary labels. Defaults to False. linear_encoder (dict, optional): Mapping for label encoding. Defaults to None. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( Sen4AgriNet , batch_size = batch_size , num_workers = num_workers , ** kwargs , ) self . bands = bands self . seed = seed self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . scenario = scenario self . requires_norm = requires_norm self . binary_labels = binary_labels self . linear_encoder = linear_encoder self . kwargs = kwargs def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = Sen4AgriNet ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = Sen4AgriNet ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , ) if stage in [ \"test\" ]: self . test_dataset = Sen4AgriNet ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , ) if stage in [ \"predict\" ]: self . predict_dataset = Sen4AgriNet ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , )","title":"Sen4AgriNetDataModule"},{"location":"data/#terratorch.datamodules.sen4agrinet.Sen4AgriNetDataModule.__init__","text":"Initializes the Sen4AgriNetDataModule for the Sen4AgriNet dataset. Parameters: bands ( list [ str ] | None , default: None ) \u2013 List of bands to use. Defaults to None. batch_size ( int , default: 8 ) \u2013 Batch size for DataLoaders. Defaults to 8. num_workers ( int , default: 0 ) \u2013 Number of workers for data loading. Defaults to 0. data_root ( str , default: './' ) \u2013 Root directory of the dataset. Defaults to \"./\". train_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for training data. val_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for validation data. test_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for test data. predict_transform ( Compose | None | list [ BasicTransform ] , default: None ) \u2013 Transformations for prediction data. seed ( int , default: 42 ) \u2013 Random seed for reproducibility. Defaults to 42. scenario ( str , default: 'random' ) \u2013 Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020). requires_norm ( bool , default: True ) \u2013 Whether normalization is required. Defaults to True. binary_labels ( bool , default: False ) \u2013 Whether to use binary labels. Defaults to False. linear_encoder ( dict , default: None ) \u2013 Mapping for label encoding. Defaults to None. **kwargs ( Any , default: {} ) \u2013 Additional keyword arguments. Source code in terratorch/datamodules/sen4agrinet.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , bands : list [ str ] | None = None , batch_size : int = 8 , num_workers : int = 0 , data_root : str = \"./\" , train_transform : A . Compose | None | list [ A . BasicTransform ] = None , val_transform : A . Compose | None | list [ A . BasicTransform ] = None , test_transform : A . Compose | None | list [ A . BasicTransform ] = None , predict_transform : A . Compose | None | list [ A . BasicTransform ] = None , seed : int = 42 , scenario : str = \"random\" , requires_norm : bool = True , binary_labels : bool = False , linear_encoder : dict = None , ** kwargs : Any , ) -> None : \"\"\" Initializes the Sen4AgriNetDataModule for the Sen4AgriNet dataset. Args: bands (list[str] | None, optional): List of bands to use. Defaults to None. batch_size (int, optional): Batch size for DataLoaders. Defaults to 8. num_workers (int, optional): Number of workers for data loading. Defaults to 0. data_root (str, optional): Root directory of the dataset. Defaults to \"./\". train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data. val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data. test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data. predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data. seed (int, optional): Random seed for reproducibility. Defaults to 42. scenario (str): Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020). requires_norm (bool, optional): Whether normalization is required. Defaults to True. binary_labels (bool, optional): Whether to use binary labels. Defaults to False. linear_encoder (dict, optional): Mapping for label encoding. Defaults to None. **kwargs: Additional keyword arguments. \"\"\" super () . __init__ ( Sen4AgriNet , batch_size = batch_size , num_workers = num_workers , ** kwargs , ) self . bands = bands self . seed = seed self . train_transform = wrap_in_compose_is_list ( train_transform ) self . val_transform = wrap_in_compose_is_list ( val_transform ) self . test_transform = wrap_in_compose_is_list ( test_transform ) self . predict_transform = wrap_in_compose_is_list ( predict_transform ) self . data_root = data_root self . scenario = scenario self . requires_norm = requires_norm self . binary_labels = binary_labels self . linear_encoder = linear_encoder self . kwargs = kwargs","title":"__init__"},{"location":"data/#terratorch.datamodules.sen4agrinet.Sen4AgriNetDataModule.setup","text":"Set up datasets. Parameters: stage ( str ) \u2013 Either fit, validate, test, or predict. Source code in terratorch/datamodules/sen4agrinet.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def setup ( self , stage : str ) -> None : \"\"\"Set up datasets. Args: stage: Either fit, validate, test, or predict. \"\"\" if stage in [ \"fit\" ]: self . train_dataset = Sen4AgriNet ( split = \"train\" , data_root = self . data_root , transform = self . train_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , ) if stage in [ \"fit\" , \"validate\" ]: self . val_dataset = Sen4AgriNet ( split = \"val\" , data_root = self . data_root , transform = self . val_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , ) if stage in [ \"test\" ]: self . test_dataset = Sen4AgriNet ( split = \"test\" , data_root = self . data_root , transform = self . test_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , ) if stage in [ \"predict\" ]: self . predict_dataset = Sen4AgriNet ( split = \"test\" , data_root = self . data_root , transform = self . predict_transform , bands = self . bands , seed = self . seed , scenario = self . scenario , requires_norm = self . requires_norm , binary_labels = self . binary_labels , linear_encoder = self . linear_encoder , ** self . kwargs , )","title":"setup"},{"location":"data/#terratorch.datamodules.sen4map","text":"","title":"sen4map"},{"location":"data/#terratorch.datamodules.sen4map.Sen4MapLucasDataModule","text":"Bases: LightningDataModule NonGeo LightningDataModule implementation for Sen4map. Source code in terratorch/datamodules/sen4map.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class Sen4MapLucasDataModule ( pl . LightningDataModule ): \"\"\"NonGeo LightningDataModule implementation for Sen4map.\"\"\" def __init__ ( self , batch_size , num_workers , prefetch_factor = 0 , # dataset_bands:list[HLSBands|int] = None, # input_bands:list[HLSBands|int] = None, train_hdf5_path = None , train_hdf5_keys_path = None , test_hdf5_path = None , test_hdf5_keys_path = None , val_hdf5_path = None , val_hdf5_keys_path = None , ** kwargs ): \"\"\" Initializes the Sen4MapLucasDataModule for handling Sen4Map monthly composites. Args: batch_size (int): Batch size for DataLoaders. num_workers (int): Number of worker processes for data loading. prefetch_factor (int, optional): Number of samples to prefetch per worker. Defaults to 0. train_hdf5_path (str, optional): Path to the training HDF5 file. train_hdf5_keys_path (str, optional): Path to the training HDF5 keys file. test_hdf5_path (str, optional): Path to the testing HDF5 file. test_hdf5_keys_path (str, optional): Path to the testing HDF5 keys file. val_hdf5_path (str, optional): Path to the validation HDF5 file. val_hdf5_keys_path (str, optional): Path to the validation HDF5 keys file. train_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated train keys. test_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated test keys. val_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated validation keys. shuffle (bool, optional): Global shuffle flag. train_shuffle (bool, optional): Shuffle flag for training data; defaults to global shuffle if unset. val_shuffle (bool, optional): Shuffle flag for validation data. test_shuffle (bool, optional): Shuffle flag for test data. train_data_fraction (float, optional): Fraction of training data to use. Defaults to 1.0. val_data_fraction (float, optional): Fraction of validation data to use. Defaults to 1.0. test_data_fraction (float, optional): Fraction of test data to use. Defaults to 1.0. all_hdf5_data_path (str, optional): General HDF5 data path for all splits. If provided, overrides specific paths. resize (bool, optional): Whether to resize images. Defaults to False. resize_to (int or tuple, optional): Target size for resizing images. resize_interpolation (str, optional): Interpolation mode for resizing ('bilinear', 'bicubic', etc.). resize_antialiasing (bool, optional): Whether to apply antialiasing during resizing. Defaults to True. **kwargs: Additional keyword arguments. \"\"\" self . prepare_data_per_node = False self . _log_hyperparams = None self . allow_zero_length_dataloader_with_multiple_devices = False self . batch_size = batch_size self . num_workers = num_workers self . prefetch_factor = prefetch_factor self . train_hdf5_path = train_hdf5_path self . test_hdf5_path = test_hdf5_path self . val_hdf5_path = val_hdf5_path self . train_hdf5_keys_path = train_hdf5_keys_path self . test_hdf5_keys_path = test_hdf5_keys_path self . val_hdf5_keys_path = val_hdf5_keys_path if train_hdf5_path and not train_hdf5_keys_path : print ( f \"Train dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\" ) if test_hdf5_path and not test_hdf5_keys_path : print ( f \"Test dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\" ) if val_hdf5_path and not val_hdf5_keys_path : print ( f \"Val dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\" ) self . train_hdf5_keys_save_path = kwargs . pop ( \"train_hdf5_keys_save_path\" , None ) self . test_hdf5_keys_save_path = kwargs . pop ( \"test_hdf5_keys_save_path\" , None ) self . val_hdf5_keys_save_path = kwargs . pop ( \"val_hdf5_keys_save_path\" , None ) self . shuffle = kwargs . pop ( \"shuffle\" , None ) self . train_shuffle = kwargs . pop ( \"train_shuffle\" , None ) or self . shuffle self . val_shuffle = kwargs . pop ( \"val_shuffle\" , None ) self . test_shuffle = kwargs . pop ( \"test_shuffle\" , None ) self . train_data_fraction = kwargs . pop ( \"train_data_fraction\" , 1.0 ) self . val_data_fraction = kwargs . pop ( \"val_data_fraction\" , 1.0 ) self . test_data_fraction = kwargs . pop ( \"test_data_fraction\" , 1.0 ) if self . train_data_fraction != 1.0 and not train_hdf5_keys_path : raise ValueError ( f \"train_data_fraction provided as non-unity but train_hdf5_keys_path is unset.\" ) if self . val_data_fraction != 1.0 and not val_hdf5_keys_path : raise ValueError ( f \"val_data_fraction provided as non-unity but val_hdf5_keys_path is unset.\" ) if self . test_data_fraction != 1.0 and not test_hdf5_keys_path : raise ValueError ( f \"test_data_fraction provided as non-unity but test_hdf5_keys_path is unset.\" ) all_hdf5_data_path = kwargs . pop ( \"all_hdf5_data_path\" , None ) if all_hdf5_data_path is not None : print ( f \"all_hdf5_data_path provided, will be interpreted as the general data path for all splits. \\n Keys in provided train_hdf5_keys_path assumed to encompass all keys for entire data. Validation and Test keys will be subtracted from Train keys.\" ) if self . train_hdf5_path : raise ValueError ( f \"Both general all_hdf5_data_path provided and a specific train_hdf5_path, remove the train_hdf5_path\" ) if self . val_hdf5_path : raise ValueError ( f \"Both general all_hdf5_data_path provided and a specific val_hdf5_path, remove the val_hdf5_path\" ) if self . test_hdf5_path : raise ValueError ( f \"Both general all_hdf5_data_path provided and a specific test_hdf5_path, remove the test_hdf5_path\" ) self . train_hdf5_path = all_hdf5_data_path self . val_hdf5_path = all_hdf5_data_path self . test_hdf5_path = all_hdf5_data_path self . reduce_train_keys = True else : self . reduce_train_keys = False self . resize = kwargs . pop ( \"resize\" , False ) self . resize_to = kwargs . pop ( \"resize_to\" , None ) if self . resize and self . resize_to is None : raise ValueError ( f \"Config provided resize as True, but resize_to parameter not given\" ) self . resize_interpolation = kwargs . pop ( \"resize_interpolation\" , None ) if self . resize and self . resize_interpolation is None : print ( f \"Config provided resize as True, but resize_interpolation mode not given. Will assume default bilinear\" ) self . resize_interpolation = \"bilinear\" interpolation_dict = { \"bilinear\" : InterpolationMode . BILINEAR , \"bicubic\" : InterpolationMode . BICUBIC , \"nearest\" : InterpolationMode . NEAREST , \"nearest_exact\" : InterpolationMode . NEAREST_EXACT } if self . resize : if self . resize_interpolation not in interpolation_dict . keys (): raise ValueError ( f \"resize_interpolation provided as { self . resize_interpolation } , but valid options are: { interpolation_dict . keys () } \" ) self . resize_interpolation = interpolation_dict [ self . resize_interpolation ] self . resize_antialiasing = kwargs . pop ( \"resize_antialiasing\" , True ) self . kwargs = kwargs def _load_hdf5_keys_from_path ( self , path , fraction = 1.0 ): if path is None : return None with open ( path , \"rb\" ) as f : keys = pickle . load ( f ) return keys [: int ( fraction * len ( keys ))] def setup ( self , stage : str ): \"\"\"Set up datasets. Args: stage: Either fit, test. \"\"\" if stage == \"fit\" : train_keys = self . _load_hdf5_keys_from_path ( self . train_hdf5_keys_path , fraction = self . train_data_fraction ) val_keys = self . _load_hdf5_keys_from_path ( self . val_hdf5_keys_path , fraction = self . val_data_fraction ) if self . reduce_train_keys : test_keys = self . _load_hdf5_keys_from_path ( self . test_hdf5_keys_path , fraction = self . test_data_fraction ) train_keys = list ( set ( train_keys ) - set ( val_keys ) - set ( test_keys )) train_file = h5py . File ( self . train_hdf5_path , 'r' ) self . lucasS2_train = Sen4MapDatasetMonthlyComposites ( train_file , h5data_keys = train_keys , resize = self . resize , resize_to = self . resize_to , resize_interpolation = self . resize_interpolation , resize_antialiasing = self . resize_antialiasing , save_keys_path = self . train_hdf5_keys_save_path , ** self . kwargs ) val_file = h5py . File ( self . val_hdf5_path , 'r' ) self . lucasS2_val = Sen4MapDatasetMonthlyComposites ( val_file , h5data_keys = val_keys , resize = self . resize , resize_to = self . resize_to , resize_interpolation = self . resize_interpolation , resize_antialiasing = self . resize_antialiasing , save_keys_path = self . val_hdf5_keys_save_path , ** self . kwargs ) if stage == \"test\" : test_file = h5py . File ( self . test_hdf5_path , 'r' ) test_keys = self . _load_hdf5_keys_from_path ( self . test_hdf5_keys_path , fraction = self . test_data_fraction ) self . lucasS2_test = Sen4MapDatasetMonthlyComposites ( test_file , h5data_keys = test_keys , resize = self . resize , resize_to = self . resize_to , resize_interpolation = self . resize_interpolation , resize_antialiasing = self . resize_antialiasing , save_keys_path = self . test_hdf5_keys_save_path , ** self . kwargs ) def train_dataloader ( self ): return DataLoader ( self . lucasS2_train , batch_size = self . batch_size , num_workers = self . num_workers , prefetch_factor = self . prefetch_factor , shuffle = self . train_shuffle ) def val_dataloader ( self ): return DataLoader ( self . lucasS2_val , batch_size = self . batch_size , num_workers = self . num_workers , prefetch_factor = self . prefetch_factor , shuffle = self . val_shuffle ) def test_dataloader ( self ): return DataLoader ( self . lucasS2_test , batch_size = self . batch_size , num_workers = self . num_workers , prefetch_factor = self . prefetch_factor , shuffle = self . test_shuffle )","title":"Sen4MapLucasDataModule"},{"location":"data/#terratorch.datamodules.sen4map.Sen4MapLucasDataModule.__init__","text":"Initializes the Sen4MapLucasDataModule for handling Sen4Map monthly composites. Parameters: batch_size ( int ) \u2013 Batch size for DataLoaders. num_workers ( int ) \u2013 Number of worker processes for data loading. prefetch_factor ( int , default: 0 ) \u2013 Number of samples to prefetch per worker. Defaults to 0. train_hdf5_path ( str , default: None ) \u2013 Path to the training HDF5 file. train_hdf5_keys_path ( str , default: None ) \u2013 Path to the training HDF5 keys file. test_hdf5_path ( str , default: None ) \u2013 Path to the testing HDF5 file. test_hdf5_keys_path ( str , default: None ) \u2013 Path to the testing HDF5 keys file. val_hdf5_path ( str , default: None ) \u2013 Path to the validation HDF5 file. val_hdf5_keys_path ( str , default: None ) \u2013 Path to the validation HDF5 keys file. train_hdf5_keys_save_path ( str ) \u2013 (from kwargs) Path to save generated train keys. test_hdf5_keys_save_path ( str ) \u2013 (from kwargs) Path to save generated test keys. val_hdf5_keys_save_path ( str ) \u2013 (from kwargs) Path to save generated validation keys. shuffle ( bool ) \u2013 Global shuffle flag. train_shuffle ( bool ) \u2013 Shuffle flag for training data; defaults to global shuffle if unset. val_shuffle ( bool ) \u2013 Shuffle flag for validation data. test_shuffle ( bool ) \u2013 Shuffle flag for test data. train_data_fraction ( float ) \u2013 Fraction of training data to use. Defaults to 1.0. val_data_fraction ( float ) \u2013 Fraction of validation data to use. Defaults to 1.0. test_data_fraction ( float ) \u2013 Fraction of test data to use. Defaults to 1.0. all_hdf5_data_path ( str ) \u2013 General HDF5 data path for all splits. If provided, overrides specific paths. resize ( bool ) \u2013 Whether to resize images. Defaults to False. resize_to ( int or tuple ) \u2013 Target size for resizing images. resize_interpolation ( str ) \u2013 Interpolation mode for resizing ('bilinear', 'bicubic', etc.). resize_antialiasing ( bool ) \u2013 Whether to apply antialiasing during resizing. Defaults to True. **kwargs \u2013 Additional keyword arguments. Source code in terratorch/datamodules/sen4map.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def __init__ ( self , batch_size , num_workers , prefetch_factor = 0 , # dataset_bands:list[HLSBands|int] = None, # input_bands:list[HLSBands|int] = None, train_hdf5_path = None , train_hdf5_keys_path = None , test_hdf5_path = None , test_hdf5_keys_path = None , val_hdf5_path = None , val_hdf5_keys_path = None , ** kwargs ): \"\"\" Initializes the Sen4MapLucasDataModule for handling Sen4Map monthly composites. Args: batch_size (int): Batch size for DataLoaders. num_workers (int): Number of worker processes for data loading. prefetch_factor (int, optional): Number of samples to prefetch per worker. Defaults to 0. train_hdf5_path (str, optional): Path to the training HDF5 file. train_hdf5_keys_path (str, optional): Path to the training HDF5 keys file. test_hdf5_path (str, optional): Path to the testing HDF5 file. test_hdf5_keys_path (str, optional): Path to the testing HDF5 keys file. val_hdf5_path (str, optional): Path to the validation HDF5 file. val_hdf5_keys_path (str, optional): Path to the validation HDF5 keys file. train_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated train keys. test_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated test keys. val_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated validation keys. shuffle (bool, optional): Global shuffle flag. train_shuffle (bool, optional): Shuffle flag for training data; defaults to global shuffle if unset. val_shuffle (bool, optional): Shuffle flag for validation data. test_shuffle (bool, optional): Shuffle flag for test data. train_data_fraction (float, optional): Fraction of training data to use. Defaults to 1.0. val_data_fraction (float, optional): Fraction of validation data to use. Defaults to 1.0. test_data_fraction (float, optional): Fraction of test data to use. Defaults to 1.0. all_hdf5_data_path (str, optional): General HDF5 data path for all splits. If provided, overrides specific paths. resize (bool, optional): Whether to resize images. Defaults to False. resize_to (int or tuple, optional): Target size for resizing images. resize_interpolation (str, optional): Interpolation mode for resizing ('bilinear', 'bicubic', etc.). resize_antialiasing (bool, optional): Whether to apply antialiasing during resizing. Defaults to True. **kwargs: Additional keyword arguments. \"\"\" self . prepare_data_per_node = False self . _log_hyperparams = None self . allow_zero_length_dataloader_with_multiple_devices = False self . batch_size = batch_size self . num_workers = num_workers self . prefetch_factor = prefetch_factor self . train_hdf5_path = train_hdf5_path self . test_hdf5_path = test_hdf5_path self . val_hdf5_path = val_hdf5_path self . train_hdf5_keys_path = train_hdf5_keys_path self . test_hdf5_keys_path = test_hdf5_keys_path self . val_hdf5_keys_path = val_hdf5_keys_path if train_hdf5_path and not train_hdf5_keys_path : print ( f \"Train dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\" ) if test_hdf5_path and not test_hdf5_keys_path : print ( f \"Test dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\" ) if val_hdf5_path and not val_hdf5_keys_path : print ( f \"Val dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\" ) self . train_hdf5_keys_save_path = kwargs . pop ( \"train_hdf5_keys_save_path\" , None ) self . test_hdf5_keys_save_path = kwargs . pop ( \"test_hdf5_keys_save_path\" , None ) self . val_hdf5_keys_save_path = kwargs . pop ( \"val_hdf5_keys_save_path\" , None ) self . shuffle = kwargs . pop ( \"shuffle\" , None ) self . train_shuffle = kwargs . pop ( \"train_shuffle\" , None ) or self . shuffle self . val_shuffle = kwargs . pop ( \"val_shuffle\" , None ) self . test_shuffle = kwargs . pop ( \"test_shuffle\" , None ) self . train_data_fraction = kwargs . pop ( \"train_data_fraction\" , 1.0 ) self . val_data_fraction = kwargs . pop ( \"val_data_fraction\" , 1.0 ) self . test_data_fraction = kwargs . pop ( \"test_data_fraction\" , 1.0 ) if self . train_data_fraction != 1.0 and not train_hdf5_keys_path : raise ValueError ( f \"train_data_fraction provided as non-unity but train_hdf5_keys_path is unset.\" ) if self . val_data_fraction != 1.0 and not val_hdf5_keys_path : raise ValueError ( f \"val_data_fraction provided as non-unity but val_hdf5_keys_path is unset.\" ) if self . test_data_fraction != 1.0 and not test_hdf5_keys_path : raise ValueError ( f \"test_data_fraction provided as non-unity but test_hdf5_keys_path is unset.\" ) all_hdf5_data_path = kwargs . pop ( \"all_hdf5_data_path\" , None ) if all_hdf5_data_path is not None : print ( f \"all_hdf5_data_path provided, will be interpreted as the general data path for all splits. \\n Keys in provided train_hdf5_keys_path assumed to encompass all keys for entire data. Validation and Test keys will be subtracted from Train keys.\" ) if self . train_hdf5_path : raise ValueError ( f \"Both general all_hdf5_data_path provided and a specific train_hdf5_path, remove the train_hdf5_path\" ) if self . val_hdf5_path : raise ValueError ( f \"Both general all_hdf5_data_path provided and a specific val_hdf5_path, remove the val_hdf5_path\" ) if self . test_hdf5_path : raise ValueError ( f \"Both general all_hdf5_data_path provided and a specific test_hdf5_path, remove the test_hdf5_path\" ) self . train_hdf5_path = all_hdf5_data_path self . val_hdf5_path = all_hdf5_data_path self . test_hdf5_path = all_hdf5_data_path self . reduce_train_keys = True else : self . reduce_train_keys = False self . resize = kwargs . pop ( \"resize\" , False ) self . resize_to = kwargs . pop ( \"resize_to\" , None ) if self . resize and self . resize_to is None : raise ValueError ( f \"Config provided resize as True, but resize_to parameter not given\" ) self . resize_interpolation = kwargs . pop ( \"resize_interpolation\" , None ) if self . resize and self . resize_interpolation is None : print ( f \"Config provided resize as True, but resize_interpolation mode not given. Will assume default bilinear\" ) self . resize_interpolation = \"bilinear\" interpolation_dict = { \"bilinear\" : InterpolationMode . BILINEAR , \"bicubic\" : InterpolationMode . BICUBIC , \"nearest\" : InterpolationMode . NEAREST , \"nearest_exact\" : InterpolationMode . NEAREST_EXACT } if self . resize : if self . resize_interpolation not in interpolation_dict . keys (): raise ValueError ( f \"resize_interpolation provided as { self . resize_interpolation } , but valid options are: { interpolation_dict . keys () } \" ) self . resize_interpolation = interpolation_dict [ self . resize_interpolation ] self . resize_antialiasing = kwargs . pop ( \"resize_antialiasing\" , True ) self . kwargs = kwargs","title":"__init__"},{"location":"data/#terratorch.datamodules.sen4map.Sen4MapLucasDataModule.setup","text":"Set up datasets. Parameters: stage ( str ) \u2013 Either fit, test. Source code in terratorch/datamodules/sen4map.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def setup ( self , stage : str ): \"\"\"Set up datasets. Args: stage: Either fit, test. \"\"\" if stage == \"fit\" : train_keys = self . _load_hdf5_keys_from_path ( self . train_hdf5_keys_path , fraction = self . train_data_fraction ) val_keys = self . _load_hdf5_keys_from_path ( self . val_hdf5_keys_path , fraction = self . val_data_fraction ) if self . reduce_train_keys : test_keys = self . _load_hdf5_keys_from_path ( self . test_hdf5_keys_path , fraction = self . test_data_fraction ) train_keys = list ( set ( train_keys ) - set ( val_keys ) - set ( test_keys )) train_file = h5py . File ( self . train_hdf5_path , 'r' ) self . lucasS2_train = Sen4MapDatasetMonthlyComposites ( train_file , h5data_keys = train_keys , resize = self . resize , resize_to = self . resize_to , resize_interpolation = self . resize_interpolation , resize_antialiasing = self . resize_antialiasing , save_keys_path = self . train_hdf5_keys_save_path , ** self . kwargs ) val_file = h5py . File ( self . val_hdf5_path , 'r' ) self . lucasS2_val = Sen4MapDatasetMonthlyComposites ( val_file , h5data_keys = val_keys , resize = self . resize , resize_to = self . resize_to , resize_interpolation = self . resize_interpolation , resize_antialiasing = self . resize_antialiasing , save_keys_path = self . val_hdf5_keys_save_path , ** self . kwargs ) if stage == \"test\" : test_file = h5py . File ( self . test_hdf5_path , 'r' ) test_keys = self . _load_hdf5_keys_from_path ( self . test_hdf5_keys_path , fraction = self . test_data_fraction ) self . lucasS2_test = Sen4MapDatasetMonthlyComposites ( test_file , h5data_keys = test_keys , resize = self . resize , resize_to = self . resize_to , resize_interpolation = self . resize_interpolation , resize_antialiasing = self . resize_antialiasing , save_keys_path = self . test_hdf5_keys_save_path , ** self . kwargs )","title":"setup"},{"location":"data/#transforms","text":"The transforms module provides a set of specialized image transformations designed to manipulate spatial, temporal, and multimodal data efficiently. These transformations allow for greater flexibility when working with multi-temporal, multi-channel, and multi-modal datasets, ensuring that data can be formatted appropriately for different model architectures.","title":"Transforms"},{"location":"data/#terratorch.datasets.transforms","text":"","title":"transforms"},{"location":"data/#terratorch.datasets.transforms.FlattenSamplesIntoChannels","text":"Bases: ImageOnlyTransform FlattenSamplesIntoChannels is an image transformation that merges the sample (and optionally temporal) dimensions into the channel dimension. This transform rearranges an input tensor by flattening the sample dimension, and if specified, also the temporal dimension, thereby concatenating these dimensions into a single channel dimension. Source code in terratorch/datasets/transforms.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class FlattenSamplesIntoChannels ( ImageOnlyTransform ): \"\"\" FlattenSamplesIntoChannels is an image transformation that merges the sample (and optionally temporal) dimensions into the channel dimension. This transform rearranges an input tensor by flattening the sample dimension, and if specified, also the temporal dimension, thereby concatenating these dimensions into a single channel dimension. \"\"\" def __init__ ( self , time_dim : bool = True ): \"\"\" Initialize the FlattenSamplesIntoChannels transform. Args: time_dim (bool): If True, the temporal dimension is included in the flattening process. Default is True. \"\"\" super () . __init__ ( True , 1 ) self . time_dim = time_dim def apply ( self , img , ** params ): if self . time_dim : rearranged = rearrange ( img , \"samples time height width channels -> height width (samples time channels)\" ) else : rearranged = rearrange ( img , \"samples height width channels -> height width (samples channels)\" ) return rearranged def get_transform_init_args_names ( self ): return ()","title":"FlattenSamplesIntoChannels"},{"location":"data/#terratorch.datasets.transforms.FlattenSamplesIntoChannels.__init__","text":"Initialize the FlattenSamplesIntoChannels transform. Parameters: time_dim ( bool , default: True ) \u2013 If True, the temporal dimension is included in the flattening process. Default is True. Source code in terratorch/datasets/transforms.py 121 122 123 124 125 126 127 128 129 def __init__ ( self , time_dim : bool = True ): \"\"\" Initialize the FlattenSamplesIntoChannels transform. Args: time_dim (bool): If True, the temporal dimension is included in the flattening process. Default is True. \"\"\" super () . __init__ ( True , 1 ) self . time_dim = time_dim","title":"__init__"},{"location":"data/#terratorch.datasets.transforms.FlattenTemporalIntoChannels","text":"Bases: ImageOnlyTransform FlattenTemporalIntoChannels is an image transformation that flattens the temporal dimension into the channel dimension. This transform rearranges an input tensor with a temporal dimension into one where the time and channel dimensions are merged. It expects the input to have a fixed number of dimensions defined by N_DIMS_FOR_TEMPORAL. Source code in terratorch/datasets/transforms.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 class FlattenTemporalIntoChannels ( ImageOnlyTransform ): \"\"\" FlattenTemporalIntoChannels is an image transformation that flattens the temporal dimension into the channel dimension. This transform rearranges an input tensor with a temporal dimension into one where the time and channel dimensions are merged. It expects the input to have a fixed number of dimensions defined by N_DIMS_FOR_TEMPORAL. \"\"\" def __init__ ( self ): \"\"\" Initialize the FlattenTemporalIntoChannels transform. \"\"\" super () . __init__ ( True , 1 ) def apply ( self , img , ** params ): if len ( img . shape ) != N_DIMS_FOR_TEMPORAL : msg = f \"Expected input temporal sequence to have { N_DIMS_FOR_TEMPORAL } dimensions, but got { len ( img . shape ) } \" raise Exception ( msg ) rearranged = rearrange ( img , \"time height width channels -> height width (time channels)\" ) return rearranged def get_transform_init_args_names ( self ): return ()","title":"FlattenTemporalIntoChannels"},{"location":"data/#terratorch.datasets.transforms.FlattenTemporalIntoChannels.__init__","text":"Initialize the FlattenTemporalIntoChannels transform. Source code in terratorch/datasets/transforms.py 58 59 60 61 62 def __init__ ( self ): \"\"\" Initialize the FlattenTemporalIntoChannels transform. \"\"\" super () . __init__ ( True , 1 )","title":"__init__"},{"location":"data/#terratorch.datasets.transforms.MultimodalTransforms","text":"MultimodalTransforms applies albumentations transforms to multiple image modalities. This class supports both shared transformations across modalities and separate transformations for each modality. It also handles non-image modalities by applying a specified non-image transform. Source code in terratorch/datasets/transforms.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 class MultimodalTransforms : \"\"\" MultimodalTransforms applies albumentations transforms to multiple image modalities. This class supports both shared transformations across modalities and separate transformations for each modality. It also handles non-image modalities by applying a specified non-image transform. \"\"\" def __init__ ( self , transforms : dict | A . Compose , shared : bool = True , non_image_modalities : list [ str ] | None = None , non_image_transform : object | None = None , ): \"\"\" Initialize the MultimodalTransforms. Args: transforms (dict or A.Compose): The transformation(s) to apply to the data. shared (bool): If True, the same transform is applied to all modalities; if False, separate transforms are used. non_image_modalities (list[str] | None): List of keys corresponding to non-image modalities. non_image_transform (object | None): A transform to apply to non-image modalities. If None, a default transform is used. \"\"\" self . transforms = transforms self . shared = shared self . non_image_modalities = non_image_modalities self . non_image_transform = non_image_transform or default_non_image_transform def __call__ ( self , data : dict ): if self . shared : # albumentations requires a key 'image' and treats all other keys as additional targets image_modality = list ( set ( data . keys ()) - set ( self . non_image_modalities ))[ 0 ] data [ 'image' ] = data . pop ( image_modality ) data = self . transforms ( ** data ) data [ image_modality ] = data . pop ( 'image' ) # Process sequence data which is ignored by albumentations as 'global_label' for modality in self . non_image_modalities : data [ modality ] = self . non_image_transform ( data [ modality ]) else : # Applies transformations for each modality separate for key , value in data . items (): data [ key ] = self . transforms [ key ]( image = value )[ 'image' ] # Only works with image modalities return data","title":"MultimodalTransforms"},{"location":"data/#terratorch.datasets.transforms.MultimodalTransforms.__init__","text":"Initialize the MultimodalTransforms. Parameters: transforms ( dict or Compose ) \u2013 The transformation(s) to apply to the data. shared ( bool , default: True ) \u2013 If True, the same transform is applied to all modalities; if False, separate transforms are used. non_image_modalities ( list [ str ] | None , default: None ) \u2013 List of keys corresponding to non-image modalities. non_image_transform ( object | None , default: None ) \u2013 A transform to apply to non-image modalities. If None, a default transform is used. Source code in terratorch/datasets/transforms.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def __init__ ( self , transforms : dict | A . Compose , shared : bool = True , non_image_modalities : list [ str ] | None = None , non_image_transform : object | None = None , ): \"\"\" Initialize the MultimodalTransforms. Args: transforms (dict or A.Compose): The transformation(s) to apply to the data. shared (bool): If True, the same transform is applied to all modalities; if False, separate transforms are used. non_image_modalities (list[str] | None): List of keys corresponding to non-image modalities. non_image_transform (object | None): A transform to apply to non-image modalities. If None, a default transform is used. \"\"\" self . transforms = transforms self . shared = shared self . non_image_modalities = non_image_modalities self . non_image_transform = non_image_transform or default_non_image_transform","title":"__init__"},{"location":"data/#terratorch.datasets.transforms.Padding","text":"Bases: ImageOnlyTransform Padding to adjust (slight) discrepancies between input images Source code in terratorch/datasets/transforms.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class Padding ( ImageOnlyTransform ): \"\"\"Padding to adjust (slight) discrepancies between input images\"\"\" def __init__ ( self , input_shape : list = None ): super () . __init__ ( True , 1 ) self . input_shape = input_shape def apply ( self , img , ** params ): shape = img . shape [ - 2 :] pad_values_ = [ j - i for i , j in zip ( shape , self . input_shape )] if all ([ i % 2 == 0 for i in pad_values_ ]): pad_values = sum ([[ int ( j / 2 ), int ( j / 2 )] for j in pad_values_ ], []) else : pad_values = sum ([[ 0 , j ] for j in pad_values_ ], []) return F . pad ( img , pad_values ) def get_transform_init_args_names ( self ): return ()","title":"Padding"},{"location":"data/#terratorch.datasets.transforms.Rearrange","text":"Bases: ImageOnlyTransform Rearrange is a generic image transformation that reshapes an input tensor using a custom einops pattern. This transform allows flexible reordering of tensor dimensions based on the provided pattern and arguments. Source code in terratorch/datasets/transforms.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 class Rearrange ( ImageOnlyTransform ): \"\"\" Rearrange is a generic image transformation that reshapes an input tensor using a custom einops pattern. This transform allows flexible reordering of tensor dimensions based on the provided pattern and arguments. \"\"\" def __init__ ( self , rearrange : str , rearrange_args : dict [ str , int ] | None = None , always_apply : bool = True , p : float = 1 ): \"\"\" Initialize the Rearrange transform. Args: rearrange (str): The einops rearrangement pattern to apply. rearrange_args (dict[str, int] | None): Additional arguments for the rearrangement pattern. always_apply (bool): Whether to always apply this transform. Default is True. p (float): The probability of applying the transform. Default is 1. \"\"\" super () . __init__ ( always_apply , p ) self . rearrange = rearrange self . vars = rearrange_args if rearrange_args else {} def apply ( self , img , ** params ): return rearrange ( img , self . rearrange , ** self . vars ) def get_transform_init_args_names ( self ): return \"rearrange\"","title":"Rearrange"},{"location":"data/#terratorch.datasets.transforms.Rearrange.__init__","text":"Initialize the Rearrange transform. Parameters: rearrange ( str ) \u2013 The einops rearrangement pattern to apply. rearrange_args ( dict [ str , int ] | None , default: None ) \u2013 Additional arguments for the rearrangement pattern. always_apply ( bool , default: True ) \u2013 Whether to always apply this transform. Default is True. p ( float , default: 1 ) \u2013 The probability of applying the transform. Default is 1. Source code in terratorch/datasets/transforms.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def __init__ ( self , rearrange : str , rearrange_args : dict [ str , int ] | None = None , always_apply : bool = True , p : float = 1 ): \"\"\" Initialize the Rearrange transform. Args: rearrange (str): The einops rearrangement pattern to apply. rearrange_args (dict[str, int] | None): Additional arguments for the rearrangement pattern. always_apply (bool): Whether to always apply this transform. Default is True. p (float): The probability of applying the transform. Default is 1. \"\"\" super () . __init__ ( always_apply , p ) self . rearrange = rearrange self . vars = rearrange_args if rearrange_args else {}","title":"__init__"},{"location":"data/#terratorch.datasets.transforms.SelectBands","text":"Bases: ImageOnlyTransform SelectBands is an image transformation that selects a subset of bands (channels) from an input image. This transform uses specified band indices to filter and output only the desired channels from the image tensor. Source code in terratorch/datasets/transforms.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 class SelectBands ( ImageOnlyTransform ): \"\"\" SelectBands is an image transformation that selects a subset of bands (channels) from an input image. This transform uses specified band indices to filter and output only the desired channels from the image tensor. \"\"\" def __init__ ( self , band_indices : list [ int ]): \"\"\" Initialize the SelectBands transform. Args: band_indices (list[int]): A list of indices specifying which bands to select. \"\"\" super () . __init__ ( True , 1 ) self . band_indices = band_indices def apply ( self , img , ** params ): return img [ ... , self . band_indices ] def get_transform_init_args_names ( self ): return \"band_indices\"","title":"SelectBands"},{"location":"data/#terratorch.datasets.transforms.SelectBands.__init__","text":"Initialize the SelectBands transform. Parameters: band_indices ( list [ int ] ) \u2013 A list of indices specifying which bands to select. Source code in terratorch/datasets/transforms.py 242 243 244 245 246 247 248 249 250 def __init__ ( self , band_indices : list [ int ]): \"\"\" Initialize the SelectBands transform. Args: band_indices (list[int]): A list of indices specifying which bands to select. \"\"\" super () . __init__ ( True , 1 ) self . band_indices = band_indices","title":"__init__"},{"location":"data/#terratorch.datasets.transforms.UnflattenSamplesFromChannels","text":"Bases: ImageOnlyTransform UnflattenSamplesFromChannels is an image transformation that restores the sample (and optionally temporal) dimensions from the channel dimension. This transform is designed to reverse the flattening performed by FlattenSamplesIntoChannels and is typically applied after converting images to a channels-first format. Source code in terratorch/datasets/transforms.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 class UnflattenSamplesFromChannels ( ImageOnlyTransform ): \"\"\" UnflattenSamplesFromChannels is an image transformation that restores the sample (and optionally temporal) dimensions from the channel dimension. This transform is designed to reverse the flattening performed by FlattenSamplesIntoChannels and is typically applied after converting images to a channels-first format. \"\"\" def __init__ ( self , time_dim : bool = True , n_samples : int | None = None , n_timesteps : int | None = None , n_channels : int | None = None ): \"\"\" Initialize the UnflattenSamplesFromChannels transform. Args: time_dim (bool): If True, the temporal dimension is considered during unflattening. n_samples (int | None): The number of samples. n_timesteps (int | None): The number of time steps. n_channels (int | None): The number of channels per time step. Raises: Exception: If time_dim is True and fewer than two of n_channels, n_timesteps, and n_samples are provided. Exception: If time_dim is False and neither n_channels nor n_samples is provided. \"\"\" super () . __init__ ( True , 1 ) self . time_dim = time_dim if self . time_dim : if bool ( n_channels ) + bool ( n_timesteps ) + bool ( n_samples ) < 2 : msg = \"Two of n_channels, n_timesteps, and n_channels must be provided\" raise Exception ( msg ) if n_timesteps and n_channels : self . additional_info = { \"channels\" : n_channels , \"time\" : n_timesteps } elif n_timesteps and n_samples : self . additional_info = { \"time\" : n_timesteps , \"samples\" : n_samples } else : self . additional_info = { \"channels\" : n_channels , \"samples\" : n_samples } else : if n_channels is None and n_samples is None : msg = \"One of n_channels or n_samples must be provided\" raise Exception ( msg ) self . additional_info = { \"channels\" : n_channels } if n_channels else { \"samples\" : n_samples } def apply ( self , img , ** params ): if self . time_dim : rearranged = rearrange ( img , \"(samples time channels) height width -> samples channels time height width\" , ** self . additional_info ) else : rearranged = rearrange ( img , \"(samples channels) height width -> samples channels height width\" , ** self . additional_info ) return rearranged def get_transform_init_args_names ( self ): return ( \"n_timesteps\" , \"n_channels\" )","title":"UnflattenSamplesFromChannels"},{"location":"data/#terratorch.datasets.transforms.UnflattenSamplesFromChannels.__init__","text":"Initialize the UnflattenSamplesFromChannels transform. Parameters: time_dim ( bool , default: True ) \u2013 If True, the temporal dimension is considered during unflattening. n_samples ( int | None , default: None ) \u2013 The number of samples. n_timesteps ( int | None , default: None ) \u2013 The number of time steps. n_channels ( int | None , default: None ) \u2013 The number of channels per time step. Raises: Exception \u2013 If time_dim is True and fewer than two of n_channels, n_timesteps, and n_samples are provided. Exception \u2013 If time_dim is False and neither n_channels nor n_samples is provided. Source code in terratorch/datasets/transforms.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def __init__ ( self , time_dim : bool = True , n_samples : int | None = None , n_timesteps : int | None = None , n_channels : int | None = None ): \"\"\" Initialize the UnflattenSamplesFromChannels transform. Args: time_dim (bool): If True, the temporal dimension is considered during unflattening. n_samples (int | None): The number of samples. n_timesteps (int | None): The number of time steps. n_channels (int | None): The number of channels per time step. Raises: Exception: If time_dim is True and fewer than two of n_channels, n_timesteps, and n_samples are provided. Exception: If time_dim is False and neither n_channels nor n_samples is provided. \"\"\" super () . __init__ ( True , 1 ) self . time_dim = time_dim if self . time_dim : if bool ( n_channels ) + bool ( n_timesteps ) + bool ( n_samples ) < 2 : msg = \"Two of n_channels, n_timesteps, and n_channels must be provided\" raise Exception ( msg ) if n_timesteps and n_channels : self . additional_info = { \"channels\" : n_channels , \"time\" : n_timesteps } elif n_timesteps and n_samples : self . additional_info = { \"time\" : n_timesteps , \"samples\" : n_samples } else : self . additional_info = { \"channels\" : n_channels , \"samples\" : n_samples } else : if n_channels is None and n_samples is None : msg = \"One of n_channels or n_samples must be provided\" raise Exception ( msg ) self . additional_info = { \"channels\" : n_channels } if n_channels else { \"samples\" : n_samples }","title":"__init__"},{"location":"data/#terratorch.datasets.transforms.UnflattenTemporalFromChannels","text":"Bases: ImageOnlyTransform UnflattenTemporalFromChannels is an image transformation that restores the temporal dimension from the channel dimension. This transform is typically applied after converting images to a channels-first format (e.g., after ToTensorV2) and rearranges the flattened temporal information back into separate time and channel dimensions. Source code in terratorch/datasets/transforms.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class UnflattenTemporalFromChannels ( ImageOnlyTransform ): \"\"\" UnflattenTemporalFromChannels is an image transformation that restores the temporal dimension from the channel dimension. This transform is typically applied after converting images to a channels-first format (e.g., after ToTensorV2) and rearranges the flattened temporal information back into separate time and channel dimensions. \"\"\" def __init__ ( self , n_timesteps : int | None = None , n_channels : int | None = None ): super () . __init__ ( True , 1 ) \"\"\" Initialize the UnflattenTemporalFromChannels transform. Args: n_timesteps (int | None): The number of time steps. Must be provided if n_channels is not provided. n_channels (int | None): The number of channels per time step. Must be provided if n_timesteps is not provided. Raises: Exception: If neither n_timesteps nor n_channels is provided. \"\"\" if n_timesteps is None and n_channels is None : msg = \"One of n_timesteps or n_channels must be provided\" raise Exception ( msg ) self . additional_info = { \"channels\" : n_channels } if n_channels else { \"time\" : n_timesteps } def apply ( self , img , ** params ): if len ( img . shape ) != N_DIMS_FLATTENED_TEMPORAL : msg = f \"Expected input temporal sequence to have { N_DIMS_FLATTENED_TEMPORAL } dimensions \\ , but got { len ( img . shape ) } \" raise Exception ( msg ) rearranged = rearrange ( img , \"(time channels) height width -> channels time height width\" , ** self . additional_info ) return rearranged def get_transform_init_args_names ( self ): return ( \"n_timesteps\" , \"n_channels\" )","title":"UnflattenTemporalFromChannels"},{"location":"encoder_decoder_factory/","text":"EncoderDecoderFactory # Check the Glossary for more information about the terms used in this page. The EncoderDecoderFactory is the main class used to instantiate and compose models for general tasks. This factory leverages the BACKBONE_REGISTRY , DECODER_REGISTRY and NECK_REGISTRY to compose models formed as encoder + decoder, with some optional glue in between provided by the necks. As most current models work this way, this is a particularly important factory, allowing for great flexibility in combining encoders and decoders from different sources. The factory allows arguments to be passed to the encoder, decoder and head. Arguments with the prefix backbone_ will be routed to the backbone constructor, with decoder_ and head_ working the same way. These are accepted dynamically and not checked. Any unused arguments will raise a ValueError . Both encoder and decoder may be passed as strings, in which case they will be looked in the respective registry, or as nn.Modules , in which case they will be used as is. In the second case, the factory assumes in good faith that the encoder or decoder which is passed conforms to the expected contract. Not all decoders will readily accept the raw output of the given encoder. This is where necks come in. Necks are a sequence of operations which are applied to the output of the encoder before it is passed to the decoder. They must be instances of Neck , which is a subclass of nn.Module , meaning they can even define new trainable parameters. The EncoderDecoderFactory returns a PixelWiseModel or a ScalarOutputModel depending on the task. terratorch.models.encoder_decoder_factory.EncoderDecoderFactory # Bases: ModelFactory Source code in terratorch/models/encoder_decoder_factory.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 @MODEL_FACTORY_REGISTRY . register class EncoderDecoderFactory ( ModelFactory ): def build_model ( self , task : str , backbone : str | nn . Module , decoder : str | nn . Module , num_classes : int | None = None , necks : list [ dict ] | None = None , aux_decoders : list [ AuxiliaryHead ] | None = None , rescale : bool = True , # noqa: FBT002, FBT001 peft_config : dict | None = None , ** kwargs , ) -> Model : \"\"\"Generic model factory that combines an encoder and decoder, together with a head, for a specific task. Further arguments to be passed to the backbone, decoder or head. They should be prefixed with `backbone_`, `decoder_` and `head_` respectively. Args: task (str): Task to be performed. Currently supports \"segmentation\", \"regression\" and \"classification\". backbone (str, nn.Module): Backbone to be used. If a string, will look for such models in the different registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it directly. The backbone should have and `out_channels` attribute and its `forward` should return a list[Tensor]. decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model. If a string, will look for such decoders in the different registries supported (internal terratorch registry, smp, ...). If an nn.Module, we expect it to expose a property `decoder.out_channels`. Pixel wise tasks will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\". num_classes (int, optional): Number of classes. None for regression tasks. necks (list[dict]): nn.Modules to be called in succession on encoder features before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry. Expects each one to have a key \"name\" and subsequent keys for arguments, if any. Defaults to None, which applies the identity function. aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead decoders to be added to the model. These decoders take the input from the encoder as well. rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True. peft_config (dict): Configuration options for using [PEFT](https://huggingface.co/docs/peft/index). The dictionary should have the following keys: - \"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available [here](https://huggingface.co/docs/peft/package_reference/peft_types#peft.PeftType). - \"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep. This should be used when the qkv matrices are merged together in a single linear layer and the PEFT method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in Q and V matrices). e.g. If using Prithvi this should be \"qkv\" - \"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to [PeftConfig](https://huggingface.co/docs/peft/package_reference/config#peft.PeftConfig) Returns: nn.Module: Full model with encoder, decoder and head. \"\"\" task = task . lower () if task not in SUPPORTED_TASKS : msg = f \"Task { task } not supported. Please choose one of { SUPPORTED_TASKS } \" raise NotImplementedError ( msg ) backbone_kwargs , kwargs = extract_prefix_keys ( kwargs , \"backbone_\" ) backbone = _get_backbone ( backbone , ** backbone_kwargs ) # If patch size is not provided in the config or by the model, it might lead to errors due to irregular images. patch_size = backbone_kwargs . get ( \"patch_size\" , None ) if patch_size is None : # Infer patch size from model by checking all backbone modules for module in backbone . modules (): if hasattr ( module , \"patch_size\" ): patch_size = module . patch_size break padding = backbone_kwargs . get ( \"padding\" , \"reflect\" ) if peft_config is not None : if not backbone_kwargs . get ( \"pretrained\" , False ): msg = ( \"You are using PEFT without a pretrained backbone. If you are loading a checkpoint afterwards \" \"this is probably fine, but if you are training a model check the backbone_pretrained parameter.\" ) warnings . warn ( msg , stacklevel = 1 ) backbone = get_peft_backbone ( peft_config , backbone ) try : out_channels = backbone . out_channels except AttributeError as e : msg = \"backbone must have out_channels attribute\" raise AttributeError ( msg ) from e if necks is None : necks = [] neck_list , channel_list = build_neck_list ( necks , out_channels ) # some decoders already include a head # for these, we pass the num_classes to them # others dont include a head # for those, we dont pass num_classes decoder_kwargs , kwargs = extract_prefix_keys ( kwargs , \"decoder_\" ) head_kwargs , kwargs = extract_prefix_keys ( kwargs , \"head_\" ) decoder , head_kwargs , decoder_includes_head = _get_decoder_and_head_kwargs ( decoder , channel_list , decoder_kwargs , head_kwargs , num_classes = num_classes ) if aux_decoders is None : _check_all_args_used ( kwargs ) return _build_appropriate_model ( task , backbone , decoder , head_kwargs , patch_size = patch_size , padding = padding , necks = neck_list , decoder_includes_head = decoder_includes_head , rescale = rescale , ) to_be_aux_decoders : list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] = [] for aux_decoder in aux_decoders : args = aux_decoder . decoder_args if aux_decoder . decoder_args else {} aux_decoder_kwargs , args = extract_prefix_keys ( args , \"decoder_\" ) aux_head_kwargs , args = extract_prefix_keys ( args , \"head_\" ) aux_decoder_instance , aux_head_kwargs , aux_decoder_includes_head = _get_decoder_and_head_kwargs ( aux_decoder . decoder , channel_list , aux_decoder_kwargs , aux_head_kwargs , num_classes = num_classes ) to_be_aux_decoders . append ( AuxiliaryHeadWithDecoderWithoutInstantiatedHead ( aux_decoder . name , aux_decoder_instance , aux_head_kwargs ) ) _check_all_args_used ( args ) _check_all_args_used ( kwargs ) return _build_appropriate_model ( task , backbone , decoder , head_kwargs , patch_size = patch_size , padding = padding , necks = neck_list , decoder_includes_head = decoder_includes_head , rescale = rescale , auxiliary_heads = to_be_aux_decoders , ) build_model ( task , backbone , decoder , num_classes = None , necks = None , aux_decoders = None , rescale = True , peft_config = None , ** kwargs ) # Generic model factory that combines an encoder and decoder, together with a head, for a specific task. Further arguments to be passed to the backbone, decoder or head. They should be prefixed with backbone_ , decoder_ and head_ respectively. Parameters: task ( str ) \u2013 Task to be performed. Currently supports \"segmentation\", \"regression\" and \"classification\". backbone ( ( str , Module ) ) \u2013 Backbone to be used. If a string, will look for such models in the different registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it directly. The backbone should have and out_channels attribute and its forward should return a list[Tensor]. decoder ( Union [ str , Module ] ) \u2013 Decoder to be used for the segmentation model. If a string, will look for such decoders in the different registries supported (internal terratorch registry, smp, ...). If an nn.Module, we expect it to expose a property decoder.out_channels . Pixel wise tasks will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\". num_classes ( int , default: None ) \u2013 Number of classes. None for regression tasks. necks ( list [ dict ] , default: None ) \u2013 nn.Modules to be called in succession on encoder features before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry. Expects each one to have a key \"name\" and subsequent keys for arguments, if any. Defaults to None, which applies the identity function. aux_decoders ( list [ AuxiliaryHead ] | None , default: None ) \u2013 List of AuxiliaryHead decoders to be added to the model. These decoders take the input from the encoder as well. rescale ( bool , default: True ) \u2013 Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True. peft_config ( dict , default: None ) \u2013 Configuration options for using PEFT . The dictionary should have the following keys: \"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available here . \"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep. This should be used when the qkv matrices are merged together in a single linear layer and the PEFT method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in Q and V matrices). e.g. If using Prithvi this should be \"qkv\" \"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to PeftConfig Returns: Model \u2013 nn.Module: Full model with encoder, decoder and head. Source code in terratorch/models/encoder_decoder_factory.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def build_model ( self , task : str , backbone : str | nn . Module , decoder : str | nn . Module , num_classes : int | None = None , necks : list [ dict ] | None = None , aux_decoders : list [ AuxiliaryHead ] | None = None , rescale : bool = True , # noqa: FBT002, FBT001 peft_config : dict | None = None , ** kwargs , ) -> Model : \"\"\"Generic model factory that combines an encoder and decoder, together with a head, for a specific task. Further arguments to be passed to the backbone, decoder or head. They should be prefixed with `backbone_`, `decoder_` and `head_` respectively. Args: task (str): Task to be performed. Currently supports \"segmentation\", \"regression\" and \"classification\". backbone (str, nn.Module): Backbone to be used. If a string, will look for such models in the different registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it directly. The backbone should have and `out_channels` attribute and its `forward` should return a list[Tensor]. decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model. If a string, will look for such decoders in the different registries supported (internal terratorch registry, smp, ...). If an nn.Module, we expect it to expose a property `decoder.out_channels`. Pixel wise tasks will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\". num_classes (int, optional): Number of classes. None for regression tasks. necks (list[dict]): nn.Modules to be called in succession on encoder features before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry. Expects each one to have a key \"name\" and subsequent keys for arguments, if any. Defaults to None, which applies the identity function. aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead decoders to be added to the model. These decoders take the input from the encoder as well. rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True. peft_config (dict): Configuration options for using [PEFT](https://huggingface.co/docs/peft/index). The dictionary should have the following keys: - \"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available [here](https://huggingface.co/docs/peft/package_reference/peft_types#peft.PeftType). - \"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep. This should be used when the qkv matrices are merged together in a single linear layer and the PEFT method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in Q and V matrices). e.g. If using Prithvi this should be \"qkv\" - \"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to [PeftConfig](https://huggingface.co/docs/peft/package_reference/config#peft.PeftConfig) Returns: nn.Module: Full model with encoder, decoder and head. \"\"\" task = task . lower () if task not in SUPPORTED_TASKS : msg = f \"Task { task } not supported. Please choose one of { SUPPORTED_TASKS } \" raise NotImplementedError ( msg ) backbone_kwargs , kwargs = extract_prefix_keys ( kwargs , \"backbone_\" ) backbone = _get_backbone ( backbone , ** backbone_kwargs ) # If patch size is not provided in the config or by the model, it might lead to errors due to irregular images. patch_size = backbone_kwargs . get ( \"patch_size\" , None ) if patch_size is None : # Infer patch size from model by checking all backbone modules for module in backbone . modules (): if hasattr ( module , \"patch_size\" ): patch_size = module . patch_size break padding = backbone_kwargs . get ( \"padding\" , \"reflect\" ) if peft_config is not None : if not backbone_kwargs . get ( \"pretrained\" , False ): msg = ( \"You are using PEFT without a pretrained backbone. If you are loading a checkpoint afterwards \" \"this is probably fine, but if you are training a model check the backbone_pretrained parameter.\" ) warnings . warn ( msg , stacklevel = 1 ) backbone = get_peft_backbone ( peft_config , backbone ) try : out_channels = backbone . out_channels except AttributeError as e : msg = \"backbone must have out_channels attribute\" raise AttributeError ( msg ) from e if necks is None : necks = [] neck_list , channel_list = build_neck_list ( necks , out_channels ) # some decoders already include a head # for these, we pass the num_classes to them # others dont include a head # for those, we dont pass num_classes decoder_kwargs , kwargs = extract_prefix_keys ( kwargs , \"decoder_\" ) head_kwargs , kwargs = extract_prefix_keys ( kwargs , \"head_\" ) decoder , head_kwargs , decoder_includes_head = _get_decoder_and_head_kwargs ( decoder , channel_list , decoder_kwargs , head_kwargs , num_classes = num_classes ) if aux_decoders is None : _check_all_args_used ( kwargs ) return _build_appropriate_model ( task , backbone , decoder , head_kwargs , patch_size = patch_size , padding = padding , necks = neck_list , decoder_includes_head = decoder_includes_head , rescale = rescale , ) to_be_aux_decoders : list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] = [] for aux_decoder in aux_decoders : args = aux_decoder . decoder_args if aux_decoder . decoder_args else {} aux_decoder_kwargs , args = extract_prefix_keys ( args , \"decoder_\" ) aux_head_kwargs , args = extract_prefix_keys ( args , \"head_\" ) aux_decoder_instance , aux_head_kwargs , aux_decoder_includes_head = _get_decoder_and_head_kwargs ( aux_decoder . decoder , channel_list , aux_decoder_kwargs , aux_head_kwargs , num_classes = num_classes ) to_be_aux_decoders . append ( AuxiliaryHeadWithDecoderWithoutInstantiatedHead ( aux_decoder . name , aux_decoder_instance , aux_head_kwargs ) ) _check_all_args_used ( args ) _check_all_args_used ( kwargs ) return _build_appropriate_model ( task , backbone , decoder , head_kwargs , patch_size = patch_size , padding = padding , necks = neck_list , decoder_includes_head = decoder_includes_head , rescale = rescale , auxiliary_heads = to_be_aux_decoders , ) terratorch.models.pixel_wise_model.PixelWiseModel # terratorch.models.scalar_output_model.ScalarOutputModel # Encoders # To be a valid encoder, an object must be an nn.Module and contain an attribute out_channels , basically a list of the channel dimensions corresponding to the features it returns. The forward method of any encoder should return a list of torch.Tensor . In [ 19 ] : backbone = BACKBONE_REGISTRY.build ( \"prithvi_eo_v2_300\" , pretrained = True ) In [ 20 ] : import numpy as np In [ 21 ] : import torch In [ 22 ] : input_image = torch.tensor ( np.random.rand ( 1 ,6,224,224 ) .astype ( \"float32\" )) In [ 23 ] : output = backbone.forward ( input_image ) In [ 24 ] : [ item.shape for item in output ] Out [ 24 ] : [ torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ])] Necks # Necks are the connectors between encoder and decoder. They can perform operations such as selecting elements from the output of the encoder ( SelectIndices ), reshaping the outputs of ViTs so they are compatible with CNNs ( ReshapeTokensToImage ), amongst others. Necks are nn.Modules , with an additional method process_channel_list which informs the EncoderDecoderFactory about how it will alter the channel list provided by encoder.out_channels . terratorch.models.necks.Neck # Bases: ABC , Module Base class for Neck A neck must must implement self.process_channel_list which returns the new channel list. Source code in terratorch/models/necks.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Neck ( ABC , nn . Module ): \"\"\"Base class for Neck A neck must must implement `self.process_channel_list` which returns the new channel list. \"\"\" def __init__ ( self , channel_list : list [ int ]) -> None : super () . __init__ () self . channel_list = channel_list @abstractmethod def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: return channel_list @abstractmethod def forward ( self , channel_list : list [ torch . Tensor ]) -> list [ torch . Tensor ]: ... terratorch.models.necks.SelectIndices # Bases: Neck Source code in terratorch/models/necks.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @TERRATORCH_NECK_REGISTRY . register class SelectIndices ( Neck ): def __init__ ( self , channel_list : list [ int ], indices : list [ int ]): \"\"\"Select indices from the embedding list Args: indices (list[int]): list of indices to select. \"\"\" super () . __init__ ( channel_list ) self . indices = indices def forward ( self , features : list [ torch . Tensor ]) -> list [ torch . Tensor ]: features = [ features [ i ] for i in self . indices ] return features def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: channel_list = [ channel_list [ i ] for i in self . indices ] return channel_list __init__ ( channel_list , indices ) # Select indices from the embedding list Parameters: indices ( list [ int ] ) \u2013 list of indices to select. Source code in terratorch/models/necks.py 33 34 35 36 37 38 39 40 def __init__ ( self , channel_list : list [ int ], indices : list [ int ]): \"\"\"Select indices from the embedding list Args: indices (list[int]): list of indices to select. \"\"\" super () . __init__ ( channel_list ) self . indices = indices terratorch.models.necks.PermuteDims # Bases: Neck Source code in terratorch/models/necks.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @TERRATORCH_NECK_REGISTRY . register class PermuteDims ( Neck ): def __init__ ( self , channel_list : list [ int ], new_order : list [ int ]): \"\"\"Permute dimensions of each element in the embedding list Args: new_order (list[int]): list of indices to be passed to tensor.permute() \"\"\" super () . __init__ ( channel_list ) self . new_order = new_order def forward ( self , features : list [ torch . Tensor ]) -> list [ torch . Tensor ]: features = [ feat . permute ( * self . new_order ) . contiguous () for feat in features ] return features def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: return super () . process_channel_list ( channel_list ) __init__ ( channel_list , new_order ) # Permute dimensions of each element in the embedding list Parameters: new_order ( list [ int ] ) \u2013 list of indices to be passed to tensor.permute() Source code in terratorch/models/necks.py 52 53 54 55 56 57 58 59 def __init__ ( self , channel_list : list [ int ], new_order : list [ int ]): \"\"\"Permute dimensions of each element in the embedding list Args: new_order (list[int]): list of indices to be passed to tensor.permute() \"\"\" super () . __init__ ( channel_list ) self . new_order = new_order terratorch.models.necks.InterpolateToPyramidal # Bases: Neck Source code in terratorch/models/necks.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 @TERRATORCH_NECK_REGISTRY . register class InterpolateToPyramidal ( Neck ): def __init__ ( self , channel_list : list [ int ], scale_factor : int = 2 , mode : str = \"nearest\" ): \"\"\"Spatially interpolate embeddings so that embedding[i - 1] is scale_factor times larger than embedding[i] Useful to make non-pyramidal backbones compatible with hierarachical ones Args: scale_factor (int): Amount to scale embeddings by each layer. Defaults to 2. mode (str): Interpolation mode to be passed to torch.nn.functional.interpolate. Defaults to 'nearest'. \"\"\" super () . __init__ ( channel_list ) self . scale_factor = scale_factor self . mode = mode def forward ( self , features : list [ torch . Tensor ]) -> list [ torch . Tensor ]: out = [] scale_exponents = list ( range ( len ( features ), 0 , - 1 )) for x , exponent in zip ( features , scale_exponents , strict = True ): out . append ( F . interpolate ( x , scale_factor = self . scale_factor ** exponent , mode = self . mode )) return out def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: return super () . process_channel_list ( channel_list ) __init__ ( channel_list , scale_factor = 2 , mode = 'nearest' ) # Spatially interpolate embeddings so that embedding[i - 1] is scale_factor times larger than embedding[i] Useful to make non-pyramidal backbones compatible with hierarachical ones Args: scale_factor (int): Amount to scale embeddings by each layer. Defaults to 2. mode (str): Interpolation mode to be passed to torch.nn.functional.interpolate. Defaults to 'nearest'. Source code in terratorch/models/necks.py 70 71 72 73 74 75 76 77 78 79 80 def __init__ ( self , channel_list : list [ int ], scale_factor : int = 2 , mode : str = \"nearest\" ): \"\"\"Spatially interpolate embeddings so that embedding[i - 1] is scale_factor times larger than embedding[i] Useful to make non-pyramidal backbones compatible with hierarachical ones Args: scale_factor (int): Amount to scale embeddings by each layer. Defaults to 2. mode (str): Interpolation mode to be passed to torch.nn.functional.interpolate. Defaults to 'nearest'. \"\"\" super () . __init__ ( channel_list ) self . scale_factor = scale_factor self . mode = mode terratorch.models.necks.MaxpoolToPyramidal # Bases: Neck Source code in terratorch/models/necks.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 @TERRATORCH_NECK_REGISTRY . register class MaxpoolToPyramidal ( Neck ): def __init__ ( self , channel_list : list [ int ], kernel_size : int = 2 ): \"\"\"Spatially downsample embeddings so that embedding[i - 1] is scale_factor times smaller than embedding[i] Useful to make non-pyramidal backbones compatible with hierarachical ones Args: kernel_size (int). Base kernel size to use for maxpool. Defaults to 2. \"\"\" super () . __init__ ( channel_list ) self . kernel_size = kernel_size def forward ( self , features : list [ torch . Tensor ]) -> list [ torch . Tensor ]: out = [] scale_exponents = list ( range ( len ( features ))) for x , exponent in zip ( features , scale_exponents , strict = True ): if exponent == 0 : out . append ( x . clone ()) else : out . append ( F . max_pool2d ( x , kernel_size = self . kernel_size ** exponent )) return out def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: return super () . process_channel_list ( channel_list ) __init__ ( channel_list , kernel_size = 2 ) # Spatially downsample embeddings so that embedding[i - 1] is scale_factor times smaller than embedding[i] Useful to make non-pyramidal backbones compatible with hierarachical ones Args: kernel_size (int). Base kernel size to use for maxpool. Defaults to 2. Source code in terratorch/models/necks.py 96 97 98 99 100 101 102 103 104 def __init__ ( self , channel_list : list [ int ], kernel_size : int = 2 ): \"\"\"Spatially downsample embeddings so that embedding[i - 1] is scale_factor times smaller than embedding[i] Useful to make non-pyramidal backbones compatible with hierarachical ones Args: kernel_size (int). Base kernel size to use for maxpool. Defaults to 2. \"\"\" super () . __init__ ( channel_list ) self . kernel_size = kernel_size terratorch.models.necks.ReshapeTokensToImage # Bases: Neck Source code in terratorch/models/necks.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 @TERRATORCH_NECK_REGISTRY . register class ReshapeTokensToImage ( Neck ): def __init__ ( self , channel_list : list [ int ], remove_cls_token = True , effective_time_dim : int = 1 ): # noqa: FBT002 \"\"\"Reshape output of transformer encoder so it can be passed to a conv net. Args: remove_cls_token (bool, optional): Whether to remove the cls token from the first position. Defaults to True. effective_time_dim (int, optional): The effective temporal dimension the transformer processes. For a ViT, his will be given by `num_frames // tubelet size`. This is used to determine the temporal dimension of the embedding, which is concatenated with the embedding dimension. For example: - A model which processes 1 frame with a tubelet size of 1 has an effective_time_dim of 1. The embedding produced by this model has embedding size embed_dim * 1. - A model which processes 3 frames with a tubelet size of 1 has an effective_time_dim of 3. The embedding produced by this model has embedding size embed_dim * 3. - A model which processes 12 frames with a tubelet size of 4 has an effective_time_dim of 3. The embedding produced by this model has an embedding size embed_dim * 3. Defaults to 1. \"\"\" super () . __init__ ( channel_list ) self . remove_cls_token = remove_cls_token self . effective_time_dim = effective_time_dim def collapse_dims ( self , x ): \"\"\" When the encoder output has more than 3 dimensions, is necessary to reshape it. \"\"\" shape = x . shape batch = x . shape [ 0 ] e = x . shape [ - 1 ] collapsed_dim = np . prod ( x . shape [ 1 : - 1 ]) return x . reshape ( batch , collapsed_dim , e ) def forward ( self , features : list [ torch . Tensor ]) -> list [ torch . Tensor ]: out = [] for x in features : if self . remove_cls_token : x_no_token = x [:, 1 :, :] else : x_no_token = x x_no_token = self . collapse_dims ( x_no_token ) number_of_tokens = x_no_token . shape [ 1 ] tokens_per_timestep = number_of_tokens // self . effective_time_dim h = int ( math . sqrt ( tokens_per_timestep )) encoded = rearrange ( x_no_token , \"batch (t h w) e -> batch (t e) h w\" , batch = x_no_token . shape [ 0 ], t = self . effective_time_dim , h = h , ) out . append ( encoded ) return out def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: return super () . process_channel_list ( channel_list ) __init__ ( channel_list , remove_cls_token = True , effective_time_dim = 1 ) # Reshape output of transformer encoder so it can be passed to a conv net. Parameters: remove_cls_token ( bool , default: True ) \u2013 Whether to remove the cls token from the first position. Defaults to True. effective_time_dim ( int , default: 1 ) \u2013 The effective temporal dimension the transformer processes. For a ViT, his will be given by num_frames // tubelet size . This is used to determine the temporal dimension of the embedding, which is concatenated with the embedding dimension. For example: - A model which processes 1 frame with a tubelet size of 1 has an effective_time_dim of 1. The embedding produced by this model has embedding size embed_dim * 1. - A model which processes 3 frames with a tubelet size of 1 has an effective_time_dim of 3. The embedding produced by this model has embedding size embed_dim * 3. - A model which processes 12 frames with a tubelet size of 4 has an effective_time_dim of 3. The embedding produced by this model has an embedding size embed_dim * 3. Defaults to 1. Source code in terratorch/models/necks.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def __init__ ( self , channel_list : list [ int ], remove_cls_token = True , effective_time_dim : int = 1 ): # noqa: FBT002 \"\"\"Reshape output of transformer encoder so it can be passed to a conv net. Args: remove_cls_token (bool, optional): Whether to remove the cls token from the first position. Defaults to True. effective_time_dim (int, optional): The effective temporal dimension the transformer processes. For a ViT, his will be given by `num_frames // tubelet size`. This is used to determine the temporal dimension of the embedding, which is concatenated with the embedding dimension. For example: - A model which processes 1 frame with a tubelet size of 1 has an effective_time_dim of 1. The embedding produced by this model has embedding size embed_dim * 1. - A model which processes 3 frames with a tubelet size of 1 has an effective_time_dim of 3. The embedding produced by this model has embedding size embed_dim * 3. - A model which processes 12 frames with a tubelet size of 4 has an effective_time_dim of 3. The embedding produced by this model has an embedding size embed_dim * 3. Defaults to 1. \"\"\" super () . __init__ ( channel_list ) self . remove_cls_token = remove_cls_token self . effective_time_dim = effective_time_dim collapse_dims ( x ) # When the encoder output has more than 3 dimensions, is necessary to reshape it. Source code in terratorch/models/necks.py 145 146 147 148 149 150 151 152 153 154 155 def collapse_dims ( self , x ): \"\"\" When the encoder output has more than 3 dimensions, is necessary to reshape it. \"\"\" shape = x . shape batch = x . shape [ 0 ] e = x . shape [ - 1 ] collapsed_dim = np . prod ( x . shape [ 1 : - 1 ]) return x . reshape ( batch , collapsed_dim , e ) terratorch.models.necks.AddBottleneckLayer # Bases: Neck Add a layer that reduces the channel dimension of the final embedding by half, and concatenates it Useful for compatibility with some smp decoders. Source code in terratorch/models/necks.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 @TERRATORCH_NECK_REGISTRY . register class AddBottleneckLayer ( Neck ): \"\"\"Add a layer that reduces the channel dimension of the final embedding by half, and concatenates it Useful for compatibility with some smp decoders. \"\"\" def __init__ ( self , channel_list : list [ int ]): super () . __init__ ( channel_list ) self . bottleneck = nn . Conv2d ( channel_list [ - 1 ], channel_list [ - 1 ] // 2 , kernel_size = 1 ) def forward ( self , features : list [ torch . Tensor ]) -> list [ torch . Tensor ]: new_embedding = self . bottleneck ( features [ - 1 ]) features . append ( new_embedding ) return features def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: return [ * channel_list , channel_list [ - 1 ] // 2 ] terratorch.models.necks.LearnedInterpolateToPyramidal # Bases: Neck Use learned convolutions to transform the output of a non-pyramidal encoder into pyramidal ones Always requires exactly 4 embeddings Source code in terratorch/models/necks.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 @TERRATORCH_NECK_REGISTRY . register class LearnedInterpolateToPyramidal ( Neck ): \"\"\"Use learned convolutions to transform the output of a non-pyramidal encoder into pyramidal ones Always requires exactly 4 embeddings \"\"\" def __init__ ( self , channel_list : list [ int ]): super () . __init__ ( channel_list ) if len ( channel_list ) != 4 : msg = \"This class can only handle exactly 4 input embeddings\" raise Exception ( msg ) self . fpn1 = nn . Sequential ( nn . ConvTranspose2d ( channel_list [ 0 ], channel_list [ 0 ] // 2 , 2 , 2 ), nn . BatchNorm2d ( channel_list [ 0 ] // 2 ), nn . GELU (), nn . ConvTranspose2d ( channel_list [ 0 ] // 2 , channel_list [ 0 ] // 4 , 2 , 2 ), ) self . fpn2 = nn . Sequential ( nn . ConvTranspose2d ( channel_list [ 1 ], channel_list [ 1 ] // 2 , 2 , 2 )) self . fpn3 = nn . Sequential ( nn . Identity ()) self . fpn4 = nn . Sequential ( nn . MaxPool2d ( kernel_size = 2 , stride = 2 )) self . embedding_dim = [ channel_list [ 0 ] // 4 , channel_list [ 1 ] // 2 , channel_list [ 2 ], channel_list [ 3 ]] def forward ( self , features : list [ torch . Tensor ]) -> list [ torch . Tensor ]: scaled_inputs = [] scaled_inputs . append ( self . fpn1 ( features [ 0 ])) scaled_inputs . append ( self . fpn2 ( features [ 1 ])) scaled_inputs . append ( self . fpn3 ( features [ 2 ])) scaled_inputs . append ( self . fpn4 ( features [ 3 ])) return scaled_inputs def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: return [ channel_list [ 0 ] // 4 , channel_list [ 1 ] // 2 , channel_list [ 2 ], channel_list [ 3 ]] Decoders # To be a valid decoder, an object must be an nn.Module with an attribute out_channels , an int representing the channel dimension of the output. The first argument to its constructor will be a list of channel dimensions it should expect as input. It's forward method should accept a list of embeddings. Heads # Most decoders require a final head to be added for a specific task (e.g. semantic segmentation vs pixel wise regression). Those registries producing decoders that dont require a head must expose the attribute includes_head=True so that a head is not added. Decoders passed as nn.Modules which do not require a head must expose the same attribute themselves. terratorch.models.heads.classification_head.ClassificationHead # Bases: Module Classification head Source code in terratorch/models/heads/classification_head.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class ClassificationHead ( nn . Module ): \"\"\"Classification head\"\"\" # how to allow cls token? def __init__ ( self , in_dim : int , num_classes : int , dim_list : list [ int ] | None = None , dropout : float = 0 , linear_after_pool : bool = False , ) -> None : \"\"\"Constructor Args: in_dim (int): Input dimensionality num_classes (int): Number of output classes dim_list (list[int] | None, optional): List with number of dimensions for each Linear layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. linear_after_pool (bool, optional): Apply pooling first, then apply the linear layer. Defaults to False \"\"\" super () . __init__ () self . num_classes = num_classes self . linear_after_pool = linear_after_pool if dim_list is None : pre_head = nn . Identity () else : def block ( in_dim , out_dim ): return nn . Sequential ( nn . Linear ( in_features = in_dim , out_features = out_dim ), nn . ReLU ()) dim_list = [ in_dim , * dim_list ] pre_head = nn . Sequential ( * [ block ( dim_list [ i ], dim_list [ i + 1 ]) for i in range ( len ( dim_list ) - 1 )]) in_dim = dim_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Linear ( in_features = in_dim , out_features = num_classes ), ) def forward ( self , x : Tensor ): x = x . reshape ( x . shape [ 0 ], x . shape [ 1 ], - 1 ) . permute ( 0 , 2 , 1 ) if self . linear_after_pool : x = x . mean ( axis = 1 ) out = self . head ( x ) else : x = self . head ( x ) out = x . mean ( axis = 1 ) return out __init__ ( in_dim , num_classes , dim_list = None , dropout = 0 , linear_after_pool = False ) # Constructor Parameters: in_dim ( int ) \u2013 Input dimensionality num_classes ( int ) \u2013 Number of output classes dim_list ( list [ int ] | None , default: None ) \u2013 List with number of dimensions for each Linear layer to be created. Defaults to None. dropout ( float , default: 0 ) \u2013 Dropout value to apply. Defaults to 0. linear_after_pool ( bool , default: False ) \u2013 Apply pooling first, then apply the linear layer. Defaults to False Source code in terratorch/models/heads/classification_head.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , in_dim : int , num_classes : int , dim_list : list [ int ] | None = None , dropout : float = 0 , linear_after_pool : bool = False , ) -> None : \"\"\"Constructor Args: in_dim (int): Input dimensionality num_classes (int): Number of output classes dim_list (list[int] | None, optional): List with number of dimensions for each Linear layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. linear_after_pool (bool, optional): Apply pooling first, then apply the linear layer. Defaults to False \"\"\" super () . __init__ () self . num_classes = num_classes self . linear_after_pool = linear_after_pool if dim_list is None : pre_head = nn . Identity () else : def block ( in_dim , out_dim ): return nn . Sequential ( nn . Linear ( in_features = in_dim , out_features = out_dim ), nn . ReLU ()) dim_list = [ in_dim , * dim_list ] pre_head = nn . Sequential ( * [ block ( dim_list [ i ], dim_list [ i + 1 ]) for i in range ( len ( dim_list ) - 1 )]) in_dim = dim_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Linear ( in_features = in_dim , out_features = num_classes ), ) terratorch.models.heads.regression_head.RegressionHead # Bases: Module Regression head Source code in terratorch/models/heads/regression_head.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class RegressionHead ( nn . Module ): \"\"\"Regression head\"\"\" def __init__ ( self , in_channels : int , final_act : nn . Module | str | None = None , learned_upscale_layers : int = 0 , channel_list : list [ int ] | None = None , batch_norm : bool = True , dropout : float = 0 , ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels final_act (nn.Module | None, optional): Final activation to be applied. Defaults to None. learned_upscale_layers (int, optional): Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0. channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. batch_norm (bool, optional): Whether to apply batch norm. Defaults to True. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . learned_upscale_layers = learned_upscale_layers self . final_act = final_act if final_act else nn . Identity () if isinstance ( final_act , str ): module_name , class_name = final_act . rsplit ( \".\" , 1 ) target_class = getattr ( importlib . import_module ( module_name ), class_name ) self . final_act = target_class () pre_layers = [] if learned_upscale_layers != 0 : learned_upscale = nn . Sequential ( * [ PixelShuffleUpscale ( in_channels ) for _ in range ( self . learned_upscale_layers )] ) pre_layers . append ( learned_upscale ) if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 , bias = False ), nn . BatchNorm2d ( out_channels ), nn . ReLU ( inplace = True ), ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] pre_layers . append ( pre_head ) dropout = nn . Dropout2d ( dropout ) final_layer = nn . Conv2d ( in_channels = in_channels , out_channels = 1 , kernel_size = 1 ) self . head = nn . Sequential ( * [ * pre_layers , dropout , final_layer ]) def forward ( self , x ): output = self . head ( x ) return self . final_act ( output ) __init__ ( in_channels , final_act = None , learned_upscale_layers = 0 , channel_list = None , batch_norm = True , dropout = 0 ) # Constructor Parameters: in_channels ( int ) \u2013 Number of input channels final_act ( Module | None , default: None ) \u2013 Final activation to be applied. Defaults to None. learned_upscale_layers ( int , default: 0 ) \u2013 Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0. channel_list ( list [ int ] | None , default: None ) \u2013 List with number of channels for each Conv layer to be created. Defaults to None. batch_norm ( bool , default: True ) \u2013 Whether to apply batch norm. Defaults to True. dropout ( float , default: 0 ) \u2013 Dropout value to apply. Defaults to 0. Source code in terratorch/models/heads/regression_head.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , in_channels : int , final_act : nn . Module | str | None = None , learned_upscale_layers : int = 0 , channel_list : list [ int ] | None = None , batch_norm : bool = True , dropout : float = 0 , ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels final_act (nn.Module | None, optional): Final activation to be applied. Defaults to None. learned_upscale_layers (int, optional): Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0. channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. batch_norm (bool, optional): Whether to apply batch norm. Defaults to True. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . learned_upscale_layers = learned_upscale_layers self . final_act = final_act if final_act else nn . Identity () if isinstance ( final_act , str ): module_name , class_name = final_act . rsplit ( \".\" , 1 ) target_class = getattr ( importlib . import_module ( module_name ), class_name ) self . final_act = target_class () pre_layers = [] if learned_upscale_layers != 0 : learned_upscale = nn . Sequential ( * [ PixelShuffleUpscale ( in_channels ) for _ in range ( self . learned_upscale_layers )] ) pre_layers . append ( learned_upscale ) if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 , bias = False ), nn . BatchNorm2d ( out_channels ), nn . ReLU ( inplace = True ), ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] pre_layers . append ( pre_head ) dropout = nn . Dropout2d ( dropout ) final_layer = nn . Conv2d ( in_channels = in_channels , out_channels = 1 , kernel_size = 1 ) self . head = nn . Sequential ( * [ * pre_layers , dropout , final_layer ]) terratorch.models.heads.segmentation_head.SegmentationHead # Bases: Module Segmentation head Source code in terratorch/models/heads/segmentation_head.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class SegmentationHead ( nn . Module ): \"\"\"Segmentation head\"\"\" def __init__ ( self , in_channels : int , num_classes : int , channel_list : list [ int ] | None = None , dropout : float = 0 ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels num_classes (int): Number of output classes channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . num_classes = num_classes if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 ), nn . ReLU () ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Conv2d ( in_channels = in_channels , out_channels = num_classes , kernel_size = 1 , ), ) def forward ( self , x ): return self . head ( x ) __init__ ( in_channels , num_classes , channel_list = None , dropout = 0 ) # Constructor Parameters: in_channels ( int ) \u2013 Number of input channels num_classes ( int ) \u2013 Number of output classes channel_list ( list [ int ] | None , default: None ) \u2013 List with number of channels for each Conv layer to be created. Defaults to None. dropout ( float , default: 0 ) \u2013 Dropout value to apply. Defaults to 0. Source code in terratorch/models/heads/segmentation_head.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , in_channels : int , num_classes : int , channel_list : list [ int ] | None = None , dropout : float = 0 ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels num_classes (int): Number of output classes channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . num_classes = num_classes if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 ), nn . ReLU () ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Conv2d ( in_channels = in_channels , out_channels = num_classes , kernel_size = 1 , ), ) Decoder compatibilities # Not all encoders and decoders are compatible. Below we include some caveats. Some decoders expect pyramidal outputs, but some encoders do not produce such outputs (e.g. vanilla ViT models). In this case, the InterpolateToPyramidal , MaxpoolToPyramidal and LearnedInterpolateToPyramidal necks may be particularly useful. SMP decoders # Not all decoders are guaranteed to work with all encoders without additional necks. Please check smp documentation to understand the embedding spatial dimensions expected by each decoder. In particular, smp seems to assume the first feature in the passed feature list has the same spatial resolution as the input, which may not always be true, and may break some decoders. In addition, for some decoders, the final 2 features have the same spatial resolution. Adding the AddBottleneckLayer neck will make this compatible. Some smp decoders require additional parameters, such as decoder_channels . These must be passed through the factory. In the case of decoder_channels , it would be passed as decoder_decoder_channels (the first decoder_ routes the parameter to the decoder, where it is passed as decoder_channels ). MMSegmentation decoders # MMSegmentation decoders are available through the BACKBONE_REGISTRY. Warning MMSegmentation currently requires mmcv==2.1.0 . Pre-built wheels for this only exist for torch==2.1.0 . In order to use mmseg without building from source, you must downgrade your torch to this version. Install mmseg with: pip install -U openmim mim install mmengine mim install mmcv == 2 .1.0 pip install regex ftfy mmsegmentation We provide access to mmseg decoders as an external source of decoders, but are not directly responsible for the maintainence of that library. Some mmseg decoders require the parameter in_index , which performs the same function as the SelectIndices neck. For use for pixel wise regression, mmseg decoders should take num_classes=1 .","title":"EncoderDecoderFactory"},{"location":"encoder_decoder_factory/#encoderdecoderfactory","text":"Check the Glossary for more information about the terms used in this page. The EncoderDecoderFactory is the main class used to instantiate and compose models for general tasks. This factory leverages the BACKBONE_REGISTRY , DECODER_REGISTRY and NECK_REGISTRY to compose models formed as encoder + decoder, with some optional glue in between provided by the necks. As most current models work this way, this is a particularly important factory, allowing for great flexibility in combining encoders and decoders from different sources. The factory allows arguments to be passed to the encoder, decoder and head. Arguments with the prefix backbone_ will be routed to the backbone constructor, with decoder_ and head_ working the same way. These are accepted dynamically and not checked. Any unused arguments will raise a ValueError . Both encoder and decoder may be passed as strings, in which case they will be looked in the respective registry, or as nn.Modules , in which case they will be used as is. In the second case, the factory assumes in good faith that the encoder or decoder which is passed conforms to the expected contract. Not all decoders will readily accept the raw output of the given encoder. This is where necks come in. Necks are a sequence of operations which are applied to the output of the encoder before it is passed to the decoder. They must be instances of Neck , which is a subclass of nn.Module , meaning they can even define new trainable parameters. The EncoderDecoderFactory returns a PixelWiseModel or a ScalarOutputModel depending on the task.","title":"EncoderDecoderFactory"},{"location":"encoder_decoder_factory/#terratorch.models.encoder_decoder_factory.EncoderDecoderFactory","text":"Bases: ModelFactory Source code in terratorch/models/encoder_decoder_factory.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 @MODEL_FACTORY_REGISTRY . register class EncoderDecoderFactory ( ModelFactory ): def build_model ( self , task : str , backbone : str | nn . Module , decoder : str | nn . Module , num_classes : int | None = None , necks : list [ dict ] | None = None , aux_decoders : list [ AuxiliaryHead ] | None = None , rescale : bool = True , # noqa: FBT002, FBT001 peft_config : dict | None = None , ** kwargs , ) -> Model : \"\"\"Generic model factory that combines an encoder and decoder, together with a head, for a specific task. Further arguments to be passed to the backbone, decoder or head. They should be prefixed with `backbone_`, `decoder_` and `head_` respectively. Args: task (str): Task to be performed. Currently supports \"segmentation\", \"regression\" and \"classification\". backbone (str, nn.Module): Backbone to be used. If a string, will look for such models in the different registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it directly. The backbone should have and `out_channels` attribute and its `forward` should return a list[Tensor]. decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model. If a string, will look for such decoders in the different registries supported (internal terratorch registry, smp, ...). If an nn.Module, we expect it to expose a property `decoder.out_channels`. Pixel wise tasks will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\". num_classes (int, optional): Number of classes. None for regression tasks. necks (list[dict]): nn.Modules to be called in succession on encoder features before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry. Expects each one to have a key \"name\" and subsequent keys for arguments, if any. Defaults to None, which applies the identity function. aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead decoders to be added to the model. These decoders take the input from the encoder as well. rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True. peft_config (dict): Configuration options for using [PEFT](https://huggingface.co/docs/peft/index). The dictionary should have the following keys: - \"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available [here](https://huggingface.co/docs/peft/package_reference/peft_types#peft.PeftType). - \"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep. This should be used when the qkv matrices are merged together in a single linear layer and the PEFT method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in Q and V matrices). e.g. If using Prithvi this should be \"qkv\" - \"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to [PeftConfig](https://huggingface.co/docs/peft/package_reference/config#peft.PeftConfig) Returns: nn.Module: Full model with encoder, decoder and head. \"\"\" task = task . lower () if task not in SUPPORTED_TASKS : msg = f \"Task { task } not supported. Please choose one of { SUPPORTED_TASKS } \" raise NotImplementedError ( msg ) backbone_kwargs , kwargs = extract_prefix_keys ( kwargs , \"backbone_\" ) backbone = _get_backbone ( backbone , ** backbone_kwargs ) # If patch size is not provided in the config or by the model, it might lead to errors due to irregular images. patch_size = backbone_kwargs . get ( \"patch_size\" , None ) if patch_size is None : # Infer patch size from model by checking all backbone modules for module in backbone . modules (): if hasattr ( module , \"patch_size\" ): patch_size = module . patch_size break padding = backbone_kwargs . get ( \"padding\" , \"reflect\" ) if peft_config is not None : if not backbone_kwargs . get ( \"pretrained\" , False ): msg = ( \"You are using PEFT without a pretrained backbone. If you are loading a checkpoint afterwards \" \"this is probably fine, but if you are training a model check the backbone_pretrained parameter.\" ) warnings . warn ( msg , stacklevel = 1 ) backbone = get_peft_backbone ( peft_config , backbone ) try : out_channels = backbone . out_channels except AttributeError as e : msg = \"backbone must have out_channels attribute\" raise AttributeError ( msg ) from e if necks is None : necks = [] neck_list , channel_list = build_neck_list ( necks , out_channels ) # some decoders already include a head # for these, we pass the num_classes to them # others dont include a head # for those, we dont pass num_classes decoder_kwargs , kwargs = extract_prefix_keys ( kwargs , \"decoder_\" ) head_kwargs , kwargs = extract_prefix_keys ( kwargs , \"head_\" ) decoder , head_kwargs , decoder_includes_head = _get_decoder_and_head_kwargs ( decoder , channel_list , decoder_kwargs , head_kwargs , num_classes = num_classes ) if aux_decoders is None : _check_all_args_used ( kwargs ) return _build_appropriate_model ( task , backbone , decoder , head_kwargs , patch_size = patch_size , padding = padding , necks = neck_list , decoder_includes_head = decoder_includes_head , rescale = rescale , ) to_be_aux_decoders : list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] = [] for aux_decoder in aux_decoders : args = aux_decoder . decoder_args if aux_decoder . decoder_args else {} aux_decoder_kwargs , args = extract_prefix_keys ( args , \"decoder_\" ) aux_head_kwargs , args = extract_prefix_keys ( args , \"head_\" ) aux_decoder_instance , aux_head_kwargs , aux_decoder_includes_head = _get_decoder_and_head_kwargs ( aux_decoder . decoder , channel_list , aux_decoder_kwargs , aux_head_kwargs , num_classes = num_classes ) to_be_aux_decoders . append ( AuxiliaryHeadWithDecoderWithoutInstantiatedHead ( aux_decoder . name , aux_decoder_instance , aux_head_kwargs ) ) _check_all_args_used ( args ) _check_all_args_used ( kwargs ) return _build_appropriate_model ( task , backbone , decoder , head_kwargs , patch_size = patch_size , padding = padding , necks = neck_list , decoder_includes_head = decoder_includes_head , rescale = rescale , auxiliary_heads = to_be_aux_decoders , )","title":"EncoderDecoderFactory"},{"location":"encoder_decoder_factory/#terratorch.models.encoder_decoder_factory.EncoderDecoderFactory.build_model","text":"Generic model factory that combines an encoder and decoder, together with a head, for a specific task. Further arguments to be passed to the backbone, decoder or head. They should be prefixed with backbone_ , decoder_ and head_ respectively. Parameters: task ( str ) \u2013 Task to be performed. Currently supports \"segmentation\", \"regression\" and \"classification\". backbone ( ( str , Module ) ) \u2013 Backbone to be used. If a string, will look for such models in the different registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it directly. The backbone should have and out_channels attribute and its forward should return a list[Tensor]. decoder ( Union [ str , Module ] ) \u2013 Decoder to be used for the segmentation model. If a string, will look for such decoders in the different registries supported (internal terratorch registry, smp, ...). If an nn.Module, we expect it to expose a property decoder.out_channels . Pixel wise tasks will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\". num_classes ( int , default: None ) \u2013 Number of classes. None for regression tasks. necks ( list [ dict ] , default: None ) \u2013 nn.Modules to be called in succession on encoder features before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry. Expects each one to have a key \"name\" and subsequent keys for arguments, if any. Defaults to None, which applies the identity function. aux_decoders ( list [ AuxiliaryHead ] | None , default: None ) \u2013 List of AuxiliaryHead decoders to be added to the model. These decoders take the input from the encoder as well. rescale ( bool , default: True ) \u2013 Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True. peft_config ( dict , default: None ) \u2013 Configuration options for using PEFT . The dictionary should have the following keys: \"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available here . \"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep. This should be used when the qkv matrices are merged together in a single linear layer and the PEFT method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in Q and V matrices). e.g. If using Prithvi this should be \"qkv\" \"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to PeftConfig Returns: Model \u2013 nn.Module: Full model with encoder, decoder and head. Source code in terratorch/models/encoder_decoder_factory.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def build_model ( self , task : str , backbone : str | nn . Module , decoder : str | nn . Module , num_classes : int | None = None , necks : list [ dict ] | None = None , aux_decoders : list [ AuxiliaryHead ] | None = None , rescale : bool = True , # noqa: FBT002, FBT001 peft_config : dict | None = None , ** kwargs , ) -> Model : \"\"\"Generic model factory that combines an encoder and decoder, together with a head, for a specific task. Further arguments to be passed to the backbone, decoder or head. They should be prefixed with `backbone_`, `decoder_` and `head_` respectively. Args: task (str): Task to be performed. Currently supports \"segmentation\", \"regression\" and \"classification\". backbone (str, nn.Module): Backbone to be used. If a string, will look for such models in the different registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it directly. The backbone should have and `out_channels` attribute and its `forward` should return a list[Tensor]. decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model. If a string, will look for such decoders in the different registries supported (internal terratorch registry, smp, ...). If an nn.Module, we expect it to expose a property `decoder.out_channels`. Pixel wise tasks will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\". num_classes (int, optional): Number of classes. None for regression tasks. necks (list[dict]): nn.Modules to be called in succession on encoder features before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry. Expects each one to have a key \"name\" and subsequent keys for arguments, if any. Defaults to None, which applies the identity function. aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead decoders to be added to the model. These decoders take the input from the encoder as well. rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True. peft_config (dict): Configuration options for using [PEFT](https://huggingface.co/docs/peft/index). The dictionary should have the following keys: - \"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available [here](https://huggingface.co/docs/peft/package_reference/peft_types#peft.PeftType). - \"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep. This should be used when the qkv matrices are merged together in a single linear layer and the PEFT method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in Q and V matrices). e.g. If using Prithvi this should be \"qkv\" - \"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to [PeftConfig](https://huggingface.co/docs/peft/package_reference/config#peft.PeftConfig) Returns: nn.Module: Full model with encoder, decoder and head. \"\"\" task = task . lower () if task not in SUPPORTED_TASKS : msg = f \"Task { task } not supported. Please choose one of { SUPPORTED_TASKS } \" raise NotImplementedError ( msg ) backbone_kwargs , kwargs = extract_prefix_keys ( kwargs , \"backbone_\" ) backbone = _get_backbone ( backbone , ** backbone_kwargs ) # If patch size is not provided in the config or by the model, it might lead to errors due to irregular images. patch_size = backbone_kwargs . get ( \"patch_size\" , None ) if patch_size is None : # Infer patch size from model by checking all backbone modules for module in backbone . modules (): if hasattr ( module , \"patch_size\" ): patch_size = module . patch_size break padding = backbone_kwargs . get ( \"padding\" , \"reflect\" ) if peft_config is not None : if not backbone_kwargs . get ( \"pretrained\" , False ): msg = ( \"You are using PEFT without a pretrained backbone. If you are loading a checkpoint afterwards \" \"this is probably fine, but if you are training a model check the backbone_pretrained parameter.\" ) warnings . warn ( msg , stacklevel = 1 ) backbone = get_peft_backbone ( peft_config , backbone ) try : out_channels = backbone . out_channels except AttributeError as e : msg = \"backbone must have out_channels attribute\" raise AttributeError ( msg ) from e if necks is None : necks = [] neck_list , channel_list = build_neck_list ( necks , out_channels ) # some decoders already include a head # for these, we pass the num_classes to them # others dont include a head # for those, we dont pass num_classes decoder_kwargs , kwargs = extract_prefix_keys ( kwargs , \"decoder_\" ) head_kwargs , kwargs = extract_prefix_keys ( kwargs , \"head_\" ) decoder , head_kwargs , decoder_includes_head = _get_decoder_and_head_kwargs ( decoder , channel_list , decoder_kwargs , head_kwargs , num_classes = num_classes ) if aux_decoders is None : _check_all_args_used ( kwargs ) return _build_appropriate_model ( task , backbone , decoder , head_kwargs , patch_size = patch_size , padding = padding , necks = neck_list , decoder_includes_head = decoder_includes_head , rescale = rescale , ) to_be_aux_decoders : list [ AuxiliaryHeadWithDecoderWithoutInstantiatedHead ] = [] for aux_decoder in aux_decoders : args = aux_decoder . decoder_args if aux_decoder . decoder_args else {} aux_decoder_kwargs , args = extract_prefix_keys ( args , \"decoder_\" ) aux_head_kwargs , args = extract_prefix_keys ( args , \"head_\" ) aux_decoder_instance , aux_head_kwargs , aux_decoder_includes_head = _get_decoder_and_head_kwargs ( aux_decoder . decoder , channel_list , aux_decoder_kwargs , aux_head_kwargs , num_classes = num_classes ) to_be_aux_decoders . append ( AuxiliaryHeadWithDecoderWithoutInstantiatedHead ( aux_decoder . name , aux_decoder_instance , aux_head_kwargs ) ) _check_all_args_used ( args ) _check_all_args_used ( kwargs ) return _build_appropriate_model ( task , backbone , decoder , head_kwargs , patch_size = patch_size , padding = padding , necks = neck_list , decoder_includes_head = decoder_includes_head , rescale = rescale , auxiliary_heads = to_be_aux_decoders , )","title":"build_model"},{"location":"encoder_decoder_factory/#terratorchmodelspixel_wise_modelpixelwisemodel","text":"","title":"terratorch.models.pixel_wise_model.PixelWiseModel"},{"location":"encoder_decoder_factory/#terratorchmodelsscalar_output_modelscalaroutputmodel","text":"","title":"terratorch.models.scalar_output_model.ScalarOutputModel"},{"location":"encoder_decoder_factory/#encoders","text":"To be a valid encoder, an object must be an nn.Module and contain an attribute out_channels , basically a list of the channel dimensions corresponding to the features it returns. The forward method of any encoder should return a list of torch.Tensor . In [ 19 ] : backbone = BACKBONE_REGISTRY.build ( \"prithvi_eo_v2_300\" , pretrained = True ) In [ 20 ] : import numpy as np In [ 21 ] : import torch In [ 22 ] : input_image = torch.tensor ( np.random.rand ( 1 ,6,224,224 ) .astype ( \"float32\" )) In [ 23 ] : output = backbone.forward ( input_image ) In [ 24 ] : [ item.shape for item in output ] Out [ 24 ] : [ torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ]) , torch.Size ([ 1 , 197 , 1024 ])]","title":"Encoders"},{"location":"encoder_decoder_factory/#necks","text":"Necks are the connectors between encoder and decoder. They can perform operations such as selecting elements from the output of the encoder ( SelectIndices ), reshaping the outputs of ViTs so they are compatible with CNNs ( ReshapeTokensToImage ), amongst others. Necks are nn.Modules , with an additional method process_channel_list which informs the EncoderDecoderFactory about how it will alter the channel list provided by encoder.out_channels .","title":"Necks"},{"location":"encoder_decoder_factory/#terratorch.models.necks.Neck","text":"Bases: ABC , Module Base class for Neck A neck must must implement self.process_channel_list which returns the new channel list. Source code in terratorch/models/necks.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Neck ( ABC , nn . Module ): \"\"\"Base class for Neck A neck must must implement `self.process_channel_list` which returns the new channel list. \"\"\" def __init__ ( self , channel_list : list [ int ]) -> None : super () . __init__ () self . channel_list = channel_list @abstractmethod def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: return channel_list @abstractmethod def forward ( self , channel_list : list [ torch . Tensor ]) -> list [ torch . Tensor ]: ...","title":"Neck"},{"location":"encoder_decoder_factory/#terratorch.models.necks.SelectIndices","text":"Bases: Neck Source code in terratorch/models/necks.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @TERRATORCH_NECK_REGISTRY . register class SelectIndices ( Neck ): def __init__ ( self , channel_list : list [ int ], indices : list [ int ]): \"\"\"Select indices from the embedding list Args: indices (list[int]): list of indices to select. \"\"\" super () . __init__ ( channel_list ) self . indices = indices def forward ( self , features : list [ torch . Tensor ]) -> list [ torch . Tensor ]: features = [ features [ i ] for i in self . indices ] return features def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: channel_list = [ channel_list [ i ] for i in self . indices ] return channel_list","title":"SelectIndices"},{"location":"encoder_decoder_factory/#terratorch.models.necks.SelectIndices.__init__","text":"Select indices from the embedding list Parameters: indices ( list [ int ] ) \u2013 list of indices to select. Source code in terratorch/models/necks.py 33 34 35 36 37 38 39 40 def __init__ ( self , channel_list : list [ int ], indices : list [ int ]): \"\"\"Select indices from the embedding list Args: indices (list[int]): list of indices to select. \"\"\" super () . __init__ ( channel_list ) self . indices = indices","title":"__init__"},{"location":"encoder_decoder_factory/#terratorch.models.necks.PermuteDims","text":"Bases: Neck Source code in terratorch/models/necks.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @TERRATORCH_NECK_REGISTRY . register class PermuteDims ( Neck ): def __init__ ( self , channel_list : list [ int ], new_order : list [ int ]): \"\"\"Permute dimensions of each element in the embedding list Args: new_order (list[int]): list of indices to be passed to tensor.permute() \"\"\" super () . __init__ ( channel_list ) self . new_order = new_order def forward ( self , features : list [ torch . Tensor ]) -> list [ torch . Tensor ]: features = [ feat . permute ( * self . new_order ) . contiguous () for feat in features ] return features def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: return super () . process_channel_list ( channel_list )","title":"PermuteDims"},{"location":"encoder_decoder_factory/#terratorch.models.necks.PermuteDims.__init__","text":"Permute dimensions of each element in the embedding list Parameters: new_order ( list [ int ] ) \u2013 list of indices to be passed to tensor.permute() Source code in terratorch/models/necks.py 52 53 54 55 56 57 58 59 def __init__ ( self , channel_list : list [ int ], new_order : list [ int ]): \"\"\"Permute dimensions of each element in the embedding list Args: new_order (list[int]): list of indices to be passed to tensor.permute() \"\"\" super () . __init__ ( channel_list ) self . new_order = new_order","title":"__init__"},{"location":"encoder_decoder_factory/#terratorch.models.necks.InterpolateToPyramidal","text":"Bases: Neck Source code in terratorch/models/necks.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 @TERRATORCH_NECK_REGISTRY . register class InterpolateToPyramidal ( Neck ): def __init__ ( self , channel_list : list [ int ], scale_factor : int = 2 , mode : str = \"nearest\" ): \"\"\"Spatially interpolate embeddings so that embedding[i - 1] is scale_factor times larger than embedding[i] Useful to make non-pyramidal backbones compatible with hierarachical ones Args: scale_factor (int): Amount to scale embeddings by each layer. Defaults to 2. mode (str): Interpolation mode to be passed to torch.nn.functional.interpolate. Defaults to 'nearest'. \"\"\" super () . __init__ ( channel_list ) self . scale_factor = scale_factor self . mode = mode def forward ( self , features : list [ torch . Tensor ]) -> list [ torch . Tensor ]: out = [] scale_exponents = list ( range ( len ( features ), 0 , - 1 )) for x , exponent in zip ( features , scale_exponents , strict = True ): out . append ( F . interpolate ( x , scale_factor = self . scale_factor ** exponent , mode = self . mode )) return out def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: return super () . process_channel_list ( channel_list )","title":"InterpolateToPyramidal"},{"location":"encoder_decoder_factory/#terratorch.models.necks.InterpolateToPyramidal.__init__","text":"Spatially interpolate embeddings so that embedding[i - 1] is scale_factor times larger than embedding[i] Useful to make non-pyramidal backbones compatible with hierarachical ones Args: scale_factor (int): Amount to scale embeddings by each layer. Defaults to 2. mode (str): Interpolation mode to be passed to torch.nn.functional.interpolate. Defaults to 'nearest'. Source code in terratorch/models/necks.py 70 71 72 73 74 75 76 77 78 79 80 def __init__ ( self , channel_list : list [ int ], scale_factor : int = 2 , mode : str = \"nearest\" ): \"\"\"Spatially interpolate embeddings so that embedding[i - 1] is scale_factor times larger than embedding[i] Useful to make non-pyramidal backbones compatible with hierarachical ones Args: scale_factor (int): Amount to scale embeddings by each layer. Defaults to 2. mode (str): Interpolation mode to be passed to torch.nn.functional.interpolate. Defaults to 'nearest'. \"\"\" super () . __init__ ( channel_list ) self . scale_factor = scale_factor self . mode = mode","title":"__init__"},{"location":"encoder_decoder_factory/#terratorch.models.necks.MaxpoolToPyramidal","text":"Bases: Neck Source code in terratorch/models/necks.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 @TERRATORCH_NECK_REGISTRY . register class MaxpoolToPyramidal ( Neck ): def __init__ ( self , channel_list : list [ int ], kernel_size : int = 2 ): \"\"\"Spatially downsample embeddings so that embedding[i - 1] is scale_factor times smaller than embedding[i] Useful to make non-pyramidal backbones compatible with hierarachical ones Args: kernel_size (int). Base kernel size to use for maxpool. Defaults to 2. \"\"\" super () . __init__ ( channel_list ) self . kernel_size = kernel_size def forward ( self , features : list [ torch . Tensor ]) -> list [ torch . Tensor ]: out = [] scale_exponents = list ( range ( len ( features ))) for x , exponent in zip ( features , scale_exponents , strict = True ): if exponent == 0 : out . append ( x . clone ()) else : out . append ( F . max_pool2d ( x , kernel_size = self . kernel_size ** exponent )) return out def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: return super () . process_channel_list ( channel_list )","title":"MaxpoolToPyramidal"},{"location":"encoder_decoder_factory/#terratorch.models.necks.MaxpoolToPyramidal.__init__","text":"Spatially downsample embeddings so that embedding[i - 1] is scale_factor times smaller than embedding[i] Useful to make non-pyramidal backbones compatible with hierarachical ones Args: kernel_size (int). Base kernel size to use for maxpool. Defaults to 2. Source code in terratorch/models/necks.py 96 97 98 99 100 101 102 103 104 def __init__ ( self , channel_list : list [ int ], kernel_size : int = 2 ): \"\"\"Spatially downsample embeddings so that embedding[i - 1] is scale_factor times smaller than embedding[i] Useful to make non-pyramidal backbones compatible with hierarachical ones Args: kernel_size (int). Base kernel size to use for maxpool. Defaults to 2. \"\"\" super () . __init__ ( channel_list ) self . kernel_size = kernel_size","title":"__init__"},{"location":"encoder_decoder_factory/#terratorch.models.necks.ReshapeTokensToImage","text":"Bases: Neck Source code in terratorch/models/necks.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 @TERRATORCH_NECK_REGISTRY . register class ReshapeTokensToImage ( Neck ): def __init__ ( self , channel_list : list [ int ], remove_cls_token = True , effective_time_dim : int = 1 ): # noqa: FBT002 \"\"\"Reshape output of transformer encoder so it can be passed to a conv net. Args: remove_cls_token (bool, optional): Whether to remove the cls token from the first position. Defaults to True. effective_time_dim (int, optional): The effective temporal dimension the transformer processes. For a ViT, his will be given by `num_frames // tubelet size`. This is used to determine the temporal dimension of the embedding, which is concatenated with the embedding dimension. For example: - A model which processes 1 frame with a tubelet size of 1 has an effective_time_dim of 1. The embedding produced by this model has embedding size embed_dim * 1. - A model which processes 3 frames with a tubelet size of 1 has an effective_time_dim of 3. The embedding produced by this model has embedding size embed_dim * 3. - A model which processes 12 frames with a tubelet size of 4 has an effective_time_dim of 3. The embedding produced by this model has an embedding size embed_dim * 3. Defaults to 1. \"\"\" super () . __init__ ( channel_list ) self . remove_cls_token = remove_cls_token self . effective_time_dim = effective_time_dim def collapse_dims ( self , x ): \"\"\" When the encoder output has more than 3 dimensions, is necessary to reshape it. \"\"\" shape = x . shape batch = x . shape [ 0 ] e = x . shape [ - 1 ] collapsed_dim = np . prod ( x . shape [ 1 : - 1 ]) return x . reshape ( batch , collapsed_dim , e ) def forward ( self , features : list [ torch . Tensor ]) -> list [ torch . Tensor ]: out = [] for x in features : if self . remove_cls_token : x_no_token = x [:, 1 :, :] else : x_no_token = x x_no_token = self . collapse_dims ( x_no_token ) number_of_tokens = x_no_token . shape [ 1 ] tokens_per_timestep = number_of_tokens // self . effective_time_dim h = int ( math . sqrt ( tokens_per_timestep )) encoded = rearrange ( x_no_token , \"batch (t h w) e -> batch (t e) h w\" , batch = x_no_token . shape [ 0 ], t = self . effective_time_dim , h = h , ) out . append ( encoded ) return out def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: return super () . process_channel_list ( channel_list )","title":"ReshapeTokensToImage"},{"location":"encoder_decoder_factory/#terratorch.models.necks.ReshapeTokensToImage.__init__","text":"Reshape output of transformer encoder so it can be passed to a conv net. Parameters: remove_cls_token ( bool , default: True ) \u2013 Whether to remove the cls token from the first position. Defaults to True. effective_time_dim ( int , default: 1 ) \u2013 The effective temporal dimension the transformer processes. For a ViT, his will be given by num_frames // tubelet size . This is used to determine the temporal dimension of the embedding, which is concatenated with the embedding dimension. For example: - A model which processes 1 frame with a tubelet size of 1 has an effective_time_dim of 1. The embedding produced by this model has embedding size embed_dim * 1. - A model which processes 3 frames with a tubelet size of 1 has an effective_time_dim of 3. The embedding produced by this model has embedding size embed_dim * 3. - A model which processes 12 frames with a tubelet size of 4 has an effective_time_dim of 3. The embedding produced by this model has an embedding size embed_dim * 3. Defaults to 1. Source code in terratorch/models/necks.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def __init__ ( self , channel_list : list [ int ], remove_cls_token = True , effective_time_dim : int = 1 ): # noqa: FBT002 \"\"\"Reshape output of transformer encoder so it can be passed to a conv net. Args: remove_cls_token (bool, optional): Whether to remove the cls token from the first position. Defaults to True. effective_time_dim (int, optional): The effective temporal dimension the transformer processes. For a ViT, his will be given by `num_frames // tubelet size`. This is used to determine the temporal dimension of the embedding, which is concatenated with the embedding dimension. For example: - A model which processes 1 frame with a tubelet size of 1 has an effective_time_dim of 1. The embedding produced by this model has embedding size embed_dim * 1. - A model which processes 3 frames with a tubelet size of 1 has an effective_time_dim of 3. The embedding produced by this model has embedding size embed_dim * 3. - A model which processes 12 frames with a tubelet size of 4 has an effective_time_dim of 3. The embedding produced by this model has an embedding size embed_dim * 3. Defaults to 1. \"\"\" super () . __init__ ( channel_list ) self . remove_cls_token = remove_cls_token self . effective_time_dim = effective_time_dim","title":"__init__"},{"location":"encoder_decoder_factory/#terratorch.models.necks.ReshapeTokensToImage.collapse_dims","text":"When the encoder output has more than 3 dimensions, is necessary to reshape it. Source code in terratorch/models/necks.py 145 146 147 148 149 150 151 152 153 154 155 def collapse_dims ( self , x ): \"\"\" When the encoder output has more than 3 dimensions, is necessary to reshape it. \"\"\" shape = x . shape batch = x . shape [ 0 ] e = x . shape [ - 1 ] collapsed_dim = np . prod ( x . shape [ 1 : - 1 ]) return x . reshape ( batch , collapsed_dim , e )","title":"collapse_dims"},{"location":"encoder_decoder_factory/#terratorch.models.necks.AddBottleneckLayer","text":"Bases: Neck Add a layer that reduces the channel dimension of the final embedding by half, and concatenates it Useful for compatibility with some smp decoders. Source code in terratorch/models/necks.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 @TERRATORCH_NECK_REGISTRY . register class AddBottleneckLayer ( Neck ): \"\"\"Add a layer that reduces the channel dimension of the final embedding by half, and concatenates it Useful for compatibility with some smp decoders. \"\"\" def __init__ ( self , channel_list : list [ int ]): super () . __init__ ( channel_list ) self . bottleneck = nn . Conv2d ( channel_list [ - 1 ], channel_list [ - 1 ] // 2 , kernel_size = 1 ) def forward ( self , features : list [ torch . Tensor ]) -> list [ torch . Tensor ]: new_embedding = self . bottleneck ( features [ - 1 ]) features . append ( new_embedding ) return features def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: return [ * channel_list , channel_list [ - 1 ] // 2 ]","title":"AddBottleneckLayer"},{"location":"encoder_decoder_factory/#terratorch.models.necks.LearnedInterpolateToPyramidal","text":"Bases: Neck Use learned convolutions to transform the output of a non-pyramidal encoder into pyramidal ones Always requires exactly 4 embeddings Source code in terratorch/models/necks.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 @TERRATORCH_NECK_REGISTRY . register class LearnedInterpolateToPyramidal ( Neck ): \"\"\"Use learned convolutions to transform the output of a non-pyramidal encoder into pyramidal ones Always requires exactly 4 embeddings \"\"\" def __init__ ( self , channel_list : list [ int ]): super () . __init__ ( channel_list ) if len ( channel_list ) != 4 : msg = \"This class can only handle exactly 4 input embeddings\" raise Exception ( msg ) self . fpn1 = nn . Sequential ( nn . ConvTranspose2d ( channel_list [ 0 ], channel_list [ 0 ] // 2 , 2 , 2 ), nn . BatchNorm2d ( channel_list [ 0 ] // 2 ), nn . GELU (), nn . ConvTranspose2d ( channel_list [ 0 ] // 2 , channel_list [ 0 ] // 4 , 2 , 2 ), ) self . fpn2 = nn . Sequential ( nn . ConvTranspose2d ( channel_list [ 1 ], channel_list [ 1 ] // 2 , 2 , 2 )) self . fpn3 = nn . Sequential ( nn . Identity ()) self . fpn4 = nn . Sequential ( nn . MaxPool2d ( kernel_size = 2 , stride = 2 )) self . embedding_dim = [ channel_list [ 0 ] // 4 , channel_list [ 1 ] // 2 , channel_list [ 2 ], channel_list [ 3 ]] def forward ( self , features : list [ torch . Tensor ]) -> list [ torch . Tensor ]: scaled_inputs = [] scaled_inputs . append ( self . fpn1 ( features [ 0 ])) scaled_inputs . append ( self . fpn2 ( features [ 1 ])) scaled_inputs . append ( self . fpn3 ( features [ 2 ])) scaled_inputs . append ( self . fpn4 ( features [ 3 ])) return scaled_inputs def process_channel_list ( self , channel_list : list [ int ]) -> list [ int ]: return [ channel_list [ 0 ] // 4 , channel_list [ 1 ] // 2 , channel_list [ 2 ], channel_list [ 3 ]]","title":"LearnedInterpolateToPyramidal"},{"location":"encoder_decoder_factory/#decoders","text":"To be a valid decoder, an object must be an nn.Module with an attribute out_channels , an int representing the channel dimension of the output. The first argument to its constructor will be a list of channel dimensions it should expect as input. It's forward method should accept a list of embeddings.","title":"Decoders"},{"location":"encoder_decoder_factory/#heads","text":"Most decoders require a final head to be added for a specific task (e.g. semantic segmentation vs pixel wise regression). Those registries producing decoders that dont require a head must expose the attribute includes_head=True so that a head is not added. Decoders passed as nn.Modules which do not require a head must expose the same attribute themselves.","title":"Heads"},{"location":"encoder_decoder_factory/#terratorch.models.heads.classification_head.ClassificationHead","text":"Bases: Module Classification head Source code in terratorch/models/heads/classification_head.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class ClassificationHead ( nn . Module ): \"\"\"Classification head\"\"\" # how to allow cls token? def __init__ ( self , in_dim : int , num_classes : int , dim_list : list [ int ] | None = None , dropout : float = 0 , linear_after_pool : bool = False , ) -> None : \"\"\"Constructor Args: in_dim (int): Input dimensionality num_classes (int): Number of output classes dim_list (list[int] | None, optional): List with number of dimensions for each Linear layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. linear_after_pool (bool, optional): Apply pooling first, then apply the linear layer. Defaults to False \"\"\" super () . __init__ () self . num_classes = num_classes self . linear_after_pool = linear_after_pool if dim_list is None : pre_head = nn . Identity () else : def block ( in_dim , out_dim ): return nn . Sequential ( nn . Linear ( in_features = in_dim , out_features = out_dim ), nn . ReLU ()) dim_list = [ in_dim , * dim_list ] pre_head = nn . Sequential ( * [ block ( dim_list [ i ], dim_list [ i + 1 ]) for i in range ( len ( dim_list ) - 1 )]) in_dim = dim_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Linear ( in_features = in_dim , out_features = num_classes ), ) def forward ( self , x : Tensor ): x = x . reshape ( x . shape [ 0 ], x . shape [ 1 ], - 1 ) . permute ( 0 , 2 , 1 ) if self . linear_after_pool : x = x . mean ( axis = 1 ) out = self . head ( x ) else : x = self . head ( x ) out = x . mean ( axis = 1 ) return out","title":"ClassificationHead"},{"location":"encoder_decoder_factory/#terratorch.models.heads.classification_head.ClassificationHead.__init__","text":"Constructor Parameters: in_dim ( int ) \u2013 Input dimensionality num_classes ( int ) \u2013 Number of output classes dim_list ( list [ int ] | None , default: None ) \u2013 List with number of dimensions for each Linear layer to be created. Defaults to None. dropout ( float , default: 0 ) \u2013 Dropout value to apply. Defaults to 0. linear_after_pool ( bool , default: False ) \u2013 Apply pooling first, then apply the linear layer. Defaults to False Source code in terratorch/models/heads/classification_head.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , in_dim : int , num_classes : int , dim_list : list [ int ] | None = None , dropout : float = 0 , linear_after_pool : bool = False , ) -> None : \"\"\"Constructor Args: in_dim (int): Input dimensionality num_classes (int): Number of output classes dim_list (list[int] | None, optional): List with number of dimensions for each Linear layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. linear_after_pool (bool, optional): Apply pooling first, then apply the linear layer. Defaults to False \"\"\" super () . __init__ () self . num_classes = num_classes self . linear_after_pool = linear_after_pool if dim_list is None : pre_head = nn . Identity () else : def block ( in_dim , out_dim ): return nn . Sequential ( nn . Linear ( in_features = in_dim , out_features = out_dim ), nn . ReLU ()) dim_list = [ in_dim , * dim_list ] pre_head = nn . Sequential ( * [ block ( dim_list [ i ], dim_list [ i + 1 ]) for i in range ( len ( dim_list ) - 1 )]) in_dim = dim_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Linear ( in_features = in_dim , out_features = num_classes ), )","title":"__init__"},{"location":"encoder_decoder_factory/#terratorch.models.heads.regression_head.RegressionHead","text":"Bases: Module Regression head Source code in terratorch/models/heads/regression_head.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class RegressionHead ( nn . Module ): \"\"\"Regression head\"\"\" def __init__ ( self , in_channels : int , final_act : nn . Module | str | None = None , learned_upscale_layers : int = 0 , channel_list : list [ int ] | None = None , batch_norm : bool = True , dropout : float = 0 , ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels final_act (nn.Module | None, optional): Final activation to be applied. Defaults to None. learned_upscale_layers (int, optional): Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0. channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. batch_norm (bool, optional): Whether to apply batch norm. Defaults to True. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . learned_upscale_layers = learned_upscale_layers self . final_act = final_act if final_act else nn . Identity () if isinstance ( final_act , str ): module_name , class_name = final_act . rsplit ( \".\" , 1 ) target_class = getattr ( importlib . import_module ( module_name ), class_name ) self . final_act = target_class () pre_layers = [] if learned_upscale_layers != 0 : learned_upscale = nn . Sequential ( * [ PixelShuffleUpscale ( in_channels ) for _ in range ( self . learned_upscale_layers )] ) pre_layers . append ( learned_upscale ) if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 , bias = False ), nn . BatchNorm2d ( out_channels ), nn . ReLU ( inplace = True ), ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] pre_layers . append ( pre_head ) dropout = nn . Dropout2d ( dropout ) final_layer = nn . Conv2d ( in_channels = in_channels , out_channels = 1 , kernel_size = 1 ) self . head = nn . Sequential ( * [ * pre_layers , dropout , final_layer ]) def forward ( self , x ): output = self . head ( x ) return self . final_act ( output )","title":"RegressionHead"},{"location":"encoder_decoder_factory/#terratorch.models.heads.regression_head.RegressionHead.__init__","text":"Constructor Parameters: in_channels ( int ) \u2013 Number of input channels final_act ( Module | None , default: None ) \u2013 Final activation to be applied. Defaults to None. learned_upscale_layers ( int , default: 0 ) \u2013 Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0. channel_list ( list [ int ] | None , default: None ) \u2013 List with number of channels for each Conv layer to be created. Defaults to None. batch_norm ( bool , default: True ) \u2013 Whether to apply batch norm. Defaults to True. dropout ( float , default: 0 ) \u2013 Dropout value to apply. Defaults to 0. Source code in terratorch/models/heads/regression_head.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , in_channels : int , final_act : nn . Module | str | None = None , learned_upscale_layers : int = 0 , channel_list : list [ int ] | None = None , batch_norm : bool = True , dropout : float = 0 , ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels final_act (nn.Module | None, optional): Final activation to be applied. Defaults to None. learned_upscale_layers (int, optional): Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0. channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. batch_norm (bool, optional): Whether to apply batch norm. Defaults to True. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . learned_upscale_layers = learned_upscale_layers self . final_act = final_act if final_act else nn . Identity () if isinstance ( final_act , str ): module_name , class_name = final_act . rsplit ( \".\" , 1 ) target_class = getattr ( importlib . import_module ( module_name ), class_name ) self . final_act = target_class () pre_layers = [] if learned_upscale_layers != 0 : learned_upscale = nn . Sequential ( * [ PixelShuffleUpscale ( in_channels ) for _ in range ( self . learned_upscale_layers )] ) pre_layers . append ( learned_upscale ) if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 , bias = False ), nn . BatchNorm2d ( out_channels ), nn . ReLU ( inplace = True ), ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] pre_layers . append ( pre_head ) dropout = nn . Dropout2d ( dropout ) final_layer = nn . Conv2d ( in_channels = in_channels , out_channels = 1 , kernel_size = 1 ) self . head = nn . Sequential ( * [ * pre_layers , dropout , final_layer ])","title":"__init__"},{"location":"encoder_decoder_factory/#terratorch.models.heads.segmentation_head.SegmentationHead","text":"Bases: Module Segmentation head Source code in terratorch/models/heads/segmentation_head.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class SegmentationHead ( nn . Module ): \"\"\"Segmentation head\"\"\" def __init__ ( self , in_channels : int , num_classes : int , channel_list : list [ int ] | None = None , dropout : float = 0 ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels num_classes (int): Number of output classes channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . num_classes = num_classes if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 ), nn . ReLU () ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Conv2d ( in_channels = in_channels , out_channels = num_classes , kernel_size = 1 , ), ) def forward ( self , x ): return self . head ( x )","title":"SegmentationHead"},{"location":"encoder_decoder_factory/#terratorch.models.heads.segmentation_head.SegmentationHead.__init__","text":"Constructor Parameters: in_channels ( int ) \u2013 Number of input channels num_classes ( int ) \u2013 Number of output classes channel_list ( list [ int ] | None , default: None ) \u2013 List with number of channels for each Conv layer to be created. Defaults to None. dropout ( float , default: 0 ) \u2013 Dropout value to apply. Defaults to 0. Source code in terratorch/models/heads/segmentation_head.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , in_channels : int , num_classes : int , channel_list : list [ int ] | None = None , dropout : float = 0 ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels num_classes (int): Number of output classes channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . num_classes = num_classes if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 ), nn . ReLU () ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Conv2d ( in_channels = in_channels , out_channels = num_classes , kernel_size = 1 , ), )","title":"__init__"},{"location":"encoder_decoder_factory/#decoder-compatibilities","text":"Not all encoders and decoders are compatible. Below we include some caveats. Some decoders expect pyramidal outputs, but some encoders do not produce such outputs (e.g. vanilla ViT models). In this case, the InterpolateToPyramidal , MaxpoolToPyramidal and LearnedInterpolateToPyramidal necks may be particularly useful.","title":"Decoder compatibilities"},{"location":"encoder_decoder_factory/#smp-decoders","text":"Not all decoders are guaranteed to work with all encoders without additional necks. Please check smp documentation to understand the embedding spatial dimensions expected by each decoder. In particular, smp seems to assume the first feature in the passed feature list has the same spatial resolution as the input, which may not always be true, and may break some decoders. In addition, for some decoders, the final 2 features have the same spatial resolution. Adding the AddBottleneckLayer neck will make this compatible. Some smp decoders require additional parameters, such as decoder_channels . These must be passed through the factory. In the case of decoder_channels , it would be passed as decoder_decoder_channels (the first decoder_ routes the parameter to the decoder, where it is passed as decoder_channels ).","title":"SMP decoders"},{"location":"encoder_decoder_factory/#mmsegmentation-decoders","text":"MMSegmentation decoders are available through the BACKBONE_REGISTRY. Warning MMSegmentation currently requires mmcv==2.1.0 . Pre-built wheels for this only exist for torch==2.1.0 . In order to use mmseg without building from source, you must downgrade your torch to this version. Install mmseg with: pip install -U openmim mim install mmengine mim install mmcv == 2 .1.0 pip install regex ftfy mmsegmentation We provide access to mmseg decoders as an external source of decoders, but are not directly responsible for the maintainence of that library. Some mmseg decoders require the parameter in_index , which performs the same function as the SelectIndices neck. For use for pixel wise regression, mmseg decoders should take num_classes=1 .","title":"MMSegmentation decoders"},{"location":"examples/","text":"Performing an inference task with TerraTorch # Step 1: Download the test case from HuggingFace # We will use the burn scars identification test case, in which we are interested in estimating the area affected by wildfires using a finetuned model (Prithvi-EO backbone + CNN decoder). To download the complete example, do: git clone https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-300M-BurnScars/ Step 2: Run the default inference case # The example you download already contains some sample images to be used as input, so you just need to go to the local repository and create a directory to save the outputs: cd Prithvi-EO-2.0-300M-BurnScars mkdir outputs and to execute a command line like: terratorch predict -c burn_scars_config.yaml --predict_output_dir outputs/ --data.init_args.predict_data_root examples/ --ckpt_path Prithvi_EO_V2_300M_BurnScars.pt You will see the outputs being saved in the outputs directory. Input image (RGB components) # Predicted mask # } More examples # For some examples of training using the existing tasks, check out the following pages on our github repo: From config files # Under examples/confs Flood Segmentation with ViT: sen1floods11_vit.yaml Flood Segmentation with ViT and an SMP head: sen1floods11_vit_smp.yaml Flood Segmentation with ViT and an MMSeg head: sen1floods11_vit_mmseg.yaml Multitemporal Crop Segmentation: multitemporal_crop.yaml Burn Scar Segmentation: burn_scars.yaml Scene Classification: eurosat.yaml External examples available in Prithvi-EO-2.0 Carbon Flux Landslide Multitemporal Crop","title":"Examples"},{"location":"examples/#performing-an-inference-task-with-terratorch","text":"","title":"Performing an inference task with TerraTorch"},{"location":"examples/#step-1-download-the-test-case-from-huggingface","text":"We will use the burn scars identification test case, in which we are interested in estimating the area affected by wildfires using a finetuned model (Prithvi-EO backbone + CNN decoder). To download the complete example, do: git clone https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-300M-BurnScars/","title":"Step 1: Download the test case from HuggingFace"},{"location":"examples/#step-2-run-the-default-inference-case","text":"The example you download already contains some sample images to be used as input, so you just need to go to the local repository and create a directory to save the outputs: cd Prithvi-EO-2.0-300M-BurnScars mkdir outputs and to execute a command line like: terratorch predict -c burn_scars_config.yaml --predict_output_dir outputs/ --data.init_args.predict_data_root examples/ --ckpt_path Prithvi_EO_V2_300M_BurnScars.pt You will see the outputs being saved in the outputs directory.","title":"Step 2: Run the default inference case"},{"location":"examples/#input-image-rgb-components","text":"","title":"Input image (RGB components)"},{"location":"examples/#predicted-mask","text":"}","title":"Predicted mask"},{"location":"examples/#more-examples","text":"For some examples of training using the existing tasks, check out the following pages on our github repo:","title":"More examples"},{"location":"examples/#from-config-files","text":"Under examples/confs Flood Segmentation with ViT: sen1floods11_vit.yaml Flood Segmentation with ViT and an SMP head: sen1floods11_vit_smp.yaml Flood Segmentation with ViT and an MMSeg head: sen1floods11_vit_mmseg.yaml Multitemporal Crop Segmentation: multitemporal_crop.yaml Burn Scar Segmentation: burn_scars.yaml Scene Classification: eurosat.yaml External examples available in Prithvi-EO-2.0 Carbon Flux Landslide Multitemporal Crop","title":"From config files"},{"location":"glossary/","text":"Glossary of terms used in this Documentation and in the Geospatial AI area # Encoder # The neural network used to map between the inputs and the intermdiary stage (usually referred as embedding or sometimes as latent space) of the forward step. The encoder is also frequently called backbone and, for finetuning tasks, it is usually the part of the model which is not updated/trained. Decoder # The neural network employed to map between the intermediary stage (embedding/latent space) and the target output. For finetuning tasks, the decoder is the most essential part, since it is trained to map the embedding produced by a previoulsy trained encoder to a new task. Head # A network, usually very small when compared to the encoder and decoder, which is used as final step to adapt the decoder output to a specific task, for example, by applying a determined activation to it. Neck # Necks are operations placed between the encoder and the decoder stages aimed at adjusting possible discrepancies, as incompatible shapes, or applying some specific transform, as a normalization required for the task being executed. Factory # A Factory is a class which organizes the instantiation of a complete model, as a backbone-neck-decoder-head architecture. A class is intended to receive lists and dictionaries containing the required arguments used to build the model and returns a new instance already ready to be used.","title":"Glossary"},{"location":"glossary/#glossary-of-terms-used-in-this-documentation-and-in-the-geospatial-ai-area","text":"","title":"Glossary of terms used in this Documentation and in the Geospatial AI area"},{"location":"glossary/#encoder","text":"The neural network used to map between the inputs and the intermdiary stage (usually referred as embedding or sometimes as latent space) of the forward step. The encoder is also frequently called backbone and, for finetuning tasks, it is usually the part of the model which is not updated/trained.","title":"Encoder"},{"location":"glossary/#decoder","text":"The neural network employed to map between the intermediary stage (embedding/latent space) and the target output. For finetuning tasks, the decoder is the most essential part, since it is trained to map the embedding produced by a previoulsy trained encoder to a new task.","title":"Decoder"},{"location":"glossary/#head","text":"A network, usually very small when compared to the encoder and decoder, which is used as final step to adapt the decoder output to a specific task, for example, by applying a determined activation to it.","title":"Head"},{"location":"glossary/#neck","text":"Necks are operations placed between the encoder and the decoder stages aimed at adjusting possible discrepancies, as incompatible shapes, or applying some specific transform, as a normalization required for the task being executed.","title":"Neck"},{"location":"glossary/#factory","text":"A Factory is a class which organizes the instantiation of a complete model, as a backbone-neck-decoder-head architecture. A class is intended to receive lists and dictionaries containing the required arguments used to build the model and returns a new instance already ready to be used.","title":"Factory"},{"location":"license/","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright yyyy Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"models/","text":"Models # To interface with terratorch tasks correctly, models must inherit from the Model parent class: terratorch.models.model.Model # Bases: ABC , Module Source code in terratorch/models/model.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Model ( ABC , nn . Module ): def __init__ ( self , * args , ** kwargs ) -> None : super () . __init__ ( * args , ** kwargs ) @abstractmethod def freeze_encoder ( self ): pass @abstractmethod def freeze_decoder ( self ): pass @abstractmethod def forward ( self , * args , ** kwargs ) -> ModelOutput : pass and have a forward method which returns an object ModelOutput : terratorch.models.model.ModelOutput dataclass # Source code in terratorch/models/model.py 10 11 12 13 @dataclass class ModelOutput : output : Tensor auxiliary_heads : dict [ str , Tensor ] = None Model Factories # In order to be used by tasks, models must have a Model Factory which builds them. Factories must conform to the ModelFactory parent class. terratorch.models.model.ModelFactory # Bases: Protocol Source code in terratorch/models/model.py 32 33 class ModelFactory ( typing . Protocol ): def build_model ( self , * args , ** kwargs ) -> Model : ... You most likely do not need to implement your own model factory, unless you are wrapping another library which generates full models. For most cases, the encoder decoder factory can be used to combine a backbone with a decoder. To add new backbones or decoders, to be used with the encoder decoder factory they should be registered . To add a new model factory, it should be registered in the MODEL_FACTORY_REGISTRY . Adding a new model # To add a new backbone, simply create a class and annotate it (or a constructor function that instantiates it) with @TERRATORCH_BACKBONE_FACTORY.register . The model will be registered with the same name as the function. To create many model variants from the same class, the reccomended approach is to annotate a constructor function from each with a fully descriptive name. from terratorch.registry import TERRATORCH_BACKBONE_REGISTRY , BACKBONE_REGISTRY from torch import nn # make sure this is in the import path for terratorch @TERRATORCH_BACKBONE_REGISTRY . register class BasicBackbone ( nn . Module ): def __init__ ( self , out_channels = 64 ): super () . __init__ () self . flatten = nn . Flatten () self . layer = nn . Linear ( 224 * 224 , out_channels ) self . out_channels = [ out_channels ] def forward ( self , x ): return self . layer ( self . flatten ( x )) # you can build directly with the TERRATORCH_BACKBONE_REGISTRY # but typically this will be accessed from the BACKBONE_REGISTRY >>> BACKBONE_REGISTRY . build ( \"BasicBackbone\" , out_channels = 64 ) BasicBackbone ( ( flatten ): Flatten ( start_dim = 1 , end_dim =- 1 ) ( layer ): Linear ( in_features = 50176 , out_features = 64 , bias = True ) ) @TERRATORCH_BACKBONE_REGISTRY . register def basic_backbone_128 (): return BasicBackbone ( out_channels = 128 ) >>> BACKBONE_REGISTRY . build ( \"basic_backbone_128\" ) BasicBackbone ( ( flatten ): Flatten ( start_dim = 1 , end_dim =- 1 ) ( layer ): Linear ( in_features = 50176 , out_features = 128 , bias = True ) ) Adding a new decoder can be done in the same way with the TERRATORCH_DECODER_REGISTRY . Info All decoders will be passed the channel_list as the first argument for initialization. To pass your own path from where to load the weights with the PrithviModelFactory, you can make use of timm's pretrained_cfg_overlay . E.g. to pass a local path, you can pass the parameter backbone_pretrained_cfg_overlay = {\"file\": \"<local_path>\"} to the model factory. Besides file , you can also pass url , hf_hub_id , amongst others. Check timm's documentation for full details. terratorch.models.backbones.select_patch_embed_weights # select_patch_embed_weights ( state_dict , model , pretrained_bands , model_bands , proj_key = None , encoder_only = True ) # Filter out the patch embedding weights according to the bands being used. If a band exists in the pretrained_bands, but not in model_bands, drop it. If a band exists in model_bands, but not pretrained_bands, randomly initialize those weights. Parameters: state_dict ( dict ) \u2013 State Dict model ( Module ) \u2013 Model to load the weights onto. pretrained_bands ( list [ HLSBands | int ] ) \u2013 List of bands the model was pretrained on, in the correct order. model_bands ( list [ HLSBands | int ] ) \u2013 List of bands the model is going to be finetuned on, in the correct order proj_key ( str , default: None ) \u2013 Key to patch embedding projection weight in state_dict. Returns: dict ( dict ) \u2013 New state dict Source code in terratorch/models/backbones/select_patch_embed_weights.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def select_patch_embed_weights ( state_dict : dict , model : nn . Module , pretrained_bands : list [ HLSBands | int | OpticalBands | SARBands ], model_bands : list [ HLSBands | int | OpticalBands | SARBands ], proj_key : str | None = None , encoder_only : bool = True ) -> dict : \"\"\"Filter out the patch embedding weights according to the bands being used. If a band exists in the pretrained_bands, but not in model_bands, drop it. If a band exists in model_bands, but not pretrained_bands, randomly initialize those weights. Args: state_dict (dict): State Dict model (nn.Module): Model to load the weights onto. pretrained_bands (list[HLSBands | int]): List of bands the model was pretrained on, in the correct order. model_bands (list[HLSBands | int]): List of bands the model is going to be finetuned on, in the correct order proj_key (str, optional): Key to patch embedding projection weight in state_dict. Returns: dict: New state dict \"\"\" if ( type ( pretrained_bands ) == type ( model_bands )) | ( type ( pretrained_bands ) == int ) | ( type ( model_bands ) == int ): state_dict = get_state_dict ( state_dict ) prefix = None # we expect no prefix will be necessary in principle if proj_key is None : # Search for patch embedding weight in state dict proj_key , prefix = get_proj_key ( state_dict , return_prefix = True , encoder_only = encoder_only ) if proj_key is None or proj_key not in state_dict : raise Exception ( \"Could not find key for patch embed weight in state_dict.\" ) patch_embed_weight = state_dict [ proj_key ] # It seems `proj_key` can have different names for # the checkpoint and the model instance proj_key_ , _ = get_proj_key ( model . state_dict (), encoder_only = encoder_only ) if proj_key_ : temp_weight = model . state_dict ()[ proj_key_ ] . clone () else : temp_weight = model . state_dict ()[ proj_key ] . clone () # only do this if the patch size and tubelet size match. If not, start with random weights if patch_embed_weights_are_compatible ( temp_weight , patch_embed_weight ): torch . nn . init . xavier_uniform_ ( temp_weight . view ([ temp_weight . shape [ 0 ], - 1 ])) for index , band in enumerate ( model_bands ): if band in pretrained_bands : logging . info ( f \"Loaded weights for { band } in position { index } of patch embed\" ) temp_weight [:, index ] = patch_embed_weight [:, pretrained_bands . index ( band )] else : warnings . warn ( f \"Incompatible shapes between patch embedding of model { temp_weight . shape } and \\ of checkpoint { patch_embed_weight . shape } \" , category = UserWarning , stacklevel = 1 , ) state_dict [ proj_key ] = temp_weight if prefix : state_dict = remove_prefixes ( state_dict , prefix ) return state_dict Decoders # terratorch.models.decoders.fcn_decoder # FCNDecoder # Bases: Module Fully Convolutional Decoder Source code in terratorch/models/decoders/fcn_decoder.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 @TERRATORCH_DECODER_REGISTRY . register class FCNDecoder ( nn . Module ): \"\"\"Fully Convolutional Decoder\"\"\" def __init__ ( self , embed_dim : int , channels : int = 256 , num_convs : int = 4 , in_index : int = - 1 ) -> None : \"\"\"Constructor Args: embed_dim (_type_): Input embedding dimension channels (int, optional): Number of channels for each conv. Defaults to 256. num_convs (int, optional): Number of convs. Defaults to 4. in_index (int, optional): Index of the input list to take. Defaults to -1. \"\"\" super () . __init__ () kernel_size = 2 stride = 2 dilation = 1 padding = 0 output_padding = 0 self . channels = channels self . num_convs = num_convs self . in_index = in_index self . embed_dim = embed_dim [ in_index ] if num_convs < 1 : msg = \"num_convs must be >= 1\" raise Exception ( msg ) convs = [] for i in range ( num_convs ): in_channels = self . embed_dim if i == 0 else self . channels convs . append ( _conv_upscale_block ( in_channels , self . channels , kernel_size , stride , dilation , padding , output_padding ) ) self . convs = nn . Sequential ( * convs ) @property def out_channels ( self ): return self . channels def forward ( self , x : list [ Tensor ]): x = x [ self . in_index ] decoded = self . convs ( x ) return decoded __init__ ( embed_dim , channels = 256 , num_convs = 4 , in_index =- 1 ) # Constructor Parameters: embed_dim ( _type_ ) \u2013 Input embedding dimension channels ( int , default: 256 ) \u2013 Number of channels for each conv. Defaults to 256. num_convs ( int , default: 4 ) \u2013 Number of convs. Defaults to 4. in_index ( int , default: -1 ) \u2013 Index of the input list to take. Defaults to -1. Source code in terratorch/models/decoders/fcn_decoder.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , embed_dim : int , channels : int = 256 , num_convs : int = 4 , in_index : int = - 1 ) -> None : \"\"\"Constructor Args: embed_dim (_type_): Input embedding dimension channels (int, optional): Number of channels for each conv. Defaults to 256. num_convs (int, optional): Number of convs. Defaults to 4. in_index (int, optional): Index of the input list to take. Defaults to -1. \"\"\" super () . __init__ () kernel_size = 2 stride = 2 dilation = 1 padding = 0 output_padding = 0 self . channels = channels self . num_convs = num_convs self . in_index = in_index self . embed_dim = embed_dim [ in_index ] if num_convs < 1 : msg = \"num_convs must be >= 1\" raise Exception ( msg ) convs = [] for i in range ( num_convs ): in_channels = self . embed_dim if i == 0 else self . channels convs . append ( _conv_upscale_block ( in_channels , self . channels , kernel_size , stride , dilation , padding , output_padding ) ) self . convs = nn . Sequential ( * convs ) terratorch.models.decoders.identity_decoder # Pass the features straight through IdentityDecoder # Bases: Module Identity decoder. Useful to pass the feature straight to the head. Source code in terratorch/models/decoders/identity_decoder.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @TERRATORCH_DECODER_REGISTRY . register class IdentityDecoder ( nn . Module ): \"\"\"Identity decoder. Useful to pass the feature straight to the head.\"\"\" def __init__ ( self , embed_dim : int , out_index =- 1 ) -> None : \"\"\"Constructor Args: embed_dim (int): Input embedding dimension out_index (int, optional): Index of the input list to take.. Defaults to -1. \"\"\" super () . __init__ () self . embed_dim = embed_dim self . dim = out_index @property def out_channels ( self ): return self . embed_dim [ self . dim ] def forward ( self , x : list [ Tensor ]): return x [ self . dim ] __init__ ( embed_dim , out_index =- 1 ) # Constructor Parameters: embed_dim ( int ) \u2013 Input embedding dimension out_index ( int , default: -1 ) \u2013 Index of the input list to take.. Defaults to -1. Source code in terratorch/models/decoders/identity_decoder.py 15 16 17 18 19 20 21 22 23 24 def __init__ ( self , embed_dim : int , out_index =- 1 ) -> None : \"\"\"Constructor Args: embed_dim (int): Input embedding dimension out_index (int, optional): Index of the input list to take.. Defaults to -1. \"\"\" super () . __init__ () self . embed_dim = embed_dim self . dim = out_index terratorch.models.decoders.upernet_decoder # PPM # Bases: ModuleList Pooling Pyramid Module used in PSPNet. Source code in terratorch/models/decoders/upernet_decoder.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 class PPM ( nn . ModuleList ): \"\"\"Pooling Pyramid Module used in PSPNet.\"\"\" def __init__ ( self , pool_scales , in_channels , channels , align_corners ): \"\"\"Constructor Args: pool_scales (tuple[int]): Pooling scales used in Pooling Pyramid Module. in_channels (int): Input channels. channels (int): Channels after modules, before conv_seg. align_corners (bool): align_corners argument of F.interpolate. \"\"\" super () . __init__ () self . pool_scales = pool_scales self . align_corners = align_corners self . in_channels = in_channels self . channels = channels for pool_scale in pool_scales : self . append ( nn . Sequential ( nn . AdaptiveAvgPool2d ( pool_scale ), ConvModule ( self . in_channels , self . channels , 1 , inplace = True ), ) ) def forward ( self , x ): \"\"\"Forward function.\"\"\" ppm_outs = [] for ppm in self : ppm_out = ppm ( x ) upsampled_ppm_out = torch . nn . functional . interpolate ( ppm_out , size = x . size ()[ 2 :], mode = \"bilinear\" , align_corners = self . align_corners ) ppm_outs . append ( upsampled_ppm_out ) return ppm_outs __init__ ( pool_scales , in_channels , channels , align_corners ) # Constructor Parameters: pool_scales ( tuple [ int ] ) \u2013 Pooling scales used in Pooling Pyramid Module. in_channels ( int ) \u2013 Input channels. channels ( int ) \u2013 Channels after modules, before conv_seg. align_corners ( bool ) \u2013 align_corners argument of F.interpolate. Source code in terratorch/models/decoders/upernet_decoder.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def __init__ ( self , pool_scales , in_channels , channels , align_corners ): \"\"\"Constructor Args: pool_scales (tuple[int]): Pooling scales used in Pooling Pyramid Module. in_channels (int): Input channels. channels (int): Channels after modules, before conv_seg. align_corners (bool): align_corners argument of F.interpolate. \"\"\" super () . __init__ () self . pool_scales = pool_scales self . align_corners = align_corners self . in_channels = in_channels self . channels = channels for pool_scale in pool_scales : self . append ( nn . Sequential ( nn . AdaptiveAvgPool2d ( pool_scale ), ConvModule ( self . in_channels , self . channels , 1 , inplace = True ), ) ) forward ( x ) # Forward function. Source code in terratorch/models/decoders/upernet_decoder.py 176 177 178 179 180 181 182 183 184 185 def forward ( self , x ): \"\"\"Forward function.\"\"\" ppm_outs = [] for ppm in self : ppm_out = ppm ( x ) upsampled_ppm_out = torch . nn . functional . interpolate ( ppm_out , size = x . size ()[ 2 :], mode = \"bilinear\" , align_corners = self . align_corners ) ppm_outs . append ( upsampled_ppm_out ) return ppm_outs UperNetDecoder # Bases: Module UperNetDecoder. Adapted from MMSegmentation. Source code in terratorch/models/decoders/upernet_decoder.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 @TERRATORCH_DECODER_REGISTRY . register class UperNetDecoder ( nn . Module ): \"\"\"UperNetDecoder. Adapted from MMSegmentation.\"\"\" def __init__ ( self , embed_dim : list [ int ], pool_scales : tuple [ int ] = ( 1 , 2 , 3 , 6 ), channels : int = 256 , align_corners : bool = True , # noqa: FBT001, FBT002 scale_modules : bool = False , ): \"\"\"Constructor Args: embed_dim (list[int]): Input embedding dimension for each input. pool_scales (tuple[int], optional): Pooling scales used in Pooling Pyramid Module applied on the last feature. Default: (1, 2, 3, 6). channels (int, optional): Channels used in the decoder. Defaults to 256. align_corners (bool, optional): Wheter to align corners in rescaling. Defaults to True. scale_modules (bool, optional): Whether to apply scale modules to the inputs. Needed for plain ViT. Defaults to False. \"\"\" super () . __init__ () if scale_modules : # TODO: remove scale_modules before v1? warnings . warn ( \"DeprecationWarning: scale_modules is deprecated and will be removed in future versions. \" \"Use LearnedInterpolateToPyramidal neck instead.\" , stacklevel = 1 , ) self . scale_modules = scale_modules if scale_modules : self . fpn1 = nn . Sequential ( nn . ConvTranspose2d ( embed_dim [ 0 ], embed_dim [ 0 ] // 2 , 2 , 2 ), nn . BatchNorm2d ( embed_dim [ 0 ] // 2 ), nn . GELU (), nn . ConvTranspose2d ( embed_dim [ 0 ] // 2 , embed_dim [ 0 ] // 4 , 2 , 2 )) self . fpn2 = nn . Sequential ( nn . ConvTranspose2d ( embed_dim [ 1 ], embed_dim [ 1 ] // 2 , 2 , 2 )) self . fpn3 = nn . Sequential ( nn . Identity ()) self . fpn4 = nn . Sequential ( nn . MaxPool2d ( kernel_size = 2 , stride = 2 )) self . embed_dim = [ embed_dim [ 0 ] // 4 , embed_dim [ 1 ] // 2 , embed_dim [ 2 ], embed_dim [ 3 ]] else : self . embed_dim = embed_dim self . out_channels = channels self . channels = channels self . align_corners = align_corners # PSP Module self . psp_modules = PPM ( pool_scales , self . embed_dim [ - 1 ], self . channels , align_corners = self . align_corners , ) self . bottleneck = ConvModule ( self . embed_dim [ - 1 ] + len ( pool_scales ) * self . channels , self . channels , 3 , padding = 1 , inplace = True ) # FPN Module self . lateral_convs = nn . ModuleList () self . fpn_convs = nn . ModuleList () for embed_dim in self . embed_dim [: - 1 ]: # skip the top layer l_conv = ConvModule ( embed_dim , self . channels , 1 , inplace = False , ) fpn_conv = ConvModule ( self . channels , self . channels , 3 , padding = 1 , inplace = False , ) self . lateral_convs . append ( l_conv ) self . fpn_convs . append ( fpn_conv ) self . fpn_bottleneck = ConvModule ( len ( self . embed_dim ) * self . channels , self . channels , 3 , padding = 1 , inplace = True ) def psp_forward ( self , inputs ): \"\"\"Forward function of PSP module.\"\"\" x = inputs [ - 1 ] psp_outs = [ x ] psp_outs . extend ( self . psp_modules ( x )) psp_outs = torch . cat ( psp_outs , dim = 1 ) output = self . bottleneck ( psp_outs ) return output def forward ( self , inputs ): \"\"\"Forward function for feature maps before classifying each pixel with Args: inputs (list[Tensor]): List of multi-level img features. Returns: feats (Tensor): A tensor of shape (batch_size, self.channels, H, W) which is feature map for last layer of decoder head. \"\"\" if self . scale_modules : scaled_inputs = [] scaled_inputs . append ( self . fpn1 ( inputs [ 0 ])) scaled_inputs . append ( self . fpn2 ( inputs [ 1 ])) scaled_inputs . append ( self . fpn3 ( inputs [ 2 ])) scaled_inputs . append ( self . fpn4 ( inputs [ 3 ])) inputs = scaled_inputs # build laterals laterals = [ lateral_conv ( inputs [ i ]) for i , lateral_conv in enumerate ( self . lateral_convs )] laterals . append ( self . psp_forward ( inputs )) # build top-down path used_backbone_levels = len ( laterals ) for i in range ( used_backbone_levels - 1 , 0 , - 1 ): prev_shape = laterals [ i - 1 ] . shape [ 2 :] laterals [ i - 1 ] = laterals [ i - 1 ] + torch . nn . functional . interpolate ( laterals [ i ], size = prev_shape , mode = \"bilinear\" , align_corners = self . align_corners ) # build outputs fpn_outs = [ self . fpn_convs [ i ]( laterals [ i ]) for i in range ( used_backbone_levels - 1 )] # append psp feature fpn_outs . append ( laterals [ - 1 ]) for i in range ( used_backbone_levels - 1 , 0 , - 1 ): fpn_outs [ i ] = torch . nn . functional . interpolate ( fpn_outs [ i ], size = fpn_outs [ 0 ] . shape [ 2 :], mode = \"bilinear\" , align_corners = self . align_corners ) fpn_outs = torch . cat ( fpn_outs , dim = 1 ) feats = self . fpn_bottleneck ( fpn_outs ) return feats __init__ ( embed_dim , pool_scales = ( 1 , 2 , 3 , 6 ), channels = 256 , align_corners = True , scale_modules = False ) # Constructor Parameters: embed_dim ( list [ int ] ) \u2013 Input embedding dimension for each input. pool_scales ( tuple [ int ] , default: (1, 2, 3, 6) ) \u2013 Pooling scales used in Pooling Pyramid Module applied on the last feature. Default: (1, 2, 3, 6). channels ( int , default: 256 ) \u2013 Channels used in the decoder. Defaults to 256. align_corners ( bool , default: True ) \u2013 Wheter to align corners in rescaling. Defaults to True. scale_modules ( bool , default: False ) \u2013 Whether to apply scale modules to the inputs. Needed for plain ViT. Defaults to False. Source code in terratorch/models/decoders/upernet_decoder.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , embed_dim : list [ int ], pool_scales : tuple [ int ] = ( 1 , 2 , 3 , 6 ), channels : int = 256 , align_corners : bool = True , # noqa: FBT001, FBT002 scale_modules : bool = False , ): \"\"\"Constructor Args: embed_dim (list[int]): Input embedding dimension for each input. pool_scales (tuple[int], optional): Pooling scales used in Pooling Pyramid Module applied on the last feature. Default: (1, 2, 3, 6). channels (int, optional): Channels used in the decoder. Defaults to 256. align_corners (bool, optional): Wheter to align corners in rescaling. Defaults to True. scale_modules (bool, optional): Whether to apply scale modules to the inputs. Needed for plain ViT. Defaults to False. \"\"\" super () . __init__ () if scale_modules : # TODO: remove scale_modules before v1? warnings . warn ( \"DeprecationWarning: scale_modules is deprecated and will be removed in future versions. \" \"Use LearnedInterpolateToPyramidal neck instead.\" , stacklevel = 1 , ) self . scale_modules = scale_modules if scale_modules : self . fpn1 = nn . Sequential ( nn . ConvTranspose2d ( embed_dim [ 0 ], embed_dim [ 0 ] // 2 , 2 , 2 ), nn . BatchNorm2d ( embed_dim [ 0 ] // 2 ), nn . GELU (), nn . ConvTranspose2d ( embed_dim [ 0 ] // 2 , embed_dim [ 0 ] // 4 , 2 , 2 )) self . fpn2 = nn . Sequential ( nn . ConvTranspose2d ( embed_dim [ 1 ], embed_dim [ 1 ] // 2 , 2 , 2 )) self . fpn3 = nn . Sequential ( nn . Identity ()) self . fpn4 = nn . Sequential ( nn . MaxPool2d ( kernel_size = 2 , stride = 2 )) self . embed_dim = [ embed_dim [ 0 ] // 4 , embed_dim [ 1 ] // 2 , embed_dim [ 2 ], embed_dim [ 3 ]] else : self . embed_dim = embed_dim self . out_channels = channels self . channels = channels self . align_corners = align_corners # PSP Module self . psp_modules = PPM ( pool_scales , self . embed_dim [ - 1 ], self . channels , align_corners = self . align_corners , ) self . bottleneck = ConvModule ( self . embed_dim [ - 1 ] + len ( pool_scales ) * self . channels , self . channels , 3 , padding = 1 , inplace = True ) # FPN Module self . lateral_convs = nn . ModuleList () self . fpn_convs = nn . ModuleList () for embed_dim in self . embed_dim [: - 1 ]: # skip the top layer l_conv = ConvModule ( embed_dim , self . channels , 1 , inplace = False , ) fpn_conv = ConvModule ( self . channels , self . channels , 3 , padding = 1 , inplace = False , ) self . lateral_convs . append ( l_conv ) self . fpn_convs . append ( fpn_conv ) self . fpn_bottleneck = ConvModule ( len ( self . embed_dim ) * self . channels , self . channels , 3 , padding = 1 , inplace = True ) forward ( inputs ) # Forward function for feature maps before classifying each pixel with Args: inputs (list[Tensor]): List of multi-level img features. Returns: feats ( Tensor ) \u2013 A tensor of shape (batch_size, self.channels, H, W) which is feature map for last layer of decoder head. Source code in terratorch/models/decoders/upernet_decoder.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def forward ( self , inputs ): \"\"\"Forward function for feature maps before classifying each pixel with Args: inputs (list[Tensor]): List of multi-level img features. Returns: feats (Tensor): A tensor of shape (batch_size, self.channels, H, W) which is feature map for last layer of decoder head. \"\"\" if self . scale_modules : scaled_inputs = [] scaled_inputs . append ( self . fpn1 ( inputs [ 0 ])) scaled_inputs . append ( self . fpn2 ( inputs [ 1 ])) scaled_inputs . append ( self . fpn3 ( inputs [ 2 ])) scaled_inputs . append ( self . fpn4 ( inputs [ 3 ])) inputs = scaled_inputs # build laterals laterals = [ lateral_conv ( inputs [ i ]) for i , lateral_conv in enumerate ( self . lateral_convs )] laterals . append ( self . psp_forward ( inputs )) # build top-down path used_backbone_levels = len ( laterals ) for i in range ( used_backbone_levels - 1 , 0 , - 1 ): prev_shape = laterals [ i - 1 ] . shape [ 2 :] laterals [ i - 1 ] = laterals [ i - 1 ] + torch . nn . functional . interpolate ( laterals [ i ], size = prev_shape , mode = \"bilinear\" , align_corners = self . align_corners ) # build outputs fpn_outs = [ self . fpn_convs [ i ]( laterals [ i ]) for i in range ( used_backbone_levels - 1 )] # append psp feature fpn_outs . append ( laterals [ - 1 ]) for i in range ( used_backbone_levels - 1 , 0 , - 1 ): fpn_outs [ i ] = torch . nn . functional . interpolate ( fpn_outs [ i ], size = fpn_outs [ 0 ] . shape [ 2 :], mode = \"bilinear\" , align_corners = self . align_corners ) fpn_outs = torch . cat ( fpn_outs , dim = 1 ) feats = self . fpn_bottleneck ( fpn_outs ) return feats psp_forward ( inputs ) # Forward function of PSP module. Source code in terratorch/models/decoders/upernet_decoder.py 96 97 98 99 100 101 102 103 104 def psp_forward ( self , inputs ): \"\"\"Forward function of PSP module.\"\"\" x = inputs [ - 1 ] psp_outs = [ x ] psp_outs . extend ( self . psp_modules ( x )) psp_outs = torch . cat ( psp_outs , dim = 1 ) output = self . bottleneck ( psp_outs ) return output Heads # terratorch.models.heads.regression_head # RegressionHead # Bases: Module Regression head Source code in terratorch/models/heads/regression_head.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class RegressionHead ( nn . Module ): \"\"\"Regression head\"\"\" def __init__ ( self , in_channels : int , final_act : nn . Module | str | None = None , learned_upscale_layers : int = 0 , channel_list : list [ int ] | None = None , batch_norm : bool = True , dropout : float = 0 , ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels final_act (nn.Module | None, optional): Final activation to be applied. Defaults to None. learned_upscale_layers (int, optional): Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0. channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. batch_norm (bool, optional): Whether to apply batch norm. Defaults to True. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . learned_upscale_layers = learned_upscale_layers self . final_act = final_act if final_act else nn . Identity () if isinstance ( final_act , str ): module_name , class_name = final_act . rsplit ( \".\" , 1 ) target_class = getattr ( importlib . import_module ( module_name ), class_name ) self . final_act = target_class () pre_layers = [] if learned_upscale_layers != 0 : learned_upscale = nn . Sequential ( * [ PixelShuffleUpscale ( in_channels ) for _ in range ( self . learned_upscale_layers )] ) pre_layers . append ( learned_upscale ) if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 , bias = False ), nn . BatchNorm2d ( out_channels ), nn . ReLU ( inplace = True ), ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] pre_layers . append ( pre_head ) dropout = nn . Dropout2d ( dropout ) final_layer = nn . Conv2d ( in_channels = in_channels , out_channels = 1 , kernel_size = 1 ) self . head = nn . Sequential ( * [ * pre_layers , dropout , final_layer ]) def forward ( self , x ): output = self . head ( x ) return self . final_act ( output ) __init__ ( in_channels , final_act = None , learned_upscale_layers = 0 , channel_list = None , batch_norm = True , dropout = 0 ) # Constructor Parameters: in_channels ( int ) \u2013 Number of input channels final_act ( Module | None , default: None ) \u2013 Final activation to be applied. Defaults to None. learned_upscale_layers ( int , default: 0 ) \u2013 Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0. channel_list ( list [ int ] | None , default: None ) \u2013 List with number of channels for each Conv layer to be created. Defaults to None. batch_norm ( bool , default: True ) \u2013 Whether to apply batch norm. Defaults to True. dropout ( float , default: 0 ) \u2013 Dropout value to apply. Defaults to 0. Source code in terratorch/models/heads/regression_head.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , in_channels : int , final_act : nn . Module | str | None = None , learned_upscale_layers : int = 0 , channel_list : list [ int ] | None = None , batch_norm : bool = True , dropout : float = 0 , ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels final_act (nn.Module | None, optional): Final activation to be applied. Defaults to None. learned_upscale_layers (int, optional): Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0. channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. batch_norm (bool, optional): Whether to apply batch norm. Defaults to True. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . learned_upscale_layers = learned_upscale_layers self . final_act = final_act if final_act else nn . Identity () if isinstance ( final_act , str ): module_name , class_name = final_act . rsplit ( \".\" , 1 ) target_class = getattr ( importlib . import_module ( module_name ), class_name ) self . final_act = target_class () pre_layers = [] if learned_upscale_layers != 0 : learned_upscale = nn . Sequential ( * [ PixelShuffleUpscale ( in_channels ) for _ in range ( self . learned_upscale_layers )] ) pre_layers . append ( learned_upscale ) if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 , bias = False ), nn . BatchNorm2d ( out_channels ), nn . ReLU ( inplace = True ), ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] pre_layers . append ( pre_head ) dropout = nn . Dropout2d ( dropout ) final_layer = nn . Conv2d ( in_channels = in_channels , out_channels = 1 , kernel_size = 1 ) self . head = nn . Sequential ( * [ * pre_layers , dropout , final_layer ]) terratorch.models.heads.segmentation_head # SegmentationHead # Bases: Module Segmentation head Source code in terratorch/models/heads/segmentation_head.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class SegmentationHead ( nn . Module ): \"\"\"Segmentation head\"\"\" def __init__ ( self , in_channels : int , num_classes : int , channel_list : list [ int ] | None = None , dropout : float = 0 ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels num_classes (int): Number of output classes channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . num_classes = num_classes if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 ), nn . ReLU () ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Conv2d ( in_channels = in_channels , out_channels = num_classes , kernel_size = 1 , ), ) def forward ( self , x ): return self . head ( x ) __init__ ( in_channels , num_classes , channel_list = None , dropout = 0 ) # Constructor Parameters: in_channels ( int ) \u2013 Number of input channels num_classes ( int ) \u2013 Number of output classes channel_list ( list [ int ] | None , default: None ) \u2013 List with number of channels for each Conv layer to be created. Defaults to None. dropout ( float , default: 0 ) \u2013 Dropout value to apply. Defaults to 0. Source code in terratorch/models/heads/segmentation_head.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , in_channels : int , num_classes : int , channel_list : list [ int ] | None = None , dropout : float = 0 ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels num_classes (int): Number of output classes channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . num_classes = num_classes if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 ), nn . ReLU () ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Conv2d ( in_channels = in_channels , out_channels = num_classes , kernel_size = 1 , ), ) terratorch.models.heads.classification_head # ClassificationHead # Bases: Module Classification head Source code in terratorch/models/heads/classification_head.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class ClassificationHead ( nn . Module ): \"\"\"Classification head\"\"\" # how to allow cls token? def __init__ ( self , in_dim : int , num_classes : int , dim_list : list [ int ] | None = None , dropout : float = 0 , linear_after_pool : bool = False , ) -> None : \"\"\"Constructor Args: in_dim (int): Input dimensionality num_classes (int): Number of output classes dim_list (list[int] | None, optional): List with number of dimensions for each Linear layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. linear_after_pool (bool, optional): Apply pooling first, then apply the linear layer. Defaults to False \"\"\" super () . __init__ () self . num_classes = num_classes self . linear_after_pool = linear_after_pool if dim_list is None : pre_head = nn . Identity () else : def block ( in_dim , out_dim ): return nn . Sequential ( nn . Linear ( in_features = in_dim , out_features = out_dim ), nn . ReLU ()) dim_list = [ in_dim , * dim_list ] pre_head = nn . Sequential ( * [ block ( dim_list [ i ], dim_list [ i + 1 ]) for i in range ( len ( dim_list ) - 1 )]) in_dim = dim_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Linear ( in_features = in_dim , out_features = num_classes ), ) def forward ( self , x : Tensor ): x = x . reshape ( x . shape [ 0 ], x . shape [ 1 ], - 1 ) . permute ( 0 , 2 , 1 ) if self . linear_after_pool : x = x . mean ( axis = 1 ) out = self . head ( x ) else : x = self . head ( x ) out = x . mean ( axis = 1 ) return out __init__ ( in_dim , num_classes , dim_list = None , dropout = 0 , linear_after_pool = False ) # Constructor Parameters: in_dim ( int ) \u2013 Input dimensionality num_classes ( int ) \u2013 Number of output classes dim_list ( list [ int ] | None , default: None ) \u2013 List with number of dimensions for each Linear layer to be created. Defaults to None. dropout ( float , default: 0 ) \u2013 Dropout value to apply. Defaults to 0. linear_after_pool ( bool , default: False ) \u2013 Apply pooling first, then apply the linear layer. Defaults to False Source code in terratorch/models/heads/classification_head.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , in_dim : int , num_classes : int , dim_list : list [ int ] | None = None , dropout : float = 0 , linear_after_pool : bool = False , ) -> None : \"\"\"Constructor Args: in_dim (int): Input dimensionality num_classes (int): Number of output classes dim_list (list[int] | None, optional): List with number of dimensions for each Linear layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. linear_after_pool (bool, optional): Apply pooling first, then apply the linear layer. Defaults to False \"\"\" super () . __init__ () self . num_classes = num_classes self . linear_after_pool = linear_after_pool if dim_list is None : pre_head = nn . Identity () else : def block ( in_dim , out_dim ): return nn . Sequential ( nn . Linear ( in_features = in_dim , out_features = out_dim ), nn . ReLU ()) dim_list = [ in_dim , * dim_list ] pre_head = nn . Sequential ( * [ block ( dim_list [ i ], dim_list [ i + 1 ]) for i in range ( len ( dim_list ) - 1 )]) in_dim = dim_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Linear ( in_features = in_dim , out_features = num_classes ), ) Auxiliary Heads # terratorch.models.model.AuxiliaryHead dataclass # Class containing all information to create auxiliary heads. Parameters: name ( str ) \u2013 Name of the head. Should match the name given to the auxiliary loss. decoder ( str ) \u2013 Name of the decoder class to be used. decoder_args ( dict | None ) \u2013 parameters to be passed to the decoder constructor. Parameters for the decoder should be prefixed with decoder_ . Parameters for the head should be prefixed with head_ . Source code in terratorch/models/model.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @dataclass class AuxiliaryHead : \"\"\"Class containing all information to create auxiliary heads. Args: name (str): Name of the head. Should match the name given to the auxiliary loss. decoder (str): Name of the decoder class to be used. decoder_args (dict | None): parameters to be passed to the decoder constructor. Parameters for the decoder should be prefixed with `decoder_`. Parameters for the head should be prefixed with `head_`. \"\"\" name : str decoder : str decoder_args : dict | None Model Output # terratorch.models.model.ModelOutput dataclass # Source code in terratorch/models/model.py 10 11 12 13 @dataclass class ModelOutput : output : Tensor auxiliary_heads : dict [ str , Tensor ] = None Model Factory # terratorch.models.PrithviModelFactory # Bases: ModelFactory Source code in terratorch/models/prithvi_model_factory.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @MODEL_FACTORY_REGISTRY . register class PrithviModelFactory ( ModelFactory ): def __init__ ( self ) -> None : self . _factory : EncoderDecoderFactory = EncoderDecoderFactory () def build_model ( self , task : str , backbone : str | nn . Module , decoder : str | nn . Module , bands : list [ HLSBands | int ], in_channels : int | None = None , # this should be removed, can be derived from bands. But it is a breaking change num_classes : int | None = None , pretrained : bool = True , # noqa: FBT001, FBT002 num_frames : int = 1 , prepare_features_for_image_model : Callable | None = None , aux_decoders : list [ AuxiliaryHead ] | None = None , rescale : bool = True , # noqa: FBT002, FBT001 ** kwargs , ) -> Model : \"\"\"Model factory for prithvi models. Further arguments to be passed to the backbone, decoder or head. They should be prefixed with `backbone_`, `decoder_` and `head_` respectively. Args: task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\". backbone (str, nn.Module): Backbone to be used. If string, should be able to be parsed by the specified factory. Defaults to \"prithvi_100\". decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model. If a string, it will be created from a class exposed in decoder.__init__.py with the same name. If an nn.Module, we expect it to expose a property `decoder.out_channels`. Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\". in_channels (int, optional): Number of input channels. Defaults to 3. bands (list[terratorch.datasets.HLSBands], optional): Bands the model will be trained on. Should be a list of terratorch.datasets.HLSBands. Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE]. num_classes (int, optional): Number of classes. None for regression tasks. pretrained (Union[bool, Path], optional): Whether to load pretrained weights for the backbone, if available. Defaults to True. num_frames (int, optional): Number of timesteps for the model to handle. Defaults to 1. prepare_features_for_image_model (Callable | None): Function to be called on encoder features before passing them to the decoder. Defaults to None, which applies the identity function. aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead deciders to be added to the model. These decoders take the input from the encoder as well. rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True. Returns: nn.Module: Full model with encoder, decoder and head. \"\"\" warnings . warn ( \"PrithviModelFactory is deprecated. Please switch to EncoderDecoderFactory.\" , stacklevel = 1 ) if in_channels is None : in_channels = len ( bands ) # TODO: support auxiliary heads kwargs [ \"backbone_bands\" ] = bands kwargs [ \"backbone_in_chans\" ] = in_channels kwargs [ \"backbone_pretrained\" ] = pretrained kwargs [ \"backbone_num_frames\" ] = num_frames if prepare_features_for_image_model : msg = ( \"This functionality is no longer supported. Please migrate to EncoderDecoderFactory \\ and use necks.\" ) raise RuntimeError ( msg ) if not isinstance ( backbone , nn . Module ): if not backbone . startswith ( \"prithvi_\" ): msg = \"This class only handles models for `prithvi` encoders\" raise NotImplementedError ( msg ) return self . _factory . build_model ( task , backbone , decoder , num_classes = num_classes , necks = None , aux_decoders = aux_decoders , rescale = rescale , ** kwargs ) build_model ( task , backbone , decoder , bands , in_channels = None , num_classes = None , pretrained = True , num_frames = 1 , prepare_features_for_image_model = None , aux_decoders = None , rescale = True , ** kwargs ) # Model factory for prithvi models. Further arguments to be passed to the backbone, decoder or head. They should be prefixed with backbone_ , decoder_ and head_ respectively. Parameters: task ( str ) \u2013 Task to be performed. Currently supports \"segmentation\" and \"regression\". backbone ( ( str , Module ) ) \u2013 Backbone to be used. If string, should be able to be parsed by the specified factory. Defaults to \"prithvi_100\". decoder ( Union [ str , Module ] ) \u2013 Decoder to be used for the segmentation model. If a string, it will be created from a class exposed in decoder. init .py with the same name. If an nn.Module, we expect it to expose a property decoder.out_channels . Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\". in_channels ( int , default: None ) \u2013 Number of input channels. Defaults to 3. bands ( list [ HLSBands ] ) \u2013 Bands the model will be trained on. Should be a list of terratorch.datasets.HLSBands. Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE]. num_classes ( int , default: None ) \u2013 Number of classes. None for regression tasks. pretrained ( Union [ bool , Path ] , default: True ) \u2013 Whether to load pretrained weights for the backbone, if available. Defaults to True. num_frames ( int , default: 1 ) \u2013 Number of timesteps for the model to handle. Defaults to 1. prepare_features_for_image_model ( Callable | None , default: None ) \u2013 Function to be called on encoder features before passing them to the decoder. Defaults to None, which applies the identity function. aux_decoders ( list [ AuxiliaryHead ] | None , default: None ) \u2013 List of AuxiliaryHead deciders to be added to the model. These decoders take the input from the encoder as well. rescale ( bool , default: True ) \u2013 Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True. Returns: Model \u2013 nn.Module: Full model with encoder, decoder and head. Source code in terratorch/models/prithvi_model_factory.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def build_model ( self , task : str , backbone : str | nn . Module , decoder : str | nn . Module , bands : list [ HLSBands | int ], in_channels : int | None = None , # this should be removed, can be derived from bands. But it is a breaking change num_classes : int | None = None , pretrained : bool = True , # noqa: FBT001, FBT002 num_frames : int = 1 , prepare_features_for_image_model : Callable | None = None , aux_decoders : list [ AuxiliaryHead ] | None = None , rescale : bool = True , # noqa: FBT002, FBT001 ** kwargs , ) -> Model : \"\"\"Model factory for prithvi models. Further arguments to be passed to the backbone, decoder or head. They should be prefixed with `backbone_`, `decoder_` and `head_` respectively. Args: task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\". backbone (str, nn.Module): Backbone to be used. If string, should be able to be parsed by the specified factory. Defaults to \"prithvi_100\". decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model. If a string, it will be created from a class exposed in decoder.__init__.py with the same name. If an nn.Module, we expect it to expose a property `decoder.out_channels`. Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\". in_channels (int, optional): Number of input channels. Defaults to 3. bands (list[terratorch.datasets.HLSBands], optional): Bands the model will be trained on. Should be a list of terratorch.datasets.HLSBands. Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE]. num_classes (int, optional): Number of classes. None for regression tasks. pretrained (Union[bool, Path], optional): Whether to load pretrained weights for the backbone, if available. Defaults to True. num_frames (int, optional): Number of timesteps for the model to handle. Defaults to 1. prepare_features_for_image_model (Callable | None): Function to be called on encoder features before passing them to the decoder. Defaults to None, which applies the identity function. aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead deciders to be added to the model. These decoders take the input from the encoder as well. rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True. Returns: nn.Module: Full model with encoder, decoder and head. \"\"\" warnings . warn ( \"PrithviModelFactory is deprecated. Please switch to EncoderDecoderFactory.\" , stacklevel = 1 ) if in_channels is None : in_channels = len ( bands ) # TODO: support auxiliary heads kwargs [ \"backbone_bands\" ] = bands kwargs [ \"backbone_in_chans\" ] = in_channels kwargs [ \"backbone_pretrained\" ] = pretrained kwargs [ \"backbone_num_frames\" ] = num_frames if prepare_features_for_image_model : msg = ( \"This functionality is no longer supported. Please migrate to EncoderDecoderFactory \\ and use necks.\" ) raise RuntimeError ( msg ) if not isinstance ( backbone , nn . Module ): if not backbone . startswith ( \"prithvi_\" ): msg = \"This class only handles models for `prithvi` encoders\" raise NotImplementedError ( msg ) return self . _factory . build_model ( task , backbone , decoder , num_classes = num_classes , necks = None , aux_decoders = aux_decoders , rescale = rescale , ** kwargs ) terratorch.models.SMPModelFactory # Bases: ModelFactory Source code in terratorch/models/smp_model_factory.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 @MODEL_FACTORY_REGISTRY . register class SMPModelFactory ( ModelFactory ): def build_model ( self , task : str , backbone : str , model : str , bands : list [ HLSBands | int ], in_channels : int | None = None , num_classes : int = 1 , pretrained : str | bool | None = True , # noqa: FBT002 prepare_features_for_image_model : Callable | None = None , regression_relu : bool = False , # noqa: FBT001, FBT002 ** kwargs , ) -> Model : \"\"\" Factory class for creating SMP (Segmentation Models Pytorch) based models with optional customization. This factory handles the instantiation of segmentation and regression models using specified encoders and decoders from the SMP library, along with custom modifications and extensions such as auxiliary decoders or modified encoders. Attributes: task (str): Specifies the task for which the model is being built. Supported tasks are \"segmentation\". backbone (str): Specifies the backbone model to be used. decoder (str): Specifies the decoder to be used for constructing the segmentation model. bands (list[terratorch.datasets.HLSBands | int]): A list specifying the bands that the model will operate on. These are expected to be from terratorch.datasets.HLSBands. in_channels (int, optional): Specifies the number of input channels. Defaults to None. num_classes (int, optional): The number of output classes for the model. pretrained (bool | Path, optional): Indicates whether to load pretrained weights for the backbone. Can also specify a path to weights. Defaults to True. num_frames (int, optional): Specifies the number of timesteps the model should handle. Useful for temporal models. regression_relu (bool): Whether to apply ReLU activation in the case of regression tasks. **kwargs: Additional arguments that might be passed to further customize the backbone, decoder, or any auxiliary heads. These should be prefixed appropriately Raises: ValueError: If the specified decoder is not supported by SMP. Exception: If the specified task is not \"segmentation\" Returns: nn.Module: A model instance wrapped in SMPModelWrapper configured according to the specified parameters and tasks. \"\"\" if task != \"segmentation\" : msg = f \"SMP models can only perform segmentation, but got task { task } \" raise Exception ( msg ) bands = [ HLSBands . try_convert_to_hls_bands_enum ( b ) for b in bands ] if in_channels is None : in_channels = len ( bands ) # Gets decoder module. model_module = getattr ( smp , model , None ) if model_module is None : msg = f \"Decoder { model } is not supported in SMP.\" raise ValueError ( msg ) backbone_kwargs , kwargs = extract_prefix_keys ( kwargs , \"backbone_\" ) # Encoder params should be prefixed backbone_ smp_kwargs , kwargs = extract_prefix_keys ( backbone_kwargs , \"smp_\" ) # Smp model params should be prefixed smp_ aux_params , kwargs = extract_prefix_keys ( backbone_kwargs , \"aux_\" ) # Auxiliary head params should be prefixed aux_ aux_params = None if aux_params == {} else aux_params if isinstance ( pretrained , bool ): if pretrained : pretrained = \"imagenet\" else : pretrained = None # If encoder not currently supported by SMP (custom encoder). if backbone not in smp_encoders : # These params must be included in the config file with appropriate prefix. required_params = { \"encoder_depth\" : smp_kwargs , \"out_channels\" : backbone_kwargs , \"output_stride\" : backbone_kwargs , } for param , config_dict in required_params . items (): if param not in config_dict : msg = f \"Config must include the ' { param } ' parameter\" raise ValueError ( msg ) # Using new encoder. backbone_class = make_smp_encoder ( backbone ) backbone_kwargs [ \"prepare_features_for_image_model\" ] = prepare_features_for_image_model # Registering custom encoder into SMP. register_custom_encoder ( backbone_class , backbone_kwargs , pretrained ) model_args = { \"encoder_name\" : \"SMPEncoderWrapperWithPFFIM\" , \"encoder_weights\" : pretrained , \"in_channels\" : in_channels , \"classes\" : num_classes , ** smp_kwargs , } # Using SMP encoder. else : model_args = { \"encoder_name\" : backbone , \"encoder_weights\" : pretrained , \"in_channels\" : in_channels , \"classes\" : num_classes , ** smp_kwargs , } model = model_module ( ** model_args , aux_params = aux_params ) return SMPModelWrapper ( model , relu = task == \"regression\" and regression_relu , squeeze_single_class = task == \"regression\" ) build_model ( task , backbone , model , bands , in_channels = None , num_classes = 1 , pretrained = True , prepare_features_for_image_model = None , regression_relu = False , ** kwargs ) # Factory class for creating SMP (Segmentation Models Pytorch) based models with optional customization. This factory handles the instantiation of segmentation and regression models using specified encoders and decoders from the SMP library, along with custom modifications and extensions such as auxiliary decoders or modified encoders. Attributes: task ( str ) \u2013 Specifies the task for which the model is being built. Supported tasks are \"segmentation\". backbone ( str ) \u2013 Specifies the backbone model to be used. decoder ( str ) \u2013 Specifies the decoder to be used for constructing the segmentation model. bands ( list [ HLSBands | int ] ) \u2013 A list specifying the bands that the model will operate on. These are expected to be from terratorch.datasets.HLSBands. in_channels ( int ) \u2013 Specifies the number of input channels. Defaults to None. num_classes ( int ) \u2013 The number of output classes for the model. pretrained ( bool | Path ) \u2013 Indicates whether to load pretrained weights for the backbone. Can also specify a path to weights. Defaults to True. num_frames ( int ) \u2013 Specifies the number of timesteps the model should handle. Useful for temporal models. regression_relu ( bool ) \u2013 Whether to apply ReLU activation in the case of regression tasks. **kwargs ( bool ) \u2013 Additional arguments that might be passed to further customize the backbone, decoder, or any auxiliary heads. These should be prefixed appropriately Raises: ValueError \u2013 If the specified decoder is not supported by SMP. Exception \u2013 If the specified task is not \"segmentation\" Returns: Model \u2013 nn.Module: A model instance wrapped in SMPModelWrapper configured according to the specified parameters and tasks. Source code in terratorch/models/smp_model_factory.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def build_model ( self , task : str , backbone : str , model : str , bands : list [ HLSBands | int ], in_channels : int | None = None , num_classes : int = 1 , pretrained : str | bool | None = True , # noqa: FBT002 prepare_features_for_image_model : Callable | None = None , regression_relu : bool = False , # noqa: FBT001, FBT002 ** kwargs , ) -> Model : \"\"\" Factory class for creating SMP (Segmentation Models Pytorch) based models with optional customization. This factory handles the instantiation of segmentation and regression models using specified encoders and decoders from the SMP library, along with custom modifications and extensions such as auxiliary decoders or modified encoders. Attributes: task (str): Specifies the task for which the model is being built. Supported tasks are \"segmentation\". backbone (str): Specifies the backbone model to be used. decoder (str): Specifies the decoder to be used for constructing the segmentation model. bands (list[terratorch.datasets.HLSBands | int]): A list specifying the bands that the model will operate on. These are expected to be from terratorch.datasets.HLSBands. in_channels (int, optional): Specifies the number of input channels. Defaults to None. num_classes (int, optional): The number of output classes for the model. pretrained (bool | Path, optional): Indicates whether to load pretrained weights for the backbone. Can also specify a path to weights. Defaults to True. num_frames (int, optional): Specifies the number of timesteps the model should handle. Useful for temporal models. regression_relu (bool): Whether to apply ReLU activation in the case of regression tasks. **kwargs: Additional arguments that might be passed to further customize the backbone, decoder, or any auxiliary heads. These should be prefixed appropriately Raises: ValueError: If the specified decoder is not supported by SMP. Exception: If the specified task is not \"segmentation\" Returns: nn.Module: A model instance wrapped in SMPModelWrapper configured according to the specified parameters and tasks. \"\"\" if task != \"segmentation\" : msg = f \"SMP models can only perform segmentation, but got task { task } \" raise Exception ( msg ) bands = [ HLSBands . try_convert_to_hls_bands_enum ( b ) for b in bands ] if in_channels is None : in_channels = len ( bands ) # Gets decoder module. model_module = getattr ( smp , model , None ) if model_module is None : msg = f \"Decoder { model } is not supported in SMP.\" raise ValueError ( msg ) backbone_kwargs , kwargs = extract_prefix_keys ( kwargs , \"backbone_\" ) # Encoder params should be prefixed backbone_ smp_kwargs , kwargs = extract_prefix_keys ( backbone_kwargs , \"smp_\" ) # Smp model params should be prefixed smp_ aux_params , kwargs = extract_prefix_keys ( backbone_kwargs , \"aux_\" ) # Auxiliary head params should be prefixed aux_ aux_params = None if aux_params == {} else aux_params if isinstance ( pretrained , bool ): if pretrained : pretrained = \"imagenet\" else : pretrained = None # If encoder not currently supported by SMP (custom encoder). if backbone not in smp_encoders : # These params must be included in the config file with appropriate prefix. required_params = { \"encoder_depth\" : smp_kwargs , \"out_channels\" : backbone_kwargs , \"output_stride\" : backbone_kwargs , } for param , config_dict in required_params . items (): if param not in config_dict : msg = f \"Config must include the ' { param } ' parameter\" raise ValueError ( msg ) # Using new encoder. backbone_class = make_smp_encoder ( backbone ) backbone_kwargs [ \"prepare_features_for_image_model\" ] = prepare_features_for_image_model # Registering custom encoder into SMP. register_custom_encoder ( backbone_class , backbone_kwargs , pretrained ) model_args = { \"encoder_name\" : \"SMPEncoderWrapperWithPFFIM\" , \"encoder_weights\" : pretrained , \"in_channels\" : in_channels , \"classes\" : num_classes , ** smp_kwargs , } # Using SMP encoder. else : model_args = { \"encoder_name\" : backbone , \"encoder_weights\" : pretrained , \"in_channels\" : in_channels , \"classes\" : num_classes , ** smp_kwargs , } model = model_module ( ** model_args , aux_params = aux_params ) return SMPModelWrapper ( model , relu = task == \"regression\" and regression_relu , squeeze_single_class = task == \"regression\" ) Adding new model types # Adding new model types is as simple as creating a new factory that produces models. See for instance the example below for a potential SMPModelFactory from terratorch.models.model import register_factory @register_factory class SMPModelFactory ( ModelFactory ): def build_model ( self , task : str , backbone : str | nn . Module , decoder : str | nn . Module , in_channels : int , ** kwargs , ) -> Model : model = smp . Unet ( encoder_name = \"resnet34\" , encoder_weights = None , in_channels = in_channels , classes = 1 ) return SMPModelWrapper ( model ) @register_factory class SMPModelWrapper ( Model , nn . Module ): def __init__ ( self , smp_model ) -> None : super () . __init__ () self . smp_model = smp_model def forward ( self , * args , ** kwargs ): return ModelOutput ( self . smp_model ( * args , ** kwargs ) . squeeze ( 1 )) def freeze_encoder ( self ): pass def freeze_decoder ( self ): pass Custom modules with CLI # Custom modules must be in the import path in order to be registered in the appropriate registries. In order to do this without modifying the code when using the CLI, you may place your modules under a custom_modules directory. This must be in the directory from which you execute terratorch.","title":"Models"},{"location":"models/#models","text":"To interface with terratorch tasks correctly, models must inherit from the Model parent class:","title":"Models"},{"location":"models/#terratorch.models.model.Model","text":"Bases: ABC , Module Source code in terratorch/models/model.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Model ( ABC , nn . Module ): def __init__ ( self , * args , ** kwargs ) -> None : super () . __init__ ( * args , ** kwargs ) @abstractmethod def freeze_encoder ( self ): pass @abstractmethod def freeze_decoder ( self ): pass @abstractmethod def forward ( self , * args , ** kwargs ) -> ModelOutput : pass and have a forward method which returns an object ModelOutput :","title":"Model"},{"location":"models/#terratorch.models.model.ModelOutput","text":"Source code in terratorch/models/model.py 10 11 12 13 @dataclass class ModelOutput : output : Tensor auxiliary_heads : dict [ str , Tensor ] = None","title":"ModelOutput"},{"location":"models/#model-factories","text":"In order to be used by tasks, models must have a Model Factory which builds them. Factories must conform to the ModelFactory parent class.","title":"Model Factories"},{"location":"models/#terratorch.models.model.ModelFactory","text":"Bases: Protocol Source code in terratorch/models/model.py 32 33 class ModelFactory ( typing . Protocol ): def build_model ( self , * args , ** kwargs ) -> Model : ... You most likely do not need to implement your own model factory, unless you are wrapping another library which generates full models. For most cases, the encoder decoder factory can be used to combine a backbone with a decoder. To add new backbones or decoders, to be used with the encoder decoder factory they should be registered . To add a new model factory, it should be registered in the MODEL_FACTORY_REGISTRY .","title":"ModelFactory"},{"location":"models/#adding-a-new-model","text":"To add a new backbone, simply create a class and annotate it (or a constructor function that instantiates it) with @TERRATORCH_BACKBONE_FACTORY.register . The model will be registered with the same name as the function. To create many model variants from the same class, the reccomended approach is to annotate a constructor function from each with a fully descriptive name. from terratorch.registry import TERRATORCH_BACKBONE_REGISTRY , BACKBONE_REGISTRY from torch import nn # make sure this is in the import path for terratorch @TERRATORCH_BACKBONE_REGISTRY . register class BasicBackbone ( nn . Module ): def __init__ ( self , out_channels = 64 ): super () . __init__ () self . flatten = nn . Flatten () self . layer = nn . Linear ( 224 * 224 , out_channels ) self . out_channels = [ out_channels ] def forward ( self , x ): return self . layer ( self . flatten ( x )) # you can build directly with the TERRATORCH_BACKBONE_REGISTRY # but typically this will be accessed from the BACKBONE_REGISTRY >>> BACKBONE_REGISTRY . build ( \"BasicBackbone\" , out_channels = 64 ) BasicBackbone ( ( flatten ): Flatten ( start_dim = 1 , end_dim =- 1 ) ( layer ): Linear ( in_features = 50176 , out_features = 64 , bias = True ) ) @TERRATORCH_BACKBONE_REGISTRY . register def basic_backbone_128 (): return BasicBackbone ( out_channels = 128 ) >>> BACKBONE_REGISTRY . build ( \"basic_backbone_128\" ) BasicBackbone ( ( flatten ): Flatten ( start_dim = 1 , end_dim =- 1 ) ( layer ): Linear ( in_features = 50176 , out_features = 128 , bias = True ) ) Adding a new decoder can be done in the same way with the TERRATORCH_DECODER_REGISTRY . Info All decoders will be passed the channel_list as the first argument for initialization. To pass your own path from where to load the weights with the PrithviModelFactory, you can make use of timm's pretrained_cfg_overlay . E.g. to pass a local path, you can pass the parameter backbone_pretrained_cfg_overlay = {\"file\": \"<local_path>\"} to the model factory. Besides file , you can also pass url , hf_hub_id , amongst others. Check timm's documentation for full details.","title":"Adding a new model"},{"location":"models/#terratorch.models.backbones.select_patch_embed_weights","text":"","title":"select_patch_embed_weights"},{"location":"models/#terratorch.models.backbones.select_patch_embed_weights.select_patch_embed_weights","text":"Filter out the patch embedding weights according to the bands being used. If a band exists in the pretrained_bands, but not in model_bands, drop it. If a band exists in model_bands, but not pretrained_bands, randomly initialize those weights. Parameters: state_dict ( dict ) \u2013 State Dict model ( Module ) \u2013 Model to load the weights onto. pretrained_bands ( list [ HLSBands | int ] ) \u2013 List of bands the model was pretrained on, in the correct order. model_bands ( list [ HLSBands | int ] ) \u2013 List of bands the model is going to be finetuned on, in the correct order proj_key ( str , default: None ) \u2013 Key to patch embedding projection weight in state_dict. Returns: dict ( dict ) \u2013 New state dict Source code in terratorch/models/backbones/select_patch_embed_weights.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def select_patch_embed_weights ( state_dict : dict , model : nn . Module , pretrained_bands : list [ HLSBands | int | OpticalBands | SARBands ], model_bands : list [ HLSBands | int | OpticalBands | SARBands ], proj_key : str | None = None , encoder_only : bool = True ) -> dict : \"\"\"Filter out the patch embedding weights according to the bands being used. If a band exists in the pretrained_bands, but not in model_bands, drop it. If a band exists in model_bands, but not pretrained_bands, randomly initialize those weights. Args: state_dict (dict): State Dict model (nn.Module): Model to load the weights onto. pretrained_bands (list[HLSBands | int]): List of bands the model was pretrained on, in the correct order. model_bands (list[HLSBands | int]): List of bands the model is going to be finetuned on, in the correct order proj_key (str, optional): Key to patch embedding projection weight in state_dict. Returns: dict: New state dict \"\"\" if ( type ( pretrained_bands ) == type ( model_bands )) | ( type ( pretrained_bands ) == int ) | ( type ( model_bands ) == int ): state_dict = get_state_dict ( state_dict ) prefix = None # we expect no prefix will be necessary in principle if proj_key is None : # Search for patch embedding weight in state dict proj_key , prefix = get_proj_key ( state_dict , return_prefix = True , encoder_only = encoder_only ) if proj_key is None or proj_key not in state_dict : raise Exception ( \"Could not find key for patch embed weight in state_dict.\" ) patch_embed_weight = state_dict [ proj_key ] # It seems `proj_key` can have different names for # the checkpoint and the model instance proj_key_ , _ = get_proj_key ( model . state_dict (), encoder_only = encoder_only ) if proj_key_ : temp_weight = model . state_dict ()[ proj_key_ ] . clone () else : temp_weight = model . state_dict ()[ proj_key ] . clone () # only do this if the patch size and tubelet size match. If not, start with random weights if patch_embed_weights_are_compatible ( temp_weight , patch_embed_weight ): torch . nn . init . xavier_uniform_ ( temp_weight . view ([ temp_weight . shape [ 0 ], - 1 ])) for index , band in enumerate ( model_bands ): if band in pretrained_bands : logging . info ( f \"Loaded weights for { band } in position { index } of patch embed\" ) temp_weight [:, index ] = patch_embed_weight [:, pretrained_bands . index ( band )] else : warnings . warn ( f \"Incompatible shapes between patch embedding of model { temp_weight . shape } and \\ of checkpoint { patch_embed_weight . shape } \" , category = UserWarning , stacklevel = 1 , ) state_dict [ proj_key ] = temp_weight if prefix : state_dict = remove_prefixes ( state_dict , prefix ) return state_dict","title":"select_patch_embed_weights"},{"location":"models/#decoders","text":"","title":"Decoders"},{"location":"models/#terratorch.models.decoders.fcn_decoder","text":"","title":"fcn_decoder"},{"location":"models/#terratorch.models.decoders.fcn_decoder.FCNDecoder","text":"Bases: Module Fully Convolutional Decoder Source code in terratorch/models/decoders/fcn_decoder.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 @TERRATORCH_DECODER_REGISTRY . register class FCNDecoder ( nn . Module ): \"\"\"Fully Convolutional Decoder\"\"\" def __init__ ( self , embed_dim : int , channels : int = 256 , num_convs : int = 4 , in_index : int = - 1 ) -> None : \"\"\"Constructor Args: embed_dim (_type_): Input embedding dimension channels (int, optional): Number of channels for each conv. Defaults to 256. num_convs (int, optional): Number of convs. Defaults to 4. in_index (int, optional): Index of the input list to take. Defaults to -1. \"\"\" super () . __init__ () kernel_size = 2 stride = 2 dilation = 1 padding = 0 output_padding = 0 self . channels = channels self . num_convs = num_convs self . in_index = in_index self . embed_dim = embed_dim [ in_index ] if num_convs < 1 : msg = \"num_convs must be >= 1\" raise Exception ( msg ) convs = [] for i in range ( num_convs ): in_channels = self . embed_dim if i == 0 else self . channels convs . append ( _conv_upscale_block ( in_channels , self . channels , kernel_size , stride , dilation , padding , output_padding ) ) self . convs = nn . Sequential ( * convs ) @property def out_channels ( self ): return self . channels def forward ( self , x : list [ Tensor ]): x = x [ self . in_index ] decoded = self . convs ( x ) return decoded","title":"FCNDecoder"},{"location":"models/#terratorch.models.decoders.fcn_decoder.FCNDecoder.__init__","text":"Constructor Parameters: embed_dim ( _type_ ) \u2013 Input embedding dimension channels ( int , default: 256 ) \u2013 Number of channels for each conv. Defaults to 256. num_convs ( int , default: 4 ) \u2013 Number of convs. Defaults to 4. in_index ( int , default: -1 ) \u2013 Index of the input list to take. Defaults to -1. Source code in terratorch/models/decoders/fcn_decoder.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , embed_dim : int , channels : int = 256 , num_convs : int = 4 , in_index : int = - 1 ) -> None : \"\"\"Constructor Args: embed_dim (_type_): Input embedding dimension channels (int, optional): Number of channels for each conv. Defaults to 256. num_convs (int, optional): Number of convs. Defaults to 4. in_index (int, optional): Index of the input list to take. Defaults to -1. \"\"\" super () . __init__ () kernel_size = 2 stride = 2 dilation = 1 padding = 0 output_padding = 0 self . channels = channels self . num_convs = num_convs self . in_index = in_index self . embed_dim = embed_dim [ in_index ] if num_convs < 1 : msg = \"num_convs must be >= 1\" raise Exception ( msg ) convs = [] for i in range ( num_convs ): in_channels = self . embed_dim if i == 0 else self . channels convs . append ( _conv_upscale_block ( in_channels , self . channels , kernel_size , stride , dilation , padding , output_padding ) ) self . convs = nn . Sequential ( * convs )","title":"__init__"},{"location":"models/#terratorch.models.decoders.identity_decoder","text":"Pass the features straight through","title":"identity_decoder"},{"location":"models/#terratorch.models.decoders.identity_decoder.IdentityDecoder","text":"Bases: Module Identity decoder. Useful to pass the feature straight to the head. Source code in terratorch/models/decoders/identity_decoder.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @TERRATORCH_DECODER_REGISTRY . register class IdentityDecoder ( nn . Module ): \"\"\"Identity decoder. Useful to pass the feature straight to the head.\"\"\" def __init__ ( self , embed_dim : int , out_index =- 1 ) -> None : \"\"\"Constructor Args: embed_dim (int): Input embedding dimension out_index (int, optional): Index of the input list to take.. Defaults to -1. \"\"\" super () . __init__ () self . embed_dim = embed_dim self . dim = out_index @property def out_channels ( self ): return self . embed_dim [ self . dim ] def forward ( self , x : list [ Tensor ]): return x [ self . dim ]","title":"IdentityDecoder"},{"location":"models/#terratorch.models.decoders.identity_decoder.IdentityDecoder.__init__","text":"Constructor Parameters: embed_dim ( int ) \u2013 Input embedding dimension out_index ( int , default: -1 ) \u2013 Index of the input list to take.. Defaults to -1. Source code in terratorch/models/decoders/identity_decoder.py 15 16 17 18 19 20 21 22 23 24 def __init__ ( self , embed_dim : int , out_index =- 1 ) -> None : \"\"\"Constructor Args: embed_dim (int): Input embedding dimension out_index (int, optional): Index of the input list to take.. Defaults to -1. \"\"\" super () . __init__ () self . embed_dim = embed_dim self . dim = out_index","title":"__init__"},{"location":"models/#terratorch.models.decoders.upernet_decoder","text":"","title":"upernet_decoder"},{"location":"models/#terratorch.models.decoders.upernet_decoder.PPM","text":"Bases: ModuleList Pooling Pyramid Module used in PSPNet. Source code in terratorch/models/decoders/upernet_decoder.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 class PPM ( nn . ModuleList ): \"\"\"Pooling Pyramid Module used in PSPNet.\"\"\" def __init__ ( self , pool_scales , in_channels , channels , align_corners ): \"\"\"Constructor Args: pool_scales (tuple[int]): Pooling scales used in Pooling Pyramid Module. in_channels (int): Input channels. channels (int): Channels after modules, before conv_seg. align_corners (bool): align_corners argument of F.interpolate. \"\"\" super () . __init__ () self . pool_scales = pool_scales self . align_corners = align_corners self . in_channels = in_channels self . channels = channels for pool_scale in pool_scales : self . append ( nn . Sequential ( nn . AdaptiveAvgPool2d ( pool_scale ), ConvModule ( self . in_channels , self . channels , 1 , inplace = True ), ) ) def forward ( self , x ): \"\"\"Forward function.\"\"\" ppm_outs = [] for ppm in self : ppm_out = ppm ( x ) upsampled_ppm_out = torch . nn . functional . interpolate ( ppm_out , size = x . size ()[ 2 :], mode = \"bilinear\" , align_corners = self . align_corners ) ppm_outs . append ( upsampled_ppm_out ) return ppm_outs","title":"PPM"},{"location":"models/#terratorch.models.decoders.upernet_decoder.PPM.__init__","text":"Constructor Parameters: pool_scales ( tuple [ int ] ) \u2013 Pooling scales used in Pooling Pyramid Module. in_channels ( int ) \u2013 Input channels. channels ( int ) \u2013 Channels after modules, before conv_seg. align_corners ( bool ) \u2013 align_corners argument of F.interpolate. Source code in terratorch/models/decoders/upernet_decoder.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def __init__ ( self , pool_scales , in_channels , channels , align_corners ): \"\"\"Constructor Args: pool_scales (tuple[int]): Pooling scales used in Pooling Pyramid Module. in_channels (int): Input channels. channels (int): Channels after modules, before conv_seg. align_corners (bool): align_corners argument of F.interpolate. \"\"\" super () . __init__ () self . pool_scales = pool_scales self . align_corners = align_corners self . in_channels = in_channels self . channels = channels for pool_scale in pool_scales : self . append ( nn . Sequential ( nn . AdaptiveAvgPool2d ( pool_scale ), ConvModule ( self . in_channels , self . channels , 1 , inplace = True ), ) )","title":"__init__"},{"location":"models/#terratorch.models.decoders.upernet_decoder.PPM.forward","text":"Forward function. Source code in terratorch/models/decoders/upernet_decoder.py 176 177 178 179 180 181 182 183 184 185 def forward ( self , x ): \"\"\"Forward function.\"\"\" ppm_outs = [] for ppm in self : ppm_out = ppm ( x ) upsampled_ppm_out = torch . nn . functional . interpolate ( ppm_out , size = x . size ()[ 2 :], mode = \"bilinear\" , align_corners = self . align_corners ) ppm_outs . append ( upsampled_ppm_out ) return ppm_outs","title":"forward"},{"location":"models/#terratorch.models.decoders.upernet_decoder.UperNetDecoder","text":"Bases: Module UperNetDecoder. Adapted from MMSegmentation. Source code in terratorch/models/decoders/upernet_decoder.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 @TERRATORCH_DECODER_REGISTRY . register class UperNetDecoder ( nn . Module ): \"\"\"UperNetDecoder. Adapted from MMSegmentation.\"\"\" def __init__ ( self , embed_dim : list [ int ], pool_scales : tuple [ int ] = ( 1 , 2 , 3 , 6 ), channels : int = 256 , align_corners : bool = True , # noqa: FBT001, FBT002 scale_modules : bool = False , ): \"\"\"Constructor Args: embed_dim (list[int]): Input embedding dimension for each input. pool_scales (tuple[int], optional): Pooling scales used in Pooling Pyramid Module applied on the last feature. Default: (1, 2, 3, 6). channels (int, optional): Channels used in the decoder. Defaults to 256. align_corners (bool, optional): Wheter to align corners in rescaling. Defaults to True. scale_modules (bool, optional): Whether to apply scale modules to the inputs. Needed for plain ViT. Defaults to False. \"\"\" super () . __init__ () if scale_modules : # TODO: remove scale_modules before v1? warnings . warn ( \"DeprecationWarning: scale_modules is deprecated and will be removed in future versions. \" \"Use LearnedInterpolateToPyramidal neck instead.\" , stacklevel = 1 , ) self . scale_modules = scale_modules if scale_modules : self . fpn1 = nn . Sequential ( nn . ConvTranspose2d ( embed_dim [ 0 ], embed_dim [ 0 ] // 2 , 2 , 2 ), nn . BatchNorm2d ( embed_dim [ 0 ] // 2 ), nn . GELU (), nn . ConvTranspose2d ( embed_dim [ 0 ] // 2 , embed_dim [ 0 ] // 4 , 2 , 2 )) self . fpn2 = nn . Sequential ( nn . ConvTranspose2d ( embed_dim [ 1 ], embed_dim [ 1 ] // 2 , 2 , 2 )) self . fpn3 = nn . Sequential ( nn . Identity ()) self . fpn4 = nn . Sequential ( nn . MaxPool2d ( kernel_size = 2 , stride = 2 )) self . embed_dim = [ embed_dim [ 0 ] // 4 , embed_dim [ 1 ] // 2 , embed_dim [ 2 ], embed_dim [ 3 ]] else : self . embed_dim = embed_dim self . out_channels = channels self . channels = channels self . align_corners = align_corners # PSP Module self . psp_modules = PPM ( pool_scales , self . embed_dim [ - 1 ], self . channels , align_corners = self . align_corners , ) self . bottleneck = ConvModule ( self . embed_dim [ - 1 ] + len ( pool_scales ) * self . channels , self . channels , 3 , padding = 1 , inplace = True ) # FPN Module self . lateral_convs = nn . ModuleList () self . fpn_convs = nn . ModuleList () for embed_dim in self . embed_dim [: - 1 ]: # skip the top layer l_conv = ConvModule ( embed_dim , self . channels , 1 , inplace = False , ) fpn_conv = ConvModule ( self . channels , self . channels , 3 , padding = 1 , inplace = False , ) self . lateral_convs . append ( l_conv ) self . fpn_convs . append ( fpn_conv ) self . fpn_bottleneck = ConvModule ( len ( self . embed_dim ) * self . channels , self . channels , 3 , padding = 1 , inplace = True ) def psp_forward ( self , inputs ): \"\"\"Forward function of PSP module.\"\"\" x = inputs [ - 1 ] psp_outs = [ x ] psp_outs . extend ( self . psp_modules ( x )) psp_outs = torch . cat ( psp_outs , dim = 1 ) output = self . bottleneck ( psp_outs ) return output def forward ( self , inputs ): \"\"\"Forward function for feature maps before classifying each pixel with Args: inputs (list[Tensor]): List of multi-level img features. Returns: feats (Tensor): A tensor of shape (batch_size, self.channels, H, W) which is feature map for last layer of decoder head. \"\"\" if self . scale_modules : scaled_inputs = [] scaled_inputs . append ( self . fpn1 ( inputs [ 0 ])) scaled_inputs . append ( self . fpn2 ( inputs [ 1 ])) scaled_inputs . append ( self . fpn3 ( inputs [ 2 ])) scaled_inputs . append ( self . fpn4 ( inputs [ 3 ])) inputs = scaled_inputs # build laterals laterals = [ lateral_conv ( inputs [ i ]) for i , lateral_conv in enumerate ( self . lateral_convs )] laterals . append ( self . psp_forward ( inputs )) # build top-down path used_backbone_levels = len ( laterals ) for i in range ( used_backbone_levels - 1 , 0 , - 1 ): prev_shape = laterals [ i - 1 ] . shape [ 2 :] laterals [ i - 1 ] = laterals [ i - 1 ] + torch . nn . functional . interpolate ( laterals [ i ], size = prev_shape , mode = \"bilinear\" , align_corners = self . align_corners ) # build outputs fpn_outs = [ self . fpn_convs [ i ]( laterals [ i ]) for i in range ( used_backbone_levels - 1 )] # append psp feature fpn_outs . append ( laterals [ - 1 ]) for i in range ( used_backbone_levels - 1 , 0 , - 1 ): fpn_outs [ i ] = torch . nn . functional . interpolate ( fpn_outs [ i ], size = fpn_outs [ 0 ] . shape [ 2 :], mode = \"bilinear\" , align_corners = self . align_corners ) fpn_outs = torch . cat ( fpn_outs , dim = 1 ) feats = self . fpn_bottleneck ( fpn_outs ) return feats","title":"UperNetDecoder"},{"location":"models/#terratorch.models.decoders.upernet_decoder.UperNetDecoder.__init__","text":"Constructor Parameters: embed_dim ( list [ int ] ) \u2013 Input embedding dimension for each input. pool_scales ( tuple [ int ] , default: (1, 2, 3, 6) ) \u2013 Pooling scales used in Pooling Pyramid Module applied on the last feature. Default: (1, 2, 3, 6). channels ( int , default: 256 ) \u2013 Channels used in the decoder. Defaults to 256. align_corners ( bool , default: True ) \u2013 Wheter to align corners in rescaling. Defaults to True. scale_modules ( bool , default: False ) \u2013 Whether to apply scale modules to the inputs. Needed for plain ViT. Defaults to False. Source code in terratorch/models/decoders/upernet_decoder.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , embed_dim : list [ int ], pool_scales : tuple [ int ] = ( 1 , 2 , 3 , 6 ), channels : int = 256 , align_corners : bool = True , # noqa: FBT001, FBT002 scale_modules : bool = False , ): \"\"\"Constructor Args: embed_dim (list[int]): Input embedding dimension for each input. pool_scales (tuple[int], optional): Pooling scales used in Pooling Pyramid Module applied on the last feature. Default: (1, 2, 3, 6). channels (int, optional): Channels used in the decoder. Defaults to 256. align_corners (bool, optional): Wheter to align corners in rescaling. Defaults to True. scale_modules (bool, optional): Whether to apply scale modules to the inputs. Needed for plain ViT. Defaults to False. \"\"\" super () . __init__ () if scale_modules : # TODO: remove scale_modules before v1? warnings . warn ( \"DeprecationWarning: scale_modules is deprecated and will be removed in future versions. \" \"Use LearnedInterpolateToPyramidal neck instead.\" , stacklevel = 1 , ) self . scale_modules = scale_modules if scale_modules : self . fpn1 = nn . Sequential ( nn . ConvTranspose2d ( embed_dim [ 0 ], embed_dim [ 0 ] // 2 , 2 , 2 ), nn . BatchNorm2d ( embed_dim [ 0 ] // 2 ), nn . GELU (), nn . ConvTranspose2d ( embed_dim [ 0 ] // 2 , embed_dim [ 0 ] // 4 , 2 , 2 )) self . fpn2 = nn . Sequential ( nn . ConvTranspose2d ( embed_dim [ 1 ], embed_dim [ 1 ] // 2 , 2 , 2 )) self . fpn3 = nn . Sequential ( nn . Identity ()) self . fpn4 = nn . Sequential ( nn . MaxPool2d ( kernel_size = 2 , stride = 2 )) self . embed_dim = [ embed_dim [ 0 ] // 4 , embed_dim [ 1 ] // 2 , embed_dim [ 2 ], embed_dim [ 3 ]] else : self . embed_dim = embed_dim self . out_channels = channels self . channels = channels self . align_corners = align_corners # PSP Module self . psp_modules = PPM ( pool_scales , self . embed_dim [ - 1 ], self . channels , align_corners = self . align_corners , ) self . bottleneck = ConvModule ( self . embed_dim [ - 1 ] + len ( pool_scales ) * self . channels , self . channels , 3 , padding = 1 , inplace = True ) # FPN Module self . lateral_convs = nn . ModuleList () self . fpn_convs = nn . ModuleList () for embed_dim in self . embed_dim [: - 1 ]: # skip the top layer l_conv = ConvModule ( embed_dim , self . channels , 1 , inplace = False , ) fpn_conv = ConvModule ( self . channels , self . channels , 3 , padding = 1 , inplace = False , ) self . lateral_convs . append ( l_conv ) self . fpn_convs . append ( fpn_conv ) self . fpn_bottleneck = ConvModule ( len ( self . embed_dim ) * self . channels , self . channels , 3 , padding = 1 , inplace = True )","title":"__init__"},{"location":"models/#terratorch.models.decoders.upernet_decoder.UperNetDecoder.forward","text":"Forward function for feature maps before classifying each pixel with Args: inputs (list[Tensor]): List of multi-level img features. Returns: feats ( Tensor ) \u2013 A tensor of shape (batch_size, self.channels, H, W) which is feature map for last layer of decoder head. Source code in terratorch/models/decoders/upernet_decoder.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def forward ( self , inputs ): \"\"\"Forward function for feature maps before classifying each pixel with Args: inputs (list[Tensor]): List of multi-level img features. Returns: feats (Tensor): A tensor of shape (batch_size, self.channels, H, W) which is feature map for last layer of decoder head. \"\"\" if self . scale_modules : scaled_inputs = [] scaled_inputs . append ( self . fpn1 ( inputs [ 0 ])) scaled_inputs . append ( self . fpn2 ( inputs [ 1 ])) scaled_inputs . append ( self . fpn3 ( inputs [ 2 ])) scaled_inputs . append ( self . fpn4 ( inputs [ 3 ])) inputs = scaled_inputs # build laterals laterals = [ lateral_conv ( inputs [ i ]) for i , lateral_conv in enumerate ( self . lateral_convs )] laterals . append ( self . psp_forward ( inputs )) # build top-down path used_backbone_levels = len ( laterals ) for i in range ( used_backbone_levels - 1 , 0 , - 1 ): prev_shape = laterals [ i - 1 ] . shape [ 2 :] laterals [ i - 1 ] = laterals [ i - 1 ] + torch . nn . functional . interpolate ( laterals [ i ], size = prev_shape , mode = \"bilinear\" , align_corners = self . align_corners ) # build outputs fpn_outs = [ self . fpn_convs [ i ]( laterals [ i ]) for i in range ( used_backbone_levels - 1 )] # append psp feature fpn_outs . append ( laterals [ - 1 ]) for i in range ( used_backbone_levels - 1 , 0 , - 1 ): fpn_outs [ i ] = torch . nn . functional . interpolate ( fpn_outs [ i ], size = fpn_outs [ 0 ] . shape [ 2 :], mode = \"bilinear\" , align_corners = self . align_corners ) fpn_outs = torch . cat ( fpn_outs , dim = 1 ) feats = self . fpn_bottleneck ( fpn_outs ) return feats","title":"forward"},{"location":"models/#terratorch.models.decoders.upernet_decoder.UperNetDecoder.psp_forward","text":"Forward function of PSP module. Source code in terratorch/models/decoders/upernet_decoder.py 96 97 98 99 100 101 102 103 104 def psp_forward ( self , inputs ): \"\"\"Forward function of PSP module.\"\"\" x = inputs [ - 1 ] psp_outs = [ x ] psp_outs . extend ( self . psp_modules ( x )) psp_outs = torch . cat ( psp_outs , dim = 1 ) output = self . bottleneck ( psp_outs ) return output","title":"psp_forward"},{"location":"models/#heads","text":"","title":"Heads"},{"location":"models/#terratorch.models.heads.regression_head","text":"","title":"regression_head"},{"location":"models/#terratorch.models.heads.regression_head.RegressionHead","text":"Bases: Module Regression head Source code in terratorch/models/heads/regression_head.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class RegressionHead ( nn . Module ): \"\"\"Regression head\"\"\" def __init__ ( self , in_channels : int , final_act : nn . Module | str | None = None , learned_upscale_layers : int = 0 , channel_list : list [ int ] | None = None , batch_norm : bool = True , dropout : float = 0 , ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels final_act (nn.Module | None, optional): Final activation to be applied. Defaults to None. learned_upscale_layers (int, optional): Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0. channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. batch_norm (bool, optional): Whether to apply batch norm. Defaults to True. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . learned_upscale_layers = learned_upscale_layers self . final_act = final_act if final_act else nn . Identity () if isinstance ( final_act , str ): module_name , class_name = final_act . rsplit ( \".\" , 1 ) target_class = getattr ( importlib . import_module ( module_name ), class_name ) self . final_act = target_class () pre_layers = [] if learned_upscale_layers != 0 : learned_upscale = nn . Sequential ( * [ PixelShuffleUpscale ( in_channels ) for _ in range ( self . learned_upscale_layers )] ) pre_layers . append ( learned_upscale ) if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 , bias = False ), nn . BatchNorm2d ( out_channels ), nn . ReLU ( inplace = True ), ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] pre_layers . append ( pre_head ) dropout = nn . Dropout2d ( dropout ) final_layer = nn . Conv2d ( in_channels = in_channels , out_channels = 1 , kernel_size = 1 ) self . head = nn . Sequential ( * [ * pre_layers , dropout , final_layer ]) def forward ( self , x ): output = self . head ( x ) return self . final_act ( output )","title":"RegressionHead"},{"location":"models/#terratorch.models.heads.regression_head.RegressionHead.__init__","text":"Constructor Parameters: in_channels ( int ) \u2013 Number of input channels final_act ( Module | None , default: None ) \u2013 Final activation to be applied. Defaults to None. learned_upscale_layers ( int , default: 0 ) \u2013 Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0. channel_list ( list [ int ] | None , default: None ) \u2013 List with number of channels for each Conv layer to be created. Defaults to None. batch_norm ( bool , default: True ) \u2013 Whether to apply batch norm. Defaults to True. dropout ( float , default: 0 ) \u2013 Dropout value to apply. Defaults to 0. Source code in terratorch/models/heads/regression_head.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , in_channels : int , final_act : nn . Module | str | None = None , learned_upscale_layers : int = 0 , channel_list : list [ int ] | None = None , batch_norm : bool = True , dropout : float = 0 , ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels final_act (nn.Module | None, optional): Final activation to be applied. Defaults to None. learned_upscale_layers (int, optional): Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0. channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. batch_norm (bool, optional): Whether to apply batch norm. Defaults to True. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . learned_upscale_layers = learned_upscale_layers self . final_act = final_act if final_act else nn . Identity () if isinstance ( final_act , str ): module_name , class_name = final_act . rsplit ( \".\" , 1 ) target_class = getattr ( importlib . import_module ( module_name ), class_name ) self . final_act = target_class () pre_layers = [] if learned_upscale_layers != 0 : learned_upscale = nn . Sequential ( * [ PixelShuffleUpscale ( in_channels ) for _ in range ( self . learned_upscale_layers )] ) pre_layers . append ( learned_upscale ) if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 , bias = False ), nn . BatchNorm2d ( out_channels ), nn . ReLU ( inplace = True ), ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] pre_layers . append ( pre_head ) dropout = nn . Dropout2d ( dropout ) final_layer = nn . Conv2d ( in_channels = in_channels , out_channels = 1 , kernel_size = 1 ) self . head = nn . Sequential ( * [ * pre_layers , dropout , final_layer ])","title":"__init__"},{"location":"models/#terratorch.models.heads.segmentation_head","text":"","title":"segmentation_head"},{"location":"models/#terratorch.models.heads.segmentation_head.SegmentationHead","text":"Bases: Module Segmentation head Source code in terratorch/models/heads/segmentation_head.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class SegmentationHead ( nn . Module ): \"\"\"Segmentation head\"\"\" def __init__ ( self , in_channels : int , num_classes : int , channel_list : list [ int ] | None = None , dropout : float = 0 ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels num_classes (int): Number of output classes channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . num_classes = num_classes if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 ), nn . ReLU () ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Conv2d ( in_channels = in_channels , out_channels = num_classes , kernel_size = 1 , ), ) def forward ( self , x ): return self . head ( x )","title":"SegmentationHead"},{"location":"models/#terratorch.models.heads.segmentation_head.SegmentationHead.__init__","text":"Constructor Parameters: in_channels ( int ) \u2013 Number of input channels num_classes ( int ) \u2013 Number of output classes channel_list ( list [ int ] | None , default: None ) \u2013 List with number of channels for each Conv layer to be created. Defaults to None. dropout ( float , default: 0 ) \u2013 Dropout value to apply. Defaults to 0. Source code in terratorch/models/heads/segmentation_head.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , in_channels : int , num_classes : int , channel_list : list [ int ] | None = None , dropout : float = 0 ) -> None : \"\"\"Constructor Args: in_channels (int): Number of input channels num_classes (int): Number of output classes channel_list (list[int] | None, optional): List with number of channels for each Conv layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. \"\"\" super () . __init__ () self . num_classes = num_classes if channel_list is None : pre_head = nn . Identity () else : def block ( in_channels , out_channels ): return nn . Sequential ( nn . Conv2d ( in_channels = in_channels , out_channels = out_channels , kernel_size = 3 , padding = 1 ), nn . ReLU () ) channel_list = [ in_channels , * channel_list ] pre_head = nn . Sequential ( * [ block ( channel_list [ i ], channel_list [ i + 1 ]) for i in range ( len ( channel_list ) - 1 )] ) in_channels = channel_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Conv2d ( in_channels = in_channels , out_channels = num_classes , kernel_size = 1 , ), )","title":"__init__"},{"location":"models/#terratorch.models.heads.classification_head","text":"","title":"classification_head"},{"location":"models/#terratorch.models.heads.classification_head.ClassificationHead","text":"Bases: Module Classification head Source code in terratorch/models/heads/classification_head.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class ClassificationHead ( nn . Module ): \"\"\"Classification head\"\"\" # how to allow cls token? def __init__ ( self , in_dim : int , num_classes : int , dim_list : list [ int ] | None = None , dropout : float = 0 , linear_after_pool : bool = False , ) -> None : \"\"\"Constructor Args: in_dim (int): Input dimensionality num_classes (int): Number of output classes dim_list (list[int] | None, optional): List with number of dimensions for each Linear layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. linear_after_pool (bool, optional): Apply pooling first, then apply the linear layer. Defaults to False \"\"\" super () . __init__ () self . num_classes = num_classes self . linear_after_pool = linear_after_pool if dim_list is None : pre_head = nn . Identity () else : def block ( in_dim , out_dim ): return nn . Sequential ( nn . Linear ( in_features = in_dim , out_features = out_dim ), nn . ReLU ()) dim_list = [ in_dim , * dim_list ] pre_head = nn . Sequential ( * [ block ( dim_list [ i ], dim_list [ i + 1 ]) for i in range ( len ( dim_list ) - 1 )]) in_dim = dim_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Linear ( in_features = in_dim , out_features = num_classes ), ) def forward ( self , x : Tensor ): x = x . reshape ( x . shape [ 0 ], x . shape [ 1 ], - 1 ) . permute ( 0 , 2 , 1 ) if self . linear_after_pool : x = x . mean ( axis = 1 ) out = self . head ( x ) else : x = self . head ( x ) out = x . mean ( axis = 1 ) return out","title":"ClassificationHead"},{"location":"models/#terratorch.models.heads.classification_head.ClassificationHead.__init__","text":"Constructor Parameters: in_dim ( int ) \u2013 Input dimensionality num_classes ( int ) \u2013 Number of output classes dim_list ( list [ int ] | None , default: None ) \u2013 List with number of dimensions for each Linear layer to be created. Defaults to None. dropout ( float , default: 0 ) \u2013 Dropout value to apply. Defaults to 0. linear_after_pool ( bool , default: False ) \u2013 Apply pooling first, then apply the linear layer. Defaults to False Source code in terratorch/models/heads/classification_head.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , in_dim : int , num_classes : int , dim_list : list [ int ] | None = None , dropout : float = 0 , linear_after_pool : bool = False , ) -> None : \"\"\"Constructor Args: in_dim (int): Input dimensionality num_classes (int): Number of output classes dim_list (list[int] | None, optional): List with number of dimensions for each Linear layer to be created. Defaults to None. dropout (float, optional): Dropout value to apply. Defaults to 0. linear_after_pool (bool, optional): Apply pooling first, then apply the linear layer. Defaults to False \"\"\" super () . __init__ () self . num_classes = num_classes self . linear_after_pool = linear_after_pool if dim_list is None : pre_head = nn . Identity () else : def block ( in_dim , out_dim ): return nn . Sequential ( nn . Linear ( in_features = in_dim , out_features = out_dim ), nn . ReLU ()) dim_list = [ in_dim , * dim_list ] pre_head = nn . Sequential ( * [ block ( dim_list [ i ], dim_list [ i + 1 ]) for i in range ( len ( dim_list ) - 1 )]) in_dim = dim_list [ - 1 ] dropout = nn . Identity () if dropout == 0 else nn . Dropout ( dropout ) self . head = nn . Sequential ( pre_head , dropout , nn . Linear ( in_features = in_dim , out_features = num_classes ), )","title":"__init__"},{"location":"models/#auxiliary-heads","text":"","title":"Auxiliary Heads"},{"location":"models/#terratorch.models.model.AuxiliaryHead","text":"Class containing all information to create auxiliary heads. Parameters: name ( str ) \u2013 Name of the head. Should match the name given to the auxiliary loss. decoder ( str ) \u2013 Name of the decoder class to be used. decoder_args ( dict | None ) \u2013 parameters to be passed to the decoder constructor. Parameters for the decoder should be prefixed with decoder_ . Parameters for the head should be prefixed with head_ . Source code in terratorch/models/model.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @dataclass class AuxiliaryHead : \"\"\"Class containing all information to create auxiliary heads. Args: name (str): Name of the head. Should match the name given to the auxiliary loss. decoder (str): Name of the decoder class to be used. decoder_args (dict | None): parameters to be passed to the decoder constructor. Parameters for the decoder should be prefixed with `decoder_`. Parameters for the head should be prefixed with `head_`. \"\"\" name : str decoder : str decoder_args : dict | None","title":"AuxiliaryHead"},{"location":"models/#model-output","text":"","title":"Model Output"},{"location":"models/#terratorch.models.model.ModelOutput","text":"Source code in terratorch/models/model.py 10 11 12 13 @dataclass class ModelOutput : output : Tensor auxiliary_heads : dict [ str , Tensor ] = None","title":"ModelOutput"},{"location":"models/#model-factory","text":"","title":"Model Factory"},{"location":"models/#terratorch.models.PrithviModelFactory","text":"Bases: ModelFactory Source code in terratorch/models/prithvi_model_factory.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 @MODEL_FACTORY_REGISTRY . register class PrithviModelFactory ( ModelFactory ): def __init__ ( self ) -> None : self . _factory : EncoderDecoderFactory = EncoderDecoderFactory () def build_model ( self , task : str , backbone : str | nn . Module , decoder : str | nn . Module , bands : list [ HLSBands | int ], in_channels : int | None = None , # this should be removed, can be derived from bands. But it is a breaking change num_classes : int | None = None , pretrained : bool = True , # noqa: FBT001, FBT002 num_frames : int = 1 , prepare_features_for_image_model : Callable | None = None , aux_decoders : list [ AuxiliaryHead ] | None = None , rescale : bool = True , # noqa: FBT002, FBT001 ** kwargs , ) -> Model : \"\"\"Model factory for prithvi models. Further arguments to be passed to the backbone, decoder or head. They should be prefixed with `backbone_`, `decoder_` and `head_` respectively. Args: task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\". backbone (str, nn.Module): Backbone to be used. If string, should be able to be parsed by the specified factory. Defaults to \"prithvi_100\". decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model. If a string, it will be created from a class exposed in decoder.__init__.py with the same name. If an nn.Module, we expect it to expose a property `decoder.out_channels`. Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\". in_channels (int, optional): Number of input channels. Defaults to 3. bands (list[terratorch.datasets.HLSBands], optional): Bands the model will be trained on. Should be a list of terratorch.datasets.HLSBands. Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE]. num_classes (int, optional): Number of classes. None for regression tasks. pretrained (Union[bool, Path], optional): Whether to load pretrained weights for the backbone, if available. Defaults to True. num_frames (int, optional): Number of timesteps for the model to handle. Defaults to 1. prepare_features_for_image_model (Callable | None): Function to be called on encoder features before passing them to the decoder. Defaults to None, which applies the identity function. aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead deciders to be added to the model. These decoders take the input from the encoder as well. rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True. Returns: nn.Module: Full model with encoder, decoder and head. \"\"\" warnings . warn ( \"PrithviModelFactory is deprecated. Please switch to EncoderDecoderFactory.\" , stacklevel = 1 ) if in_channels is None : in_channels = len ( bands ) # TODO: support auxiliary heads kwargs [ \"backbone_bands\" ] = bands kwargs [ \"backbone_in_chans\" ] = in_channels kwargs [ \"backbone_pretrained\" ] = pretrained kwargs [ \"backbone_num_frames\" ] = num_frames if prepare_features_for_image_model : msg = ( \"This functionality is no longer supported. Please migrate to EncoderDecoderFactory \\ and use necks.\" ) raise RuntimeError ( msg ) if not isinstance ( backbone , nn . Module ): if not backbone . startswith ( \"prithvi_\" ): msg = \"This class only handles models for `prithvi` encoders\" raise NotImplementedError ( msg ) return self . _factory . build_model ( task , backbone , decoder , num_classes = num_classes , necks = None , aux_decoders = aux_decoders , rescale = rescale , ** kwargs )","title":"PrithviModelFactory"},{"location":"models/#terratorch.models.PrithviModelFactory.build_model","text":"Model factory for prithvi models. Further arguments to be passed to the backbone, decoder or head. They should be prefixed with backbone_ , decoder_ and head_ respectively. Parameters: task ( str ) \u2013 Task to be performed. Currently supports \"segmentation\" and \"regression\". backbone ( ( str , Module ) ) \u2013 Backbone to be used. If string, should be able to be parsed by the specified factory. Defaults to \"prithvi_100\". decoder ( Union [ str , Module ] ) \u2013 Decoder to be used for the segmentation model. If a string, it will be created from a class exposed in decoder. init .py with the same name. If an nn.Module, we expect it to expose a property decoder.out_channels . Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\". in_channels ( int , default: None ) \u2013 Number of input channels. Defaults to 3. bands ( list [ HLSBands ] ) \u2013 Bands the model will be trained on. Should be a list of terratorch.datasets.HLSBands. Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE]. num_classes ( int , default: None ) \u2013 Number of classes. None for regression tasks. pretrained ( Union [ bool , Path ] , default: True ) \u2013 Whether to load pretrained weights for the backbone, if available. Defaults to True. num_frames ( int , default: 1 ) \u2013 Number of timesteps for the model to handle. Defaults to 1. prepare_features_for_image_model ( Callable | None , default: None ) \u2013 Function to be called on encoder features before passing them to the decoder. Defaults to None, which applies the identity function. aux_decoders ( list [ AuxiliaryHead ] | None , default: None ) \u2013 List of AuxiliaryHead deciders to be added to the model. These decoders take the input from the encoder as well. rescale ( bool , default: True ) \u2013 Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True. Returns: Model \u2013 nn.Module: Full model with encoder, decoder and head. Source code in terratorch/models/prithvi_model_factory.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def build_model ( self , task : str , backbone : str | nn . Module , decoder : str | nn . Module , bands : list [ HLSBands | int ], in_channels : int | None = None , # this should be removed, can be derived from bands. But it is a breaking change num_classes : int | None = None , pretrained : bool = True , # noqa: FBT001, FBT002 num_frames : int = 1 , prepare_features_for_image_model : Callable | None = None , aux_decoders : list [ AuxiliaryHead ] | None = None , rescale : bool = True , # noqa: FBT002, FBT001 ** kwargs , ) -> Model : \"\"\"Model factory for prithvi models. Further arguments to be passed to the backbone, decoder or head. They should be prefixed with `backbone_`, `decoder_` and `head_` respectively. Args: task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\". backbone (str, nn.Module): Backbone to be used. If string, should be able to be parsed by the specified factory. Defaults to \"prithvi_100\". decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model. If a string, it will be created from a class exposed in decoder.__init__.py with the same name. If an nn.Module, we expect it to expose a property `decoder.out_channels`. Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\". in_channels (int, optional): Number of input channels. Defaults to 3. bands (list[terratorch.datasets.HLSBands], optional): Bands the model will be trained on. Should be a list of terratorch.datasets.HLSBands. Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE]. num_classes (int, optional): Number of classes. None for regression tasks. pretrained (Union[bool, Path], optional): Whether to load pretrained weights for the backbone, if available. Defaults to True. num_frames (int, optional): Number of timesteps for the model to handle. Defaults to 1. prepare_features_for_image_model (Callable | None): Function to be called on encoder features before passing them to the decoder. Defaults to None, which applies the identity function. aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead deciders to be added to the model. These decoders take the input from the encoder as well. rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True. Returns: nn.Module: Full model with encoder, decoder and head. \"\"\" warnings . warn ( \"PrithviModelFactory is deprecated. Please switch to EncoderDecoderFactory.\" , stacklevel = 1 ) if in_channels is None : in_channels = len ( bands ) # TODO: support auxiliary heads kwargs [ \"backbone_bands\" ] = bands kwargs [ \"backbone_in_chans\" ] = in_channels kwargs [ \"backbone_pretrained\" ] = pretrained kwargs [ \"backbone_num_frames\" ] = num_frames if prepare_features_for_image_model : msg = ( \"This functionality is no longer supported. Please migrate to EncoderDecoderFactory \\ and use necks.\" ) raise RuntimeError ( msg ) if not isinstance ( backbone , nn . Module ): if not backbone . startswith ( \"prithvi_\" ): msg = \"This class only handles models for `prithvi` encoders\" raise NotImplementedError ( msg ) return self . _factory . build_model ( task , backbone , decoder , num_classes = num_classes , necks = None , aux_decoders = aux_decoders , rescale = rescale , ** kwargs )","title":"build_model"},{"location":"models/#terratorch.models.SMPModelFactory","text":"Bases: ModelFactory Source code in terratorch/models/smp_model_factory.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 @MODEL_FACTORY_REGISTRY . register class SMPModelFactory ( ModelFactory ): def build_model ( self , task : str , backbone : str , model : str , bands : list [ HLSBands | int ], in_channels : int | None = None , num_classes : int = 1 , pretrained : str | bool | None = True , # noqa: FBT002 prepare_features_for_image_model : Callable | None = None , regression_relu : bool = False , # noqa: FBT001, FBT002 ** kwargs , ) -> Model : \"\"\" Factory class for creating SMP (Segmentation Models Pytorch) based models with optional customization. This factory handles the instantiation of segmentation and regression models using specified encoders and decoders from the SMP library, along with custom modifications and extensions such as auxiliary decoders or modified encoders. Attributes: task (str): Specifies the task for which the model is being built. Supported tasks are \"segmentation\". backbone (str): Specifies the backbone model to be used. decoder (str): Specifies the decoder to be used for constructing the segmentation model. bands (list[terratorch.datasets.HLSBands | int]): A list specifying the bands that the model will operate on. These are expected to be from terratorch.datasets.HLSBands. in_channels (int, optional): Specifies the number of input channels. Defaults to None. num_classes (int, optional): The number of output classes for the model. pretrained (bool | Path, optional): Indicates whether to load pretrained weights for the backbone. Can also specify a path to weights. Defaults to True. num_frames (int, optional): Specifies the number of timesteps the model should handle. Useful for temporal models. regression_relu (bool): Whether to apply ReLU activation in the case of regression tasks. **kwargs: Additional arguments that might be passed to further customize the backbone, decoder, or any auxiliary heads. These should be prefixed appropriately Raises: ValueError: If the specified decoder is not supported by SMP. Exception: If the specified task is not \"segmentation\" Returns: nn.Module: A model instance wrapped in SMPModelWrapper configured according to the specified parameters and tasks. \"\"\" if task != \"segmentation\" : msg = f \"SMP models can only perform segmentation, but got task { task } \" raise Exception ( msg ) bands = [ HLSBands . try_convert_to_hls_bands_enum ( b ) for b in bands ] if in_channels is None : in_channels = len ( bands ) # Gets decoder module. model_module = getattr ( smp , model , None ) if model_module is None : msg = f \"Decoder { model } is not supported in SMP.\" raise ValueError ( msg ) backbone_kwargs , kwargs = extract_prefix_keys ( kwargs , \"backbone_\" ) # Encoder params should be prefixed backbone_ smp_kwargs , kwargs = extract_prefix_keys ( backbone_kwargs , \"smp_\" ) # Smp model params should be prefixed smp_ aux_params , kwargs = extract_prefix_keys ( backbone_kwargs , \"aux_\" ) # Auxiliary head params should be prefixed aux_ aux_params = None if aux_params == {} else aux_params if isinstance ( pretrained , bool ): if pretrained : pretrained = \"imagenet\" else : pretrained = None # If encoder not currently supported by SMP (custom encoder). if backbone not in smp_encoders : # These params must be included in the config file with appropriate prefix. required_params = { \"encoder_depth\" : smp_kwargs , \"out_channels\" : backbone_kwargs , \"output_stride\" : backbone_kwargs , } for param , config_dict in required_params . items (): if param not in config_dict : msg = f \"Config must include the ' { param } ' parameter\" raise ValueError ( msg ) # Using new encoder. backbone_class = make_smp_encoder ( backbone ) backbone_kwargs [ \"prepare_features_for_image_model\" ] = prepare_features_for_image_model # Registering custom encoder into SMP. register_custom_encoder ( backbone_class , backbone_kwargs , pretrained ) model_args = { \"encoder_name\" : \"SMPEncoderWrapperWithPFFIM\" , \"encoder_weights\" : pretrained , \"in_channels\" : in_channels , \"classes\" : num_classes , ** smp_kwargs , } # Using SMP encoder. else : model_args = { \"encoder_name\" : backbone , \"encoder_weights\" : pretrained , \"in_channels\" : in_channels , \"classes\" : num_classes , ** smp_kwargs , } model = model_module ( ** model_args , aux_params = aux_params ) return SMPModelWrapper ( model , relu = task == \"regression\" and regression_relu , squeeze_single_class = task == \"regression\" )","title":"SMPModelFactory"},{"location":"models/#terratorch.models.SMPModelFactory.build_model","text":"Factory class for creating SMP (Segmentation Models Pytorch) based models with optional customization. This factory handles the instantiation of segmentation and regression models using specified encoders and decoders from the SMP library, along with custom modifications and extensions such as auxiliary decoders or modified encoders. Attributes: task ( str ) \u2013 Specifies the task for which the model is being built. Supported tasks are \"segmentation\". backbone ( str ) \u2013 Specifies the backbone model to be used. decoder ( str ) \u2013 Specifies the decoder to be used for constructing the segmentation model. bands ( list [ HLSBands | int ] ) \u2013 A list specifying the bands that the model will operate on. These are expected to be from terratorch.datasets.HLSBands. in_channels ( int ) \u2013 Specifies the number of input channels. Defaults to None. num_classes ( int ) \u2013 The number of output classes for the model. pretrained ( bool | Path ) \u2013 Indicates whether to load pretrained weights for the backbone. Can also specify a path to weights. Defaults to True. num_frames ( int ) \u2013 Specifies the number of timesteps the model should handle. Useful for temporal models. regression_relu ( bool ) \u2013 Whether to apply ReLU activation in the case of regression tasks. **kwargs ( bool ) \u2013 Additional arguments that might be passed to further customize the backbone, decoder, or any auxiliary heads. These should be prefixed appropriately Raises: ValueError \u2013 If the specified decoder is not supported by SMP. Exception \u2013 If the specified task is not \"segmentation\" Returns: Model \u2013 nn.Module: A model instance wrapped in SMPModelWrapper configured according to the specified parameters and tasks. Source code in terratorch/models/smp_model_factory.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def build_model ( self , task : str , backbone : str , model : str , bands : list [ HLSBands | int ], in_channels : int | None = None , num_classes : int = 1 , pretrained : str | bool | None = True , # noqa: FBT002 prepare_features_for_image_model : Callable | None = None , regression_relu : bool = False , # noqa: FBT001, FBT002 ** kwargs , ) -> Model : \"\"\" Factory class for creating SMP (Segmentation Models Pytorch) based models with optional customization. This factory handles the instantiation of segmentation and regression models using specified encoders and decoders from the SMP library, along with custom modifications and extensions such as auxiliary decoders or modified encoders. Attributes: task (str): Specifies the task for which the model is being built. Supported tasks are \"segmentation\". backbone (str): Specifies the backbone model to be used. decoder (str): Specifies the decoder to be used for constructing the segmentation model. bands (list[terratorch.datasets.HLSBands | int]): A list specifying the bands that the model will operate on. These are expected to be from terratorch.datasets.HLSBands. in_channels (int, optional): Specifies the number of input channels. Defaults to None. num_classes (int, optional): The number of output classes for the model. pretrained (bool | Path, optional): Indicates whether to load pretrained weights for the backbone. Can also specify a path to weights. Defaults to True. num_frames (int, optional): Specifies the number of timesteps the model should handle. Useful for temporal models. regression_relu (bool): Whether to apply ReLU activation in the case of regression tasks. **kwargs: Additional arguments that might be passed to further customize the backbone, decoder, or any auxiliary heads. These should be prefixed appropriately Raises: ValueError: If the specified decoder is not supported by SMP. Exception: If the specified task is not \"segmentation\" Returns: nn.Module: A model instance wrapped in SMPModelWrapper configured according to the specified parameters and tasks. \"\"\" if task != \"segmentation\" : msg = f \"SMP models can only perform segmentation, but got task { task } \" raise Exception ( msg ) bands = [ HLSBands . try_convert_to_hls_bands_enum ( b ) for b in bands ] if in_channels is None : in_channels = len ( bands ) # Gets decoder module. model_module = getattr ( smp , model , None ) if model_module is None : msg = f \"Decoder { model } is not supported in SMP.\" raise ValueError ( msg ) backbone_kwargs , kwargs = extract_prefix_keys ( kwargs , \"backbone_\" ) # Encoder params should be prefixed backbone_ smp_kwargs , kwargs = extract_prefix_keys ( backbone_kwargs , \"smp_\" ) # Smp model params should be prefixed smp_ aux_params , kwargs = extract_prefix_keys ( backbone_kwargs , \"aux_\" ) # Auxiliary head params should be prefixed aux_ aux_params = None if aux_params == {} else aux_params if isinstance ( pretrained , bool ): if pretrained : pretrained = \"imagenet\" else : pretrained = None # If encoder not currently supported by SMP (custom encoder). if backbone not in smp_encoders : # These params must be included in the config file with appropriate prefix. required_params = { \"encoder_depth\" : smp_kwargs , \"out_channels\" : backbone_kwargs , \"output_stride\" : backbone_kwargs , } for param , config_dict in required_params . items (): if param not in config_dict : msg = f \"Config must include the ' { param } ' parameter\" raise ValueError ( msg ) # Using new encoder. backbone_class = make_smp_encoder ( backbone ) backbone_kwargs [ \"prepare_features_for_image_model\" ] = prepare_features_for_image_model # Registering custom encoder into SMP. register_custom_encoder ( backbone_class , backbone_kwargs , pretrained ) model_args = { \"encoder_name\" : \"SMPEncoderWrapperWithPFFIM\" , \"encoder_weights\" : pretrained , \"in_channels\" : in_channels , \"classes\" : num_classes , ** smp_kwargs , } # Using SMP encoder. else : model_args = { \"encoder_name\" : backbone , \"encoder_weights\" : pretrained , \"in_channels\" : in_channels , \"classes\" : num_classes , ** smp_kwargs , } model = model_module ( ** model_args , aux_params = aux_params ) return SMPModelWrapper ( model , relu = task == \"regression\" and regression_relu , squeeze_single_class = task == \"regression\" )","title":"build_model"},{"location":"models/#adding-new-model-types","text":"Adding new model types is as simple as creating a new factory that produces models. See for instance the example below for a potential SMPModelFactory from terratorch.models.model import register_factory @register_factory class SMPModelFactory ( ModelFactory ): def build_model ( self , task : str , backbone : str | nn . Module , decoder : str | nn . Module , in_channels : int , ** kwargs , ) -> Model : model = smp . Unet ( encoder_name = \"resnet34\" , encoder_weights = None , in_channels = in_channels , classes = 1 ) return SMPModelWrapper ( model ) @register_factory class SMPModelWrapper ( Model , nn . Module ): def __init__ ( self , smp_model ) -> None : super () . __init__ () self . smp_model = smp_model def forward ( self , * args , ** kwargs ): return ModelOutput ( self . smp_model ( * args , ** kwargs ) . squeeze ( 1 )) def freeze_encoder ( self ): pass def freeze_decoder ( self ): pass","title":"Adding new model types"},{"location":"models/#custom-modules-with-cli","text":"Custom modules must be in the import path in order to be registered in the appropriate registries. In order to do this without modifying the code when using the CLI, you may place your modules under a custom_modules directory. This must be in the directory from which you execute terratorch.","title":"Custom modules with CLI"},{"location":"quick_start/","text":"Quick start # Configuring the environment # Python # TerraTorch is currently tested for Python in 3.10 <= Python <= 3.12 . GDAL # GDAL is required to read and write TIFF images. It is usually easy to install in Unix/Linux systems, but if it is not your case we reccomend using a conda environment and installing it with conda install -c conda-forge gdal . Installing TerraTorch # For a stable point-release, use pip install terratorch . If you prefer to get the most recent version of the main branch, install the library with pip install git+https://github.com/IBM/terratorch.git . To install as a developer (e.g. to extend the library) clone this repo, and run pip install -e . . Creating Backbones # You can interact with the library at several levels of abstraction. Each deeper level of abstraction trades off some amount of flexibility for ease of use and configuration. In the simplest case, we might only want access a backbone and code all the rest ourselves. In this case, we can simply use the library as a backbone factory: Instantiating a prithvi backbone from terratorch import BACKBONE_REGISTRY # find available prithvi models print ([ model_name for model_name in BACKBONE_REGISTRY if \"terratorch_prithvi\" in model_name ]) >>> [ 'terratorch_prithvi_eo_tiny' , 'terratorch_prithvi_eo_v1_100' , 'terratorch_prithvi_eo_v2_300' , 'terratorch_prithvi_eo_v2_600' , 'terratorch_prithvi_eo_v2_300_tl' , 'terratorch_prithvi_eo_v2_600_tl' ] # show all models with list(BACKBONE_REGISTRY) # check a model is in the registry \"terratorch_prithvi_eo_v2_300\" in BACKBONE_REGISTRY >>> True # without the prefix, all internal registries will be searched until the first match is found \"prithvi_eo_v1_100\" in BACKBONE_REGISTRY >>> True # instantiate your desired model # the backbone registry prefix (e.g. `terratorch` or `timm`) is optional # in this case, the underlying registry is terratorch. model = BACKBONE_REGISTRY . build ( \"prithvi_eo_v1_100\" , pretrained = True ) # instantiate your model with more options, for instance, passing weights from your own file model = BACKBONE_REGISTRY . build ( \"prithvi_eo_v2_300\" , num_frames = 1 , ckpt_path = 'path/to/model.pt' ) # Rest of your PyTorch / PyTorchLightning code Internally, terratorch maintains several registries for components such as backbones or decoders. The top-level BACKBONE_REGISTRY collects all of them. The name passed to build is used to find the appropriate model constructor, which will be the first model from the first registry found with that name. To explicitly determine the registry that will build the model, you may prepend a prefix such as timm_ to the model name. In this case, the timm model registry will be exclusively searched for the model. Directly creating a full model # We also provide a model factory for a task specific model built on one a backbones: Building a full model, with task specific decoder import terratorch # even though we don't use the import directly, we need it so that the models are available in the timm registry from terratorch.models import EncoderDecoderFactory from terratorch.datasets import HLSBands model_factory = EncoderDecoderFactory () # Let's build a segmentation model # Parameters prefixed with backbone_ get passed to the backbone # Parameters prefixed with decoder_ get passed to the decoder # Parameters prefixed with head_ get passed to the head model = model_factory . build_model ( task = \"segmentation\" , backbone = \"prithvi_eo_v2_300\" , backbone_pretrained = True , backbone_bands = [ HLSBands . BLUE , HLSBands . GREEN , HLSBands . RED , HLSBands . NIR_NARROW , HLSBands . SWIR_1 , HLSBands . SWIR_2 , ], necks = [{ \"name\" : \"SelectIndices\" , \"indices\" : [ - 1 ]}, { \"name\" : \"ReshapeTokensToImage\" }], decoder = \"FCNDecoder\" , decoder_channels = 128 , head_dropout = 0.1 , num_classes = 4 , ) # Rest of your PyTorch / PyTorchLightning code . . . Training with Lightning Tasks # At the highest level of abstraction, you can directly obtain a LightningModule ready to be trained. Building a full Pixel-Wise Regression task model_args = dict ( backbone = \"prithvi_eo_v2_300\" , backbone_pretrained = True , backbone_num_frames = 1 , backbone_bands = [ HLSBands . BLUE , HLSBands . GREEN , HLSBands . RED , HLSBands . NIR_NARROW , HLSBands . SWIR_1 , HLSBands . SWIR_2 , ], necks = [{ \"name\" : \"SelectIndices\" , \"indices\" : [ - 1 ]}, { \"name\" : \"ReshapeTokensToImage\" }], decoder = \"FCNDecoder\" , decoder_channels = 128 , head_dropout = 0.1 ) task = PixelwiseRegressionTask ( model_args , \"EncoderDecoderFactory\" , loss = \"rmse\" , lr = lr , ignore_index =- 1 , optimizer = \"AdamW\" , optimizer_hparams = { \"weight_decay\" : 0.05 }, ) # Pass this LightningModule to a Lightning Trainer, together with some LightningDataModule Alternatively, all the process can be summarized in configuration files written in YAML format, as seen below. Configuration file for a Semantic Segmentation Task # lightning.pytorch==2.1.1 seed_everything : 0 trainer : accelerator : auto strategy : auto devices : auto num_nodes : 1 precision : bf16 logger : class_path : TensorBoardLogger init_args : save_dir : <path_to_experiment_dir> name : <experiment_name> callbacks : - class_path : RichProgressBar - class_path : LearningRateMonitor init_args : logging_interval : epoch max_epochs : 200 check_val_every_n_epoch : 1 log_every_n_steps : 50 enable_checkpointing : true default_root_dir : <path_to_experiment_dir> data : class_path : terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule init_args : batch_size : 16 num_workers : 8 dict_kwargs : data_root : <path_to_data_root> bands : - 1 - 2 - 3 - 8 - 11 - 12 model : class_path : terratorch.tasks.SemanticSegmentationTask init_args : model_factory : EncoderDecoderFactory model_args : backbone : prithvi_eo_v2_300 backbone_img_size : 512 backbone_pretrained : True backbone_bands : - BLUE - GREEN - RED - NIR_NARROW - SWIR_1 - SWIR_2 necks : - name : SelectIndices indices : [ 5 , 11 , 17 , 23 ] - name : ReshapeTokensToImage - name : LearnedInterpolateToPyramidal decoder : UperNetDecoder decoder_channels : 256 head_channel_list : [ 256 ] head_dropout : 0.1 num_classes : 2 loss : dice ignore_index : -1 freeze_backbone : false freeze_decoder : false optimizer : class_path : torch.optim.AdamW init_args : lr : 1.e-4 weight_decay : 0.1 lr_scheduler : class_path : ReduceLROnPlateau init_args : monitor : val/loss To run this training task using the YAML, simply execute: terratorch fit --config <path_to_config_file> To test your model on the test set, execute: terratorch test --config <path_to_config_file> --ckpt_path <path_to_checkpoint_file> For inference, execute: terratorch predict -c <path_to_config_file> --ckpt_path<path_to_checkpoint> --predict_output_dir <path_to_output_dir> --data.init_args.predict_data_root <path_to_input_dir> --data.init_args.predict_dataset_bands <all bands in the predicted dataset, e.g. [ BLUE,GREEN,RED,NIR_NARROW,SWIR_1,SWIR_2,0 ] > Experimental feature : Users that want to optimize hyperparameters or repeat best experiment might be interest in in terratorch-iterate, a terratorch's plugin. For instance, to run terratorch-iterate to optimize hyperparameters, one can run: terratorch iterate --hpo --config <path_to_config_file> Please see how to install terratorch-iterate on this link and how to use it on this link .","title":"Getting Started"},{"location":"quick_start/#quick-start","text":"","title":"Quick start"},{"location":"quick_start/#configuring-the-environment","text":"","title":"Configuring the environment"},{"location":"quick_start/#python","text":"TerraTorch is currently tested for Python in 3.10 <= Python <= 3.12 .","title":"Python"},{"location":"quick_start/#gdal","text":"GDAL is required to read and write TIFF images. It is usually easy to install in Unix/Linux systems, but if it is not your case we reccomend using a conda environment and installing it with conda install -c conda-forge gdal .","title":"GDAL"},{"location":"quick_start/#installing-terratorch","text":"For a stable point-release, use pip install terratorch . If you prefer to get the most recent version of the main branch, install the library with pip install git+https://github.com/IBM/terratorch.git . To install as a developer (e.g. to extend the library) clone this repo, and run pip install -e . .","title":"Installing TerraTorch"},{"location":"quick_start/#creating-backbones","text":"You can interact with the library at several levels of abstraction. Each deeper level of abstraction trades off some amount of flexibility for ease of use and configuration. In the simplest case, we might only want access a backbone and code all the rest ourselves. In this case, we can simply use the library as a backbone factory: Instantiating a prithvi backbone from terratorch import BACKBONE_REGISTRY # find available prithvi models print ([ model_name for model_name in BACKBONE_REGISTRY if \"terratorch_prithvi\" in model_name ]) >>> [ 'terratorch_prithvi_eo_tiny' , 'terratorch_prithvi_eo_v1_100' , 'terratorch_prithvi_eo_v2_300' , 'terratorch_prithvi_eo_v2_600' , 'terratorch_prithvi_eo_v2_300_tl' , 'terratorch_prithvi_eo_v2_600_tl' ] # show all models with list(BACKBONE_REGISTRY) # check a model is in the registry \"terratorch_prithvi_eo_v2_300\" in BACKBONE_REGISTRY >>> True # without the prefix, all internal registries will be searched until the first match is found \"prithvi_eo_v1_100\" in BACKBONE_REGISTRY >>> True # instantiate your desired model # the backbone registry prefix (e.g. `terratorch` or `timm`) is optional # in this case, the underlying registry is terratorch. model = BACKBONE_REGISTRY . build ( \"prithvi_eo_v1_100\" , pretrained = True ) # instantiate your model with more options, for instance, passing weights from your own file model = BACKBONE_REGISTRY . build ( \"prithvi_eo_v2_300\" , num_frames = 1 , ckpt_path = 'path/to/model.pt' ) # Rest of your PyTorch / PyTorchLightning code Internally, terratorch maintains several registries for components such as backbones or decoders. The top-level BACKBONE_REGISTRY collects all of them. The name passed to build is used to find the appropriate model constructor, which will be the first model from the first registry found with that name. To explicitly determine the registry that will build the model, you may prepend a prefix such as timm_ to the model name. In this case, the timm model registry will be exclusively searched for the model.","title":"Creating Backbones"},{"location":"quick_start/#directly-creating-a-full-model","text":"We also provide a model factory for a task specific model built on one a backbones: Building a full model, with task specific decoder import terratorch # even though we don't use the import directly, we need it so that the models are available in the timm registry from terratorch.models import EncoderDecoderFactory from terratorch.datasets import HLSBands model_factory = EncoderDecoderFactory () # Let's build a segmentation model # Parameters prefixed with backbone_ get passed to the backbone # Parameters prefixed with decoder_ get passed to the decoder # Parameters prefixed with head_ get passed to the head model = model_factory . build_model ( task = \"segmentation\" , backbone = \"prithvi_eo_v2_300\" , backbone_pretrained = True , backbone_bands = [ HLSBands . BLUE , HLSBands . GREEN , HLSBands . RED , HLSBands . NIR_NARROW , HLSBands . SWIR_1 , HLSBands . SWIR_2 , ], necks = [{ \"name\" : \"SelectIndices\" , \"indices\" : [ - 1 ]}, { \"name\" : \"ReshapeTokensToImage\" }], decoder = \"FCNDecoder\" , decoder_channels = 128 , head_dropout = 0.1 , num_classes = 4 , ) # Rest of your PyTorch / PyTorchLightning code . . .","title":"Directly creating a full model"},{"location":"quick_start/#training-with-lightning-tasks","text":"At the highest level of abstraction, you can directly obtain a LightningModule ready to be trained. Building a full Pixel-Wise Regression task model_args = dict ( backbone = \"prithvi_eo_v2_300\" , backbone_pretrained = True , backbone_num_frames = 1 , backbone_bands = [ HLSBands . BLUE , HLSBands . GREEN , HLSBands . RED , HLSBands . NIR_NARROW , HLSBands . SWIR_1 , HLSBands . SWIR_2 , ], necks = [{ \"name\" : \"SelectIndices\" , \"indices\" : [ - 1 ]}, { \"name\" : \"ReshapeTokensToImage\" }], decoder = \"FCNDecoder\" , decoder_channels = 128 , head_dropout = 0.1 ) task = PixelwiseRegressionTask ( model_args , \"EncoderDecoderFactory\" , loss = \"rmse\" , lr = lr , ignore_index =- 1 , optimizer = \"AdamW\" , optimizer_hparams = { \"weight_decay\" : 0.05 }, ) # Pass this LightningModule to a Lightning Trainer, together with some LightningDataModule Alternatively, all the process can be summarized in configuration files written in YAML format, as seen below. Configuration file for a Semantic Segmentation Task # lightning.pytorch==2.1.1 seed_everything : 0 trainer : accelerator : auto strategy : auto devices : auto num_nodes : 1 precision : bf16 logger : class_path : TensorBoardLogger init_args : save_dir : <path_to_experiment_dir> name : <experiment_name> callbacks : - class_path : RichProgressBar - class_path : LearningRateMonitor init_args : logging_interval : epoch max_epochs : 200 check_val_every_n_epoch : 1 log_every_n_steps : 50 enable_checkpointing : true default_root_dir : <path_to_experiment_dir> data : class_path : terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule init_args : batch_size : 16 num_workers : 8 dict_kwargs : data_root : <path_to_data_root> bands : - 1 - 2 - 3 - 8 - 11 - 12 model : class_path : terratorch.tasks.SemanticSegmentationTask init_args : model_factory : EncoderDecoderFactory model_args : backbone : prithvi_eo_v2_300 backbone_img_size : 512 backbone_pretrained : True backbone_bands : - BLUE - GREEN - RED - NIR_NARROW - SWIR_1 - SWIR_2 necks : - name : SelectIndices indices : [ 5 , 11 , 17 , 23 ] - name : ReshapeTokensToImage - name : LearnedInterpolateToPyramidal decoder : UperNetDecoder decoder_channels : 256 head_channel_list : [ 256 ] head_dropout : 0.1 num_classes : 2 loss : dice ignore_index : -1 freeze_backbone : false freeze_decoder : false optimizer : class_path : torch.optim.AdamW init_args : lr : 1.e-4 weight_decay : 0.1 lr_scheduler : class_path : ReduceLROnPlateau init_args : monitor : val/loss To run this training task using the YAML, simply execute: terratorch fit --config <path_to_config_file> To test your model on the test set, execute: terratorch test --config <path_to_config_file> --ckpt_path <path_to_checkpoint_file> For inference, execute: terratorch predict -c <path_to_config_file> --ckpt_path<path_to_checkpoint> --predict_output_dir <path_to_output_dir> --data.init_args.predict_data_root <path_to_input_dir> --data.init_args.predict_dataset_bands <all bands in the predicted dataset, e.g. [ BLUE,GREEN,RED,NIR_NARROW,SWIR_1,SWIR_2,0 ] > Experimental feature : Users that want to optimize hyperparameters or repeat best experiment might be interest in in terratorch-iterate, a terratorch's plugin. For instance, to run terratorch-iterate to optimize hyperparameters, one can run: terratorch iterate --hpo --config <path_to_config_file> Please see how to install terratorch-iterate on this link and how to use it on this link .","title":"Training with Lightning Tasks"},{"location":"registry/","text":"Registries # TerraTorch keeps a set of registries which map strings to instances of those strings. They can be imported from terratorch.registry . Info If you are using tasks with existing models, you may never have to interact with registries directly. The model factory will handle interactions with registries. Registries behave like python sets, exposing the usual contains and iter operations. This means you can easily operate on them in a pythonic way, such as \"model\" in registry or list(registry) . To create the desired instance, registries expose a build method, which accepts the name and the arguments to be passed to the constructor. Using registries from terratorch import BACKBONE_REGISTRY # find available prithvi models print ([ model_name for model_name in BACKBONE_REGISTRY if \"terratorch_prithvi\" in model_name ]) >>> [ 'terratorch_prithvi_eo_tiny' , 'terratorch_prithvi_eo_v1_100' , 'terratorch_prithvi_eo_v2_300' , 'terratorch_prithvi_eo_v2_600' , 'terratorch_prithvi_eo_v2_300_tl' , 'terratorch_prithvi_eo_v2_600_tl' ] # show all models with list(BACKBONE_REGISTRY) # check a model is in the registry \"terratorch_prithvi_eo_v2_300\" in BACKBONE_REGISTRY >>> True # without the prefix, all internal registries will be searched until the first match is found \"prithvi_eo_v1_100\" in BACKBONE_REGISTRY >>> True # instantiate your desired model # the backbone registry prefix (e.g. `terratorch` or `timm`) is optional # in this case, the underlying registry is terratorch. model = BACKBONE_REGISTRY . build ( \"prithvi_eo_v1_100\" , pretrained = True ) # instantiate your model with more options, for instance, passing weights from your own file model = BACKBONE_REGISTRY . build ( \"prithvi_eo_v2_300\" , num_frames = 1 , ckpt_path = 'path/to/model.pt' ) # Rest of your PyTorch / PyTorchLightning code MultiSourceRegistries # BACKBONE_REGISTRY and DECODER_REGISTRY are special registries which dynamically aggregate multiple registries. They behave as if they were a single large registry by searching over multiple registries. For instance, the DECODER_REGISTRY holds the TERRATORCH_DECODER_REGISTRY , which is responsible for decoders implemented in terratorch, as well as the SMP_DECODER_REGISTRY and the MMSEG_DECODER_REGISTRY (if mmseg is installed). To make sure you access the object from a particular registry, you may prepend your string with the prefix from that registry. from terratorch import DECODER_REGISTRY # decoder registries always take at least one extra argument, the channel list with the channel dimension of each embedding passed to it DECODER_REGISTRY . build ( \"FCNDecoder\" , [ 32 , 64 , 128 ]) DECODER_REGISTRY . build ( \"terratorch_FCNDecoder\" , [ 32 , 64 , 128 ]) # Find all prefixes DECODER_REGISTRY . keys () >>> odict_keys ([ 'terratorch' , 'smp' , 'mmseg' ]) If a prefix is not added, the MultiSourceRegistry will search each registry in the order it was added (starting with the TERRATORCH_ registry) until it finds the first match. For both of these registries, only TERRATORCH_X_REGISTRY is mutable. To register backbones or decoders to terratorch, you should decorate the constructor function (or the model class itself) with @TERRATORCH_DECODER_REGISTRY.register or @TERRATORCH_BACKBONE_REGISTRY.register . To add a new registry to these top level registries, you should use the .register method, taking the register and the prefix that will be used for it. terratorch.registry.registry.MultiSourceRegistry # Bases: Mapping [ str , T ] , Generic [ T ] Registry that searches in multiple sources Correct functioning of this class depends on registries raising a KeyError when the model is not found. Source code in terratorch/registry/registry.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 class MultiSourceRegistry ( Mapping [ str , T ], typing . Generic [ T ]): \"\"\"Registry that searches in multiple sources Correct functioning of this class depends on registries raising a KeyError when the model is not found. \"\"\" def __init__ ( self , ** sources ) -> None : self . _sources : OrderedDict [ str , T ] = OrderedDict ( sources ) def _parse_prefix ( self , name ) -> tuple [ str , str ] | None : split = name . split ( \"_\" ) if len ( split ) > 1 and split [ 0 ] in self . _sources : prefix = split [ 0 ] name_without_prefix = \"_\" . join ( split [ 1 :]) return prefix , name_without_prefix return None def find_registry ( self , name : str ) -> T : parsed_prefix = self . _parse_prefix ( name ) if parsed_prefix : prefix , name_without_prefix = parsed_prefix registry = self . _sources [ prefix ] return registry # if no prefix is given, go through all sources in order for registry in self . _sources . values (): if name in registry : return registry msg = f \"Model { name } not found in any registry\" raise KeyError ( msg ) def build ( self , name : str , * constructor_args , ** constructor_kwargs ): parsed_prefix = self . _parse_prefix ( name ) if parsed_prefix : prefix , name_without_prefix = parsed_prefix registry = self . _sources [ prefix ] return registry . build ( name_without_prefix , * constructor_args , ** constructor_kwargs ) # if no prefix, try to build in order for source in self . _sources . values (): with suppress ( KeyError ): return source . build ( name , * constructor_args , ** constructor_kwargs ) msg = f \"Could not instantiate model { name } not from any source.\" raise KeyError ( msg ) def register_source ( self , prefix : str , registry : T ) -> None : \"\"\"Register a source in the registry\"\"\" if prefix in self . _sources : msg = f \"Source for prefix { prefix } already exists.\" raise KeyError ( msg ) self . _sources [ prefix ] = registry def __iter__ ( self ): for prefix in self . _sources : for element in self . _sources [ prefix ]: yield prefix + \"_\" + element def __len__ ( self ): return sum ( len ( source ) for source in self . _sources . values ()) def __getitem__ ( self , name ): return self . _sources [ name ] def __contains__ ( self , name ): parsed_prefix = self . _parse_prefix ( name ) if parsed_prefix : prefix , name_without_prefix = parsed_prefix return name_without_prefix in self . _sources [ prefix ] return any ( name in source for source in self . _sources . values ()) @_recursive_repr () def __repr__ ( self ): args = [ f \" { name } = { source !r} \" for name , source in self . _sources . items ()] return f ' { self . __class__ . __name__ } ( { \", \" . join ( args ) } )' def __str__ ( self ): sources_str = str ( \" | \" . join ([ f \" { prefix } : { source !s} \" for prefix , source in self . _sources . items ()])) return f \"Multi source registry with { len ( self ) } items: { sources_str } \" def keys ( self ): return self . _sources . keys () register_source ( prefix , registry ) # Register a source in the registry Source code in terratorch/registry/registry.py 68 69 70 71 72 73 def register_source ( self , prefix : str , registry : T ) -> None : \"\"\"Register a source in the registry\"\"\" if prefix in self . _sources : msg = f \"Source for prefix { prefix } already exists.\" raise KeyError ( msg ) self . _sources [ prefix ] = registry terratorch.registry.registry.Registry # Bases: Set Registry holding model constructors and multiple additional sources. This registry behaves as a set of strings, which are model names, to model classes or functions which instantiate model classes. In addition, it can instantiate models with the build method. Add constructors to the registry by annotating them with @registry.register. registry = Registry() @registry.register ... def model( args, *kwargs): ... return object() \"model\" in registry True model_instance = registry.build(\"model\") Source code in terratorch/registry/registry.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class Registry ( Set ): \"\"\"Registry holding model constructors and multiple additional sources. This registry behaves as a set of strings, which are model names, to model classes or functions which instantiate model classes. In addition, it can instantiate models with the build method. Add constructors to the registry by annotating them with @registry.register. >>> registry = Registry() >>> @registry.register ... def model(*args, **kwargs): ... return object() >>> \"model\" in registry True >>> model_instance = registry.build(\"model\") \"\"\" def __init__ ( self , ** elements ) -> None : self . _registry : dict [ str , Callable ] = dict ( elements ) def register ( self , constructor : Callable | type ) -> Callable : \"\"\"Register a component in the registry. Used as a decorator. Args: constructor (Callable | type): Function or class to be decorated with @register. \"\"\" if not callable ( constructor ): msg = f \"Invalid argument. Decorate a function or class with @ { self . __class__ . __name__ } .register\" raise TypeError ( msg ) self . _registry [ constructor . __name__ ] = constructor return constructor def build ( self , name : str , * constructor_args , ** constructor_kwargs ): \"\"\"Build and return the component. Use prefixes ending with _ to forward to a specific source \"\"\" return self . _registry [ name ]( * constructor_args , ** constructor_kwargs ) def __iter__ ( self ): return iter ( self . _registry ) # def __getitem__(self, key): # return self._registry[key] def __len__ ( self ): return len ( self . _registry ) def __contains__ ( self , key ): return key in self . _registry def __repr__ ( self ): return f \" { self . __class__ . __name__ } ( { self . _registry !r} )\" def __str__ ( self ): return f \"Registry with { len ( self ) } registered items\" build ( name , * constructor_args , ** constructor_kwargs ) # Build and return the component. Use prefixes ending with _ to forward to a specific source Source code in terratorch/registry/registry.py 140 141 142 143 144 def build ( self , name : str , * constructor_args , ** constructor_kwargs ): \"\"\"Build and return the component. Use prefixes ending with _ to forward to a specific source \"\"\" return self . _registry [ name ]( * constructor_args , ** constructor_kwargs ) register ( constructor ) # Register a component in the registry. Used as a decorator. Parameters: constructor ( Callable | type ) \u2013 Function or class to be decorated with @register. Source code in terratorch/registry/registry.py 128 129 130 131 132 133 134 135 136 137 138 def register ( self , constructor : Callable | type ) -> Callable : \"\"\"Register a component in the registry. Used as a decorator. Args: constructor (Callable | type): Function or class to be decorated with @register. \"\"\" if not callable ( constructor ): msg = f \"Invalid argument. Decorate a function or class with @ { self . __class__ . __name__ } .register\" raise TypeError ( msg ) self . _registry [ constructor . __name__ ] = constructor return constructor Other Registries # Additionally, terratorch has the NECK_REGISTRY , where all necks must be registered, and the MODEL_FACTORY_REGISTRY , where all model factories must be registered.","title":"Registries"},{"location":"registry/#registries","text":"TerraTorch keeps a set of registries which map strings to instances of those strings. They can be imported from terratorch.registry . Info If you are using tasks with existing models, you may never have to interact with registries directly. The model factory will handle interactions with registries. Registries behave like python sets, exposing the usual contains and iter operations. This means you can easily operate on them in a pythonic way, such as \"model\" in registry or list(registry) . To create the desired instance, registries expose a build method, which accepts the name and the arguments to be passed to the constructor. Using registries from terratorch import BACKBONE_REGISTRY # find available prithvi models print ([ model_name for model_name in BACKBONE_REGISTRY if \"terratorch_prithvi\" in model_name ]) >>> [ 'terratorch_prithvi_eo_tiny' , 'terratorch_prithvi_eo_v1_100' , 'terratorch_prithvi_eo_v2_300' , 'terratorch_prithvi_eo_v2_600' , 'terratorch_prithvi_eo_v2_300_tl' , 'terratorch_prithvi_eo_v2_600_tl' ] # show all models with list(BACKBONE_REGISTRY) # check a model is in the registry \"terratorch_prithvi_eo_v2_300\" in BACKBONE_REGISTRY >>> True # without the prefix, all internal registries will be searched until the first match is found \"prithvi_eo_v1_100\" in BACKBONE_REGISTRY >>> True # instantiate your desired model # the backbone registry prefix (e.g. `terratorch` or `timm`) is optional # in this case, the underlying registry is terratorch. model = BACKBONE_REGISTRY . build ( \"prithvi_eo_v1_100\" , pretrained = True ) # instantiate your model with more options, for instance, passing weights from your own file model = BACKBONE_REGISTRY . build ( \"prithvi_eo_v2_300\" , num_frames = 1 , ckpt_path = 'path/to/model.pt' ) # Rest of your PyTorch / PyTorchLightning code","title":"Registries"},{"location":"registry/#multisourceregistries","text":"BACKBONE_REGISTRY and DECODER_REGISTRY are special registries which dynamically aggregate multiple registries. They behave as if they were a single large registry by searching over multiple registries. For instance, the DECODER_REGISTRY holds the TERRATORCH_DECODER_REGISTRY , which is responsible for decoders implemented in terratorch, as well as the SMP_DECODER_REGISTRY and the MMSEG_DECODER_REGISTRY (if mmseg is installed). To make sure you access the object from a particular registry, you may prepend your string with the prefix from that registry. from terratorch import DECODER_REGISTRY # decoder registries always take at least one extra argument, the channel list with the channel dimension of each embedding passed to it DECODER_REGISTRY . build ( \"FCNDecoder\" , [ 32 , 64 , 128 ]) DECODER_REGISTRY . build ( \"terratorch_FCNDecoder\" , [ 32 , 64 , 128 ]) # Find all prefixes DECODER_REGISTRY . keys () >>> odict_keys ([ 'terratorch' , 'smp' , 'mmseg' ]) If a prefix is not added, the MultiSourceRegistry will search each registry in the order it was added (starting with the TERRATORCH_ registry) until it finds the first match. For both of these registries, only TERRATORCH_X_REGISTRY is mutable. To register backbones or decoders to terratorch, you should decorate the constructor function (or the model class itself) with @TERRATORCH_DECODER_REGISTRY.register or @TERRATORCH_BACKBONE_REGISTRY.register . To add a new registry to these top level registries, you should use the .register method, taking the register and the prefix that will be used for it.","title":"MultiSourceRegistries"},{"location":"registry/#terratorch.registry.registry.MultiSourceRegistry","text":"Bases: Mapping [ str , T ] , Generic [ T ] Registry that searches in multiple sources Correct functioning of this class depends on registries raising a KeyError when the model is not found. Source code in terratorch/registry/registry.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 class MultiSourceRegistry ( Mapping [ str , T ], typing . Generic [ T ]): \"\"\"Registry that searches in multiple sources Correct functioning of this class depends on registries raising a KeyError when the model is not found. \"\"\" def __init__ ( self , ** sources ) -> None : self . _sources : OrderedDict [ str , T ] = OrderedDict ( sources ) def _parse_prefix ( self , name ) -> tuple [ str , str ] | None : split = name . split ( \"_\" ) if len ( split ) > 1 and split [ 0 ] in self . _sources : prefix = split [ 0 ] name_without_prefix = \"_\" . join ( split [ 1 :]) return prefix , name_without_prefix return None def find_registry ( self , name : str ) -> T : parsed_prefix = self . _parse_prefix ( name ) if parsed_prefix : prefix , name_without_prefix = parsed_prefix registry = self . _sources [ prefix ] return registry # if no prefix is given, go through all sources in order for registry in self . _sources . values (): if name in registry : return registry msg = f \"Model { name } not found in any registry\" raise KeyError ( msg ) def build ( self , name : str , * constructor_args , ** constructor_kwargs ): parsed_prefix = self . _parse_prefix ( name ) if parsed_prefix : prefix , name_without_prefix = parsed_prefix registry = self . _sources [ prefix ] return registry . build ( name_without_prefix , * constructor_args , ** constructor_kwargs ) # if no prefix, try to build in order for source in self . _sources . values (): with suppress ( KeyError ): return source . build ( name , * constructor_args , ** constructor_kwargs ) msg = f \"Could not instantiate model { name } not from any source.\" raise KeyError ( msg ) def register_source ( self , prefix : str , registry : T ) -> None : \"\"\"Register a source in the registry\"\"\" if prefix in self . _sources : msg = f \"Source for prefix { prefix } already exists.\" raise KeyError ( msg ) self . _sources [ prefix ] = registry def __iter__ ( self ): for prefix in self . _sources : for element in self . _sources [ prefix ]: yield prefix + \"_\" + element def __len__ ( self ): return sum ( len ( source ) for source in self . _sources . values ()) def __getitem__ ( self , name ): return self . _sources [ name ] def __contains__ ( self , name ): parsed_prefix = self . _parse_prefix ( name ) if parsed_prefix : prefix , name_without_prefix = parsed_prefix return name_without_prefix in self . _sources [ prefix ] return any ( name in source for source in self . _sources . values ()) @_recursive_repr () def __repr__ ( self ): args = [ f \" { name } = { source !r} \" for name , source in self . _sources . items ()] return f ' { self . __class__ . __name__ } ( { \", \" . join ( args ) } )' def __str__ ( self ): sources_str = str ( \" | \" . join ([ f \" { prefix } : { source !s} \" for prefix , source in self . _sources . items ()])) return f \"Multi source registry with { len ( self ) } items: { sources_str } \" def keys ( self ): return self . _sources . keys ()","title":"MultiSourceRegistry"},{"location":"registry/#terratorch.registry.registry.MultiSourceRegistry.register_source","text":"Register a source in the registry Source code in terratorch/registry/registry.py 68 69 70 71 72 73 def register_source ( self , prefix : str , registry : T ) -> None : \"\"\"Register a source in the registry\"\"\" if prefix in self . _sources : msg = f \"Source for prefix { prefix } already exists.\" raise KeyError ( msg ) self . _sources [ prefix ] = registry","title":"register_source"},{"location":"registry/#terratorch.registry.registry.Registry","text":"Bases: Set Registry holding model constructors and multiple additional sources. This registry behaves as a set of strings, which are model names, to model classes or functions which instantiate model classes. In addition, it can instantiate models with the build method. Add constructors to the registry by annotating them with @registry.register. registry = Registry() @registry.register ... def model( args, *kwargs): ... return object() \"model\" in registry True model_instance = registry.build(\"model\") Source code in terratorch/registry/registry.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class Registry ( Set ): \"\"\"Registry holding model constructors and multiple additional sources. This registry behaves as a set of strings, which are model names, to model classes or functions which instantiate model classes. In addition, it can instantiate models with the build method. Add constructors to the registry by annotating them with @registry.register. >>> registry = Registry() >>> @registry.register ... def model(*args, **kwargs): ... return object() >>> \"model\" in registry True >>> model_instance = registry.build(\"model\") \"\"\" def __init__ ( self , ** elements ) -> None : self . _registry : dict [ str , Callable ] = dict ( elements ) def register ( self , constructor : Callable | type ) -> Callable : \"\"\"Register a component in the registry. Used as a decorator. Args: constructor (Callable | type): Function or class to be decorated with @register. \"\"\" if not callable ( constructor ): msg = f \"Invalid argument. Decorate a function or class with @ { self . __class__ . __name__ } .register\" raise TypeError ( msg ) self . _registry [ constructor . __name__ ] = constructor return constructor def build ( self , name : str , * constructor_args , ** constructor_kwargs ): \"\"\"Build and return the component. Use prefixes ending with _ to forward to a specific source \"\"\" return self . _registry [ name ]( * constructor_args , ** constructor_kwargs ) def __iter__ ( self ): return iter ( self . _registry ) # def __getitem__(self, key): # return self._registry[key] def __len__ ( self ): return len ( self . _registry ) def __contains__ ( self , key ): return key in self . _registry def __repr__ ( self ): return f \" { self . __class__ . __name__ } ( { self . _registry !r} )\" def __str__ ( self ): return f \"Registry with { len ( self ) } registered items\"","title":"Registry"},{"location":"registry/#terratorch.registry.registry.Registry.build","text":"Build and return the component. Use prefixes ending with _ to forward to a specific source Source code in terratorch/registry/registry.py 140 141 142 143 144 def build ( self , name : str , * constructor_args , ** constructor_kwargs ): \"\"\"Build and return the component. Use prefixes ending with _ to forward to a specific source \"\"\" return self . _registry [ name ]( * constructor_args , ** constructor_kwargs )","title":"build"},{"location":"registry/#terratorch.registry.registry.Registry.register","text":"Register a component in the registry. Used as a decorator. Parameters: constructor ( Callable | type ) \u2013 Function or class to be decorated with @register. Source code in terratorch/registry/registry.py 128 129 130 131 132 133 134 135 136 137 138 def register ( self , constructor : Callable | type ) -> Callable : \"\"\"Register a component in the registry. Used as a decorator. Args: constructor (Callable | type): Function or class to be decorated with @register. \"\"\" if not callable ( constructor ): msg = f \"Invalid argument. Decorate a function or class with @ { self . __class__ . __name__ } .register\" raise TypeError ( msg ) self . _registry [ constructor . __name__ ] = constructor return constructor","title":"register"},{"location":"registry/#other-registries","text":"Additionally, terratorch has the NECK_REGISTRY , where all necks must be registered, and the MODEL_FACTORY_REGISTRY , where all model factories must be registered.","title":"Other Registries"},{"location":"tasks/","text":"Tasks # Tasks provide a convenient abstraction over the training of a model for a specific downstream task. They encapsulate the model, optimizer, metrics, loss as well as training, validation and testing steps. The task expects to be passed a model factory, to which the model_args arguments are passed to instantiate the model that will be trained. The models produced by this model factory should output ModelOutput instances and conform to the Model ABC. Tasks are best leveraged using config files, where they are specified in the model section under class_path . You can check out some examples of config files here . Below are the details of the tasks currently implemented in TerraTorch (Pixelwise Regression, Semantic Segmentation and Classification). terratorch.tasks.segmentation_tasks.SemanticSegmentationTask # Bases: TerraTorchTask Semantic Segmentation Task that accepts models from a range of sources. This class is analog in functionality to class SemanticSegmentationTask defined by torchgeo. However, it has some important differences: - Accepts the specification of a model factory - Logs metrics per class - Does not have any callbacks by default (TorchGeo tasks do early stopping by default) - Allows the setting of optimizers in the constructor - Allows to evaluate on multiple test dataloaders Source code in terratorch/tasks/segmentation_tasks.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 class SemanticSegmentationTask ( TerraTorchTask ): \"\"\"Semantic Segmentation Task that accepts models from a range of sources. This class is analog in functionality to class SemanticSegmentationTask defined by torchgeo. However, it has some important differences: - Accepts the specification of a model factory - Logs metrics per class - Does not have any callbacks by default (TorchGeo tasks do early stopping by default) - Allows the setting of optimizers in the constructor - Allows to evaluate on multiple test dataloaders \"\"\" def __init__ ( self , model_args : dict , model_factory : str | None = None , model : torch . nn . Module | None = None , loss : str = \"ce\" , aux_heads : list [ AuxiliaryHead ] | None = None , aux_loss : dict [ str , float ] | None = None , class_weights : list [ float ] | None = None , ignore_index : int | None = None , lr : float = 0.001 , # the following are optional so CLI doesnt need to pass them optimizer : str | None = None , optimizer_hparams : dict | None = None , scheduler : str | None = None , scheduler_hparams : dict | None = None , # freeze_backbone : bool = False , # noqa: FBT001, FBT002 freeze_decoder : bool = False , # noqa: FBT002, FBT001 freeze_head : bool = False , plot_on_val : bool | int = 10 , class_names : list [ str ] | None = None , tiled_inference_parameters : TiledInferenceParameters = None , test_dataloaders_names : list [ str ] | None = None , lr_overrides : dict [ str , float ] | None = None , output_most_probable : bool = True , ) -> None : \"\"\"Constructor Args: Defaults to None. model_args (Dict): Arguments passed to the model factory. model_factory (str, optional): ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model (torch.nn.Module, optional): Custom model. loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\". aux_loss (dict[str, float] | None, optional): Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss. class_weights (list[float] | None, optional): List of class weights to be applied to the loss. Defaults to None. ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None. lr (float, optional): Learning rate to be used. Defaults to 0.001. optimizer (str | None, optional): Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False. freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False. freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False. plot_on_val (bool | int, optional): Whether to plot visualizations on validation. If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs. class_names (list[str] | None, optional): List of class names passed to metrics for better naming. Defaults to numeric ordering. tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters used to determine if inference is done on the whole image or through tiling. test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. output_most_probable (bool): A boolean to define if the output during the inference will be just for the most probable class or if it will include all of them. \"\"\" self . tiled_inference_parameters = tiled_inference_parameters self . aux_loss = aux_loss self . aux_heads = aux_heads if model is not None and model_factory is not None : logger . warning ( \"A model_factory and a model was provided. The model_factory is ignored.\" ) if model is None and model_factory is None : raise ValueError ( \"A model_factory or a model (torch.nn.Module) must be provided.\" ) if model_factory and model is None : self . model_factory = MODEL_FACTORY_REGISTRY . build ( model_factory ) super () . __init__ ( task = \"segmentation\" ) if model is not None : # Custom model self . model = model self . train_loss_handler = LossHandler ( self . train_metrics . prefix ) self . test_loss_handler : list [ LossHandler ] = [] for metrics in self . test_metrics : self . test_loss_handler . append ( LossHandler ( metrics . prefix )) self . val_loss_handler = LossHandler ( self . val_metrics . prefix ) self . monitor = f \" { self . val_metrics . prefix } loss\" self . plot_on_val = int ( plot_on_val ) self . output_most_probable = output_most_probable if output_most_probable : self . select_classes = lambda y : y . argmax ( dim = 1 ) else : self . select_classes = lambda y : y def configure_losses ( self ) -> None : \"\"\"Initialize the loss criterion. Raises: ValueError: If *loss* is invalid. \"\"\" loss : str = self . hparams [ \"loss\" ] ignore_index = self . hparams [ \"ignore_index\" ] class_weights = ( torch . Tensor ( self . hparams [ \"class_weights\" ]) if self . hparams [ \"class_weights\" ] is not None else None ) if loss == \"ce\" : ignore_value = - 100 if ignore_index is None else ignore_index self . criterion = nn . CrossEntropyLoss ( ignore_index = ignore_value , weight = class_weights ) elif loss == \"jaccard\" : if ignore_index is not None : exception_message = ( f \"Jaccard loss does not support ignore_index, but found non-None value of { ignore_index } .\" ) raise RuntimeError ( exception_message ) self . criterion = smp . losses . JaccardLoss ( mode = \"multiclass\" ) elif loss == \"focal\" : self . criterion = smp . losses . FocalLoss ( \"multiclass\" , ignore_index = ignore_index , normalized = True ) elif loss == \"dice\" : self . criterion = smp . losses . DiceLoss ( \"multiclass\" , ignore_index = ignore_index ) else : exception_message = ( f \"Loss type ' { loss } ' is not valid. Currently, supports 'ce', 'jaccard', 'dice' or 'focal' loss.\" ) raise ValueError ( exception_message ) def configure_metrics ( self ) -> None : \"\"\"Initialize the performance metrics.\"\"\" num_classes : int = self . hparams [ \"model_args\" ][ \"num_classes\" ] ignore_index : int = self . hparams [ \"ignore_index\" ] class_names = self . hparams [ \"class_names\" ] metrics = MetricCollection ( { \"Multiclass_Accuracy\" : MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , multidim_average = \"global\" , average = \"micro\" , ), \"Multiclass_Accuracy_Class\" : ClasswiseWrapper ( MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , multidim_average = \"global\" , average = None , ), labels = class_names , ), \"Multiclass_Jaccard_Index_Micro\" : MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , average = \"micro\" ), \"Multiclass_Jaccard_Index\" : MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , ), \"Multiclass_Jaccard_Index_Class\" : ClasswiseWrapper ( MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , average = None ), labels = class_names , ), \"Multiclass_F1_Score\" : MulticlassF1Score ( num_classes = num_classes , ignore_index = ignore_index , multidim_average = \"global\" , average = \"micro\" , ), } ) self . train_metrics = metrics . clone ( prefix = \"train/\" ) self . val_metrics = metrics . clone ( prefix = \"val/\" ) if self . hparams [ \"test_dataloaders_names\" ] is not None : self . test_metrics = nn . ModuleList ( [ metrics . clone ( prefix = f \"test/ { dl_name } /\" ) for dl_name in self . hparams [ \"test_dataloaders_names\" ]] ) else : self . test_metrics = nn . ModuleList ([ metrics . clone ( prefix = \"test/\" )]) def training_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the train loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . train_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . train_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = y . shape [ 0 ]) y_hat_hard = to_segmentation_prediction ( model_output ) self . train_metrics . update ( y_hat_hard , y ) return loss [ \"loss\" ] def test_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the test loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) if dataloader_idx >= len ( self . test_loss_handler ): msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\" raise ValueError ( msg ) loss = self . test_loss_handler [ dataloader_idx ] . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . test_loss_handler [ dataloader_idx ] . log_loss ( partial ( self . log , add_dataloader_idx = False ), # We don't need the dataloader idx as prefixes are different loss_dict = loss , batch_size = y . shape [ 0 ], ) y_hat_hard = to_segmentation_prediction ( model_output ) self . test_metrics [ dataloader_idx ] . update ( y_hat_hard , y ) def validation_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the validation loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . val_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . val_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = y . shape [ 0 ]) y_hat_hard = to_segmentation_prediction ( model_output ) self . val_metrics . update ( y_hat_hard , y ) if self . _do_plot_samples ( batch_idx ): try : datamodule = self . trainer . datamodule batch [ \"prediction\" ] = y_hat_hard if isinstance ( batch [ \"image\" ], dict ): if hasattr ( datamodule , \"rgb_modality\" ): # Generic multimodal dataset batch [ \"image\" ] = batch [ \"image\" ][ datamodule . rgb_modality ] else : # Multimodal dataset. Assuming first item to be the modality to visualize. batch [ \"image\" ] = batch [ \"image\" ][ list ( batch [ \"image\" ] . keys ())[ 0 ]] for key in [ \"image\" , \"mask\" , \"prediction\" ]: batch [ key ] = batch [ key ] . cpu () sample = unbind_samples ( batch )[ 0 ] fig = datamodule . val_dataset . plot ( sample ) if fig : summary_writer = self . logger . experiment if hasattr ( summary_writer , \"add_figure\" ): summary_writer . add_figure ( f \"image/ { batch_idx } \" , fig , global_step = self . global_step ) elif hasattr ( summary_writer , \"log_figure\" ): summary_writer . log_figure ( self . logger . run_id , fig , f \"epoch_ { self . current_epoch } _ { batch_idx } .png\" ) except ValueError : pass finally : plt . close () def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the predicted class probabilities. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. Returns: Output predicted probabilities. \"\"\" x = batch [ \"image\" ] file_names = batch [ \"filename\" ] if \"filename\" in batch else None other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } def model_forward ( x , ** kwargs ): return self ( x , ** kwargs ) . output if self . tiled_inference_parameters : y_hat : Tensor = tiled_inference ( model_forward , x , self . hparams [ \"model_args\" ][ \"num_classes\" ], self . tiled_inference_parameters , ** rest , ) else : y_hat : Tensor = self ( x , ** rest ) . output y_hat = self . select_classes ( y_hat ) return y_hat , file_names __init__ ( model_args , model_factory = None , model = None , loss = 'ce' , aux_heads = None , aux_loss = None , class_weights = None , ignore_index = None , lr = 0.001 , optimizer = None , optimizer_hparams = None , scheduler = None , scheduler_hparams = None , freeze_backbone = False , freeze_decoder = False , freeze_head = False , plot_on_val = 10 , class_names = None , tiled_inference_parameters = None , test_dataloaders_names = None , lr_overrides = None , output_most_probable = True ) # Constructor Parameters: model_args ( Dict ) \u2013 Arguments passed to the model factory. model_factory ( str , default: None ) \u2013 ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model ( Module , default: None ) \u2013 Custom model. loss ( str , default: 'ce' ) \u2013 Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\". aux_loss ( dict [ str , float ] | None , default: None ) \u2013 Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights ( Union [ list [ float ], None] , default: None ) \u2013 List of class weights to be applied to the loss. class_weights ( list [ float ] | None , default: None ) \u2013 List of class weights to be applied to the loss. Defaults to None. ignore_index ( int | None , default: None ) \u2013 Label to ignore in the loss computation. Defaults to None. lr ( float , default: 0.001 ) \u2013 Learning rate to be used. Defaults to 0.001. optimizer ( str | None , default: None ) \u2013 Name of optimizer class from torch.optim to be used. optimizer_hparams ( dict | None , default: None ) \u2013 Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler ( str , default: None ) \u2013 Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams ( dict | None , default: None ) \u2013 Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone ( bool , default: False ) \u2013 Whether to freeze the backbone. Defaults to False. freeze_decoder ( bool , default: False ) \u2013 Whether to freeze the decoder. Defaults to False. freeze_head ( bool , default: False ) \u2013 Whether to freeze the segmentation head. Defaults to False. plot_on_val ( bool | int , default: 10 ) \u2013 Whether to plot visualizations on validation. class_names ( list [ str ] | None , default: None ) \u2013 List of class names passed to metrics for better naming. Defaults to numeric ordering. tiled_inference_parameters ( TiledInferenceParameters | None , default: None ) \u2013 Inference parameters used to determine if inference is done on the whole image or through tiling. test_dataloaders_names ( list [ str ] | None , default: None ) \u2013 Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides ( dict [ str , float ] | None , default: None ) \u2013 Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. output_most_probable ( bool , default: True ) \u2013 A boolean to define if the output during the inference will be just for the most probable class or if it will include all of them. Source code in terratorch/tasks/segmentation_tasks.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def __init__ ( self , model_args : dict , model_factory : str | None = None , model : torch . nn . Module | None = None , loss : str = \"ce\" , aux_heads : list [ AuxiliaryHead ] | None = None , aux_loss : dict [ str , float ] | None = None , class_weights : list [ float ] | None = None , ignore_index : int | None = None , lr : float = 0.001 , # the following are optional so CLI doesnt need to pass them optimizer : str | None = None , optimizer_hparams : dict | None = None , scheduler : str | None = None , scheduler_hparams : dict | None = None , # freeze_backbone : bool = False , # noqa: FBT001, FBT002 freeze_decoder : bool = False , # noqa: FBT002, FBT001 freeze_head : bool = False , plot_on_val : bool | int = 10 , class_names : list [ str ] | None = None , tiled_inference_parameters : TiledInferenceParameters = None , test_dataloaders_names : list [ str ] | None = None , lr_overrides : dict [ str , float ] | None = None , output_most_probable : bool = True , ) -> None : \"\"\"Constructor Args: Defaults to None. model_args (Dict): Arguments passed to the model factory. model_factory (str, optional): ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model (torch.nn.Module, optional): Custom model. loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\". aux_loss (dict[str, float] | None, optional): Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss. class_weights (list[float] | None, optional): List of class weights to be applied to the loss. Defaults to None. ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None. lr (float, optional): Learning rate to be used. Defaults to 0.001. optimizer (str | None, optional): Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False. freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False. freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False. plot_on_val (bool | int, optional): Whether to plot visualizations on validation. If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs. class_names (list[str] | None, optional): List of class names passed to metrics for better naming. Defaults to numeric ordering. tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters used to determine if inference is done on the whole image or through tiling. test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. output_most_probable (bool): A boolean to define if the output during the inference will be just for the most probable class or if it will include all of them. \"\"\" self . tiled_inference_parameters = tiled_inference_parameters self . aux_loss = aux_loss self . aux_heads = aux_heads if model is not None and model_factory is not None : logger . warning ( \"A model_factory and a model was provided. The model_factory is ignored.\" ) if model is None and model_factory is None : raise ValueError ( \"A model_factory or a model (torch.nn.Module) must be provided.\" ) if model_factory and model is None : self . model_factory = MODEL_FACTORY_REGISTRY . build ( model_factory ) super () . __init__ ( task = \"segmentation\" ) if model is not None : # Custom model self . model = model self . train_loss_handler = LossHandler ( self . train_metrics . prefix ) self . test_loss_handler : list [ LossHandler ] = [] for metrics in self . test_metrics : self . test_loss_handler . append ( LossHandler ( metrics . prefix )) self . val_loss_handler = LossHandler ( self . val_metrics . prefix ) self . monitor = f \" { self . val_metrics . prefix } loss\" self . plot_on_val = int ( plot_on_val ) self . output_most_probable = output_most_probable if output_most_probable : self . select_classes = lambda y : y . argmax ( dim = 1 ) else : self . select_classes = lambda y : y configure_losses () # Initialize the loss criterion. Raises: ValueError \u2013 If loss is invalid. Source code in terratorch/tasks/segmentation_tasks.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def configure_losses ( self ) -> None : \"\"\"Initialize the loss criterion. Raises: ValueError: If *loss* is invalid. \"\"\" loss : str = self . hparams [ \"loss\" ] ignore_index = self . hparams [ \"ignore_index\" ] class_weights = ( torch . Tensor ( self . hparams [ \"class_weights\" ]) if self . hparams [ \"class_weights\" ] is not None else None ) if loss == \"ce\" : ignore_value = - 100 if ignore_index is None else ignore_index self . criterion = nn . CrossEntropyLoss ( ignore_index = ignore_value , weight = class_weights ) elif loss == \"jaccard\" : if ignore_index is not None : exception_message = ( f \"Jaccard loss does not support ignore_index, but found non-None value of { ignore_index } .\" ) raise RuntimeError ( exception_message ) self . criterion = smp . losses . JaccardLoss ( mode = \"multiclass\" ) elif loss == \"focal\" : self . criterion = smp . losses . FocalLoss ( \"multiclass\" , ignore_index = ignore_index , normalized = True ) elif loss == \"dice\" : self . criterion = smp . losses . DiceLoss ( \"multiclass\" , ignore_index = ignore_index ) else : exception_message = ( f \"Loss type ' { loss } ' is not valid. Currently, supports 'ce', 'jaccard', 'dice' or 'focal' loss.\" ) raise ValueError ( exception_message ) configure_metrics () # Initialize the performance metrics. Source code in terratorch/tasks/segmentation_tasks.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def configure_metrics ( self ) -> None : \"\"\"Initialize the performance metrics.\"\"\" num_classes : int = self . hparams [ \"model_args\" ][ \"num_classes\" ] ignore_index : int = self . hparams [ \"ignore_index\" ] class_names = self . hparams [ \"class_names\" ] metrics = MetricCollection ( { \"Multiclass_Accuracy\" : MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , multidim_average = \"global\" , average = \"micro\" , ), \"Multiclass_Accuracy_Class\" : ClasswiseWrapper ( MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , multidim_average = \"global\" , average = None , ), labels = class_names , ), \"Multiclass_Jaccard_Index_Micro\" : MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , average = \"micro\" ), \"Multiclass_Jaccard_Index\" : MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , ), \"Multiclass_Jaccard_Index_Class\" : ClasswiseWrapper ( MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , average = None ), labels = class_names , ), \"Multiclass_F1_Score\" : MulticlassF1Score ( num_classes = num_classes , ignore_index = ignore_index , multidim_average = \"global\" , average = \"micro\" , ), } ) self . train_metrics = metrics . clone ( prefix = \"train/\" ) self . val_metrics = metrics . clone ( prefix = \"val/\" ) if self . hparams [ \"test_dataloaders_names\" ] is not None : self . test_metrics = nn . ModuleList ( [ metrics . clone ( prefix = f \"test/ { dl_name } /\" ) for dl_name in self . hparams [ \"test_dataloaders_names\" ]] ) else : self . test_metrics = nn . ModuleList ([ metrics . clone ( prefix = \"test/\" )]) predict_step ( batch , batch_idx , dataloader_idx = 0 ) # Compute the predicted class probabilities. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Returns: Tensor \u2013 Output predicted probabilities. Source code in terratorch/tasks/segmentation_tasks.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the predicted class probabilities. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. Returns: Output predicted probabilities. \"\"\" x = batch [ \"image\" ] file_names = batch [ \"filename\" ] if \"filename\" in batch else None other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } def model_forward ( x , ** kwargs ): return self ( x , ** kwargs ) . output if self . tiled_inference_parameters : y_hat : Tensor = tiled_inference ( model_forward , x , self . hparams [ \"model_args\" ][ \"num_classes\" ], self . tiled_inference_parameters , ** rest , ) else : y_hat : Tensor = self ( x , ** rest ) . output y_hat = self . select_classes ( y_hat ) return y_hat , file_names test_step ( batch , batch_idx , dataloader_idx = 0 ) # Compute the test loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/segmentation_tasks.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def test_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the test loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) if dataloader_idx >= len ( self . test_loss_handler ): msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\" raise ValueError ( msg ) loss = self . test_loss_handler [ dataloader_idx ] . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . test_loss_handler [ dataloader_idx ] . log_loss ( partial ( self . log , add_dataloader_idx = False ), # We don't need the dataloader idx as prefixes are different loss_dict = loss , batch_size = y . shape [ 0 ], ) y_hat_hard = to_segmentation_prediction ( model_output ) self . test_metrics [ dataloader_idx ] . update ( y_hat_hard , y ) training_step ( batch , batch_idx , dataloader_idx = 0 ) # Compute the train loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/segmentation_tasks.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def training_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the train loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . train_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . train_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = y . shape [ 0 ]) y_hat_hard = to_segmentation_prediction ( model_output ) self . train_metrics . update ( y_hat_hard , y ) return loss [ \"loss\" ] validation_step ( batch , batch_idx , dataloader_idx = 0 ) # Compute the validation loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. Source code in terratorch/tasks/segmentation_tasks.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def validation_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the validation loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . val_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . val_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = y . shape [ 0 ]) y_hat_hard = to_segmentation_prediction ( model_output ) self . val_metrics . update ( y_hat_hard , y ) if self . _do_plot_samples ( batch_idx ): try : datamodule = self . trainer . datamodule batch [ \"prediction\" ] = y_hat_hard if isinstance ( batch [ \"image\" ], dict ): if hasattr ( datamodule , \"rgb_modality\" ): # Generic multimodal dataset batch [ \"image\" ] = batch [ \"image\" ][ datamodule . rgb_modality ] else : # Multimodal dataset. Assuming first item to be the modality to visualize. batch [ \"image\" ] = batch [ \"image\" ][ list ( batch [ \"image\" ] . keys ())[ 0 ]] for key in [ \"image\" , \"mask\" , \"prediction\" ]: batch [ key ] = batch [ key ] . cpu () sample = unbind_samples ( batch )[ 0 ] fig = datamodule . val_dataset . plot ( sample ) if fig : summary_writer = self . logger . experiment if hasattr ( summary_writer , \"add_figure\" ): summary_writer . add_figure ( f \"image/ { batch_idx } \" , fig , global_step = self . global_step ) elif hasattr ( summary_writer , \"log_figure\" ): summary_writer . log_figure ( self . logger . run_id , fig , f \"epoch_ { self . current_epoch } _ { batch_idx } .png\" ) except ValueError : pass finally : plt . close () terratorch.tasks.regression_tasks.PixelwiseRegressionTask # Bases: TerraTorchTask Pixelwise Regression Task that accepts models from a range of sources. This class is analog in functionality to PixelwiseRegressionTask defined by torchgeo. However, it has some important differences: - Accepts the specification of a model factory - Logs metrics per class - Does not have any callbacks by default (TorchGeo tasks do early stopping by default) - Allows the setting of optimizers in the constructor - Allows to evaluate on multiple test dataloaders Source code in terratorch/tasks/regression_tasks.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 class PixelwiseRegressionTask ( TerraTorchTask ): \"\"\"Pixelwise Regression Task that accepts models from a range of sources. This class is analog in functionality to PixelwiseRegressionTask defined by torchgeo. However, it has some important differences: - Accepts the specification of a model factory - Logs metrics per class - Does not have any callbacks by default (TorchGeo tasks do early stopping by default) - Allows the setting of optimizers in the constructor - Allows to evaluate on multiple test dataloaders\"\"\" def __init__ ( self , model_args : dict , model_factory : str | None = None , model : torch . nn . Module | None = None , loss : str = \"mse\" , aux_heads : list [ AuxiliaryHead ] | None = None , aux_loss : dict [ str , float ] | None = None , class_weights : list [ float ] | None = None , ignore_index : int | None = None , lr : float = 0.001 , # the following are optional so CLI doesnt need to pass them optimizer : str | None = None , optimizer_hparams : dict | None = None , scheduler : str | None = None , scheduler_hparams : dict | None = None , # freeze_backbone : bool = False , # noqa: FBT001, FBT002 freeze_decoder : bool = False , # noqa: FBT001, FBT002 freeze_head : bool = False , # noqa: FBT001, FBT002 plot_on_val : bool | int = 10 , tiled_inference_parameters : TiledInferenceParameters | None = None , test_dataloaders_names : list [ str ] | None = None , lr_overrides : dict [ str , float ] | None = None , ) -> None : \"\"\"Constructor Args: model_args (Dict): Arguments passed to the model factory. model_factory (str, optional): Name of ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model (torch.nn.Module, optional): Custom model. loss (str, optional): Loss to be used. Currently, supports 'mse', 'rmse', 'mae' or 'huber' loss. Defaults to \"mse\". aux_loss (dict[str, float] | None, optional): Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights (list[float] | None, optional): List of class weights to be applied to the loss. Defaults to None. ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None. lr (float, optional): Learning rate to be used. Defaults to 0.001. optimizer (str | None, optional): Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False. freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False. freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False. plot_on_val (bool | int, optional): Whether to plot visualizations on validation. If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs. tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters used to determine if inference is done on the whole image or through tiling. test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. \"\"\" self . tiled_inference_parameters = tiled_inference_parameters self . aux_loss = aux_loss self . aux_heads = aux_heads if model is not None and model_factory is not None : logger . warning ( \"A model_factory and a model was provided. The model_factory is ignored.\" ) if model is None and model_factory is None : raise ValueError ( \"A model_factory or a model (torch.nn.Module) must be provided.\" ) if model_factory and model is None : self . model_factory = MODEL_FACTORY_REGISTRY . build ( model_factory ) super () . __init__ ( task = \"regression\" ) if model : # Custom_model self . model = model self . train_loss_handler = LossHandler ( self . train_metrics . prefix ) self . test_loss_handler : list [ LossHandler ] = [] for metrics in self . test_metrics : self . test_loss_handler . append ( LossHandler ( metrics . prefix )) self . val_loss_handler = LossHandler ( self . val_metrics . prefix ) self . monitor = f \" { self . val_metrics . prefix } loss\" self . plot_on_val = int ( plot_on_val ) def configure_losses ( self ) -> None : \"\"\"Initialize the loss criterion. Raises: ValueError: If *loss* is invalid. \"\"\" loss : str = self . hparams [ \"loss\" ] . lower () if loss == \"mse\" : self . criterion : nn . Module = IgnoreIndexLossWrapper ( nn . MSELoss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ] ) elif loss == \"mae\" : self . criterion = IgnoreIndexLossWrapper ( nn . L1Loss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ]) elif loss == \"rmse\" : # IMPORTANT! Root is done only after ignore index! Otherwise the mean taken is incorrect self . criterion = RootLossWrapper ( IgnoreIndexLossWrapper ( nn . MSELoss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ]), reduction = None ) elif loss == \"huber\" : self . criterion = IgnoreIndexLossWrapper ( nn . HuberLoss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ]) else : exception_message = f \"Loss type ' { loss } ' is not valid. Currently, supports 'mse', 'rmse' or 'mae' loss.\" raise ValueError ( exception_message ) def configure_metrics ( self ) -> None : \"\"\"Initialize the performance metrics.\"\"\" def instantiate_metrics (): return { \"RMSE\" : MeanSquaredError ( squared = False ), \"MSE\" : MeanSquaredError ( squared = True ), \"MAE\" : MeanAbsoluteError (), } def wrap_metrics_with_ignore_index ( metrics ): return { name : IgnoreIndexMetricWrapper ( metric , ignore_index = self . hparams [ \"ignore_index\" ]) for name , metric in metrics . items () } self . train_metrics = MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = \"train/\" ) self . val_metrics = MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = \"val/\" ) if self . hparams [ \"test_dataloaders_names\" ] is not None : self . test_metrics = nn . ModuleList ( [ MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = f \"test/ { dl_name } /\" ) for dl_name in self . hparams [ \"test_dataloaders_names\" ] ] ) else : self . test_metrics = nn . ModuleList ( [ MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = \"test/\" )] ) def training_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the train loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . train_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . train_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = x . shape [ 0 ]) y_hat = model_output . output self . train_metrics . update ( y_hat , y ) return loss [ \"loss\" ] def validation_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the validation loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . val_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . val_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = y . shape [ 0 ]) y_hat = model_output . output self . val_metrics . update ( y_hat , y ) if self . _do_plot_samples ( batch_idx ): try : datamodule = self . trainer . datamodule batch [ \"prediction\" ] = y_hat if isinstance ( batch [ \"image\" ], dict ): # Multimodal input batch [ \"image\" ] = batch [ \"image\" ][ self . trainer . datamodule . rgb_modality ] for key in [ \"image\" , \"mask\" , \"prediction\" ]: batch [ key ] = batch [ key ] . cpu () sample = unbind_samples ( batch )[ 0 ] fig = datamodule . val_dataset . plot ( sample ) if fig : summary_writer = self . logger . experiment if hasattr ( summary_writer , \"add_figure\" ): summary_writer . add_figure ( f \"image/ { batch_idx } \" , fig , global_step = self . global_step ) elif hasattr ( summary_writer , \"log_figure\" ): summary_writer . log_figure ( self . logger . run_id , fig , f \"epoch_ { self . current_epoch } _ { batch_idx } .png\" ) except ValueError : pass finally : plt . close () def test_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the test loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) if dataloader_idx >= len ( self . test_loss_handler ): msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\" raise ValueError ( msg ) loss = self . test_loss_handler [ dataloader_idx ] . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . test_loss_handler [ dataloader_idx ] . log_loss ( partial ( self . log , add_dataloader_idx = False ), # We don't need the dataloader idx as prefixes are different loss_dict = loss , batch_size = x . shape [ 0 ], ) y_hat = model_output . output self . test_metrics [ dataloader_idx ] . update ( y_hat , y ) def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the predicted class probabilities. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. Returns: Output predicted probabilities. \"\"\" x = batch [ \"image\" ] file_names = batch [ \"filename\" ] if \"filename\" in batch else None other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } def model_forward ( x , ** kwargs ): return self ( x ) . output if self . tiled_inference_parameters : # TODO: tiled inference does not work with additional input data (**rest) y_hat : Tensor = tiled_inference ( model_forward , x , 1 , self . tiled_inference_parameters , ** rest ) else : y_hat : Tensor = self ( x , ** rest ) . output return y_hat , file_names __init__ ( model_args , model_factory = None , model = None , loss = 'mse' , aux_heads = None , aux_loss = None , class_weights = None , ignore_index = None , lr = 0.001 , optimizer = None , optimizer_hparams = None , scheduler = None , scheduler_hparams = None , freeze_backbone = False , freeze_decoder = False , freeze_head = False , plot_on_val = 10 , tiled_inference_parameters = None , test_dataloaders_names = None , lr_overrides = None ) # Constructor Parameters: model_args ( Dict ) \u2013 Arguments passed to the model factory. model_factory ( str , default: None ) \u2013 Name of ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model ( Module , default: None ) \u2013 Custom model. loss ( str , default: 'mse' ) \u2013 Loss to be used. Currently, supports 'mse', 'rmse', 'mae' or 'huber' loss. Defaults to \"mse\". aux_loss ( dict [ str , float ] | None , default: None ) \u2013 Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights ( list [ float ] | None , default: None ) \u2013 List of class weights to be applied to the loss. Defaults to None. ignore_index ( int | None , default: None ) \u2013 Label to ignore in the loss computation. Defaults to None. lr ( float , default: 0.001 ) \u2013 Learning rate to be used. Defaults to 0.001. optimizer ( str | None , default: None ) \u2013 Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams ( dict | None , default: None ) \u2013 Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler ( str , default: None ) \u2013 Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams ( dict | None , default: None ) \u2013 Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone ( bool , default: False ) \u2013 Whether to freeze the backbone. Defaults to False. freeze_decoder ( bool , default: False ) \u2013 Whether to freeze the decoder. Defaults to False. freeze_head ( bool , default: False ) \u2013 Whether to freeze the segmentation head. Defaults to False. plot_on_val ( bool | int , default: 10 ) \u2013 Whether to plot visualizations on validation. If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs. tiled_inference_parameters ( TiledInferenceParameters | None , default: None ) \u2013 Inference parameters used to determine if inference is done on the whole image or through tiling. test_dataloaders_names ( list [ str ] | None , default: None ) \u2013 Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides ( dict [ str , float ] | None , default: None ) \u2013 Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. Source code in terratorch/tasks/regression_tasks.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def __init__ ( self , model_args : dict , model_factory : str | None = None , model : torch . nn . Module | None = None , loss : str = \"mse\" , aux_heads : list [ AuxiliaryHead ] | None = None , aux_loss : dict [ str , float ] | None = None , class_weights : list [ float ] | None = None , ignore_index : int | None = None , lr : float = 0.001 , # the following are optional so CLI doesnt need to pass them optimizer : str | None = None , optimizer_hparams : dict | None = None , scheduler : str | None = None , scheduler_hparams : dict | None = None , # freeze_backbone : bool = False , # noqa: FBT001, FBT002 freeze_decoder : bool = False , # noqa: FBT001, FBT002 freeze_head : bool = False , # noqa: FBT001, FBT002 plot_on_val : bool | int = 10 , tiled_inference_parameters : TiledInferenceParameters | None = None , test_dataloaders_names : list [ str ] | None = None , lr_overrides : dict [ str , float ] | None = None , ) -> None : \"\"\"Constructor Args: model_args (Dict): Arguments passed to the model factory. model_factory (str, optional): Name of ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model (torch.nn.Module, optional): Custom model. loss (str, optional): Loss to be used. Currently, supports 'mse', 'rmse', 'mae' or 'huber' loss. Defaults to \"mse\". aux_loss (dict[str, float] | None, optional): Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights (list[float] | None, optional): List of class weights to be applied to the loss. Defaults to None. ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None. lr (float, optional): Learning rate to be used. Defaults to 0.001. optimizer (str | None, optional): Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False. freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False. freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False. plot_on_val (bool | int, optional): Whether to plot visualizations on validation. If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs. tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters used to determine if inference is done on the whole image or through tiling. test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. \"\"\" self . tiled_inference_parameters = tiled_inference_parameters self . aux_loss = aux_loss self . aux_heads = aux_heads if model is not None and model_factory is not None : logger . warning ( \"A model_factory and a model was provided. The model_factory is ignored.\" ) if model is None and model_factory is None : raise ValueError ( \"A model_factory or a model (torch.nn.Module) must be provided.\" ) if model_factory and model is None : self . model_factory = MODEL_FACTORY_REGISTRY . build ( model_factory ) super () . __init__ ( task = \"regression\" ) if model : # Custom_model self . model = model self . train_loss_handler = LossHandler ( self . train_metrics . prefix ) self . test_loss_handler : list [ LossHandler ] = [] for metrics in self . test_metrics : self . test_loss_handler . append ( LossHandler ( metrics . prefix )) self . val_loss_handler = LossHandler ( self . val_metrics . prefix ) self . monitor = f \" { self . val_metrics . prefix } loss\" self . plot_on_val = int ( plot_on_val ) configure_losses () # Initialize the loss criterion. Raises: ValueError \u2013 If loss is invalid. Source code in terratorch/tasks/regression_tasks.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def configure_losses ( self ) -> None : \"\"\"Initialize the loss criterion. Raises: ValueError: If *loss* is invalid. \"\"\" loss : str = self . hparams [ \"loss\" ] . lower () if loss == \"mse\" : self . criterion : nn . Module = IgnoreIndexLossWrapper ( nn . MSELoss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ] ) elif loss == \"mae\" : self . criterion = IgnoreIndexLossWrapper ( nn . L1Loss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ]) elif loss == \"rmse\" : # IMPORTANT! Root is done only after ignore index! Otherwise the mean taken is incorrect self . criterion = RootLossWrapper ( IgnoreIndexLossWrapper ( nn . MSELoss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ]), reduction = None ) elif loss == \"huber\" : self . criterion = IgnoreIndexLossWrapper ( nn . HuberLoss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ]) else : exception_message = f \"Loss type ' { loss } ' is not valid. Currently, supports 'mse', 'rmse' or 'mae' loss.\" raise ValueError ( exception_message ) configure_metrics () # Initialize the performance metrics. Source code in terratorch/tasks/regression_tasks.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def configure_metrics ( self ) -> None : \"\"\"Initialize the performance metrics.\"\"\" def instantiate_metrics (): return { \"RMSE\" : MeanSquaredError ( squared = False ), \"MSE\" : MeanSquaredError ( squared = True ), \"MAE\" : MeanAbsoluteError (), } def wrap_metrics_with_ignore_index ( metrics ): return { name : IgnoreIndexMetricWrapper ( metric , ignore_index = self . hparams [ \"ignore_index\" ]) for name , metric in metrics . items () } self . train_metrics = MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = \"train/\" ) self . val_metrics = MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = \"val/\" ) if self . hparams [ \"test_dataloaders_names\" ] is not None : self . test_metrics = nn . ModuleList ( [ MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = f \"test/ { dl_name } /\" ) for dl_name in self . hparams [ \"test_dataloaders_names\" ] ] ) else : self . test_metrics = nn . ModuleList ( [ MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = \"test/\" )] ) predict_step ( batch , batch_idx , dataloader_idx = 0 ) # Compute the predicted class probabilities. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Returns: Tensor \u2013 Output predicted probabilities. Source code in terratorch/tasks/regression_tasks.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the predicted class probabilities. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. Returns: Output predicted probabilities. \"\"\" x = batch [ \"image\" ] file_names = batch [ \"filename\" ] if \"filename\" in batch else None other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } def model_forward ( x , ** kwargs ): return self ( x ) . output if self . tiled_inference_parameters : # TODO: tiled inference does not work with additional input data (**rest) y_hat : Tensor = tiled_inference ( model_forward , x , 1 , self . tiled_inference_parameters , ** rest ) else : y_hat : Tensor = self ( x , ** rest ) . output return y_hat , file_names test_step ( batch , batch_idx , dataloader_idx = 0 ) # Compute the test loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/regression_tasks.py 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 def test_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the test loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) if dataloader_idx >= len ( self . test_loss_handler ): msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\" raise ValueError ( msg ) loss = self . test_loss_handler [ dataloader_idx ] . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . test_loss_handler [ dataloader_idx ] . log_loss ( partial ( self . log , add_dataloader_idx = False ), # We don't need the dataloader idx as prefixes are different loss_dict = loss , batch_size = x . shape [ 0 ], ) y_hat = model_output . output self . test_metrics [ dataloader_idx ] . update ( y_hat , y ) training_step ( batch , batch_idx , dataloader_idx = 0 ) # Compute the train loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/regression_tasks.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def training_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the train loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . train_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . train_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = x . shape [ 0 ]) y_hat = model_output . output self . train_metrics . update ( y_hat , y ) return loss [ \"loss\" ] validation_step ( batch , batch_idx , dataloader_idx = 0 ) # Compute the validation loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/regression_tasks.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 def validation_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the validation loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . val_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . val_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = y . shape [ 0 ]) y_hat = model_output . output self . val_metrics . update ( y_hat , y ) if self . _do_plot_samples ( batch_idx ): try : datamodule = self . trainer . datamodule batch [ \"prediction\" ] = y_hat if isinstance ( batch [ \"image\" ], dict ): # Multimodal input batch [ \"image\" ] = batch [ \"image\" ][ self . trainer . datamodule . rgb_modality ] for key in [ \"image\" , \"mask\" , \"prediction\" ]: batch [ key ] = batch [ key ] . cpu () sample = unbind_samples ( batch )[ 0 ] fig = datamodule . val_dataset . plot ( sample ) if fig : summary_writer = self . logger . experiment if hasattr ( summary_writer , \"add_figure\" ): summary_writer . add_figure ( f \"image/ { batch_idx } \" , fig , global_step = self . global_step ) elif hasattr ( summary_writer , \"log_figure\" ): summary_writer . log_figure ( self . logger . run_id , fig , f \"epoch_ { self . current_epoch } _ { batch_idx } .png\" ) except ValueError : pass finally : plt . close () terratorch.tasks.classification_tasks.ClassificationTask # Bases: TerraTorchTask Classification Task that accepts models from a range of sources. This class is analog in functionality to the class ClassificationTask defined by torchgeo. However, it has some important differences: - Accepts the specification of a model factory - Logs metrics per class - Does not have any callbacks by default (TorchGeo tasks do early stopping by default) - Allows the setting of optimizers in the constructor - It provides mIoU with both Micro and Macro averaging - Allows to evaluate on multiple test dataloaders .. note:: * 'Micro' averaging suits overall performance evaluation but may not reflect minority class accuracy. * 'Macro' averaging gives equal weight to each class, useful for balanced performance assessment across imbalanced classes. Source code in terratorch/tasks/classification_tasks.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 class ClassificationTask ( TerraTorchTask ): \"\"\"Classification Task that accepts models from a range of sources. This class is analog in functionality to the class ClassificationTask defined by torchgeo. However, it has some important differences: - Accepts the specification of a model factory - Logs metrics per class - Does not have any callbacks by default (TorchGeo tasks do early stopping by default) - Allows the setting of optimizers in the constructor - It provides mIoU with both Micro and Macro averaging - Allows to evaluate on multiple test dataloaders .. note:: * 'Micro' averaging suits overall performance evaluation but may not reflect minority class accuracy. * 'Macro' averaging gives equal weight to each class, useful for balanced performance assessment across imbalanced classes. \"\"\" def __init__ ( self , model_args : dict , model_factory : str | None = None , model : torch . nn . Module | None = None , loss : str = \"ce\" , aux_heads : list [ AuxiliaryHead ] | None = None , aux_loss : dict [ str , float ] | None = None , class_weights : list [ float ] | None = None , ignore_index : int | None = None , lr : float = 0.001 , # the following are optional so CLI doesnt need to pass them optimizer : str | None = None , optimizer_hparams : dict | None = None , scheduler : str | None = None , scheduler_hparams : dict | None = None , # # freeze_backbone : bool = False , # noqa: FBT001, FBT002 freeze_decoder : bool = False , # noqa: FBT002, FBT001 freeze_head : bool = False , # noqa: FBT002, FBT001 class_names : list [ str ] | None = None , test_dataloaders_names : list [ str ] | None = None , lr_overrides : dict [ str , float ] | None = None , ) -> None : \"\"\"Constructor Args: Defaults to None. model_args (Dict): Arguments passed to the model factory. model_factory (str, optional): ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model (torch.nn.Module, optional): Custom model. loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\". aux_loss (dict[str, float] | None, optional): Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss. class_weights (list[float] | None, optional): List of class weights to be applied to the loss. Defaults to None. ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None. lr (float, optional): Learning rate to be used. Defaults to 0.001. optimizer (str | None, optional): Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False. freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False. freeze_head (bool, optional): Whether to freeze the segmentation_head. Defaults to False. class_names (list[str] | None, optional): List of class names passed to metrics for better naming. Defaults to numeric ordering. test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. \"\"\" self . aux_loss = aux_loss self . aux_heads = aux_heads if model is not None and model_factory is not None : logger . warning ( \"A model_factory and a model was provided. The model_factory is ignored.\" ) if model is None and model_factory is None : raise ValueError ( \"A model_factory or a model (torch.nn.Module) must be provided.\" ) if model_factory and model is None : self . model_factory = MODEL_FACTORY_REGISTRY . build ( model_factory ) super () . __init__ ( task = \"classification\" ) if model : # Custom model self . model = model self . train_loss_handler = LossHandler ( self . train_metrics . prefix ) self . test_loss_handler : list [ LossHandler ] = [] for metrics in self . test_metrics : self . test_loss_handler . append ( LossHandler ( metrics . prefix )) self . val_loss_handler = LossHandler ( self . val_metrics . prefix ) self . monitor = f \" { self . val_metrics . prefix } loss\" def configure_losses ( self ) -> None : \"\"\"Initialize the loss criterion. Raises: ValueError: If *loss* is invalid. \"\"\" loss : str = self . hparams [ \"loss\" ] ignore_index = self . hparams [ \"ignore_index\" ] class_weights = ( torch . Tensor ( self . hparams [ \"class_weights\" ]) if self . hparams [ \"class_weights\" ] is not None else None ) if loss == \"ce\" : ignore_value = - 100 if ignore_index is None else ignore_index self . criterion = nn . CrossEntropyLoss ( ignore_index = ignore_value , weight = class_weights ) elif loss == \"bce\" : self . criterion = nn . BCEWithLogitsLoss () elif loss == \"jaccard\" : self . criterion = JaccardLoss ( mode = \"multiclass\" ) elif loss == \"focal\" : self . criterion = FocalLoss ( mode = \"multiclass\" , normalized = True ) else : msg = f \"Loss type ' { loss } ' is not valid.\" raise ValueError ( msg ) def configure_metrics ( self ) -> None : \"\"\"Initialize the performance metrics.\"\"\" num_classes : int = self . hparams [ \"model_args\" ][ \"num_classes\" ] ignore_index : int = self . hparams [ \"ignore_index\" ] class_names = self . hparams [ \"class_names\" ] metrics = MetricCollection ( { \"Overall_Accuracy\" : MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , average = \"micro\" , ), \"Average_Accuracy\" : MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , average = \"macro\" , ), \"Multiclass_Accuracy_Class\" : ClasswiseWrapper ( MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , average = None , ), labels = class_names , ), \"Multiclass_Jaccard_Index\" : MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index ), \"Multiclass_Jaccard_Index_Class\" : ClasswiseWrapper ( MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , average = None ), labels = class_names , ), # why FBetaScore \"Multiclass_F1_Score\" : MulticlassFBetaScore ( num_classes = num_classes , ignore_index = ignore_index , beta = 1.0 , average = \"micro\" , ), } ) self . train_metrics = metrics . clone ( prefix = \"train/\" ) self . val_metrics = metrics . clone ( prefix = \"val/\" ) if self . hparams [ \"test_dataloaders_names\" ] is not None : self . test_metrics = nn . ModuleList ( [ metrics . clone ( prefix = f \"test/ { dl_name } /\" ) for dl_name in self . hparams [ \"test_dataloaders_names\" ]] ) else : self . test_metrics = nn . ModuleList ([ metrics . clone ( prefix = \"test/\" )]) def training_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the train loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"label\" ] other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . train_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . train_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = x . shape [ 0 ]) y_hat_hard = to_class_prediction ( model_output ) self . train_metrics . update ( y_hat_hard , y ) return loss [ \"loss\" ] def validation_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the validation loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"label\" ] other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . val_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . val_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = x . shape [ 0 ]) y_hat_hard = to_class_prediction ( model_output ) self . val_metrics . update ( y_hat_hard , y ) def test_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the test loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"label\" ] other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) if dataloader_idx >= len ( self . test_loss_handler ): msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\" raise ValueError ( msg ) loss = self . test_loss_handler [ dataloader_idx ] . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . test_loss_handler [ dataloader_idx ] . log_loss ( partial ( self . log , add_dataloader_idx = False ), # We don't need the dataloader idx as prefixes are different loss_dict = loss , batch_size = x . shape [ 0 ], ) y_hat_hard = to_class_prediction ( model_output ) self . test_metrics [ dataloader_idx ] . update ( y_hat_hard , y ) def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the predicted class probabilities. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. Returns: Output predicted probabilities. \"\"\" x = batch [ \"image\" ] file_names = batch [ \"filename\" ] if \"filename\" in batch else None other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) y_hat = self ( x ) . output y_hat = y_hat . argmax ( dim = 1 ) return y_hat , file_names __init__ ( model_args , model_factory = None , model = None , loss = 'ce' , aux_heads = None , aux_loss = None , class_weights = None , ignore_index = None , lr = 0.001 , optimizer = None , optimizer_hparams = None , scheduler = None , scheduler_hparams = None , freeze_backbone = False , freeze_decoder = False , freeze_head = False , class_names = None , test_dataloaders_names = None , lr_overrides = None ) # Constructor Parameters: model_args ( Dict ) \u2013 Arguments passed to the model factory. model_factory ( str , default: None ) \u2013 ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model ( Module , default: None ) \u2013 Custom model. loss ( str , default: 'ce' ) \u2013 Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\". aux_loss ( dict [ str , float ] | None , default: None ) \u2013 Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights ( Union [ list [ float ], None] , default: None ) \u2013 List of class weights to be applied to the loss. class_weights ( list [ float ] | None , default: None ) \u2013 List of class weights to be applied to the loss. Defaults to None. ignore_index ( int | None , default: None ) \u2013 Label to ignore in the loss computation. Defaults to None. lr ( float , default: 0.001 ) \u2013 Learning rate to be used. Defaults to 0.001. optimizer ( str | None , default: None ) \u2013 Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams ( dict | None , default: None ) \u2013 Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler ( str , default: None ) \u2013 Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams ( dict | None , default: None ) \u2013 Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone ( bool , default: False ) \u2013 Whether to freeze the backbone. Defaults to False. freeze_decoder ( bool , default: False ) \u2013 Whether to freeze the decoder. Defaults to False. freeze_head ( bool , default: False ) \u2013 Whether to freeze the segmentation_head. Defaults to False. class_names ( list [ str ] | None , default: None ) \u2013 List of class names passed to metrics for better naming. Defaults to numeric ordering. test_dataloaders_names ( list [ str ] | None , default: None ) \u2013 Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides ( dict [ str , float ] | None , default: None ) \u2013 Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. Source code in terratorch/tasks/classification_tasks.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def __init__ ( self , model_args : dict , model_factory : str | None = None , model : torch . nn . Module | None = None , loss : str = \"ce\" , aux_heads : list [ AuxiliaryHead ] | None = None , aux_loss : dict [ str , float ] | None = None , class_weights : list [ float ] | None = None , ignore_index : int | None = None , lr : float = 0.001 , # the following are optional so CLI doesnt need to pass them optimizer : str | None = None , optimizer_hparams : dict | None = None , scheduler : str | None = None , scheduler_hparams : dict | None = None , # # freeze_backbone : bool = False , # noqa: FBT001, FBT002 freeze_decoder : bool = False , # noqa: FBT002, FBT001 freeze_head : bool = False , # noqa: FBT002, FBT001 class_names : list [ str ] | None = None , test_dataloaders_names : list [ str ] | None = None , lr_overrides : dict [ str , float ] | None = None , ) -> None : \"\"\"Constructor Args: Defaults to None. model_args (Dict): Arguments passed to the model factory. model_factory (str, optional): ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model (torch.nn.Module, optional): Custom model. loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\". aux_loss (dict[str, float] | None, optional): Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss. class_weights (list[float] | None, optional): List of class weights to be applied to the loss. Defaults to None. ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None. lr (float, optional): Learning rate to be used. Defaults to 0.001. optimizer (str | None, optional): Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False. freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False. freeze_head (bool, optional): Whether to freeze the segmentation_head. Defaults to False. class_names (list[str] | None, optional): List of class names passed to metrics for better naming. Defaults to numeric ordering. test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. \"\"\" self . aux_loss = aux_loss self . aux_heads = aux_heads if model is not None and model_factory is not None : logger . warning ( \"A model_factory and a model was provided. The model_factory is ignored.\" ) if model is None and model_factory is None : raise ValueError ( \"A model_factory or a model (torch.nn.Module) must be provided.\" ) if model_factory and model is None : self . model_factory = MODEL_FACTORY_REGISTRY . build ( model_factory ) super () . __init__ ( task = \"classification\" ) if model : # Custom model self . model = model self . train_loss_handler = LossHandler ( self . train_metrics . prefix ) self . test_loss_handler : list [ LossHandler ] = [] for metrics in self . test_metrics : self . test_loss_handler . append ( LossHandler ( metrics . prefix )) self . val_loss_handler = LossHandler ( self . val_metrics . prefix ) self . monitor = f \" { self . val_metrics . prefix } loss\" configure_losses () # Initialize the loss criterion. Raises: ValueError \u2013 If loss is invalid. Source code in terratorch/tasks/classification_tasks.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def configure_losses ( self ) -> None : \"\"\"Initialize the loss criterion. Raises: ValueError: If *loss* is invalid. \"\"\" loss : str = self . hparams [ \"loss\" ] ignore_index = self . hparams [ \"ignore_index\" ] class_weights = ( torch . Tensor ( self . hparams [ \"class_weights\" ]) if self . hparams [ \"class_weights\" ] is not None else None ) if loss == \"ce\" : ignore_value = - 100 if ignore_index is None else ignore_index self . criterion = nn . CrossEntropyLoss ( ignore_index = ignore_value , weight = class_weights ) elif loss == \"bce\" : self . criterion = nn . BCEWithLogitsLoss () elif loss == \"jaccard\" : self . criterion = JaccardLoss ( mode = \"multiclass\" ) elif loss == \"focal\" : self . criterion = FocalLoss ( mode = \"multiclass\" , normalized = True ) else : msg = f \"Loss type ' { loss } ' is not valid.\" raise ValueError ( msg ) configure_metrics () # Initialize the performance metrics. Source code in terratorch/tasks/classification_tasks.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def configure_metrics ( self ) -> None : \"\"\"Initialize the performance metrics.\"\"\" num_classes : int = self . hparams [ \"model_args\" ][ \"num_classes\" ] ignore_index : int = self . hparams [ \"ignore_index\" ] class_names = self . hparams [ \"class_names\" ] metrics = MetricCollection ( { \"Overall_Accuracy\" : MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , average = \"micro\" , ), \"Average_Accuracy\" : MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , average = \"macro\" , ), \"Multiclass_Accuracy_Class\" : ClasswiseWrapper ( MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , average = None , ), labels = class_names , ), \"Multiclass_Jaccard_Index\" : MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index ), \"Multiclass_Jaccard_Index_Class\" : ClasswiseWrapper ( MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , average = None ), labels = class_names , ), # why FBetaScore \"Multiclass_F1_Score\" : MulticlassFBetaScore ( num_classes = num_classes , ignore_index = ignore_index , beta = 1.0 , average = \"micro\" , ), } ) self . train_metrics = metrics . clone ( prefix = \"train/\" ) self . val_metrics = metrics . clone ( prefix = \"val/\" ) if self . hparams [ \"test_dataloaders_names\" ] is not None : self . test_metrics = nn . ModuleList ( [ metrics . clone ( prefix = f \"test/ { dl_name } /\" ) for dl_name in self . hparams [ \"test_dataloaders_names\" ]] ) else : self . test_metrics = nn . ModuleList ([ metrics . clone ( prefix = \"test/\" )]) predict_step ( batch , batch_idx , dataloader_idx = 0 ) # Compute the predicted class probabilities. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Returns: Tensor \u2013 Output predicted probabilities. Source code in terratorch/tasks/classification_tasks.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the predicted class probabilities. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. Returns: Output predicted probabilities. \"\"\" x = batch [ \"image\" ] file_names = batch [ \"filename\" ] if \"filename\" in batch else None other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) y_hat = self ( x ) . output y_hat = y_hat . argmax ( dim = 1 ) return y_hat , file_names test_step ( batch , batch_idx , dataloader_idx = 0 ) # Compute the test loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/classification_tasks.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def test_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the test loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"label\" ] other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) if dataloader_idx >= len ( self . test_loss_handler ): msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\" raise ValueError ( msg ) loss = self . test_loss_handler [ dataloader_idx ] . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . test_loss_handler [ dataloader_idx ] . log_loss ( partial ( self . log , add_dataloader_idx = False ), # We don't need the dataloader idx as prefixes are different loss_dict = loss , batch_size = x . shape [ 0 ], ) y_hat_hard = to_class_prediction ( model_output ) self . test_metrics [ dataloader_idx ] . update ( y_hat_hard , y ) training_step ( batch , batch_idx , dataloader_idx = 0 ) # Compute the train loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/classification_tasks.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def training_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the train loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"label\" ] other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . train_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . train_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = x . shape [ 0 ]) y_hat_hard = to_class_prediction ( model_output ) self . train_metrics . update ( y_hat_hard , y ) return loss [ \"loss\" ] validation_step ( batch , batch_idx , dataloader_idx = 0 ) # Compute the validation loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/classification_tasks.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def validation_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the validation loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"label\" ] other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . val_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . val_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = x . shape [ 0 ]) y_hat_hard = to_class_prediction ( model_output ) self . val_metrics . update ( y_hat_hard , y )","title":"Tasks"},{"location":"tasks/#tasks","text":"Tasks provide a convenient abstraction over the training of a model for a specific downstream task. They encapsulate the model, optimizer, metrics, loss as well as training, validation and testing steps. The task expects to be passed a model factory, to which the model_args arguments are passed to instantiate the model that will be trained. The models produced by this model factory should output ModelOutput instances and conform to the Model ABC. Tasks are best leveraged using config files, where they are specified in the model section under class_path . You can check out some examples of config files here . Below are the details of the tasks currently implemented in TerraTorch (Pixelwise Regression, Semantic Segmentation and Classification).","title":"Tasks"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask","text":"Bases: TerraTorchTask Semantic Segmentation Task that accepts models from a range of sources. This class is analog in functionality to class SemanticSegmentationTask defined by torchgeo. However, it has some important differences: - Accepts the specification of a model factory - Logs metrics per class - Does not have any callbacks by default (TorchGeo tasks do early stopping by default) - Allows the setting of optimizers in the constructor - Allows to evaluate on multiple test dataloaders Source code in terratorch/tasks/segmentation_tasks.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 class SemanticSegmentationTask ( TerraTorchTask ): \"\"\"Semantic Segmentation Task that accepts models from a range of sources. This class is analog in functionality to class SemanticSegmentationTask defined by torchgeo. However, it has some important differences: - Accepts the specification of a model factory - Logs metrics per class - Does not have any callbacks by default (TorchGeo tasks do early stopping by default) - Allows the setting of optimizers in the constructor - Allows to evaluate on multiple test dataloaders \"\"\" def __init__ ( self , model_args : dict , model_factory : str | None = None , model : torch . nn . Module | None = None , loss : str = \"ce\" , aux_heads : list [ AuxiliaryHead ] | None = None , aux_loss : dict [ str , float ] | None = None , class_weights : list [ float ] | None = None , ignore_index : int | None = None , lr : float = 0.001 , # the following are optional so CLI doesnt need to pass them optimizer : str | None = None , optimizer_hparams : dict | None = None , scheduler : str | None = None , scheduler_hparams : dict | None = None , # freeze_backbone : bool = False , # noqa: FBT001, FBT002 freeze_decoder : bool = False , # noqa: FBT002, FBT001 freeze_head : bool = False , plot_on_val : bool | int = 10 , class_names : list [ str ] | None = None , tiled_inference_parameters : TiledInferenceParameters = None , test_dataloaders_names : list [ str ] | None = None , lr_overrides : dict [ str , float ] | None = None , output_most_probable : bool = True , ) -> None : \"\"\"Constructor Args: Defaults to None. model_args (Dict): Arguments passed to the model factory. model_factory (str, optional): ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model (torch.nn.Module, optional): Custom model. loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\". aux_loss (dict[str, float] | None, optional): Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss. class_weights (list[float] | None, optional): List of class weights to be applied to the loss. Defaults to None. ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None. lr (float, optional): Learning rate to be used. Defaults to 0.001. optimizer (str | None, optional): Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False. freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False. freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False. plot_on_val (bool | int, optional): Whether to plot visualizations on validation. If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs. class_names (list[str] | None, optional): List of class names passed to metrics for better naming. Defaults to numeric ordering. tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters used to determine if inference is done on the whole image or through tiling. test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. output_most_probable (bool): A boolean to define if the output during the inference will be just for the most probable class or if it will include all of them. \"\"\" self . tiled_inference_parameters = tiled_inference_parameters self . aux_loss = aux_loss self . aux_heads = aux_heads if model is not None and model_factory is not None : logger . warning ( \"A model_factory and a model was provided. The model_factory is ignored.\" ) if model is None and model_factory is None : raise ValueError ( \"A model_factory or a model (torch.nn.Module) must be provided.\" ) if model_factory and model is None : self . model_factory = MODEL_FACTORY_REGISTRY . build ( model_factory ) super () . __init__ ( task = \"segmentation\" ) if model is not None : # Custom model self . model = model self . train_loss_handler = LossHandler ( self . train_metrics . prefix ) self . test_loss_handler : list [ LossHandler ] = [] for metrics in self . test_metrics : self . test_loss_handler . append ( LossHandler ( metrics . prefix )) self . val_loss_handler = LossHandler ( self . val_metrics . prefix ) self . monitor = f \" { self . val_metrics . prefix } loss\" self . plot_on_val = int ( plot_on_val ) self . output_most_probable = output_most_probable if output_most_probable : self . select_classes = lambda y : y . argmax ( dim = 1 ) else : self . select_classes = lambda y : y def configure_losses ( self ) -> None : \"\"\"Initialize the loss criterion. Raises: ValueError: If *loss* is invalid. \"\"\" loss : str = self . hparams [ \"loss\" ] ignore_index = self . hparams [ \"ignore_index\" ] class_weights = ( torch . Tensor ( self . hparams [ \"class_weights\" ]) if self . hparams [ \"class_weights\" ] is not None else None ) if loss == \"ce\" : ignore_value = - 100 if ignore_index is None else ignore_index self . criterion = nn . CrossEntropyLoss ( ignore_index = ignore_value , weight = class_weights ) elif loss == \"jaccard\" : if ignore_index is not None : exception_message = ( f \"Jaccard loss does not support ignore_index, but found non-None value of { ignore_index } .\" ) raise RuntimeError ( exception_message ) self . criterion = smp . losses . JaccardLoss ( mode = \"multiclass\" ) elif loss == \"focal\" : self . criterion = smp . losses . FocalLoss ( \"multiclass\" , ignore_index = ignore_index , normalized = True ) elif loss == \"dice\" : self . criterion = smp . losses . DiceLoss ( \"multiclass\" , ignore_index = ignore_index ) else : exception_message = ( f \"Loss type ' { loss } ' is not valid. Currently, supports 'ce', 'jaccard', 'dice' or 'focal' loss.\" ) raise ValueError ( exception_message ) def configure_metrics ( self ) -> None : \"\"\"Initialize the performance metrics.\"\"\" num_classes : int = self . hparams [ \"model_args\" ][ \"num_classes\" ] ignore_index : int = self . hparams [ \"ignore_index\" ] class_names = self . hparams [ \"class_names\" ] metrics = MetricCollection ( { \"Multiclass_Accuracy\" : MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , multidim_average = \"global\" , average = \"micro\" , ), \"Multiclass_Accuracy_Class\" : ClasswiseWrapper ( MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , multidim_average = \"global\" , average = None , ), labels = class_names , ), \"Multiclass_Jaccard_Index_Micro\" : MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , average = \"micro\" ), \"Multiclass_Jaccard_Index\" : MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , ), \"Multiclass_Jaccard_Index_Class\" : ClasswiseWrapper ( MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , average = None ), labels = class_names , ), \"Multiclass_F1_Score\" : MulticlassF1Score ( num_classes = num_classes , ignore_index = ignore_index , multidim_average = \"global\" , average = \"micro\" , ), } ) self . train_metrics = metrics . clone ( prefix = \"train/\" ) self . val_metrics = metrics . clone ( prefix = \"val/\" ) if self . hparams [ \"test_dataloaders_names\" ] is not None : self . test_metrics = nn . ModuleList ( [ metrics . clone ( prefix = f \"test/ { dl_name } /\" ) for dl_name in self . hparams [ \"test_dataloaders_names\" ]] ) else : self . test_metrics = nn . ModuleList ([ metrics . clone ( prefix = \"test/\" )]) def training_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the train loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . train_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . train_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = y . shape [ 0 ]) y_hat_hard = to_segmentation_prediction ( model_output ) self . train_metrics . update ( y_hat_hard , y ) return loss [ \"loss\" ] def test_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the test loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) if dataloader_idx >= len ( self . test_loss_handler ): msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\" raise ValueError ( msg ) loss = self . test_loss_handler [ dataloader_idx ] . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . test_loss_handler [ dataloader_idx ] . log_loss ( partial ( self . log , add_dataloader_idx = False ), # We don't need the dataloader idx as prefixes are different loss_dict = loss , batch_size = y . shape [ 0 ], ) y_hat_hard = to_segmentation_prediction ( model_output ) self . test_metrics [ dataloader_idx ] . update ( y_hat_hard , y ) def validation_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the validation loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . val_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . val_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = y . shape [ 0 ]) y_hat_hard = to_segmentation_prediction ( model_output ) self . val_metrics . update ( y_hat_hard , y ) if self . _do_plot_samples ( batch_idx ): try : datamodule = self . trainer . datamodule batch [ \"prediction\" ] = y_hat_hard if isinstance ( batch [ \"image\" ], dict ): if hasattr ( datamodule , \"rgb_modality\" ): # Generic multimodal dataset batch [ \"image\" ] = batch [ \"image\" ][ datamodule . rgb_modality ] else : # Multimodal dataset. Assuming first item to be the modality to visualize. batch [ \"image\" ] = batch [ \"image\" ][ list ( batch [ \"image\" ] . keys ())[ 0 ]] for key in [ \"image\" , \"mask\" , \"prediction\" ]: batch [ key ] = batch [ key ] . cpu () sample = unbind_samples ( batch )[ 0 ] fig = datamodule . val_dataset . plot ( sample ) if fig : summary_writer = self . logger . experiment if hasattr ( summary_writer , \"add_figure\" ): summary_writer . add_figure ( f \"image/ { batch_idx } \" , fig , global_step = self . global_step ) elif hasattr ( summary_writer , \"log_figure\" ): summary_writer . log_figure ( self . logger . run_id , fig , f \"epoch_ { self . current_epoch } _ { batch_idx } .png\" ) except ValueError : pass finally : plt . close () def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the predicted class probabilities. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. Returns: Output predicted probabilities. \"\"\" x = batch [ \"image\" ] file_names = batch [ \"filename\" ] if \"filename\" in batch else None other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } def model_forward ( x , ** kwargs ): return self ( x , ** kwargs ) . output if self . tiled_inference_parameters : y_hat : Tensor = tiled_inference ( model_forward , x , self . hparams [ \"model_args\" ][ \"num_classes\" ], self . tiled_inference_parameters , ** rest , ) else : y_hat : Tensor = self ( x , ** rest ) . output y_hat = self . select_classes ( y_hat ) return y_hat , file_names","title":"SemanticSegmentationTask"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.__init__","text":"Constructor Parameters: model_args ( Dict ) \u2013 Arguments passed to the model factory. model_factory ( str , default: None ) \u2013 ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model ( Module , default: None ) \u2013 Custom model. loss ( str , default: 'ce' ) \u2013 Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\". aux_loss ( dict [ str , float ] | None , default: None ) \u2013 Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights ( Union [ list [ float ], None] , default: None ) \u2013 List of class weights to be applied to the loss. class_weights ( list [ float ] | None , default: None ) \u2013 List of class weights to be applied to the loss. Defaults to None. ignore_index ( int | None , default: None ) \u2013 Label to ignore in the loss computation. Defaults to None. lr ( float , default: 0.001 ) \u2013 Learning rate to be used. Defaults to 0.001. optimizer ( str | None , default: None ) \u2013 Name of optimizer class from torch.optim to be used. optimizer_hparams ( dict | None , default: None ) \u2013 Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler ( str , default: None ) \u2013 Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams ( dict | None , default: None ) \u2013 Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone ( bool , default: False ) \u2013 Whether to freeze the backbone. Defaults to False. freeze_decoder ( bool , default: False ) \u2013 Whether to freeze the decoder. Defaults to False. freeze_head ( bool , default: False ) \u2013 Whether to freeze the segmentation head. Defaults to False. plot_on_val ( bool | int , default: 10 ) \u2013 Whether to plot visualizations on validation. class_names ( list [ str ] | None , default: None ) \u2013 List of class names passed to metrics for better naming. Defaults to numeric ordering. tiled_inference_parameters ( TiledInferenceParameters | None , default: None ) \u2013 Inference parameters used to determine if inference is done on the whole image or through tiling. test_dataloaders_names ( list [ str ] | None , default: None ) \u2013 Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides ( dict [ str , float ] | None , default: None ) \u2013 Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. output_most_probable ( bool , default: True ) \u2013 A boolean to define if the output during the inference will be just for the most probable class or if it will include all of them. Source code in terratorch/tasks/segmentation_tasks.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def __init__ ( self , model_args : dict , model_factory : str | None = None , model : torch . nn . Module | None = None , loss : str = \"ce\" , aux_heads : list [ AuxiliaryHead ] | None = None , aux_loss : dict [ str , float ] | None = None , class_weights : list [ float ] | None = None , ignore_index : int | None = None , lr : float = 0.001 , # the following are optional so CLI doesnt need to pass them optimizer : str | None = None , optimizer_hparams : dict | None = None , scheduler : str | None = None , scheduler_hparams : dict | None = None , # freeze_backbone : bool = False , # noqa: FBT001, FBT002 freeze_decoder : bool = False , # noqa: FBT002, FBT001 freeze_head : bool = False , plot_on_val : bool | int = 10 , class_names : list [ str ] | None = None , tiled_inference_parameters : TiledInferenceParameters = None , test_dataloaders_names : list [ str ] | None = None , lr_overrides : dict [ str , float ] | None = None , output_most_probable : bool = True , ) -> None : \"\"\"Constructor Args: Defaults to None. model_args (Dict): Arguments passed to the model factory. model_factory (str, optional): ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model (torch.nn.Module, optional): Custom model. loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\". aux_loss (dict[str, float] | None, optional): Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss. class_weights (list[float] | None, optional): List of class weights to be applied to the loss. Defaults to None. ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None. lr (float, optional): Learning rate to be used. Defaults to 0.001. optimizer (str | None, optional): Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False. freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False. freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False. plot_on_val (bool | int, optional): Whether to plot visualizations on validation. If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs. class_names (list[str] | None, optional): List of class names passed to metrics for better naming. Defaults to numeric ordering. tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters used to determine if inference is done on the whole image or through tiling. test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. output_most_probable (bool): A boolean to define if the output during the inference will be just for the most probable class or if it will include all of them. \"\"\" self . tiled_inference_parameters = tiled_inference_parameters self . aux_loss = aux_loss self . aux_heads = aux_heads if model is not None and model_factory is not None : logger . warning ( \"A model_factory and a model was provided. The model_factory is ignored.\" ) if model is None and model_factory is None : raise ValueError ( \"A model_factory or a model (torch.nn.Module) must be provided.\" ) if model_factory and model is None : self . model_factory = MODEL_FACTORY_REGISTRY . build ( model_factory ) super () . __init__ ( task = \"segmentation\" ) if model is not None : # Custom model self . model = model self . train_loss_handler = LossHandler ( self . train_metrics . prefix ) self . test_loss_handler : list [ LossHandler ] = [] for metrics in self . test_metrics : self . test_loss_handler . append ( LossHandler ( metrics . prefix )) self . val_loss_handler = LossHandler ( self . val_metrics . prefix ) self . monitor = f \" { self . val_metrics . prefix } loss\" self . plot_on_val = int ( plot_on_val ) self . output_most_probable = output_most_probable if output_most_probable : self . select_classes = lambda y : y . argmax ( dim = 1 ) else : self . select_classes = lambda y : y","title":"__init__"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.configure_losses","text":"Initialize the loss criterion. Raises: ValueError \u2013 If loss is invalid. Source code in terratorch/tasks/segmentation_tasks.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def configure_losses ( self ) -> None : \"\"\"Initialize the loss criterion. Raises: ValueError: If *loss* is invalid. \"\"\" loss : str = self . hparams [ \"loss\" ] ignore_index = self . hparams [ \"ignore_index\" ] class_weights = ( torch . Tensor ( self . hparams [ \"class_weights\" ]) if self . hparams [ \"class_weights\" ] is not None else None ) if loss == \"ce\" : ignore_value = - 100 if ignore_index is None else ignore_index self . criterion = nn . CrossEntropyLoss ( ignore_index = ignore_value , weight = class_weights ) elif loss == \"jaccard\" : if ignore_index is not None : exception_message = ( f \"Jaccard loss does not support ignore_index, but found non-None value of { ignore_index } .\" ) raise RuntimeError ( exception_message ) self . criterion = smp . losses . JaccardLoss ( mode = \"multiclass\" ) elif loss == \"focal\" : self . criterion = smp . losses . FocalLoss ( \"multiclass\" , ignore_index = ignore_index , normalized = True ) elif loss == \"dice\" : self . criterion = smp . losses . DiceLoss ( \"multiclass\" , ignore_index = ignore_index ) else : exception_message = ( f \"Loss type ' { loss } ' is not valid. Currently, supports 'ce', 'jaccard', 'dice' or 'focal' loss.\" ) raise ValueError ( exception_message )","title":"configure_losses"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.configure_metrics","text":"Initialize the performance metrics. Source code in terratorch/tasks/segmentation_tasks.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def configure_metrics ( self ) -> None : \"\"\"Initialize the performance metrics.\"\"\" num_classes : int = self . hparams [ \"model_args\" ][ \"num_classes\" ] ignore_index : int = self . hparams [ \"ignore_index\" ] class_names = self . hparams [ \"class_names\" ] metrics = MetricCollection ( { \"Multiclass_Accuracy\" : MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , multidim_average = \"global\" , average = \"micro\" , ), \"Multiclass_Accuracy_Class\" : ClasswiseWrapper ( MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , multidim_average = \"global\" , average = None , ), labels = class_names , ), \"Multiclass_Jaccard_Index_Micro\" : MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , average = \"micro\" ), \"Multiclass_Jaccard_Index\" : MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , ), \"Multiclass_Jaccard_Index_Class\" : ClasswiseWrapper ( MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , average = None ), labels = class_names , ), \"Multiclass_F1_Score\" : MulticlassF1Score ( num_classes = num_classes , ignore_index = ignore_index , multidim_average = \"global\" , average = \"micro\" , ), } ) self . train_metrics = metrics . clone ( prefix = \"train/\" ) self . val_metrics = metrics . clone ( prefix = \"val/\" ) if self . hparams [ \"test_dataloaders_names\" ] is not None : self . test_metrics = nn . ModuleList ( [ metrics . clone ( prefix = f \"test/ { dl_name } /\" ) for dl_name in self . hparams [ \"test_dataloaders_names\" ]] ) else : self . test_metrics = nn . ModuleList ([ metrics . clone ( prefix = \"test/\" )])","title":"configure_metrics"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.predict_step","text":"Compute the predicted class probabilities. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Returns: Tensor \u2013 Output predicted probabilities. Source code in terratorch/tasks/segmentation_tasks.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the predicted class probabilities. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. Returns: Output predicted probabilities. \"\"\" x = batch [ \"image\" ] file_names = batch [ \"filename\" ] if \"filename\" in batch else None other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } def model_forward ( x , ** kwargs ): return self ( x , ** kwargs ) . output if self . tiled_inference_parameters : y_hat : Tensor = tiled_inference ( model_forward , x , self . hparams [ \"model_args\" ][ \"num_classes\" ], self . tiled_inference_parameters , ** rest , ) else : y_hat : Tensor = self ( x , ** rest ) . output y_hat = self . select_classes ( y_hat ) return y_hat , file_names","title":"predict_step"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.test_step","text":"Compute the test loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/segmentation_tasks.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def test_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the test loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) if dataloader_idx >= len ( self . test_loss_handler ): msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\" raise ValueError ( msg ) loss = self . test_loss_handler [ dataloader_idx ] . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . test_loss_handler [ dataloader_idx ] . log_loss ( partial ( self . log , add_dataloader_idx = False ), # We don't need the dataloader idx as prefixes are different loss_dict = loss , batch_size = y . shape [ 0 ], ) y_hat_hard = to_segmentation_prediction ( model_output ) self . test_metrics [ dataloader_idx ] . update ( y_hat_hard , y )","title":"test_step"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.training_step","text":"Compute the train loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/segmentation_tasks.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 def training_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the train loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . train_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . train_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = y . shape [ 0 ]) y_hat_hard = to_segmentation_prediction ( model_output ) self . train_metrics . update ( y_hat_hard , y ) return loss [ \"loss\" ]","title":"training_step"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.validation_step","text":"Compute the validation loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. Source code in terratorch/tasks/segmentation_tasks.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def validation_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the validation loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . val_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . val_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = y . shape [ 0 ]) y_hat_hard = to_segmentation_prediction ( model_output ) self . val_metrics . update ( y_hat_hard , y ) if self . _do_plot_samples ( batch_idx ): try : datamodule = self . trainer . datamodule batch [ \"prediction\" ] = y_hat_hard if isinstance ( batch [ \"image\" ], dict ): if hasattr ( datamodule , \"rgb_modality\" ): # Generic multimodal dataset batch [ \"image\" ] = batch [ \"image\" ][ datamodule . rgb_modality ] else : # Multimodal dataset. Assuming first item to be the modality to visualize. batch [ \"image\" ] = batch [ \"image\" ][ list ( batch [ \"image\" ] . keys ())[ 0 ]] for key in [ \"image\" , \"mask\" , \"prediction\" ]: batch [ key ] = batch [ key ] . cpu () sample = unbind_samples ( batch )[ 0 ] fig = datamodule . val_dataset . plot ( sample ) if fig : summary_writer = self . logger . experiment if hasattr ( summary_writer , \"add_figure\" ): summary_writer . add_figure ( f \"image/ { batch_idx } \" , fig , global_step = self . global_step ) elif hasattr ( summary_writer , \"log_figure\" ): summary_writer . log_figure ( self . logger . run_id , fig , f \"epoch_ { self . current_epoch } _ { batch_idx } .png\" ) except ValueError : pass finally : plt . close ()","title":"validation_step"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask","text":"Bases: TerraTorchTask Pixelwise Regression Task that accepts models from a range of sources. This class is analog in functionality to PixelwiseRegressionTask defined by torchgeo. However, it has some important differences: - Accepts the specification of a model factory - Logs metrics per class - Does not have any callbacks by default (TorchGeo tasks do early stopping by default) - Allows the setting of optimizers in the constructor - Allows to evaluate on multiple test dataloaders Source code in terratorch/tasks/regression_tasks.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 class PixelwiseRegressionTask ( TerraTorchTask ): \"\"\"Pixelwise Regression Task that accepts models from a range of sources. This class is analog in functionality to PixelwiseRegressionTask defined by torchgeo. However, it has some important differences: - Accepts the specification of a model factory - Logs metrics per class - Does not have any callbacks by default (TorchGeo tasks do early stopping by default) - Allows the setting of optimizers in the constructor - Allows to evaluate on multiple test dataloaders\"\"\" def __init__ ( self , model_args : dict , model_factory : str | None = None , model : torch . nn . Module | None = None , loss : str = \"mse\" , aux_heads : list [ AuxiliaryHead ] | None = None , aux_loss : dict [ str , float ] | None = None , class_weights : list [ float ] | None = None , ignore_index : int | None = None , lr : float = 0.001 , # the following are optional so CLI doesnt need to pass them optimizer : str | None = None , optimizer_hparams : dict | None = None , scheduler : str | None = None , scheduler_hparams : dict | None = None , # freeze_backbone : bool = False , # noqa: FBT001, FBT002 freeze_decoder : bool = False , # noqa: FBT001, FBT002 freeze_head : bool = False , # noqa: FBT001, FBT002 plot_on_val : bool | int = 10 , tiled_inference_parameters : TiledInferenceParameters | None = None , test_dataloaders_names : list [ str ] | None = None , lr_overrides : dict [ str , float ] | None = None , ) -> None : \"\"\"Constructor Args: model_args (Dict): Arguments passed to the model factory. model_factory (str, optional): Name of ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model (torch.nn.Module, optional): Custom model. loss (str, optional): Loss to be used. Currently, supports 'mse', 'rmse', 'mae' or 'huber' loss. Defaults to \"mse\". aux_loss (dict[str, float] | None, optional): Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights (list[float] | None, optional): List of class weights to be applied to the loss. Defaults to None. ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None. lr (float, optional): Learning rate to be used. Defaults to 0.001. optimizer (str | None, optional): Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False. freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False. freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False. plot_on_val (bool | int, optional): Whether to plot visualizations on validation. If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs. tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters used to determine if inference is done on the whole image or through tiling. test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. \"\"\" self . tiled_inference_parameters = tiled_inference_parameters self . aux_loss = aux_loss self . aux_heads = aux_heads if model is not None and model_factory is not None : logger . warning ( \"A model_factory and a model was provided. The model_factory is ignored.\" ) if model is None and model_factory is None : raise ValueError ( \"A model_factory or a model (torch.nn.Module) must be provided.\" ) if model_factory and model is None : self . model_factory = MODEL_FACTORY_REGISTRY . build ( model_factory ) super () . __init__ ( task = \"regression\" ) if model : # Custom_model self . model = model self . train_loss_handler = LossHandler ( self . train_metrics . prefix ) self . test_loss_handler : list [ LossHandler ] = [] for metrics in self . test_metrics : self . test_loss_handler . append ( LossHandler ( metrics . prefix )) self . val_loss_handler = LossHandler ( self . val_metrics . prefix ) self . monitor = f \" { self . val_metrics . prefix } loss\" self . plot_on_val = int ( plot_on_val ) def configure_losses ( self ) -> None : \"\"\"Initialize the loss criterion. Raises: ValueError: If *loss* is invalid. \"\"\" loss : str = self . hparams [ \"loss\" ] . lower () if loss == \"mse\" : self . criterion : nn . Module = IgnoreIndexLossWrapper ( nn . MSELoss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ] ) elif loss == \"mae\" : self . criterion = IgnoreIndexLossWrapper ( nn . L1Loss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ]) elif loss == \"rmse\" : # IMPORTANT! Root is done only after ignore index! Otherwise the mean taken is incorrect self . criterion = RootLossWrapper ( IgnoreIndexLossWrapper ( nn . MSELoss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ]), reduction = None ) elif loss == \"huber\" : self . criterion = IgnoreIndexLossWrapper ( nn . HuberLoss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ]) else : exception_message = f \"Loss type ' { loss } ' is not valid. Currently, supports 'mse', 'rmse' or 'mae' loss.\" raise ValueError ( exception_message ) def configure_metrics ( self ) -> None : \"\"\"Initialize the performance metrics.\"\"\" def instantiate_metrics (): return { \"RMSE\" : MeanSquaredError ( squared = False ), \"MSE\" : MeanSquaredError ( squared = True ), \"MAE\" : MeanAbsoluteError (), } def wrap_metrics_with_ignore_index ( metrics ): return { name : IgnoreIndexMetricWrapper ( metric , ignore_index = self . hparams [ \"ignore_index\" ]) for name , metric in metrics . items () } self . train_metrics = MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = \"train/\" ) self . val_metrics = MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = \"val/\" ) if self . hparams [ \"test_dataloaders_names\" ] is not None : self . test_metrics = nn . ModuleList ( [ MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = f \"test/ { dl_name } /\" ) for dl_name in self . hparams [ \"test_dataloaders_names\" ] ] ) else : self . test_metrics = nn . ModuleList ( [ MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = \"test/\" )] ) def training_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the train loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . train_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . train_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = x . shape [ 0 ]) y_hat = model_output . output self . train_metrics . update ( y_hat , y ) return loss [ \"loss\" ] def validation_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the validation loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . val_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . val_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = y . shape [ 0 ]) y_hat = model_output . output self . val_metrics . update ( y_hat , y ) if self . _do_plot_samples ( batch_idx ): try : datamodule = self . trainer . datamodule batch [ \"prediction\" ] = y_hat if isinstance ( batch [ \"image\" ], dict ): # Multimodal input batch [ \"image\" ] = batch [ \"image\" ][ self . trainer . datamodule . rgb_modality ] for key in [ \"image\" , \"mask\" , \"prediction\" ]: batch [ key ] = batch [ key ] . cpu () sample = unbind_samples ( batch )[ 0 ] fig = datamodule . val_dataset . plot ( sample ) if fig : summary_writer = self . logger . experiment if hasattr ( summary_writer , \"add_figure\" ): summary_writer . add_figure ( f \"image/ { batch_idx } \" , fig , global_step = self . global_step ) elif hasattr ( summary_writer , \"log_figure\" ): summary_writer . log_figure ( self . logger . run_id , fig , f \"epoch_ { self . current_epoch } _ { batch_idx } .png\" ) except ValueError : pass finally : plt . close () def test_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the test loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) if dataloader_idx >= len ( self . test_loss_handler ): msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\" raise ValueError ( msg ) loss = self . test_loss_handler [ dataloader_idx ] . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . test_loss_handler [ dataloader_idx ] . log_loss ( partial ( self . log , add_dataloader_idx = False ), # We don't need the dataloader idx as prefixes are different loss_dict = loss , batch_size = x . shape [ 0 ], ) y_hat = model_output . output self . test_metrics [ dataloader_idx ] . update ( y_hat , y ) def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the predicted class probabilities. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. Returns: Output predicted probabilities. \"\"\" x = batch [ \"image\" ] file_names = batch [ \"filename\" ] if \"filename\" in batch else None other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } def model_forward ( x , ** kwargs ): return self ( x ) . output if self . tiled_inference_parameters : # TODO: tiled inference does not work with additional input data (**rest) y_hat : Tensor = tiled_inference ( model_forward , x , 1 , self . tiled_inference_parameters , ** rest ) else : y_hat : Tensor = self ( x , ** rest ) . output return y_hat , file_names","title":"PixelwiseRegressionTask"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.__init__","text":"Constructor Parameters: model_args ( Dict ) \u2013 Arguments passed to the model factory. model_factory ( str , default: None ) \u2013 Name of ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model ( Module , default: None ) \u2013 Custom model. loss ( str , default: 'mse' ) \u2013 Loss to be used. Currently, supports 'mse', 'rmse', 'mae' or 'huber' loss. Defaults to \"mse\". aux_loss ( dict [ str , float ] | None , default: None ) \u2013 Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights ( list [ float ] | None , default: None ) \u2013 List of class weights to be applied to the loss. Defaults to None. ignore_index ( int | None , default: None ) \u2013 Label to ignore in the loss computation. Defaults to None. lr ( float , default: 0.001 ) \u2013 Learning rate to be used. Defaults to 0.001. optimizer ( str | None , default: None ) \u2013 Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams ( dict | None , default: None ) \u2013 Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler ( str , default: None ) \u2013 Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams ( dict | None , default: None ) \u2013 Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone ( bool , default: False ) \u2013 Whether to freeze the backbone. Defaults to False. freeze_decoder ( bool , default: False ) \u2013 Whether to freeze the decoder. Defaults to False. freeze_head ( bool , default: False ) \u2013 Whether to freeze the segmentation head. Defaults to False. plot_on_val ( bool | int , default: 10 ) \u2013 Whether to plot visualizations on validation. If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs. tiled_inference_parameters ( TiledInferenceParameters | None , default: None ) \u2013 Inference parameters used to determine if inference is done on the whole image or through tiling. test_dataloaders_names ( list [ str ] | None , default: None ) \u2013 Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides ( dict [ str , float ] | None , default: None ) \u2013 Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. Source code in terratorch/tasks/regression_tasks.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def __init__ ( self , model_args : dict , model_factory : str | None = None , model : torch . nn . Module | None = None , loss : str = \"mse\" , aux_heads : list [ AuxiliaryHead ] | None = None , aux_loss : dict [ str , float ] | None = None , class_weights : list [ float ] | None = None , ignore_index : int | None = None , lr : float = 0.001 , # the following are optional so CLI doesnt need to pass them optimizer : str | None = None , optimizer_hparams : dict | None = None , scheduler : str | None = None , scheduler_hparams : dict | None = None , # freeze_backbone : bool = False , # noqa: FBT001, FBT002 freeze_decoder : bool = False , # noqa: FBT001, FBT002 freeze_head : bool = False , # noqa: FBT001, FBT002 plot_on_val : bool | int = 10 , tiled_inference_parameters : TiledInferenceParameters | None = None , test_dataloaders_names : list [ str ] | None = None , lr_overrides : dict [ str , float ] | None = None , ) -> None : \"\"\"Constructor Args: model_args (Dict): Arguments passed to the model factory. model_factory (str, optional): Name of ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model (torch.nn.Module, optional): Custom model. loss (str, optional): Loss to be used. Currently, supports 'mse', 'rmse', 'mae' or 'huber' loss. Defaults to \"mse\". aux_loss (dict[str, float] | None, optional): Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights (list[float] | None, optional): List of class weights to be applied to the loss. Defaults to None. ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None. lr (float, optional): Learning rate to be used. Defaults to 0.001. optimizer (str | None, optional): Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False. freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False. freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False. plot_on_val (bool | int, optional): Whether to plot visualizations on validation. If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs. tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters used to determine if inference is done on the whole image or through tiling. test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. \"\"\" self . tiled_inference_parameters = tiled_inference_parameters self . aux_loss = aux_loss self . aux_heads = aux_heads if model is not None and model_factory is not None : logger . warning ( \"A model_factory and a model was provided. The model_factory is ignored.\" ) if model is None and model_factory is None : raise ValueError ( \"A model_factory or a model (torch.nn.Module) must be provided.\" ) if model_factory and model is None : self . model_factory = MODEL_FACTORY_REGISTRY . build ( model_factory ) super () . __init__ ( task = \"regression\" ) if model : # Custom_model self . model = model self . train_loss_handler = LossHandler ( self . train_metrics . prefix ) self . test_loss_handler : list [ LossHandler ] = [] for metrics in self . test_metrics : self . test_loss_handler . append ( LossHandler ( metrics . prefix )) self . val_loss_handler = LossHandler ( self . val_metrics . prefix ) self . monitor = f \" { self . val_metrics . prefix } loss\" self . plot_on_val = int ( plot_on_val )","title":"__init__"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.configure_losses","text":"Initialize the loss criterion. Raises: ValueError \u2013 If loss is invalid. Source code in terratorch/tasks/regression_tasks.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def configure_losses ( self ) -> None : \"\"\"Initialize the loss criterion. Raises: ValueError: If *loss* is invalid. \"\"\" loss : str = self . hparams [ \"loss\" ] . lower () if loss == \"mse\" : self . criterion : nn . Module = IgnoreIndexLossWrapper ( nn . MSELoss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ] ) elif loss == \"mae\" : self . criterion = IgnoreIndexLossWrapper ( nn . L1Loss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ]) elif loss == \"rmse\" : # IMPORTANT! Root is done only after ignore index! Otherwise the mean taken is incorrect self . criterion = RootLossWrapper ( IgnoreIndexLossWrapper ( nn . MSELoss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ]), reduction = None ) elif loss == \"huber\" : self . criterion = IgnoreIndexLossWrapper ( nn . HuberLoss ( reduction = \"none\" ), self . hparams [ \"ignore_index\" ]) else : exception_message = f \"Loss type ' { loss } ' is not valid. Currently, supports 'mse', 'rmse' or 'mae' loss.\" raise ValueError ( exception_message )","title":"configure_losses"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.configure_metrics","text":"Initialize the performance metrics. Source code in terratorch/tasks/regression_tasks.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def configure_metrics ( self ) -> None : \"\"\"Initialize the performance metrics.\"\"\" def instantiate_metrics (): return { \"RMSE\" : MeanSquaredError ( squared = False ), \"MSE\" : MeanSquaredError ( squared = True ), \"MAE\" : MeanAbsoluteError (), } def wrap_metrics_with_ignore_index ( metrics ): return { name : IgnoreIndexMetricWrapper ( metric , ignore_index = self . hparams [ \"ignore_index\" ]) for name , metric in metrics . items () } self . train_metrics = MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = \"train/\" ) self . val_metrics = MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = \"val/\" ) if self . hparams [ \"test_dataloaders_names\" ] is not None : self . test_metrics = nn . ModuleList ( [ MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = f \"test/ { dl_name } /\" ) for dl_name in self . hparams [ \"test_dataloaders_names\" ] ] ) else : self . test_metrics = nn . ModuleList ( [ MetricCollection ( wrap_metrics_with_ignore_index ( instantiate_metrics ()), prefix = \"test/\" )] )","title":"configure_metrics"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.predict_step","text":"Compute the predicted class probabilities. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Returns: Tensor \u2013 Output predicted probabilities. Source code in terratorch/tasks/regression_tasks.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the predicted class probabilities. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. Returns: Output predicted probabilities. \"\"\" x = batch [ \"image\" ] file_names = batch [ \"filename\" ] if \"filename\" in batch else None other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } def model_forward ( x , ** kwargs ): return self ( x ) . output if self . tiled_inference_parameters : # TODO: tiled inference does not work with additional input data (**rest) y_hat : Tensor = tiled_inference ( model_forward , x , 1 , self . tiled_inference_parameters , ** rest ) else : y_hat : Tensor = self ( x , ** rest ) . output return y_hat , file_names","title":"predict_step"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.test_step","text":"Compute the test loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/regression_tasks.py 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 def test_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the test loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) if dataloader_idx >= len ( self . test_loss_handler ): msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\" raise ValueError ( msg ) loss = self . test_loss_handler [ dataloader_idx ] . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . test_loss_handler [ dataloader_idx ] . log_loss ( partial ( self . log , add_dataloader_idx = False ), # We don't need the dataloader idx as prefixes are different loss_dict = loss , batch_size = x . shape [ 0 ], ) y_hat = model_output . output self . test_metrics [ dataloader_idx ] . update ( y_hat , y )","title":"test_step"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.training_step","text":"Compute the train loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/regression_tasks.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def training_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the train loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . train_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . train_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = x . shape [ 0 ]) y_hat = model_output . output self . train_metrics . update ( y_hat , y ) return loss [ \"loss\" ]","title":"training_step"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.validation_step","text":"Compute the validation loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/regression_tasks.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 def validation_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the validation loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"mask\" ] other_keys = batch . keys () - { \"image\" , \"mask\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . val_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . val_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = y . shape [ 0 ]) y_hat = model_output . output self . val_metrics . update ( y_hat , y ) if self . _do_plot_samples ( batch_idx ): try : datamodule = self . trainer . datamodule batch [ \"prediction\" ] = y_hat if isinstance ( batch [ \"image\" ], dict ): # Multimodal input batch [ \"image\" ] = batch [ \"image\" ][ self . trainer . datamodule . rgb_modality ] for key in [ \"image\" , \"mask\" , \"prediction\" ]: batch [ key ] = batch [ key ] . cpu () sample = unbind_samples ( batch )[ 0 ] fig = datamodule . val_dataset . plot ( sample ) if fig : summary_writer = self . logger . experiment if hasattr ( summary_writer , \"add_figure\" ): summary_writer . add_figure ( f \"image/ { batch_idx } \" , fig , global_step = self . global_step ) elif hasattr ( summary_writer , \"log_figure\" ): summary_writer . log_figure ( self . logger . run_id , fig , f \"epoch_ { self . current_epoch } _ { batch_idx } .png\" ) except ValueError : pass finally : plt . close ()","title":"validation_step"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask","text":"Bases: TerraTorchTask Classification Task that accepts models from a range of sources. This class is analog in functionality to the class ClassificationTask defined by torchgeo. However, it has some important differences: - Accepts the specification of a model factory - Logs metrics per class - Does not have any callbacks by default (TorchGeo tasks do early stopping by default) - Allows the setting of optimizers in the constructor - It provides mIoU with both Micro and Macro averaging - Allows to evaluate on multiple test dataloaders .. note:: * 'Micro' averaging suits overall performance evaluation but may not reflect minority class accuracy. * 'Macro' averaging gives equal weight to each class, useful for balanced performance assessment across imbalanced classes. Source code in terratorch/tasks/classification_tasks.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 class ClassificationTask ( TerraTorchTask ): \"\"\"Classification Task that accepts models from a range of sources. This class is analog in functionality to the class ClassificationTask defined by torchgeo. However, it has some important differences: - Accepts the specification of a model factory - Logs metrics per class - Does not have any callbacks by default (TorchGeo tasks do early stopping by default) - Allows the setting of optimizers in the constructor - It provides mIoU with both Micro and Macro averaging - Allows to evaluate on multiple test dataloaders .. note:: * 'Micro' averaging suits overall performance evaluation but may not reflect minority class accuracy. * 'Macro' averaging gives equal weight to each class, useful for balanced performance assessment across imbalanced classes. \"\"\" def __init__ ( self , model_args : dict , model_factory : str | None = None , model : torch . nn . Module | None = None , loss : str = \"ce\" , aux_heads : list [ AuxiliaryHead ] | None = None , aux_loss : dict [ str , float ] | None = None , class_weights : list [ float ] | None = None , ignore_index : int | None = None , lr : float = 0.001 , # the following are optional so CLI doesnt need to pass them optimizer : str | None = None , optimizer_hparams : dict | None = None , scheduler : str | None = None , scheduler_hparams : dict | None = None , # # freeze_backbone : bool = False , # noqa: FBT001, FBT002 freeze_decoder : bool = False , # noqa: FBT002, FBT001 freeze_head : bool = False , # noqa: FBT002, FBT001 class_names : list [ str ] | None = None , test_dataloaders_names : list [ str ] | None = None , lr_overrides : dict [ str , float ] | None = None , ) -> None : \"\"\"Constructor Args: Defaults to None. model_args (Dict): Arguments passed to the model factory. model_factory (str, optional): ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model (torch.nn.Module, optional): Custom model. loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\". aux_loss (dict[str, float] | None, optional): Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss. class_weights (list[float] | None, optional): List of class weights to be applied to the loss. Defaults to None. ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None. lr (float, optional): Learning rate to be used. Defaults to 0.001. optimizer (str | None, optional): Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False. freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False. freeze_head (bool, optional): Whether to freeze the segmentation_head. Defaults to False. class_names (list[str] | None, optional): List of class names passed to metrics for better naming. Defaults to numeric ordering. test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. \"\"\" self . aux_loss = aux_loss self . aux_heads = aux_heads if model is not None and model_factory is not None : logger . warning ( \"A model_factory and a model was provided. The model_factory is ignored.\" ) if model is None and model_factory is None : raise ValueError ( \"A model_factory or a model (torch.nn.Module) must be provided.\" ) if model_factory and model is None : self . model_factory = MODEL_FACTORY_REGISTRY . build ( model_factory ) super () . __init__ ( task = \"classification\" ) if model : # Custom model self . model = model self . train_loss_handler = LossHandler ( self . train_metrics . prefix ) self . test_loss_handler : list [ LossHandler ] = [] for metrics in self . test_metrics : self . test_loss_handler . append ( LossHandler ( metrics . prefix )) self . val_loss_handler = LossHandler ( self . val_metrics . prefix ) self . monitor = f \" { self . val_metrics . prefix } loss\" def configure_losses ( self ) -> None : \"\"\"Initialize the loss criterion. Raises: ValueError: If *loss* is invalid. \"\"\" loss : str = self . hparams [ \"loss\" ] ignore_index = self . hparams [ \"ignore_index\" ] class_weights = ( torch . Tensor ( self . hparams [ \"class_weights\" ]) if self . hparams [ \"class_weights\" ] is not None else None ) if loss == \"ce\" : ignore_value = - 100 if ignore_index is None else ignore_index self . criterion = nn . CrossEntropyLoss ( ignore_index = ignore_value , weight = class_weights ) elif loss == \"bce\" : self . criterion = nn . BCEWithLogitsLoss () elif loss == \"jaccard\" : self . criterion = JaccardLoss ( mode = \"multiclass\" ) elif loss == \"focal\" : self . criterion = FocalLoss ( mode = \"multiclass\" , normalized = True ) else : msg = f \"Loss type ' { loss } ' is not valid.\" raise ValueError ( msg ) def configure_metrics ( self ) -> None : \"\"\"Initialize the performance metrics.\"\"\" num_classes : int = self . hparams [ \"model_args\" ][ \"num_classes\" ] ignore_index : int = self . hparams [ \"ignore_index\" ] class_names = self . hparams [ \"class_names\" ] metrics = MetricCollection ( { \"Overall_Accuracy\" : MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , average = \"micro\" , ), \"Average_Accuracy\" : MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , average = \"macro\" , ), \"Multiclass_Accuracy_Class\" : ClasswiseWrapper ( MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , average = None , ), labels = class_names , ), \"Multiclass_Jaccard_Index\" : MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index ), \"Multiclass_Jaccard_Index_Class\" : ClasswiseWrapper ( MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , average = None ), labels = class_names , ), # why FBetaScore \"Multiclass_F1_Score\" : MulticlassFBetaScore ( num_classes = num_classes , ignore_index = ignore_index , beta = 1.0 , average = \"micro\" , ), } ) self . train_metrics = metrics . clone ( prefix = \"train/\" ) self . val_metrics = metrics . clone ( prefix = \"val/\" ) if self . hparams [ \"test_dataloaders_names\" ] is not None : self . test_metrics = nn . ModuleList ( [ metrics . clone ( prefix = f \"test/ { dl_name } /\" ) for dl_name in self . hparams [ \"test_dataloaders_names\" ]] ) else : self . test_metrics = nn . ModuleList ([ metrics . clone ( prefix = \"test/\" )]) def training_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the train loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"label\" ] other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . train_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . train_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = x . shape [ 0 ]) y_hat_hard = to_class_prediction ( model_output ) self . train_metrics . update ( y_hat_hard , y ) return loss [ \"loss\" ] def validation_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the validation loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"label\" ] other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . val_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . val_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = x . shape [ 0 ]) y_hat_hard = to_class_prediction ( model_output ) self . val_metrics . update ( y_hat_hard , y ) def test_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the test loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"label\" ] other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) if dataloader_idx >= len ( self . test_loss_handler ): msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\" raise ValueError ( msg ) loss = self . test_loss_handler [ dataloader_idx ] . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . test_loss_handler [ dataloader_idx ] . log_loss ( partial ( self . log , add_dataloader_idx = False ), # We don't need the dataloader idx as prefixes are different loss_dict = loss , batch_size = x . shape [ 0 ], ) y_hat_hard = to_class_prediction ( model_output ) self . test_metrics [ dataloader_idx ] . update ( y_hat_hard , y ) def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the predicted class probabilities. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. Returns: Output predicted probabilities. \"\"\" x = batch [ \"image\" ] file_names = batch [ \"filename\" ] if \"filename\" in batch else None other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) y_hat = self ( x ) . output y_hat = y_hat . argmax ( dim = 1 ) return y_hat , file_names","title":"ClassificationTask"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.__init__","text":"Constructor Parameters: model_args ( Dict ) \u2013 Arguments passed to the model factory. model_factory ( str , default: None ) \u2013 ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model ( Module , default: None ) \u2013 Custom model. loss ( str , default: 'ce' ) \u2013 Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\". aux_loss ( dict [ str , float ] | None , default: None ) \u2013 Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights ( Union [ list [ float ], None] , default: None ) \u2013 List of class weights to be applied to the loss. class_weights ( list [ float ] | None , default: None ) \u2013 List of class weights to be applied to the loss. Defaults to None. ignore_index ( int | None , default: None ) \u2013 Label to ignore in the loss computation. Defaults to None. lr ( float , default: 0.001 ) \u2013 Learning rate to be used. Defaults to 0.001. optimizer ( str | None , default: None ) \u2013 Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams ( dict | None , default: None ) \u2013 Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler ( str , default: None ) \u2013 Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams ( dict | None , default: None ) \u2013 Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone ( bool , default: False ) \u2013 Whether to freeze the backbone. Defaults to False. freeze_decoder ( bool , default: False ) \u2013 Whether to freeze the decoder. Defaults to False. freeze_head ( bool , default: False ) \u2013 Whether to freeze the segmentation_head. Defaults to False. class_names ( list [ str ] | None , default: None ) \u2013 List of class names passed to metrics for better naming. Defaults to numeric ordering. test_dataloaders_names ( list [ str ] | None , default: None ) \u2013 Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides ( dict [ str , float ] | None , default: None ) \u2013 Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. Source code in terratorch/tasks/classification_tasks.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def __init__ ( self , model_args : dict , model_factory : str | None = None , model : torch . nn . Module | None = None , loss : str = \"ce\" , aux_heads : list [ AuxiliaryHead ] | None = None , aux_loss : dict [ str , float ] | None = None , class_weights : list [ float ] | None = None , ignore_index : int | None = None , lr : float = 0.001 , # the following are optional so CLI doesnt need to pass them optimizer : str | None = None , optimizer_hparams : dict | None = None , scheduler : str | None = None , scheduler_hparams : dict | None = None , # # freeze_backbone : bool = False , # noqa: FBT001, FBT002 freeze_decoder : bool = False , # noqa: FBT002, FBT001 freeze_head : bool = False , # noqa: FBT002, FBT001 class_names : list [ str ] | None = None , test_dataloaders_names : list [ str ] | None = None , lr_overrides : dict [ str , float ] | None = None , ) -> None : \"\"\"Constructor Args: Defaults to None. model_args (Dict): Arguments passed to the model factory. model_factory (str, optional): ModelFactory class to be used to instantiate the model. Is ignored when model is provided. model (torch.nn.Module, optional): Custom model. loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\". aux_loss (dict[str, float] | None, optional): Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None. class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss. class_weights (list[float] | None, optional): List of class weights to be applied to the loss. Defaults to None. ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None. lr (float, optional): Learning rate to be used. Defaults to 0.001. optimizer (str | None, optional): Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI. optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI. scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI. scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI. freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False. freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False. freeze_head (bool, optional): Whether to freeze the segmentation_head. Defaults to False. class_names (list[str] | None, optional): List of class names passed to metrics for better naming. Defaults to numeric ordering. test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used. lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None. \"\"\" self . aux_loss = aux_loss self . aux_heads = aux_heads if model is not None and model_factory is not None : logger . warning ( \"A model_factory and a model was provided. The model_factory is ignored.\" ) if model is None and model_factory is None : raise ValueError ( \"A model_factory or a model (torch.nn.Module) must be provided.\" ) if model_factory and model is None : self . model_factory = MODEL_FACTORY_REGISTRY . build ( model_factory ) super () . __init__ ( task = \"classification\" ) if model : # Custom model self . model = model self . train_loss_handler = LossHandler ( self . train_metrics . prefix ) self . test_loss_handler : list [ LossHandler ] = [] for metrics in self . test_metrics : self . test_loss_handler . append ( LossHandler ( metrics . prefix )) self . val_loss_handler = LossHandler ( self . val_metrics . prefix ) self . monitor = f \" { self . val_metrics . prefix } loss\"","title":"__init__"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.configure_losses","text":"Initialize the loss criterion. Raises: ValueError \u2013 If loss is invalid. Source code in terratorch/tasks/classification_tasks.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def configure_losses ( self ) -> None : \"\"\"Initialize the loss criterion. Raises: ValueError: If *loss* is invalid. \"\"\" loss : str = self . hparams [ \"loss\" ] ignore_index = self . hparams [ \"ignore_index\" ] class_weights = ( torch . Tensor ( self . hparams [ \"class_weights\" ]) if self . hparams [ \"class_weights\" ] is not None else None ) if loss == \"ce\" : ignore_value = - 100 if ignore_index is None else ignore_index self . criterion = nn . CrossEntropyLoss ( ignore_index = ignore_value , weight = class_weights ) elif loss == \"bce\" : self . criterion = nn . BCEWithLogitsLoss () elif loss == \"jaccard\" : self . criterion = JaccardLoss ( mode = \"multiclass\" ) elif loss == \"focal\" : self . criterion = FocalLoss ( mode = \"multiclass\" , normalized = True ) else : msg = f \"Loss type ' { loss } ' is not valid.\" raise ValueError ( msg )","title":"configure_losses"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.configure_metrics","text":"Initialize the performance metrics. Source code in terratorch/tasks/classification_tasks.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def configure_metrics ( self ) -> None : \"\"\"Initialize the performance metrics.\"\"\" num_classes : int = self . hparams [ \"model_args\" ][ \"num_classes\" ] ignore_index : int = self . hparams [ \"ignore_index\" ] class_names = self . hparams [ \"class_names\" ] metrics = MetricCollection ( { \"Overall_Accuracy\" : MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , average = \"micro\" , ), \"Average_Accuracy\" : MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , average = \"macro\" , ), \"Multiclass_Accuracy_Class\" : ClasswiseWrapper ( MulticlassAccuracy ( num_classes = num_classes , ignore_index = ignore_index , average = None , ), labels = class_names , ), \"Multiclass_Jaccard_Index\" : MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index ), \"Multiclass_Jaccard_Index_Class\" : ClasswiseWrapper ( MulticlassJaccardIndex ( num_classes = num_classes , ignore_index = ignore_index , average = None ), labels = class_names , ), # why FBetaScore \"Multiclass_F1_Score\" : MulticlassFBetaScore ( num_classes = num_classes , ignore_index = ignore_index , beta = 1.0 , average = \"micro\" , ), } ) self . train_metrics = metrics . clone ( prefix = \"train/\" ) self . val_metrics = metrics . clone ( prefix = \"val/\" ) if self . hparams [ \"test_dataloaders_names\" ] is not None : self . test_metrics = nn . ModuleList ( [ metrics . clone ( prefix = f \"test/ { dl_name } /\" ) for dl_name in self . hparams [ \"test_dataloaders_names\" ]] ) else : self . test_metrics = nn . ModuleList ([ metrics . clone ( prefix = \"test/\" )])","title":"configure_metrics"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.predict_step","text":"Compute the predicted class probabilities. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Returns: Tensor \u2013 Output predicted probabilities. Source code in terratorch/tasks/classification_tasks.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the predicted class probabilities. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. Returns: Output predicted probabilities. \"\"\" x = batch [ \"image\" ] file_names = batch [ \"filename\" ] if \"filename\" in batch else None other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) y_hat = self ( x ) . output y_hat = y_hat . argmax ( dim = 1 ) return y_hat , file_names","title":"predict_step"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.test_step","text":"Compute the test loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/classification_tasks.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def test_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the test loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"label\" ] other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) if dataloader_idx >= len ( self . test_loss_handler ): msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\" raise ValueError ( msg ) loss = self . test_loss_handler [ dataloader_idx ] . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . test_loss_handler [ dataloader_idx ] . log_loss ( partial ( self . log , add_dataloader_idx = False ), # We don't need the dataloader idx as prefixes are different loss_dict = loss , batch_size = x . shape [ 0 ], ) y_hat_hard = to_class_prediction ( model_output ) self . test_metrics [ dataloader_idx ] . update ( y_hat_hard , y )","title":"test_step"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.training_step","text":"Compute the train loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/classification_tasks.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def training_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> Tensor : \"\"\"Compute the train loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"label\" ] other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . train_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . train_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = x . shape [ 0 ]) y_hat_hard = to_class_prediction ( model_output ) self . train_metrics . update ( y_hat_hard , y ) return loss [ \"loss\" ]","title":"training_step"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.validation_step","text":"Compute the validation loss and additional metrics. Parameters: batch ( Any ) \u2013 The output of your DataLoader. batch_idx ( int ) \u2013 Integer displaying index of this batch. dataloader_idx ( int , default: 0 ) \u2013 Index of the current dataloader. Source code in terratorch/tasks/classification_tasks.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def validation_step ( self , batch : Any , batch_idx : int , dataloader_idx : int = 0 ) -> None : \"\"\"Compute the validation loss and additional metrics. Args: batch: The output of your DataLoader. batch_idx: Integer displaying index of this batch. dataloader_idx: Index of the current dataloader. \"\"\" x = batch [ \"image\" ] y = batch [ \"label\" ] other_keys = batch . keys () - { \"image\" , \"label\" , \"filename\" } rest = { k : batch [ k ] for k in other_keys } model_output : ModelOutput = self ( x , ** rest ) loss = self . val_loss_handler . compute_loss ( model_output , y , self . criterion , self . aux_loss ) self . val_loss_handler . log_loss ( self . log , loss_dict = loss , batch_size = x . shape [ 0 ]) y_hat_hard = to_class_prediction ( model_output ) self . val_metrics . update ( y_hat_hard , y )","title":"validation_step"}]}