{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to TerraTorch","text":""},{"location":"#overview","title":"Overview","text":"<p>The purpose of this package is to build a flexible fine-tuning framework for Geospatial Foundation Models (GFMs) based on TorchGeo and Lightning which can be employed at different abstraction levels. It currently supports models from the Prithvi and Granite series, and also have been tested with others models available on HuggingFace. </p> <p>This library provides:</p> <ul> <li>All the functionality in TorchGeo.</li> <li>Easy access to Prithvi, timm and smp backbones.</li> <li>Flexible trainers for Image Segmentation, Pixel Wise Regression and Classification (more in progress).</li> <li>Launching of fine-tuning tasks through powerful configuration files.</li> </ul> <p>A good starting place is familiarization with PyTorch Lightning, which this project is built on.  TorchGeo is also an important complementary reference. </p> <p>Check out the architecture overview for a general description about how TerraTorch is organized. </p>"},{"location":"#quick-start","title":"Quick start","text":"<p>To get started, check out the quick start guide</p>"},{"location":"#license","title":"License","text":"<p>TerraTorch is distributed under the terms of License Apache 2.0, see here for more details. </p>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>The main goal of the design is to extend TorchGeo's existing tasks to be able to handle Prithvi backbones with appropriate decoders and heads. At the same time, we wish to keep the existing TorchGeo functionality intact so it can be leveraged with pretrained models that are already included.</p> <p>We achieve this by making new tasks that accept model factory classes, containing a <code>build_model</code> method. This strategy in principle allows arbitrary models to be trained for these tasks, given they respect some reasonable minimal interface. Together with this, we provide the EncoderDecoderFactory, which should enable users to plug together different Encoders and Decoders, with the aid of Necks for intermediate operations.</p> <p>Additionally, we extend TorchGeo with generic datasets and datamodules which can be defined at runtime, rather than requiring classes to be defined beforehand.</p> <p>The glue that holds everything together is LightningCLI, allowing the model, datamodule and Lightning Trainer to be instantiated from a config file or from the CLI. We make extensive use of for training and inference.</p> <p>Initial reading for a full understanding of the platform includes:</p> <ul> <li>Familiarity with PyTorch Lightning</li> <li>Familiarity with TorchGeo</li> <li>Familiarity with LightningCLI</li> </ul> <p>The scheme below illustrates the general TerraTorch's workflow for a CLI job.   </p>"},{"location":"architecture/#tasks","title":"Tasks","text":"<p>Tasks are the main coordinators for training and inference for specific tasks. They are LightningModules that contain a model and abstract away all the logic for training steps, metric computation and inference.</p> <p>One of the most important design decisions was delegating the model construction to a model factory. This has a few advantages:</p> <ul> <li>Avoids code repetition among tasks - different tasks can use the same factory</li> <li>Prefers composition over inheritance</li> <li>Allows new models to be easily added by introducing new factories</li> </ul> <p>Models are expected to be <code>torch.nn.Module</code> and implement the Model interface, providing:</p> <ul> <li><code>freeze_encoder()</code></li> <li><code>freeze_decoder()</code></li> <li><code>forward()</code></li> </ul> <p>Additionally, the <code>forward()</code> method is expected to return an object of type ModelOutput, containing the main head's output, as well as any additional auxiliary outputs. The names of these auxiliary heads are matched with the names of the provided auxiliary losses. The tasks currently deployed in TerraTorch are described here.  </p>"},{"location":"architecture/#models","title":"Models","text":"<p>Models constructed by the EncoderDecoderFactory have an internal structure explicitly divided into backbones, necks, decoders and heads. This structure is provided by the PixelWiseModel and ScalarOutputModel classes.</p> <p>However, as long as models implement the Model interface, and return ModelOutput in their forward method, they can take on any structure.</p> <p>See the models documentation for more details about the core models ScalarOutputModel and PixelWiseModel. For details about backbones (encoders) see the backbones documentation, the same for decoders and heads.  </p>"},{"location":"architecture/#model-factories","title":"Model Factories","text":"<p>A model factory is a class desgined to search a model in the register and properly instantiate it. TerraTorch has a few types of model factories for different situations, as models which require specific wrappers and processing.</p> <p>See the models factories documentation for a better explanation about it. </p>"},{"location":"architecture/#encoderdecoderfactory","title":"EncoderDecoderFactory","text":"<p>However, as we have tried as much as possible to avoid the limitless replication of model factories dedicate to very specific models by concentrating efforts on the EncoderDecoderFactory, which intends to be more general-purpose. With that in mind, we dive deeper into it here.</p>"},{"location":"architecture/#loss","title":"Loss","text":"<p>For convenience, we provide a loss handler that can be used to compute the full loss (from the main head and auxiliary heads as well).</p>"},{"location":"architecture/#generic-datasets-datamodules","title":"Generic datasets / datamodules","text":"<p>Refer to the section on data</p>"},{"location":"architecture/#exporting-models","title":"Exporting models","text":"<p>Models are saved using the PyTorch format, which basically serializes the model weights using pickle and store them into a binary file. </p> <p>"},{"location":"backbones/","title":"Backbones","text":""},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder","title":"<code>terratorch.models.backbones.swin_encoder_decoder</code>","text":"<p>Swin transformer implementation. Mix of MMSegmentation implementation and timm implementation.</p> <p>We use this implementation instead of the original implementation or timm's. This is because it offers a few advantages, namely being able to handle a dynamic input size through padding.</p> <p>Please note the original timm implementation can still be used as a backbone via timm.create_model(\"swin_...\"). You can see the available models with 'timm.list_models(\"swin*\")'</p>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.AdaptivePadding","title":"<code>AdaptivePadding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Applies padding to input (if needed) so that input can get fully covered by filter you specified. It support two modes \"same\" and \"corner\". The \"same\" mode is same with \"SAME\" padding mode in TensorFlow, pad zero around input. The \"corner\"  mode would pad zero to bottom right.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_size</code> <code>int | tuple</code> <p>Size of the kernel:</p> <code>1</code> <code>stride</code> <code>int | tuple</code> <p>Stride of the filter. Default: 1:</p> <code>1</code> <code>dilation</code> <code>int | tuple</code> <p>Spacing between kernel elements. Default: 1.</p> <code>1</code> <code>padding</code> <code>str</code> <p>Support \"same\" and \"corner\", \"corner\" mode would pad zero to bottom right, and \"same\" mode would pad zero around input. Default: \"corner\".</p> <code>'corner'</code> <p>Example: <pre><code>    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; kernel_size = 16\n    &gt;&gt;&gt; stride = 16\n    &gt;&gt;&gt; dilation = 1\n    &gt;&gt;&gt; input = torch.rand(1, 1, 15, 17)\n    &gt;&gt;&gt; adap_pad = AdaptivePadding(\n    &gt;&gt;&gt;     kernel_size=kernel_size,\n    &gt;&gt;&gt;     stride=stride,\n    &gt;&gt;&gt;     dilation=dilation,\n    &gt;&gt;&gt;     padding=\"corner\")\n    &gt;&gt;&gt; out = adap_pad(input)\n    &gt;&gt;&gt; assert (out.shape[2], out.shape[3]) == (16, 32)\n    &gt;&gt;&gt; input = torch.rand(1, 1, 16, 17)\n    &gt;&gt;&gt; out = adap_pad(input)\n    &gt;&gt;&gt; assert (out.shape[2], out.shape[3]) == (16, 32)\n</code></pre></p> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>class AdaptivePadding(nn.Module):\n    r\"\"\"Applies padding to input (if needed) so that input can get fully covered\n    by filter you specified. It support two modes \"same\" and \"corner\". The\n    \"same\" mode is same with \"SAME\" padding mode in TensorFlow, pad zero around\n    input. The \"corner\"  mode would pad zero to bottom right.\n\n    Args:\n        kernel_size (int | tuple): Size of the kernel:\n        stride (int | tuple): Stride of the filter. Default: 1:\n        dilation (int | tuple): Spacing between kernel elements.\n            Default: 1.\n        padding (str): Support \"same\" and \"corner\", \"corner\" mode\n            would pad zero to bottom right, and \"same\" mode would\n            pad zero around input. Default: \"corner\".\n    Example:\n    ```\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; kernel_size = 16\n        &gt;&gt;&gt; stride = 16\n        &gt;&gt;&gt; dilation = 1\n        &gt;&gt;&gt; input = torch.rand(1, 1, 15, 17)\n        &gt;&gt;&gt; adap_pad = AdaptivePadding(\n        &gt;&gt;&gt;     kernel_size=kernel_size,\n        &gt;&gt;&gt;     stride=stride,\n        &gt;&gt;&gt;     dilation=dilation,\n        &gt;&gt;&gt;     padding=\"corner\")\n        &gt;&gt;&gt; out = adap_pad(input)\n        &gt;&gt;&gt; assert (out.shape[2], out.shape[3]) == (16, 32)\n        &gt;&gt;&gt; input = torch.rand(1, 1, 16, 17)\n        &gt;&gt;&gt; out = adap_pad(input)\n        &gt;&gt;&gt; assert (out.shape[2], out.shape[3]) == (16, 32)\n    ```\n    \"\"\"\n\n    def __init__(self, kernel_size=1, stride=1, dilation=1, padding=\"corner\", mode=\"constant\"):\n        super().__init__()\n\n        if padding not in (\"same\", \"corner\"):\n            msg = \"padding must be same or corner\"\n            raise Exception(msg)\n\n        kernel_size = to_2tuple(kernel_size)\n        stride = to_2tuple(stride)\n        dilation = to_2tuple(dilation)\n\n        self.padding = padding\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.dilation = dilation\n        self.mode = mode\n\n    def get_pad_shape(self, input_shape):\n        input_h, input_w = input_shape\n        kernel_h, kernel_w = self.kernel_size\n        stride_h, stride_w = self.stride\n        output_h = math.ceil(input_h / stride_h)\n        output_w = math.ceil(input_w / stride_w)\n        pad_h = max((output_h - 1) * stride_h + (kernel_h - 1) * self.dilation[0] + 1 - input_h, 0)\n        pad_w = max((output_w - 1) * stride_w + (kernel_w - 1) * self.dilation[1] + 1 - input_w, 0)\n        return pad_h, pad_w\n\n    def forward(self, x):\n        pad_h, pad_w = self.get_pad_shape(x.size()[-2:])\n        if pad_h &gt; 0 or pad_w &gt; 0:\n            if self.padding == \"corner\":\n                x = F.pad(x, [0, pad_w, 0, pad_h], mode=self.mode)\n            elif self.padding == \"same\":\n                x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n        return x\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.FFN","title":"<code>FFN</code>","text":"<p>               Bases: <code>Module</code></p> <p>Implements feed-forward networks (FFNs) with identity connection.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dims</code> <code>int</code> <p>The feature dimension. Same as <code>MultiheadAttention</code>. Defaults: 256.</p> <code>256</code> <code>feedforward_channels</code> <code>int</code> <p>The hidden dimension of FFNs. Defaults: 1024.</p> <code>1024</code> <code>num_fcs</code> <code>int</code> <p>The number of fully-connected layers in FFNs. Default: 2.</p> <code>2</code> <code>act_cfg</code> <code>dict</code> <p>The activation config for FFNs. Default: dict(type='ReLU')</p> required <code>ffn_drop</code> <code>float</code> <p>Probability of an element to be zeroed in FFN. Default 0.0.</p> <code>0.0</code> <code>add_identity</code> <code>bool</code> <p>Whether to add the identity connection. Default: <code>True</code>.</p> <code>True</code> <code>dropout_layer</code> <code>obj</code> <p><code>ConfigDict</code>): The dropout_layer used when adding the shortcut.</p> <code>None</code> <code>init_cfg</code> <code>obj</code> <p><code>mmcv.ConfigDict</code>): The Config for initialization. Default: None.</p> required Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>class FFN(nn.Module):\n    \"\"\"Implements feed-forward networks (FFNs) with identity connection.\n\n    Args:\n        embed_dims (int): The feature dimension. Same as\n            `MultiheadAttention`. Defaults: 256.\n        feedforward_channels (int): The hidden dimension of FFNs.\n            Defaults: 1024.\n        num_fcs (int, optional): The number of fully-connected layers in\n            FFNs. Default: 2.\n        act_cfg (dict, optional): The activation config for FFNs.\n            Default: dict(type='ReLU')\n        ffn_drop (float, optional): Probability of an element to be\n            zeroed in FFN. Default 0.0.\n        add_identity (bool, optional): Whether to add the\n            identity connection. Default: `True`.\n        dropout_layer (obj:`ConfigDict`): The dropout_layer used\n            when adding the shortcut.\n        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.\n            Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dims=256,\n        feedforward_channels=1024,\n        num_fcs=2,\n        ffn_drop=0.0,\n        dropout_layer=None,\n        dropout_arg=0,\n        act_layer=nn.ReLU,\n        add_identity=True,\n    ):\n        super().__init__()\n        self.embed_dims = embed_dims\n        self.feedforward_channels = feedforward_channels\n        self.num_fcs = num_fcs\n        self.activate = act_layer\n\n        layers = []\n        in_channels = embed_dims\n        for _ in range(num_fcs - 1):\n            layers.append(\n                nn.Sequential(nn.Linear(in_channels, feedforward_channels), self.activate(), nn.Dropout(ffn_drop))\n            )\n            in_channels = feedforward_channels\n        layers.append(nn.Linear(feedforward_channels, embed_dims))\n        layers.append(nn.Dropout(ffn_drop))\n        self.layers = nn.Sequential(*layers)\n        self.dropout_layer = dropout_layer(dropout_arg) if dropout_layer else torch.nn.Identity()\n        self.add_identity = add_identity\n\n    def forward(self, x, identity=None):\n        \"\"\"Forward function for `FFN`.\n\n        The function would add x to the output tensor if residue is None.\n        \"\"\"\n        out = self.layers(x)\n        if not self.add_identity:\n            return self.dropout_layer(out)\n        if identity is None:\n            identity = x\n        return identity + self.dropout_layer(out)\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.FFN.forward","title":"<code>forward(x, identity=None)</code>","text":"<p>Forward function for <code>FFN</code>.</p> <p>The function would add x to the output tensor if residue is None.</p> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>def forward(self, x, identity=None):\n    \"\"\"Forward function for `FFN`.\n\n    The function would add x to the output tensor if residue is None.\n    \"\"\"\n    out = self.layers(x)\n    if not self.add_identity:\n        return self.dropout_layer(out)\n    if identity is None:\n        identity = x\n    return identity + self.dropout_layer(out)\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.MMSegSwinTransformer","title":"<code>MMSegSwinTransformer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>class MMSegSwinTransformer(nn.Module):\n\n    def __init__(\n        self,\n        pretrain_img_size=224,\n        in_chans=3,\n        embed_dim=96,\n        patch_size=4,\n        window_size=7,\n        mlp_ratio=4,\n        depths=(2, 2, 6, 2),\n        num_heads=(3, 6, 12, 24),\n        strides=(4, 2, 2, 2),\n        num_classes: int = 1000,\n        global_pool: str = \"avg\",\n        out_indices=(0, 1, 2, 3),\n        qkv_bias=True,  # noqa: FBT002\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.1,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        with_cp=False,  # noqa: FBT002\n        frozen_stages=-1,\n    ):\n        \"\"\"MMSeg Swin Transformer backbone.\n\n        This backbone is the implementation of `Swin Transformer:\n        Hierarchical Vision Transformer using Shifted\n        Windows &lt;https://arxiv.org/abs/2103.14030&gt;`_.\n        Inspiration from https://github.com/microsoft/Swin-Transformer.\n\n        Args:\n            pretrain_img_size (int | tuple[int]): The size of input image when\n                pretrain. Defaults: 224.\n            in_chans (int): The num of input channels.\n                Defaults: 3.\n            embed_dim (int): The feature dimension. Default: 96.\n            patch_size (int | tuple[int]): Patch size. Default: 4.\n            window_size (int): Window size. Default: 7.\n            mlp_ratio (int | float): Ratio of mlp hidden dim to embedding dim.\n                Default: 4.\n            depths (tuple[int]): Depths of each Swin Transformer stage.\n                Default: (2, 2, 6, 2).\n            num_heads (tuple[int]): Parallel attention heads of each Swin\n                Transformer stage. Default: (3, 6, 12, 24).\n            strides (tuple[int]): The patch merging or patch embedding stride of\n                each Swin Transformer stage. (In swin, we set kernel size equal to\n                stride.) Default: (4, 2, 2, 2).\n            out_indices (tuple[int]): Output from which stages.\n                Default: (0, 1, 2, 3).\n            qkv_bias (bool, optional): If True, add a learnable bias to query, key,\n                value. Default: True\n            qk_scale (float | None, optional): Override default qk scale of\n                head_dim ** -0.5 if set. Default: None.\n            patch_norm (bool): If add a norm layer for patch embed and patch\n                merging. Default: True.\n            drop_rate (float): Dropout rate. Defaults: 0.\n            attn_drop_rate (float): Attention dropout rate. Default: 0.\n            drop_path_rate (float): Stochastic depth rate. Defaults: 0.1.\n            act_layer (dict): activation layer.\n                Default: nn.GELU.\n            norm_layer (dict): normalization layer at\n                output of backone. Defaults: nn.LayerNorm.\n            with_cp (bool, optional): Use checkpoint or not. Using checkpoint\n                will save some memory while slowing down the training speed.\n                Default: False.\n            frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n                -1 means not freezing any parameters.\n        \"\"\"\n\n        self.frozen_stages = frozen_stages\n        self.output_fmt = \"NHWC\"\n        if isinstance(pretrain_img_size, int):\n            pretrain_img_size = to_2tuple(pretrain_img_size)\n        elif isinstance(pretrain_img_size, tuple):\n            if len(pretrain_img_size) == 1:\n                pretrain_img_size = to_2tuple(pretrain_img_size[0])\n            if not len(pretrain_img_size) == 2:  # noqa: PLR2004\n                msg = f\"The size of image should have length 1 or 2, but got {len(pretrain_img_size)}\"\n                raise Exception(msg)\n\n        super().__init__()\n\n        self.num_layers = len(depths)\n        self.out_indices = out_indices\n        self.feature_info = []\n\n        if not strides[0] == patch_size:\n            msg = \"Use non-overlapping patch embed.\"\n            raise Exception(msg)\n\n        self.patch_embed = PatchEmbed(\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            kernel_size=patch_size,\n            stride=strides[0],\n            padding=\"corner\",\n            norm_layer=norm_layer,\n            padding_mode=\"replicate\",\n            drop_rate=drop_rate,\n        )\n\n        # self.drop_after_pos = nn.Dropout(p=drop_rate)\n\n        # set stochastic depth decay rule\n        total_depth = sum(depths)\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_depth)]\n\n        stages = []\n        in_chans = embed_dim\n        scale = 1\n        for i in range(self.num_layers):\n            if i &lt; self.num_layers - 1:\n                downsample = PatchMerging(\n                    in_chans=in_chans,\n                    out_channels=2 * in_chans,\n                    stride=strides[i + 1],\n                    norm_layer=norm_layer,\n                )\n            else:\n                downsample = None\n\n            stage = SwinBlockSequence(\n                embed_dim=in_chans,\n                num_heads=num_heads[i],\n                feedforward_channels=int(mlp_ratio * in_chans),\n                depth=depths[i],\n                window_size=window_size,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop_rate=drop_rate,\n                attn_drop_rate=attn_drop_rate,\n                drop_path_rate=dpr[sum(depths[:i]) : sum(depths[: i + 1])],\n                downsample=downsample,\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                with_cp=with_cp,\n            )\n            stages.append(stage)\n            if i &gt; 0:\n                scale *= 2\n            self.feature_info += [{\"num_chs\": in_chans, \"reduction\": 4 * scale, \"module\": f\"stages.{i}\"}]\n            if downsample:\n                in_chans = downsample.out_channels\n        self.stages = nn.Sequential(*stages)\n        self.num_features = [int(embed_dim * 2**i) for i in range(self.num_layers)]\n        # Add a norm layer for each output\n\n        self.head = ClassifierHead(\n            self.num_features[-1],\n            num_classes,\n            pool_type=global_pool,\n            drop_rate=drop_rate,\n            input_fmt=self.output_fmt,\n        )\n\n    def train(self, mode=True):  # noqa: FBT002\n        \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n        super().train(mode)\n        self._freeze_stages()\n\n    def _freeze_stages(self):\n        if self.frozen_stages &gt;= 0:\n            self.patch_embed.eval()\n            for param in self.patch_embed.parameters():\n                param.requires_grad = False\n            self.drop_after_pos.eval()\n\n        for i in range(1, self.frozen_stages + 1):\n            if (i - 1) in self.out_indices:\n                norm_layer = getattr(self, f\"norm{i-1}\")\n                norm_layer.eval()\n                for param in norm_layer.parameters():\n                    param.requires_grad = False\n\n            m = self.stages[i - 1]\n            m.eval()\n            for param in m.parameters():\n                param.requires_grad = False\n\n    @torch.jit.ignore\n    def init_weights(self, mode=\"\"):\n        modes = (\"jax\", \"jax_nlhb\", \"moco\", \"\")\n        if mode not in modes:\n            msg = f\"mode must be one of {modes}\"\n            raise Exception(msg)\n        head_bias = -math.log(self.num_classes) if \"nlhb\" in mode else 0.0\n        named_apply(get_init_weights_vit(mode, head_bias=head_bias), self)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        nwd = set()\n        for n, _ in self.named_parameters():\n            if \"relative_position_bias_table\" in n:\n                nwd.add(n)\n        return nwd\n\n    @torch.jit.ignore\n    def group_matcher(self, coarse=False):  # noqa: FBT002\n        return {\n            \"stem\": r\"^patch_embed\",  # stem and embed\n            \"blocks\": r\"^layers\\.(\\d+)\"\n            if coarse\n            else [\n                (r\"^layers\\.(\\d+).downsample\", (0,)),\n                (r\"^layers\\.(\\d+)\\.\\w+\\.(\\d+)\", None),\n                (r\"^norm\", (99999,)),\n            ],\n        }\n\n    @torch.jit.ignore\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool=None):\n        self.num_classes = num_classes\n        self.head.reset(num_classes, pool_type=global_pool)\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = self.stages(x)\n        return x\n\n    def forward_head(self, x, pre_logits: bool = False):  # noqa: FBT002, FBT001\n        return self.head(x, pre_logits=True) if pre_logits else self.head(x)\n\n    def forward(self, x):\n        features = self.forward_features(x)\n        x = self.forward_head(features[0])\n        return x\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.MMSegSwinTransformer.__init__","title":"<code>__init__(pretrain_img_size=224, in_chans=3, embed_dim=96, patch_size=4, window_size=7, mlp_ratio=4, depths=(2, 2, 6, 2), num_heads=(3, 6, 12, 24), strides=(4, 2, 2, 2), num_classes=1000, global_pool='avg', out_indices=(0, 1, 2, 3), qkv_bias=True, qk_scale=None, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.1, act_layer=nn.GELU, norm_layer=nn.LayerNorm, with_cp=False, frozen_stages=-1)</code>","text":"<p>MMSeg Swin Transformer backbone.</p> <p>This backbone is the implementation of <code>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows &lt;https://arxiv.org/abs/2103.14030&gt;</code>_. Inspiration from https://github.com/microsoft/Swin-Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>pretrain_img_size</code> <code>int | tuple[int]</code> <p>The size of input image when pretrain. Defaults: 224.</p> <code>224</code> <code>in_chans</code> <code>int</code> <p>The num of input channels. Defaults: 3.</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>The feature dimension. Default: 96.</p> <code>96</code> <code>patch_size</code> <code>int | tuple[int]</code> <p>Patch size. Default: 4.</p> <code>4</code> <code>window_size</code> <code>int</code> <p>Window size. Default: 7.</p> <code>7</code> <code>mlp_ratio</code> <code>int | float</code> <p>Ratio of mlp hidden dim to embedding dim. Default: 4.</p> <code>4</code> <code>depths</code> <code>tuple[int]</code> <p>Depths of each Swin Transformer stage. Default: (2, 2, 6, 2).</p> <code>(2, 2, 6, 2)</code> <code>num_heads</code> <code>tuple[int]</code> <p>Parallel attention heads of each Swin Transformer stage. Default: (3, 6, 12, 24).</p> <code>(3, 6, 12, 24)</code> <code>strides</code> <code>tuple[int]</code> <p>The patch merging or patch embedding stride of each Swin Transformer stage. (In swin, we set kernel size equal to stride.) Default: (4, 2, 2, 2).</p> <code>(4, 2, 2, 2)</code> <code>out_indices</code> <code>tuple[int]</code> <p>Output from which stages. Default: (0, 1, 2, 3).</p> <code>(0, 1, 2, 3)</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to query, key, value. Default: True</p> <code>True</code> <code>qk_scale</code> <code>float | None</code> <p>Override default qk scale of head_dim ** -0.5 if set. Default: None.</p> <code>None</code> <code>patch_norm</code> <code>bool</code> <p>If add a norm layer for patch embed and patch merging. Default: True.</p> required <code>drop_rate</code> <code>float</code> <p>Dropout rate. Defaults: 0.</p> <code>0.0</code> <code>attn_drop_rate</code> <code>float</code> <p>Attention dropout rate. Default: 0.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float</code> <p>Stochastic depth rate. Defaults: 0.1.</p> <code>0.1</code> <code>act_layer</code> <code>dict</code> <p>activation layer. Default: nn.GELU.</p> <code>GELU</code> <code>norm_layer</code> <code>dict</code> <p>normalization layer at output of backone. Defaults: nn.LayerNorm.</p> <code>LayerNorm</code> <code>with_cp</code> <code>bool</code> <p>Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False.</p> <code>False</code> <code>frozen_stages</code> <code>int</code> <p>Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters.</p> <code>-1</code> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>def __init__(\n    self,\n    pretrain_img_size=224,\n    in_chans=3,\n    embed_dim=96,\n    patch_size=4,\n    window_size=7,\n    mlp_ratio=4,\n    depths=(2, 2, 6, 2),\n    num_heads=(3, 6, 12, 24),\n    strides=(4, 2, 2, 2),\n    num_classes: int = 1000,\n    global_pool: str = \"avg\",\n    out_indices=(0, 1, 2, 3),\n    qkv_bias=True,  # noqa: FBT002\n    qk_scale=None,\n    drop_rate=0.0,\n    attn_drop_rate=0.0,\n    drop_path_rate=0.1,\n    act_layer=nn.GELU,\n    norm_layer=nn.LayerNorm,\n    with_cp=False,  # noqa: FBT002\n    frozen_stages=-1,\n):\n    \"\"\"MMSeg Swin Transformer backbone.\n\n    This backbone is the implementation of `Swin Transformer:\n    Hierarchical Vision Transformer using Shifted\n    Windows &lt;https://arxiv.org/abs/2103.14030&gt;`_.\n    Inspiration from https://github.com/microsoft/Swin-Transformer.\n\n    Args:\n        pretrain_img_size (int | tuple[int]): The size of input image when\n            pretrain. Defaults: 224.\n        in_chans (int): The num of input channels.\n            Defaults: 3.\n        embed_dim (int): The feature dimension. Default: 96.\n        patch_size (int | tuple[int]): Patch size. Default: 4.\n        window_size (int): Window size. Default: 7.\n        mlp_ratio (int | float): Ratio of mlp hidden dim to embedding dim.\n            Default: 4.\n        depths (tuple[int]): Depths of each Swin Transformer stage.\n            Default: (2, 2, 6, 2).\n        num_heads (tuple[int]): Parallel attention heads of each Swin\n            Transformer stage. Default: (3, 6, 12, 24).\n        strides (tuple[int]): The patch merging or patch embedding stride of\n            each Swin Transformer stage. (In swin, we set kernel size equal to\n            stride.) Default: (4, 2, 2, 2).\n        out_indices (tuple[int]): Output from which stages.\n            Default: (0, 1, 2, 3).\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key,\n            value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of\n            head_dim ** -0.5 if set. Default: None.\n        patch_norm (bool): If add a norm layer for patch embed and patch\n            merging. Default: True.\n        drop_rate (float): Dropout rate. Defaults: 0.\n        attn_drop_rate (float): Attention dropout rate. Default: 0.\n        drop_path_rate (float): Stochastic depth rate. Defaults: 0.1.\n        act_layer (dict): activation layer.\n            Default: nn.GELU.\n        norm_layer (dict): normalization layer at\n            output of backone. Defaults: nn.LayerNorm.\n        with_cp (bool, optional): Use checkpoint or not. Using checkpoint\n            will save some memory while slowing down the training speed.\n            Default: False.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n    \"\"\"\n\n    self.frozen_stages = frozen_stages\n    self.output_fmt = \"NHWC\"\n    if isinstance(pretrain_img_size, int):\n        pretrain_img_size = to_2tuple(pretrain_img_size)\n    elif isinstance(pretrain_img_size, tuple):\n        if len(pretrain_img_size) == 1:\n            pretrain_img_size = to_2tuple(pretrain_img_size[0])\n        if not len(pretrain_img_size) == 2:  # noqa: PLR2004\n            msg = f\"The size of image should have length 1 or 2, but got {len(pretrain_img_size)}\"\n            raise Exception(msg)\n\n    super().__init__()\n\n    self.num_layers = len(depths)\n    self.out_indices = out_indices\n    self.feature_info = []\n\n    if not strides[0] == patch_size:\n        msg = \"Use non-overlapping patch embed.\"\n        raise Exception(msg)\n\n    self.patch_embed = PatchEmbed(\n        in_chans=in_chans,\n        embed_dim=embed_dim,\n        kernel_size=patch_size,\n        stride=strides[0],\n        padding=\"corner\",\n        norm_layer=norm_layer,\n        padding_mode=\"replicate\",\n        drop_rate=drop_rate,\n    )\n\n    # self.drop_after_pos = nn.Dropout(p=drop_rate)\n\n    # set stochastic depth decay rule\n    total_depth = sum(depths)\n    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_depth)]\n\n    stages = []\n    in_chans = embed_dim\n    scale = 1\n    for i in range(self.num_layers):\n        if i &lt; self.num_layers - 1:\n            downsample = PatchMerging(\n                in_chans=in_chans,\n                out_channels=2 * in_chans,\n                stride=strides[i + 1],\n                norm_layer=norm_layer,\n            )\n        else:\n            downsample = None\n\n        stage = SwinBlockSequence(\n            embed_dim=in_chans,\n            num_heads=num_heads[i],\n            feedforward_channels=int(mlp_ratio * in_chans),\n            depth=depths[i],\n            window_size=window_size,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            drop_rate=drop_rate,\n            attn_drop_rate=attn_drop_rate,\n            drop_path_rate=dpr[sum(depths[:i]) : sum(depths[: i + 1])],\n            downsample=downsample,\n            act_layer=act_layer,\n            norm_layer=norm_layer,\n            with_cp=with_cp,\n        )\n        stages.append(stage)\n        if i &gt; 0:\n            scale *= 2\n        self.feature_info += [{\"num_chs\": in_chans, \"reduction\": 4 * scale, \"module\": f\"stages.{i}\"}]\n        if downsample:\n            in_chans = downsample.out_channels\n    self.stages = nn.Sequential(*stages)\n    self.num_features = [int(embed_dim * 2**i) for i in range(self.num_layers)]\n    # Add a norm layer for each output\n\n    self.head = ClassifierHead(\n        self.num_features[-1],\n        num_classes,\n        pool_type=global_pool,\n        drop_rate=drop_rate,\n        input_fmt=self.output_fmt,\n    )\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.MMSegSwinTransformer.train","title":"<code>train(mode=True)</code>","text":"<p>Convert the model into training mode while keep layers freezed.</p> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>def train(self, mode=True):  # noqa: FBT002\n    \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n    super().train(mode)\n    self._freeze_stages()\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.PatchEmbed","title":"<code>PatchEmbed</code>","text":"<p>               Bases: <code>Module</code></p> <p>Image to Patch Embedding.</p> <p>We use a conv layer to implement PatchEmbed.</p> <p>Parameters:</p> Name Type Description Default <code>in_chans</code> <code>int</code> <p>The num of input channels. Default: 3</p> <code>3</code> <code>embed_dim</code> <code>int</code> <p>The dimensions of embedding. Default: 768</p> <code>768</code> <code>kernel_size</code> <code>int</code> <p>The kernel_size of embedding conv. Default: 16.</p> <code>16</code> <code>stride</code> <code>int</code> <p>The slide stride of embedding conv. Default: None (Would be set as <code>kernel_size</code>).</p> <code>None</code> <code>padding</code> <code>int | tuple | string</code> <p>The padding length of embedding conv. When it is a string, it means the mode of adaptive padding, support \"same\" and \"corner\" now. Default: \"corner\".</p> <code>'corner'</code> <code>padding_mode</code> <code>string</code> <p>The padding mode to use. Default \"constant\".</p> <code>'constant'</code> <code>dilation</code> <code>int</code> <p>The dilation rate of embedding conv. Default: 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Bias of embed conv. Default: True.</p> <code>True</code> <code>norm_cfg</code> <code>dict</code> <p>Config dict for normalization layer. Default: None.</p> required <code>input_size</code> <code>int | tuple | None</code> <p>The size of input, which will be used to calculate the out size. Only work when <code>dynamic_size</code> is False. Default: None.</p> <code>None</code> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>class PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding.\n\n    We use a conv layer to implement PatchEmbed.\n\n    Args:\n        in_chans (int): The num of input channels. Default: 3\n        embed_dim (int): The dimensions of embedding. Default: 768\n        kernel_size (int): The kernel_size of embedding conv. Default: 16.\n        stride (int, optional): The slide stride of embedding conv.\n            Default: None (Would be set as `kernel_size`).\n        padding (int | tuple | string ): The padding length of\n            embedding conv. When it is a string, it means the mode\n            of adaptive padding, support \"same\" and \"corner\" now.\n            Default: \"corner\".\n        padding_mode (string): The padding mode to use. Default \"constant\".\n        dilation (int): The dilation rate of embedding conv. Default: 1.\n        bias (bool): Bias of embed conv. Default: True.\n        norm_cfg (dict, optional): Config dict for normalization layer.\n            Default: None.\n        input_size (int | tuple | None): The size of input, which will be\n            used to calculate the out size. Only work when `dynamic_size`\n            is False. Default: None.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_chans=3,\n        embed_dim=768,\n        kernel_size=16,\n        stride=None,\n        padding=\"corner\",\n        padding_mode=\"constant\",\n        dilation=1,\n        bias=True,  # noqa: FBT002\n        norm_layer=nn.LayerNorm,\n        input_size=None,\n        drop_rate=0,\n    ):\n        super().__init__()\n\n        self.embed_dim = embed_dim\n        if stride is None:\n            stride = kernel_size\n        self.drop = nn.Dropout(drop_rate)\n\n        kernel_size = to_2tuple(kernel_size)\n        stride = to_2tuple(stride)\n        dilation = to_2tuple(dilation)\n\n        if isinstance(padding, str):\n            self.adap_padding = AdaptivePadding(\n                kernel_size=kernel_size, stride=stride, dilation=dilation, padding=padding, mode=padding_mode\n            )\n            # disable the padding of conv\n            padding = 0\n        else:\n            self.adap_padding = None\n        padding = to_2tuple(padding)\n\n        self.projection = nn.Conv2d(\n            in_channels=in_chans,\n            out_channels=embed_dim,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            bias=bias,\n        )\n\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n        if input_size:\n            input_size = to_2tuple(input_size)\n            # `init_out_size` would be used outside to\n            # calculate the num_patches\n            # when `use_abs_pos_embed` outside\n            self.init_input_size = input_size\n            if self.adap_padding:\n                pad_h, pad_w = self.adap_padding.get_pad_shape(input_size)\n                input_h, input_w = input_size\n                input_h = input_h + pad_h\n                input_w = input_w + pad_w\n                input_size = (input_h, input_w)\n\n            # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n            h_out = (input_size[0] + 2 * padding[0] - dilation[0] * (kernel_size[0] - 1) - 1) // stride[0] + 1\n            w_out = (input_size[1] + 2 * padding[1] - dilation[1] * (kernel_size[1] - 1) - 1) // stride[1] + 1\n            self.init_out_size = (h_out, w_out)\n        else:\n            self.init_input_size = None\n            self.init_out_size = None\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): Has shape (B, C, H, W). In most case, C is 3.\n\n        Returns:\n            tuple: Contains merged results and its spatial shape.\n\n                - x (Tensor): Has shape (B, out_h * out_w, embed_dim)\n                - out_size (tuple[int]): Spatial shape of x, arrange as\n                    (out_h, out_w).\n        \"\"\"\n\n        if self.adap_padding:\n            x = self.adap_padding(x)\n\n        x = self.projection(x)\n        out_size = (x.shape[2], x.shape[3])\n        x = x.flatten(2).transpose(1, 2)\n        if self.norm is not None:\n            x = self.norm(x)\n        x = self.drop(x)\n        return x, out_size\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.PatchEmbed.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Has shape (B, C, H, W). In most case, C is 3.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>Contains merged results and its spatial shape.</p> <ul> <li>x (Tensor): Has shape (B, out_h * out_w, embed_dim)</li> <li>out_size (tuple[int]): Spatial shape of x, arrange as     (out_h, out_w).</li> </ul> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Args:\n        x (Tensor): Has shape (B, C, H, W). In most case, C is 3.\n\n    Returns:\n        tuple: Contains merged results and its spatial shape.\n\n            - x (Tensor): Has shape (B, out_h * out_w, embed_dim)\n            - out_size (tuple[int]): Spatial shape of x, arrange as\n                (out_h, out_w).\n    \"\"\"\n\n    if self.adap_padding:\n        x = self.adap_padding(x)\n\n    x = self.projection(x)\n    out_size = (x.shape[2], x.shape[3])\n    x = x.flatten(2).transpose(1, 2)\n    if self.norm is not None:\n        x = self.norm(x)\n    x = self.drop(x)\n    return x, out_size\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.PatchMerging","title":"<code>PatchMerging</code>","text":"<p>               Bases: <code>Module</code></p> <p>Merge patch feature map.</p> <p>This layer groups feature map by kernel_size, and applies norm and linear layers to the grouped feature map. Our implementation uses <code>nn.Unfold</code> to merge patch, which is about 25% faster than original implementation. Instead, we need to modify pretrained models for compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>in_chans</code> <code>int</code> <p>The num of input channels.</p> required <code>out_channels</code> <code>int</code> <p>The num of output channels.</p> required <code>kernel_size</code> <code>int | tuple</code> <p>the kernel size in the unfold layer. Defaults to 2.</p> <code>2</code> <code>stride</code> <code>int | tuple</code> <p>the stride of the sliding blocks in the unfold layer. Default: None. (Would be set as <code>kernel_size</code>)</p> <code>None</code> <code>padding</code> <code>int | tuple | string</code> <p>The padding length of embedding conv. When it is a string, it means the mode of adaptive padding, support \"same\" and \"corner\" now. Default: \"corner\".</p> <code>'corner'</code> <code>dilation</code> <code>int | tuple</code> <p>dilation parameter in the unfold layer. Default: 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to add bias in linear layer or not. Defaults: False.</p> <code>False</code> <code>norm_cfg</code> <code>dict</code> <p>Config dict for normalization layer. Default: dict(type='LN').</p> required Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>class PatchMerging(nn.Module):\n    \"\"\"Merge patch feature map.\n\n    This layer groups feature map by kernel_size, and applies norm and linear\n    layers to the grouped feature map. Our implementation uses `nn.Unfold` to\n    merge patch, which is about 25% faster than original implementation.\n    Instead, we need to modify pretrained models for compatibility.\n\n    Args:\n        in_chans (int): The num of input channels.\n        out_channels (int): The num of output channels.\n        kernel_size (int | tuple, optional): the kernel size in the unfold\n            layer. Defaults to 2.\n        stride (int | tuple, optional): the stride of the sliding blocks in the\n            unfold layer. Default: None. (Would be set as `kernel_size`)\n        padding (int | tuple | string ): The padding length of\n            embedding conv. When it is a string, it means the mode\n            of adaptive padding, support \"same\" and \"corner\" now.\n            Default: \"corner\".\n        dilation (int | tuple, optional): dilation parameter in the unfold\n            layer. Default: 1.\n        bias (bool, optional): Whether to add bias in linear layer or not.\n            Defaults: False.\n        norm_cfg (dict, optional): Config dict for normalization layer.\n            Default: dict(type='LN').\n    \"\"\"\n\n    def __init__(\n        self,\n        in_chans,\n        out_channels,\n        kernel_size=2,\n        stride=None,\n        padding=\"corner\",\n        dilation=1,\n        bias=False,  # noqa: FBT002\n        norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        self.in_chans = in_chans\n        self.out_channels = out_channels\n        if stride is None:\n            stride = kernel_size\n\n        kernel_size = to_2tuple(kernel_size)\n        stride = to_2tuple(stride)\n        dilation = to_2tuple(dilation)\n\n        if isinstance(padding, str):\n            self.adap_padding = AdaptivePadding(\n                kernel_size=kernel_size, stride=stride, dilation=dilation, padding=padding\n            )\n            # disable the padding of unfold\n            padding = 0\n        else:\n            self.adap_padding = None\n\n        padding = to_2tuple(padding)\n        self.sampler = nn.Unfold(kernel_size=kernel_size, dilation=dilation, padding=padding, stride=stride)\n\n        sample_dim = kernel_size[0] * kernel_size[1] * in_chans\n\n        if norm_layer is not None:\n            self.norm = norm_layer(sample_dim)\n        else:\n            self.norm = None\n\n        self.reduction = nn.Linear(sample_dim, out_channels, bias=bias)\n\n    def forward(self, x, input_size):\n        \"\"\"\n        Args:\n            x (Tensor): Has shape (B, H*W, C_in).\n            input_size (tuple[int]): The spatial shape of x, arrange as (H, W).\n                Default: None.\n\n        Returns:\n            tuple: Contains merged results and its spatial shape.\n\n                - x (Tensor): Has shape (B, Merged_H * Merged_W, C_out)\n                - out_size (tuple[int]): Spatial shape of x, arrange as\n                    (Merged_H, Merged_W).\n        \"\"\"\n        B, L, C = x.shape  # noqa: N806\n\n        H, W = input_size  # noqa: N806\n        if not L == H * W:\n            msg = \"input feature has wrong size\"\n            raise Exception(msg)\n\n        x = x.view(B, H, W, C).permute([0, 3, 1, 2])  # B, C, H, W\n        # Use nn.Unfold to merge patch. About 25% faster than original method,\n        # but need to modify pretrained model for compatibility\n\n        if self.adap_padding:\n            x = self.adap_padding(x)\n            H, W = x.shape[-2:]  # noqa: N806\n\n        x = self.sampler(x)\n        # if kernel_size=2 and stride=2, x should has shape (B, 4*C, H/2*W/2)\n\n        out_h = (\n            H + 2 * self.sampler.padding[0] - self.sampler.dilation[0] * (self.sampler.kernel_size[0] - 1) - 1\n        ) // self.sampler.stride[0] + 1\n        out_w = (\n            W + 2 * self.sampler.padding[1] - self.sampler.dilation[1] * (self.sampler.kernel_size[1] - 1) - 1\n        ) // self.sampler.stride[1] + 1\n\n        output_size = (out_h, out_w)\n        x = x.transpose(1, 2)  # B, H/2*W/2, 4*C\n        x = self.norm(x) if self.norm else x\n        x = self.reduction(x)\n        return x, output_size\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.PatchMerging.forward","title":"<code>forward(x, input_size)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Has shape (B, H*W, C_in).</p> required <code>input_size</code> <code>tuple[int]</code> <p>The spatial shape of x, arrange as (H, W). Default: None.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>Contains merged results and its spatial shape.</p> <ul> <li>x (Tensor): Has shape (B, Merged_H * Merged_W, C_out)</li> <li>out_size (tuple[int]): Spatial shape of x, arrange as     (Merged_H, Merged_W).</li> </ul> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>def forward(self, x, input_size):\n    \"\"\"\n    Args:\n        x (Tensor): Has shape (B, H*W, C_in).\n        input_size (tuple[int]): The spatial shape of x, arrange as (H, W).\n            Default: None.\n\n    Returns:\n        tuple: Contains merged results and its spatial shape.\n\n            - x (Tensor): Has shape (B, Merged_H * Merged_W, C_out)\n            - out_size (tuple[int]): Spatial shape of x, arrange as\n                (Merged_H, Merged_W).\n    \"\"\"\n    B, L, C = x.shape  # noqa: N806\n\n    H, W = input_size  # noqa: N806\n    if not L == H * W:\n        msg = \"input feature has wrong size\"\n        raise Exception(msg)\n\n    x = x.view(B, H, W, C).permute([0, 3, 1, 2])  # B, C, H, W\n    # Use nn.Unfold to merge patch. About 25% faster than original method,\n    # but need to modify pretrained model for compatibility\n\n    if self.adap_padding:\n        x = self.adap_padding(x)\n        H, W = x.shape[-2:]  # noqa: N806\n\n    x = self.sampler(x)\n    # if kernel_size=2 and stride=2, x should has shape (B, 4*C, H/2*W/2)\n\n    out_h = (\n        H + 2 * self.sampler.padding[0] - self.sampler.dilation[0] * (self.sampler.kernel_size[0] - 1) - 1\n    ) // self.sampler.stride[0] + 1\n    out_w = (\n        W + 2 * self.sampler.padding[1] - self.sampler.dilation[1] * (self.sampler.kernel_size[1] - 1) - 1\n    ) // self.sampler.stride[1] + 1\n\n    output_size = (out_h, out_w)\n    x = x.transpose(1, 2)  # B, H/2*W/2, 4*C\n    x = self.norm(x) if self.norm else x\n    x = self.reduction(x)\n    return x, output_size\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.ShiftWindowMSA","title":"<code>ShiftWindowMSA</code>","text":"<p>               Bases: <code>Module</code></p> <p>Shifted Window Multihead Self-Attention Module.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Number of input channels.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>window_size</code> <code>int</code> <p>The height and width of the window.</p> required <code>shift_size</code> <code>int</code> <p>The shift step of each window towards right-bottom. If zero, act as regular window-msa. Defaults to 0.</p> <code>0</code> <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to q, k, v. Default: True</p> <code>True</code> <code>qk_scale</code> <code>float | None</code> <p>Override default qk scale of head_dim ** -0.5 if set. Defaults: None.</p> <code>None</code> <code>attn_drop_rate</code> <code>float</code> <p>Dropout ratio of attention weight. Defaults: 0.</p> <code>0</code> <code>proj_drop_rate</code> <code>float</code> <p>Dropout ratio of output. Defaults: 0.</p> <code>0</code> <code>drop_path_rate</code> <code>float</code> <p>Dropout ratio of layer used before output. Defaults: 0.</p> <code>0</code> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>class ShiftWindowMSA(nn.Module):\n    \"\"\"Shifted Window Multihead Self-Attention Module.\n\n    Args:\n        embed_dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): The height and width of the window.\n        shift_size (int, optional): The shift step of each window towards\n            right-bottom. If zero, act as regular window-msa. Defaults to 0.\n        qkv_bias (bool, optional): If True, add a learnable bias to q, k, v.\n            Default: True\n        qk_scale (float | None, optional): Override default qk scale of\n            head_dim ** -0.5 if set. Defaults: None.\n        attn_drop_rate (float, optional): Dropout ratio of attention weight.\n            Defaults: 0.\n        proj_drop_rate (float, optional): Dropout ratio of output.\n            Defaults: 0.\n        drop_path_rate (float, optional): Dropout ratio of layer used before output.\n            Defaults: 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        window_size,\n        shift_size=0,\n        qkv_bias=True,  # noqa: FBT002\n        qk_scale=None,\n        attn_drop_rate=0,\n        proj_drop_rate=0,\n        drop_path_rate=0,\n    ):\n        super().__init__()\n\n        self.window_size = window_size\n        self.shift_size = shift_size\n        if not 0 &lt;= self.shift_size &lt; self.window_size:\n            msg = \"0 &lt;= self.shift_size &lt; self.window_size condition must be true\"\n            raise Exception(msg)\n\n        self.w_msa = WindowMSA(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            window_size=to_2tuple(window_size),\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop_rate=attn_drop_rate,\n            proj_drop_rate=proj_drop_rate,\n        )\n\n        self.drop = DropPath(drop_path_rate)\n\n    def forward(self, query, hw_shape):\n        B, L, C = query.shape  # noqa: N806\n        H, W = hw_shape  # noqa: N806\n        if not L == H * W:\n            msg = \"input feature has wrong size\"\n            raise Exception(msg)\n        query = query.view(B, H, W, C)\n\n        # pad feature maps to multiples of window size\n        pad_r = (self.window_size - W % self.window_size) % self.window_size\n        pad_b = (self.window_size - H % self.window_size) % self.window_size\n        query = F.pad(query, (0, 0, 0, pad_r, 0, pad_b))\n        H_pad, W_pad = query.shape[1], query.shape[2]  # noqa: N806\n\n        # cyclic shift\n        if self.shift_size &gt; 0:\n            shifted_query = torch.roll(query, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n\n            # calculate attention mask for SW-MSA\n            img_mask = torch.zeros((1, H_pad, W_pad, 1), device=query.device)\n            h_slices = (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None),\n            )\n            w_slices = (\n                slice(0, -self.window_size),\n                slice(-self.window_size, -self.shift_size),\n                slice(-self.shift_size, None),\n            )\n            cnt = 0\n            for h in h_slices:\n                for w in w_slices:\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n\n            # nW, window_size, window_size, 1\n            mask_windows = self.window_partition(img_mask)\n            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, 0.0)\n        else:\n            shifted_query = query\n            attn_mask = None\n\n        # nW*B, window_size, window_size, C\n        query_windows = self.window_partition(shifted_query)\n        # nW*B, window_size*window_size, C\n        query_windows = query_windows.view(-1, self.window_size**2, C)\n\n        # W-MSA/SW-MSA (nW*B, window_size*window_size, C)\n        attn_windows = self.w_msa(query_windows, mask=attn_mask)\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n\n        # B H' W' C\n        shifted_x = self.window_reverse(attn_windows, H_pad, W_pad)\n        # reverse cyclic shift\n        if self.shift_size &gt; 0:\n            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n        else:\n            x = shifted_x\n\n        if pad_r &gt; 0 or pad_b:\n            x = x[:, :H, :W, :].contiguous()\n\n        x = x.view(B, H * W, C)\n\n        x = self.drop(x)\n        return x\n\n    def window_reverse(self, windows, H, W):  # noqa: N803\n        \"\"\"\n        Args:\n            windows: (num_windows*B, window_size, window_size, C)\n            H (int): Height of image\n            W (int): Width of image\n\n        Returns:\n            tuple: (B, H, W, C)\n        \"\"\"\n        window_size = self.window_size\n        B = int(windows.shape[0] / (H * W / window_size / window_size))  # noqa: N806\n        x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n        return x\n\n    def window_partition(self, x):\n        \"\"\"\n        Args:\n            x: (B, H, W, C)\n\n        Returns:\n            tuple: (num_windows*B, window_size, window_size, C)\n        \"\"\"\n        B, H, W, C = x.shape  # noqa: N806\n        window_size = self.window_size\n        x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n        windows = windows.view(-1, window_size, window_size, C)\n        return windows\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.ShiftWindowMSA.window_partition","title":"<code>window_partition(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <p>(B, H, W, C)</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(num_windows*B, window_size, window_size, C)</p> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>def window_partition(self, x):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n\n    Returns:\n        tuple: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape  # noqa: N806\n    window_size = self.window_size\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n    windows = windows.view(-1, window_size, window_size, C)\n    return windows\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.ShiftWindowMSA.window_reverse","title":"<code>window_reverse(windows, H, W)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>windows</code> <p>(num_windows*B, window_size, window_size, C)</p> required <code>H</code> <code>int</code> <p>Height of image</p> required <code>W</code> <code>int</code> <p>Width of image</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(B, H, W, C)</p> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>def window_reverse(self, windows, H, W):  # noqa: N803\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        tuple: (B, H, W, C)\n    \"\"\"\n    window_size = self.window_size\n    B = int(windows.shape[0] / (H * W / window_size / window_size))  # noqa: N806\n    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.SwinBlock","title":"<code>SwinBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The feature dimension.</p> required <code>num_heads</code> <code>int</code> <p>Parallel attention heads.</p> required <code>feedforward_channels</code> <code>int</code> <p>The hidden dimension for Mlps.</p> required <code>window_size</code> <code>int</code> <p>The local window scale. Default: 7.</p> <code>7</code> <code>shift</code> <code>bool</code> <p>whether to shift window or not. Default False.</p> <code>False</code> <code>qkv_bias</code> <code>bool</code> <p>enable bias for qkv if True. Default: True.</p> <code>True</code> <code>qk_scale</code> <code>float | None</code> <p>Override default qk scale of head_dim ** -0.5 if set. Default: None.</p> <code>None</code> <code>drop_rate</code> <code>float</code> <p>Dropout rate. Default: 0.</p> <code>0.0</code> <code>attn_drop_rate</code> <code>float</code> <p>Attention dropout rate. Default: 0.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float</code> <p>Stochastic depth rate. Default: 0.</p> <code>0.0</code> <code>act_cfg</code> <code>dict</code> <p>The config dict of activation function. Default: dict(type='GELU').</p> required <code>norm_cfg</code> <code>dict</code> <p>The config dict of normalization. Default: dict(type='LN').</p> required <code>with_cp</code> <code>bool</code> <p>Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False.</p> <code>False</code> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>class SwinBlock(nn.Module):\n    \"\"\" \n    Args:\n        embed_dim (int): The feature dimension.\n        num_heads (int): Parallel attention heads.\n        feedforward_channels (int): The hidden dimension for Mlps.\n        window_size (int, optional): The local window scale. Default: 7.\n        shift (bool, optional): whether to shift window or not. Default False.\n        qkv_bias (bool, optional): enable bias for qkv if True. Default: True.\n        qk_scale (float | None, optional): Override default qk scale of\n            head_dim ** -0.5 if set. Default: None.\n        drop_rate (float, optional): Dropout rate. Default: 0.\n        attn_drop_rate (float, optional): Attention dropout rate. Default: 0.\n        drop_path_rate (float, optional): Stochastic depth rate. Default: 0.\n        act_cfg (dict, optional): The config dict of activation function.\n            Default: dict(type='GELU').\n        norm_cfg (dict, optional): The config dict of normalization.\n            Default: dict(type='LN').\n        with_cp (bool, optional): Use checkpoint or not. Using checkpoint\n            will save some memory while slowing down the training speed.\n            Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        feedforward_channels,\n        window_size=7,\n        shift=False,  # noqa: FBT002\n        qkv_bias=True,  # noqa: FBT002\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        act_layer: Callable = nn.GELU,\n        norm_layer: Callable = nn.LayerNorm,\n        with_cp=False,  # noqa: FBT002\n    ):\n        super().__init__()\n\n        self.with_cp = with_cp\n\n        self.norm1 = norm_layer(embed_dim)\n        self.attn = ShiftWindowMSA(\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            window_size=window_size,\n            shift_size=window_size // 2 if shift else 0,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop_rate=attn_drop_rate,\n            proj_drop_rate=drop_rate,\n            drop_path_rate=drop_path_rate,\n        )\n\n        self.norm2 = norm_layer(embed_dim)\n        self.ffn = FFN(\n            embed_dim,\n            feedforward_channels,\n            ffn_drop=drop_rate,\n            dropout_layer=DropPath,\n            dropout_arg=drop_path_rate,\n            act_layer=act_layer,\n        )\n\n    def forward(self, x, hw_shape):\n        def _inner_forward(x):\n            identity = x\n            x = self.norm1(x)\n            x = self.attn(x, hw_shape)\n\n            x = x + identity\n\n            identity = x\n            x = self.norm2(x)\n            x = self.ffn(x, identity=identity)\n\n            return x\n\n        if self.with_cp and x.requires_grad:\n            x = cp.checkpoint(_inner_forward, x)\n        else:\n            x = _inner_forward(x)\n\n        return x\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.SwinBlockSequence","title":"<code>SwinBlockSequence</code>","text":"<p>               Bases: <code>Module</code></p> <p>Implements one stage in Swin Transformer.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The feature dimension.</p> required <code>num_heads</code> <code>int</code> <p>Parallel attention heads.</p> required <code>feedforward_channels</code> <code>int</code> <p>The hidden dimension for Mlps.</p> required <code>depth</code> <code>int</code> <p>The number of blocks in this stage.</p> required <code>window_size</code> <code>int</code> <p>The local window scale. Default: 7.</p> <code>7</code> <code>qkv_bias</code> <code>bool</code> <p>enable bias for qkv if True. Default: True.</p> <code>True</code> <code>qk_scale</code> <code>float | None</code> <p>Override default qk scale of head_dim ** -0.5 if set. Default: None.</p> <code>None</code> <code>drop_rate</code> <code>float</code> <p>Dropout rate. Default: 0.</p> <code>0.0</code> <code>attn_drop_rate</code> <code>float</code> <p>Attention dropout rate. Default: 0.</p> <code>0.0</code> <code>drop_path_rate</code> <code>float | list[float]</code> <p>Stochastic depth rate. Default: 0.</p> <code>0.0</code> <code>downsample</code> <code>BaseModule | None</code> <p>The downsample operation module. Default: None.</p> <code>None</code> <code>act_cfg</code> <code>dict</code> <p>The config dict of activation function. Default: dict(type='GELU').</p> required <code>norm_cfg</code> <code>dict</code> <p>The config dict of normalization. Default: dict(type='LN').</p> required <code>with_cp</code> <code>bool</code> <p>Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False.</p> <code>False</code> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>class SwinBlockSequence(nn.Module):\n    \"\"\"Implements one stage in Swin Transformer.\n\n    Args:\n        embed_dim (int): The feature dimension.\n        num_heads (int): Parallel attention heads.\n        feedforward_channels (int): The hidden dimension for Mlps.\n        depth (int): The number of blocks in this stage.\n        window_size (int, optional): The local window scale. Default: 7.\n        qkv_bias (bool, optional): enable bias for qkv if True. Default: True.\n        qk_scale (float | None, optional): Override default qk scale of\n            head_dim ** -0.5 if set. Default: None.\n        drop_rate (float, optional): Dropout rate. Default: 0.\n        attn_drop_rate (float, optional): Attention dropout rate. Default: 0.\n        drop_path_rate (float | list[float], optional): Stochastic depth\n            rate. Default: 0.\n        downsample (BaseModule | None, optional): The downsample operation\n            module. Default: None.\n        act_cfg (dict, optional): The config dict of activation function.\n            Default: dict(type='GELU').\n        norm_cfg (dict, optional): The config dict of normalization.\n            Default: dict(type='LN').\n        with_cp (bool, optional): Use checkpoint or not. Using checkpoint\n            will save some memory while slowing down the training speed.\n            Default: False.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        feedforward_channels,\n        depth,\n        window_size=7,\n        qkv_bias=True,  # noqa: FBT002\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        downsample=None,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        with_cp=False,  # noqa: FBT002\n    ):\n        super().__init__()\n\n        if isinstance(drop_path_rate, list):\n            drop_path_rates = drop_path_rate\n            if not len(drop_path_rates) == depth:\n                msg = \"drop_path_rates must have same len as depth\"\n                raise Exception(msg)\n        else:\n            drop_path_rates = [deepcopy(drop_path_rate) for _ in range(depth)]\n\n        self.blocks = ModuleList()\n        for i in range(depth):\n            block = SwinBlock(\n                embed_dim=embed_dim,\n                num_heads=num_heads,\n                feedforward_channels=feedforward_channels,\n                window_size=window_size,\n                shift=False if i % 2 == 0 else True,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop_rate=drop_rate,\n                attn_drop_rate=attn_drop_rate,\n                drop_path_rate=drop_path_rates[i],\n                act_layer=act_layer,\n                norm_layer=norm_layer,\n                with_cp=with_cp,\n            )\n            self.blocks.append(block)\n\n        self.downsample = downsample\n        self.norm = norm_layer(embed_dim)\n\n    def forward(self, x_hw_shape):\n        if len(x_hw_shape) == 2:  # noqa: PLR2004\n            x, hw_shape = x_hw_shape\n        else:\n            _, _, x, hw_shape = x_hw_shape\n        for block in self.blocks:\n            x = block(x, hw_shape)\n        # RETURN ORDER IS IMPORTANT!! features_only=True from timm will pick the first element of the tuple to return\n        if self.downsample:\n            x_down, down_hw_shape = self.downsample(x, hw_shape)\n            return self.norm(x).view(x.shape[0], *hw_shape, -1), hw_shape, x_down, down_hw_shape\n        else:\n            return self.norm(x).view(x.shape[0], *hw_shape, -1), hw_shape, x, hw_shape\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.WindowMSA","title":"<code>WindowMSA</code>","text":"<p>               Bases: <code>Module</code></p> <p>Window based multi-head self-attention (W-MSA) module with relative position bias.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Number of input channels.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>window_size</code> <code>tuple[int]</code> <p>The height and width of the window.</p> required <code>qkv_bias</code> <code>bool</code> <p>If True, add a learnable bias to q, k, v. Default: True.</p> <code>True</code> <code>qk_scale</code> <code>float | None</code> <p>Override default qk scale of head_dim ** -0.5 if set. Default: None.</p> <code>None</code> <code>attn_drop_rate</code> <code>float</code> <p>Dropout ratio of attention weight. Default: 0.0</p> <code>0.0</code> <code>proj_drop_rate</code> <code>float</code> <p>Dropout ratio of output. Default: 0.</p> <code>0.0</code> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>class WindowMSA(nn.Module):\n    \"\"\"Window based multi-head self-attention (W-MSA) module with relative\n    position bias.\n\n    Args:\n        embed_dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (tuple[int]): The height and width of the window.\n        qkv_bias (bool, optional):  If True, add a learnable bias to q, k, v.\n            Default: True.\n        qk_scale (float | None, optional): Override default qk scale of\n            head_dim ** -0.5 if set. Default: None.\n        attn_drop_rate (float, optional): Dropout ratio of attention weight.\n            Default: 0.0\n        proj_drop_rate (float, optional): Dropout ratio of output. Default: 0.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        window_size,\n        qkv_bias=True,  # noqa: FBT002\n        qk_scale=None,\n        attn_drop_rate=0.0,\n        proj_drop_rate=0.0,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads\n        head_embed_dim = embed_dim // num_heads\n        self.scale = qk_scale or head_embed_dim**-0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n        )  # 2*Wh-1 * 2*Ww-1, nH\n\n        # About 2x faster than original impl\n        Wh, Ww = self.window_size  # noqa: N806\n        rel_index_coords = self.double_step_seq(2 * Ww - 1, Wh, 1, Ww)\n        rel_position_index = rel_index_coords + rel_index_coords.T\n        rel_position_index = rel_position_index.flip(1).contiguous()\n        self.register_buffer(\"relative_position_index\", rel_position_index)\n\n        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop_rate)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.proj_drop = nn.Dropout(proj_drop_rate)\n\n        self.softmax = nn.Softmax(dim=-1)\n\n    def init_weights(self):\n        trunc_normal_(self.relative_position_bias_table, std=0.02)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x (tensor): input features with shape of (num_windows*B, N, C)\n            mask (tensor | None, Optional): mask with shape of (num_windows,\n                Wh*Ww, Wh*Ww), value should be between (-inf, 0].\n        \"\"\"\n        B, N, C = x.shape  # noqa: N806\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        # make torchscript happy (cannot use tensor as tuple)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)\n\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1],\n            self.window_size[0] * self.window_size[1],\n            -1,\n        )  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            nW = mask.shape[0]  # noqa: N806\n            attn = attn.view(B // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n        attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n    @staticmethod\n    def double_step_seq(step1, len1, step2, len2):\n        seq1 = torch.arange(0, step1 * len1, step1)\n        seq2 = torch.arange(0, step2 * len2, step2)\n        return (seq1[:, None] + seq2[None, :]).reshape(1, -1)\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.swin_encoder_decoder.WindowMSA.forward","title":"<code>forward(x, mask=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>tensor</code> <p>input features with shape of (num_windows*B, N, C)</p> required <code>mask</code> <code>(tensor | None, Optional)</code> <p>mask with shape of (num_windows, WhWw, WhWw), value should be between (-inf, 0].</p> <code>None</code> Source code in <code>terratorch/models/backbones/swin_encoder_decoder.py</code> <pre><code>def forward(self, x, mask=None):\n    \"\"\"\n    Args:\n        x (tensor): input features with shape of (num_windows*B, N, C)\n        mask (tensor | None, Optional): mask with shape of (num_windows,\n            Wh*Ww, Wh*Ww), value should be between (-inf, 0].\n    \"\"\"\n    B, N, C = x.shape  # noqa: N806\n    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n    # make torchscript happy (cannot use tensor as tuple)\n    q, k, v = qkv[0], qkv[1], qkv[2]\n\n    q = q * self.scale\n    attn = q @ k.transpose(-2, -1)\n\n    relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n        self.window_size[0] * self.window_size[1],\n        self.window_size[0] * self.window_size[1],\n        -1,\n    )  # Wh*Ww,Wh*Ww,nH\n    relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n    attn = attn + relative_position_bias.unsqueeze(0)\n\n    if mask is not None:\n        nW = mask.shape[0]  # noqa: N806\n        attn = attn.view(B // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n        attn = attn.view(-1, self.num_heads, N, N)\n    attn = self.softmax(attn)\n\n    attn = self.attn_drop(attn)\n\n    x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n    x = self.proj(x)\n    x = self.proj_drop(x)\n    return x\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae","title":"<code>terratorch.models.backbones.prithvi_mae</code>","text":""},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.LocationEncoder","title":"<code>LocationEncoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>class LocationEncoder(nn.Module):\n    def __init__(self, embed_dim: int, trainable_scale: bool = False):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.lat_embed_dim = embed_dim // 2\n        self.lon_embed_dim = embed_dim - self.lat_embed_dim\n\n        # If trainable, initialize scale with small number\n        if trainable_scale:\n            self.scale = nn.Parameter(torch.full((1,), 0.1))\n        else:\n            self.register_buffer('scale', torch.ones(1))\n\n    def forward(self, location_coords: torch.Tensor):\n        \"\"\"\n        location_coords: lat and lon info with shape (B, 2).\n        \"\"\"\n        shape = location_coords.shape[:1] + (1, -1)  # B, 1, -1\n\n        lat = _get_1d_sincos_embed_from_grid_torch(\n                self.lat_embed_dim, location_coords[:, 0].flatten()).reshape(shape)\n        lon = _get_1d_sincos_embed_from_grid_torch(\n                self.lon_embed_dim, location_coords[:, 1].flatten()).reshape(shape)\n\n        embedding = self.scale * torch.cat([lat, lon], dim=-1)\n\n        return embedding  # B, 1, embed_dim\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.LocationEncoder.forward","title":"<code>forward(location_coords)</code>","text":"<p>location_coords: lat and lon info with shape (B, 2).</p> Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>def forward(self, location_coords: torch.Tensor):\n    \"\"\"\n    location_coords: lat and lon info with shape (B, 2).\n    \"\"\"\n    shape = location_coords.shape[:1] + (1, -1)  # B, 1, -1\n\n    lat = _get_1d_sincos_embed_from_grid_torch(\n            self.lat_embed_dim, location_coords[:, 0].flatten()).reshape(shape)\n    lon = _get_1d_sincos_embed_from_grid_torch(\n            self.lon_embed_dim, location_coords[:, 1].flatten()).reshape(shape)\n\n    embedding = self.scale * torch.cat([lat, lon], dim=-1)\n\n    return embedding  # B, 1, embed_dim\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.MAEDecoder","title":"<code>MAEDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Decoder used in the Prithvi MAE</p> Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>class MAEDecoder(nn.Module):\n    \"\"\" Transformer Decoder used in the Prithvi MAE\"\"\"\n    def __init__(self,\n                 patch_size: int | tuple[int, int, int] = (1, 16, 16),\n                 grid_size: list[int] | tuple[int, int, int] = (3, 14, 14),\n                 in_chans: int = 3,\n                 encoder_embed_dim: int = 1024,\n                 decoder_embed_dim: int = 512,\n                 depth: int = 8,\n                 num_heads: int = 16,\n                 mlp_ratio: float = 4.,\n                 norm_layer: nn.Module = nn.LayerNorm,\n                 coords_encoding: list[str] | None = None,\n                 coords_scale_learn: bool = False,\n                 ):\n        super().__init__()\n\n        self.decoder_embed = nn.Linear(encoder_embed_dim, decoder_embed_dim, bias=True)\n        self.decoder_embed_dim = decoder_embed_dim\n        self.grid_size = grid_size\n        if isinstance(patch_size, int):\n            patch_size = (1, patch_size, patch_size)\n        self.patch_size = patch_size\n        self.num_frames = self.grid_size[0] * patch_size[0]\n        num_patches = self.grid_size[0] * self.grid_size[1] * self.grid_size[2]\n\n        # Optional temporal and location embedding\n        coords_encoding = coords_encoding or []\n        self.temporal_encoding = 'time' in coords_encoding\n        self.location_encoding = 'location' in coords_encoding\n        if self.temporal_encoding:\n            self.temporal_embed_dec = TemporalEncoder(decoder_embed_dim, coords_scale_learn)\n        if self.location_encoding:\n            self.location_embed_dec = LocationEncoder(decoder_embed_dim, coords_scale_learn)\n\n        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n\n        self.register_buffer(\"decoder_pos_embed\", torch.zeros(1, num_patches + 1, decoder_embed_dim))\n\n        self.decoder_blocks = nn.ModuleList(\n            [Block(decoder_embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer) for _ in range(depth)]\n        )\n\n        self.decoder_norm = norm_layer(decoder_embed_dim)\n        self.decoder_pred = nn.Linear(decoder_embed_dim,\n                                      patch_size[0] * patch_size[1] * patch_size[2] * in_chans,\n                                      bias=True)\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        # initialize (and freeze) position embeddings by sin-cos embedding\n        decoder_pos_embed = get_3d_sincos_pos_embed(\n            self.decoder_pos_embed.shape[-1], self.grid_size, add_cls_token=True\n        )\n        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n\n        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n        torch.nn.init.normal_(self.mask_token, std=0.02)\n        self.apply(_init_weights)\n\n    def interpolate_pos_encoding(self, sample_shape: tuple[int, int, int]):\n\n        pos_embed = _interpolate_pos_encoding(\n            pos_embed=self.decoder_pos_embed,\n            grid_size=self.grid_size,\n            patch_size=self.patch_size,\n            shape=sample_shape,\n            embed_dim=self.decoder_embed_dim,\n        )\n\n        return pos_embed\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        ids_restore: torch.Tensor,\n        temporal_coords: None | torch.Tensor = None,\n        location_coords: None | torch.Tensor = None,\n        input_size: list[int] = None,\n    ):\n        # embed tokens\n        x = self.decoder_embed(hidden_states)\n        cls_token = x[:, :1, :]\n\n        # append mask tokens to sequence\n        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n        x = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n        # unshuffle\n        x = torch.gather(x, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]).to(x.device))\n\n        # add pos embed\n        decoder_pos_embed = self.interpolate_pos_encoding(input_size[-3:])\n        cls_token = cls_token + decoder_pos_embed[:, :1, :]\n        x = x + decoder_pos_embed[:, 1:, :]\n\n        if self.temporal_encoding and temporal_coords is not None:\n            num_tokens_per_frame = x.shape[1] // self.num_frames\n            temporal_encoding = self.temporal_embed_dec(temporal_coords, num_tokens_per_frame)\n            # Add temporal encoding w/o cls token\n            x = x + temporal_encoding\n        if self.location_encoding and location_coords is not None:\n            location_encoding = self.location_embed_dec(location_coords)\n            # Add location encoding w/o cls token\n            x = x + location_encoding\n\n        # append cls token\n        x = torch.cat([cls_token, x], dim=1)\n\n        # apply Transformer layers (blocks)\n        for block in self.decoder_blocks:\n            x = block(x)\n        x = self.decoder_norm(x)\n\n        # predictor projection\n        pred = self.decoder_pred(x)\n\n        # remove cls token\n        pred = pred[:, 1:, :]\n\n        return pred\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.PatchEmbed","title":"<code>PatchEmbed</code>","text":"<p>               Bases: <code>Module</code></p> <p>3D version of timm.models.vision_transformer.PatchEmbed</p> Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>class PatchEmbed(nn.Module):\n    \"\"\"3D version of timm.models.vision_transformer.PatchEmbed\"\"\"\n    def __init__(\n            self,\n            input_size: tuple[int, int, int] = (1, 224, 224),\n            patch_size: tuple[int, int, int] = (1, 16, 16),\n            in_chans: int = 3,\n            embed_dim: int = 768,\n            norm_layer: nn.Module | None = None,\n            flatten: bool = True,\n            bias: bool = True,\n    ):\n        super().__init__()\n        self.input_size = input_size\n        self.patch_size = patch_size\n        self.grid_size = [s // p for s, p in zip(self.input_size, self.patch_size)]\n        assert self.grid_size &gt;= [1, 1, 1], \"Patch size is bigger than input size.\"\n        self.num_patches = self.grid_size[0] * self.grid_size[1] * self.grid_size[2]\n        self.flatten = flatten\n\n        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n    def forward(self, x):\n        B, C, T, H, W = x.shape\n\n        if T / self.patch_size[0] % 1 or H / self.patch_size[1] % 1 or W / self.patch_size[2] % 1:\n            warnings.warn(f\"Input {x.shape[-3:]} is not divisible by patch size {self.patch_size}.\"\n                          f\"The border will be ignored, add backbone_padding for pixel-wise tasks.\")\n\n        x = self.proj(x)\n        if self.flatten:\n            x = x.flatten(2).transpose(1, 2)  # B,C,T,H,W -&gt; B,C,L -&gt; B,L,C\n        x = self.norm(x)\n        return x\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.PrithviMAE","title":"<code>PrithviMAE</code>","text":"<p>               Bases: <code>Module</code></p> <p>Prithvi Masked Autoencoder</p> Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>class PrithviMAE(nn.Module):\n    \"\"\" Prithvi Masked Autoencoder\"\"\"\n\n    def __init__(self,\n                 img_size: int | tuple[int, int] = 224,\n                 patch_size: int | tuple[int, int, int] = (1, 16, 16),\n                 num_frames: int = 4,\n                 in_chans: int = 6,\n                 embed_dim: int = 768,\n                 depth: int = 12,\n                 num_heads: int = 12,\n                 decoder_embed_dim: int = 512,\n                 decoder_depth: int = 8,\n                 decoder_num_heads: int = 16,\n                 mlp_ratio: float = 4.,\n                 norm_layer: nn.Module = nn.LayerNorm,\n                 norm_pix_loss: bool = False,\n                 coords_encoding: list[str] | None = None,\n                 coords_scale_learn: bool = False,\n                 drop_path: float = 0.,\n                 mask_ratio: float = 0.75,\n                 **kwargs,\n                 ):\n        super().__init__()\n\n        self.encoder = PrithviViT(\n            img_size=img_size,\n            num_frames=num_frames,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            depth=depth,\n            num_heads=num_heads,\n            mlp_ratio=mlp_ratio,\n            norm_layer=norm_layer,\n            coords_encoding=coords_encoding,\n            coords_scale_learn=coords_scale_learn,\n            drop_path=drop_path,\n        )\n\n        self.decoder = MAEDecoder(\n            patch_size=patch_size,\n            grid_size=self.encoder.patch_embed.grid_size,\n            in_chans=in_chans,\n            encoder_embed_dim=embed_dim,\n            decoder_embed_dim=decoder_embed_dim,\n            depth=decoder_depth,\n            num_heads=decoder_num_heads,\n            mlp_ratio=mlp_ratio,\n            norm_layer=norm_layer,\n            coords_encoding=coords_encoding,\n            coords_scale_learn=coords_scale_learn,\n        )\n\n        self.mask_ratio = mask_ratio\n        self.norm_pix_loss = norm_pix_loss\n        self.out_channels = self.encoder.out_channels\n\n    def patchify(self, pixel_values):\n        \"\"\"\n        Args:\n            pixel_values (torch.FloatTensor of shape `(batch_size, num_channels, time, height, width)`):\n                Pixel values.\n\n        Returns:\n            torch.FloatTensor of shape\n                `(batch_size, num_patches, patch_size[0]*patch_size[1]*patch_size[2] * num_channels)`:\n                Patchified pixel values.\n        \"\"\"\n        patch_size_t, patch_size_h, patch_size_w = self.encoder.patch_embed.patch_size\n        num_channels = self.encoder.in_chans\n\n        # patchify\n        patchified_pixel_values = rearrange(pixel_values, 'b c (t s) (h p) (w q) -&gt; b (t h w) (s p q c)',\n                                            c=num_channels, s=patch_size_t, p=patch_size_h, q=patch_size_w)\n\n        return patchified_pixel_values\n\n    def unpatchify(self, patchified_pixel_values, image_size: tuple[int, int] | None = None):\n        \"\"\"\n        Args:\n            patchified_pixel_values (`torch.FloatTensor` of shape\n                `(batch_size, num_patches, patch_size[0]*patch_size[1]*patch_size[2] * num_channels))`:\n                Patchified pixel values.\n            image_size (`tuple[int, int]`, *optional*):\n                Original image size.\n\n        Returns:\n            `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`:\n                Pixel values.\n        \"\"\"\n        patch_size_t, patch_size_h, patch_size_w = self.encoder.patch_embed.patch_size\n        image_size = to_2tuple(image_size) if image_size is not None else self.encoder.img_size\n        original_height, original_width = image_size\n        num_patches_h = original_height // patch_size_h\n        num_patches_w = original_width // patch_size_w\n        num_channels = self.encoder.in_chans\n\n        pixel_values = rearrange(patchified_pixel_values, 'b (t h w) (s p q c) -&gt; b c (t s) (h p) (w q)',\n                                 c=num_channels, h=num_patches_h, w=num_patches_w,\n                                 s=patch_size_t, p=patch_size_h, q=patch_size_w)\n        return pixel_values\n\n    def forward_loss(self, pixel_values, pred, mask):\n        \"\"\"\n        Args:\n            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, time, height, width)`):\n                Pixel values.\n            pred (`torch.FloatTensor` of shape\n                `(batch_size, num_patches, patch_size[0]*patch_size[1]*patch_size[2] * num_channels)`:\n                Predicted pixel values.\n            mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n                Tensor indicating which patches are masked (1) and which are not (0).\n\n        Returns:\n            `torch.FloatTensor`: Pixel reconstruction loss.\n        \"\"\"\n        target = self.patchify(pixel_values)\n        if self.norm_pix_loss:\n            mean = target.mean(dim=-1, keepdim=True)\n            var = target.var(dim=-1, keepdim=True)\n            target = (target - mean) / (var + 1.0e-6) ** 0.5\n\n        loss = (pred - target) ** 2\n        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n        return loss\n\n    def forward(\n        self,\n        pixel_values: torch.Tensor,\n        temporal_coords: None | torch.Tensor = None,\n        location_coords: None | torch.Tensor = None,\n        mask_ratio: float = None,\n    ):\n        if len(pixel_values.shape) == 4 and self.encoder.patch_embed.input_size[0] == 1:\n            # add time dim\n            pixel_values = pixel_values.unsqueeze(2)\n\n        mask_ratio = mask_ratio or self.mask_ratio\n        latent, mask, ids_restore = self.encoder(pixel_values, temporal_coords, location_coords, mask_ratio)\n        pred = self.decoder(latent, ids_restore, temporal_coords, location_coords, input_size=pixel_values.shape)\n        loss = self.forward_loss(pixel_values, pred, mask)\n        return loss, pred, mask\n\n    def forward_features(\n        self,\n        x: torch.Tensor,\n        temporal_coords: None | torch.Tensor = None,\n        location_coords: None | torch.Tensor = None,\n    ) -&gt; list[torch.Tensor]:\n        return self.encoder.forward_features(x, temporal_coords, location_coords)\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.PrithviMAE.forward_loss","title":"<code>forward_loss(pixel_values, pred, mask)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>`torch.FloatTensor` of shape `(batch_size, num_channels, time, height, width)`</code> <p>Pixel values.</p> required <code>mask</code> <code>`torch.FloatTensor` of shape `(batch_size, sequence_length)`</code> <p>Tensor indicating which patches are masked (1) and which are not (0).</p> required <p>Returns:</p> Type Description <p><code>torch.FloatTensor</code>: Pixel reconstruction loss.</p> Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>def forward_loss(self, pixel_values, pred, mask):\n    \"\"\"\n    Args:\n        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, time, height, width)`):\n            Pixel values.\n        pred (`torch.FloatTensor` of shape\n            `(batch_size, num_patches, patch_size[0]*patch_size[1]*patch_size[2] * num_channels)`:\n            Predicted pixel values.\n        mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n            Tensor indicating which patches are masked (1) and which are not (0).\n\n    Returns:\n        `torch.FloatTensor`: Pixel reconstruction loss.\n    \"\"\"\n    target = self.patchify(pixel_values)\n    if self.norm_pix_loss:\n        mean = target.mean(dim=-1, keepdim=True)\n        var = target.var(dim=-1, keepdim=True)\n        target = (target - mean) / (var + 1.0e-6) ** 0.5\n\n    loss = (pred - target) ** 2\n    loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n    loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n    return loss\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.PrithviMAE.patchify","title":"<code>patchify(pixel_values)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>torch.FloatTensor of shape `(batch_size, num_channels, time, height, width)`</code> <p>Pixel values.</p> required <p>Returns:</p> Type Description <p>torch.FloatTensor of shape <code>(batch_size, num_patches, patch_size[0]*patch_size[1]*patch_size[2] * num_channels)</code>: Patchified pixel values.</p> Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>def patchify(self, pixel_values):\n    \"\"\"\n    Args:\n        pixel_values (torch.FloatTensor of shape `(batch_size, num_channels, time, height, width)`):\n            Pixel values.\n\n    Returns:\n        torch.FloatTensor of shape\n            `(batch_size, num_patches, patch_size[0]*patch_size[1]*patch_size[2] * num_channels)`:\n            Patchified pixel values.\n    \"\"\"\n    patch_size_t, patch_size_h, patch_size_w = self.encoder.patch_embed.patch_size\n    num_channels = self.encoder.in_chans\n\n    # patchify\n    patchified_pixel_values = rearrange(pixel_values, 'b c (t s) (h p) (w q) -&gt; b (t h w) (s p q c)',\n                                        c=num_channels, s=patch_size_t, p=patch_size_h, q=patch_size_w)\n\n    return patchified_pixel_values\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.PrithviMAE.unpatchify","title":"<code>unpatchify(patchified_pixel_values, image_size=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>`tuple[int, int]`, *optional*</code> <p>Original image size.</p> <code>None</code> <p>Returns:</p> Type Description <p><code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>: Pixel values.</p> Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>def unpatchify(self, patchified_pixel_values, image_size: tuple[int, int] | None = None):\n    \"\"\"\n    Args:\n        patchified_pixel_values (`torch.FloatTensor` of shape\n            `(batch_size, num_patches, patch_size[0]*patch_size[1]*patch_size[2] * num_channels))`:\n            Patchified pixel values.\n        image_size (`tuple[int, int]`, *optional*):\n            Original image size.\n\n    Returns:\n        `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`:\n            Pixel values.\n    \"\"\"\n    patch_size_t, patch_size_h, patch_size_w = self.encoder.patch_embed.patch_size\n    image_size = to_2tuple(image_size) if image_size is not None else self.encoder.img_size\n    original_height, original_width = image_size\n    num_patches_h = original_height // patch_size_h\n    num_patches_w = original_width // patch_size_w\n    num_channels = self.encoder.in_chans\n\n    pixel_values = rearrange(patchified_pixel_values, 'b (t h w) (s p q c) -&gt; b c (t s) (h p) (w q)',\n                             c=num_channels, h=num_patches_h, w=num_patches_w,\n                             s=patch_size_t, p=patch_size_h, q=patch_size_w)\n    return pixel_values\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.PrithviViT","title":"<code>PrithviViT</code>","text":"<p>               Bases: <code>Module</code></p> <p>Prithvi ViT Encoder</p> Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>class PrithviViT(nn.Module):\n    \"\"\" Prithvi ViT Encoder\"\"\"\n    def __init__(self,\n                 img_size: int | tuple[int, int] = 224,\n                 patch_size: int | tuple[int, int, int] = (1, 16, 16),\n                 num_frames: int = 1,\n                 in_chans: int = 3,\n                 embed_dim: int = 1024,\n                 depth: int = 24,\n                 num_heads: int = 16,\n                 mlp_ratio: float = 4.,\n                 norm_layer: nn.Module = nn.LayerNorm,\n                 coords_encoding: list[str] | None = None,\n                 coords_scale_learn: bool = False,\n                 drop_path: float = 0.,\n                 ** kwargs,\n                ):\n        super().__init__()\n\n        self.in_chans = in_chans\n        self.num_frames = num_frames\n        self.embed_dim = embed_dim\n        self.img_size = to_2tuple(img_size)\n        if isinstance(patch_size, int):\n            patch_size = (1, patch_size, patch_size)\n\n        # 3D patch embedding\n        self.patch_embed = PatchEmbed(\n            input_size=(num_frames,) + self.img_size,\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n        )\n        self.out_channels = [embed_dim * self.patch_embed.grid_size[0]] * depth\n\n        # Optional temporal and location embedding\n        coords_encoding = coords_encoding or []\n        self.temporal_encoding = 'time' in coords_encoding\n        self.location_encoding = 'location' in coords_encoding\n        if self.temporal_encoding:\n            assert patch_size[0] == 1, f\"With temporal encoding, patch_size[0] must be 1, received {patch_size[0]}\"\n            self.temporal_embed_enc = TemporalEncoder(embed_dim, coords_scale_learn)\n        if self.location_encoding:\n            self.location_embed_enc = LocationEncoder(embed_dim, coords_scale_learn)\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.register_buffer(\"pos_embed\", torch.zeros(1, self.patch_embed.num_patches + 1, embed_dim))\n\n        # Transformer layers\n        self.blocks = []\n        for i in range(depth):\n            self.blocks.append(Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer,\n                                     drop_path=drop_path,))\n        self.blocks = nn.ModuleList(self.blocks)\n\n        self.norm = norm_layer(embed_dim)\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        # initialize (and freeze) position embeddings by sin-cos embedding\n        pos_embed = get_3d_sincos_pos_embed(\n            self.pos_embed.shape[-1], self.patch_embed.grid_size, add_cls_token=True\n        )\n        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n\n        # initialize patch_embeddings like nn.Linear (instead of nn.Conv2d)\n        w = self.patch_embed.proj.weight.data\n        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n\n        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n        torch.nn.init.normal_(self.cls_token, std=0.02)\n        self.apply(_init_weights)\n\n    def random_masking(self, sequence, mask_ratio, noise=None):\n        \"\"\"\n        Perform per-sample random masking by per-sample shuffling. Per-sample shuffling is done by argsort random\n        noise.\n\n        Args:\n            sequence (`torch.FloatTensor` of shape `(batch_size, sequence_length, dim)`)\n            mask_ratio (float): mask ratio to use.\n            noise (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) which is\n                mainly used for testing purposes to control randomness and maintain the reproducibility\n        \"\"\"\n        batch_size, seq_length, dim = sequence.shape\n        len_keep = int(seq_length * (1 - mask_ratio))\n\n        if noise is None:\n            noise = torch.rand(batch_size, seq_length, device=sequence.device)  # noise in [0, 1]\n\n        # sort noise for each sample\n        ids_shuffle = torch.argsort(noise, dim=1).to(sequence.device)  # ascend: small is keep, large is remove\n        ids_restore = torch.argsort(ids_shuffle, dim=1).to(sequence.device)\n\n        # keep the first subset\n        ids_keep = ids_shuffle[:, :len_keep]\n        sequence_unmasked = torch.gather(sequence, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, dim))\n\n        # generate the binary mask: 0 is keep, 1 is remove\n        mask = torch.ones([batch_size, seq_length], device=sequence.device)\n        mask[:, :len_keep] = 0\n        # unshuffle to get the binary mask\n        mask = torch.gather(mask, dim=1, index=ids_restore)\n\n        return sequence_unmasked, mask, ids_restore\n\n    def interpolate_pos_encoding(self, sample_shape: tuple[int, int, int]):\n\n        pos_embed = _interpolate_pos_encoding(\n            pos_embed=self.pos_embed,\n            grid_size=self.patch_embed.grid_size,\n            patch_size=self.patch_embed.patch_size,\n            shape=sample_shape,\n            embed_dim=self.embed_dim,\n        )\n        return pos_embed\n\n    def forward(\n        self, x: torch.Tensor,\n        temporal_coords: None | torch.Tensor = None,\n        location_coords: None | torch.Tensor = None,\n        mask_ratio=0.75\n    ):\n        if len(x.shape) == 4 and self.patch_embed.input_size[0] == 1:\n            # add time dim\n            x = x.unsqueeze(2)\n        sample_shape = x.shape[-3:]\n\n        # embed patches\n        x = self.patch_embed(x)\n\n        pos_embed = self.interpolate_pos_encoding(sample_shape)\n        # add pos embed w/o cls token\n        x = x + pos_embed[:, 1:, :]\n\n        if self.temporal_encoding and temporal_coords is not None:\n            num_tokens_per_frame = x.shape[1] // self.num_frames\n            temporal_encoding = self.temporal_embed_enc(temporal_coords, num_tokens_per_frame)\n            x = x + temporal_encoding\n        if self.location_encoding and location_coords is not None:\n            location_encoding = self.location_embed_enc(location_coords)\n            x = x + location_encoding\n\n        # masking: length -&gt; length * mask_ratio\n        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n\n        # append cls token\n        cls_token = self.cls_token + pos_embed[:, :1, :]\n        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        # apply Transformer blocks\n        for block in self.blocks:\n            x = block(x)\n        x = self.norm(x)\n\n        return x, mask, ids_restore\n\n    def forward_features(\n        self,\n        x: torch.Tensor,\n        temporal_coords: None | torch.Tensor = None,\n        location_coords: None | torch.Tensor = None,\n    ) -&gt; list[torch.Tensor]:\n        if len(x.shape) == 4 and self.patch_embed.input_size[0] == 1:\n            # add time dim\n            x = x.unsqueeze(2)\n        sample_shape = x.shape[-3:]\n\n        # embed patches\n        x = self.patch_embed(x)\n\n        pos_embed = self.interpolate_pos_encoding(sample_shape)\n        # add pos embed w/o cls token\n        x = x + pos_embed[:, 1:, :]\n\n        if self.temporal_encoding and temporal_coords is not None:\n            num_tokens_per_frame = x.shape[1] // self.num_frames\n            temporal_encoding = self.temporal_embed_enc(temporal_coords, num_tokens_per_frame)\n            x = x + temporal_encoding\n        if self.location_encoding and location_coords is not None:\n            location_encoding = self.location_embed_enc(location_coords)\n            x = x + location_encoding\n\n        # append cls token\n        cls_token = self.cls_token + pos_embed[:, :1, :]\n        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n\n        # apply Transformer blocks\n        out = []\n        for block in self.blocks:\n            x = block(x)\n            out.append(x.clone())\n\n        x = self.norm(x)\n        out[-1] = x\n        return out\n\n    def prepare_features_for_image_model(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        out = []\n        effective_time_dim = self.patch_embed.input_size[0] // self.patch_embed.patch_size[0]\n        for x in features:\n            x_no_token = x[:, 1:, :]\n            number_of_tokens = x_no_token.shape[1]\n            tokens_per_timestep = number_of_tokens // effective_time_dim\n            h = int(np.sqrt(tokens_per_timestep))\n            encoded = rearrange(\n                x_no_token,\n                \"batch (t h w) e -&gt; batch (t e) h w\",\n                e=self.embed_dim,\n                t=effective_time_dim,\n                h=h,\n            )\n            out.append(encoded)\n        return out\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.PrithviViT.random_masking","title":"<code>random_masking(sequence, mask_ratio, noise=None)</code>","text":"<p>Perform per-sample random masking by per-sample shuffling. Per-sample shuffling is done by argsort random noise.</p> <p>Parameters:</p> Name Type Description Default <code>mask_ratio</code> <code>float</code> <p>mask ratio to use.</p> required Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>def random_masking(self, sequence, mask_ratio, noise=None):\n    \"\"\"\n    Perform per-sample random masking by per-sample shuffling. Per-sample shuffling is done by argsort random\n    noise.\n\n    Args:\n        sequence (`torch.FloatTensor` of shape `(batch_size, sequence_length, dim)`)\n        mask_ratio (float): mask ratio to use.\n        noise (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*) which is\n            mainly used for testing purposes to control randomness and maintain the reproducibility\n    \"\"\"\n    batch_size, seq_length, dim = sequence.shape\n    len_keep = int(seq_length * (1 - mask_ratio))\n\n    if noise is None:\n        noise = torch.rand(batch_size, seq_length, device=sequence.device)  # noise in [0, 1]\n\n    # sort noise for each sample\n    ids_shuffle = torch.argsort(noise, dim=1).to(sequence.device)  # ascend: small is keep, large is remove\n    ids_restore = torch.argsort(ids_shuffle, dim=1).to(sequence.device)\n\n    # keep the first subset\n    ids_keep = ids_shuffle[:, :len_keep]\n    sequence_unmasked = torch.gather(sequence, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, dim))\n\n    # generate the binary mask: 0 is keep, 1 is remove\n    mask = torch.ones([batch_size, seq_length], device=sequence.device)\n    mask[:, :len_keep] = 0\n    # unshuffle to get the binary mask\n    mask = torch.gather(mask, dim=1, index=ids_restore)\n\n    return sequence_unmasked, mask, ids_restore\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.TemporalEncoder","title":"<code>TemporalEncoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>class TemporalEncoder(nn.Module):\n    def __init__(self, embed_dim: int, trainable_scale: bool = False):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.year_embed_dim = embed_dim // 2\n        self.julian_day_embed_dim = embed_dim - self.year_embed_dim\n\n        # If trainable, initialize scale with small number\n        if trainable_scale:\n            self.scale = nn.Parameter(torch.full((1,), 0.1))\n        else:\n            self.register_buffer('scale', torch.ones(1))\n\n    def forward(self, temporal_coords: torch.Tensor, tokens_per_frame: int | None = None):\n        \"\"\"\n        Args:\n            temporal_coords: year and day-of-year info with shape (B, T, 2).\n            tokens_per_frame: number of tokens for each frame in the sample. If provided, embeddings will be\n                repeated over T dimension, and final shape is (B, T*tokens_per_frame, embed_dim).\n        \"\"\"\n        shape = temporal_coords.shape[:2] + (-1,)  # B, T, -1\n\n        year = _get_1d_sincos_embed_from_grid_torch(\n            self.year_embed_dim, temporal_coords[:, :, 0].flatten()).reshape(shape)\n        julian_day = _get_1d_sincos_embed_from_grid_torch(\n            self.julian_day_embed_dim, temporal_coords[:, :, 1].flatten()).reshape(shape)\n\n        embedding = self.scale * torch.cat([year, julian_day], dim=-1)\n\n        if tokens_per_frame is not None:\n            embedding = torch.repeat_interleave(embedding, tokens_per_frame, dim=1)\n\n        return embedding  # B, T*tokens_per_frame, embed_dim\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.TemporalEncoder.forward","title":"<code>forward(temporal_coords, tokens_per_frame=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>temporal_coords</code> <code>Tensor</code> <p>year and day-of-year info with shape (B, T, 2).</p> required <code>tokens_per_frame</code> <code>int | None</code> <p>number of tokens for each frame in the sample. If provided, embeddings will be repeated over T dimension, and final shape is (B, T*tokens_per_frame, embed_dim).</p> <code>None</code> Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>def forward(self, temporal_coords: torch.Tensor, tokens_per_frame: int | None = None):\n    \"\"\"\n    Args:\n        temporal_coords: year and day-of-year info with shape (B, T, 2).\n        tokens_per_frame: number of tokens for each frame in the sample. If provided, embeddings will be\n            repeated over T dimension, and final shape is (B, T*tokens_per_frame, embed_dim).\n    \"\"\"\n    shape = temporal_coords.shape[:2] + (-1,)  # B, T, -1\n\n    year = _get_1d_sincos_embed_from_grid_torch(\n        self.year_embed_dim, temporal_coords[:, :, 0].flatten()).reshape(shape)\n    julian_day = _get_1d_sincos_embed_from_grid_torch(\n        self.julian_day_embed_dim, temporal_coords[:, :, 1].flatten()).reshape(shape)\n\n    embedding = self.scale * torch.cat([year, julian_day], dim=-1)\n\n    if tokens_per_frame is not None:\n        embedding = torch.repeat_interleave(embedding, tokens_per_frame, dim=1)\n\n    return embedding  # B, T*tokens_per_frame, embed_dim\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.get_1d_sincos_pos_embed_from_grid","title":"<code>get_1d_sincos_pos_embed_from_grid(embed_dim, pos)</code>","text":"<p>embed_dim: output dimension for each position pos: a list of positions to be encoded: size (M,) out: (M, D)</p> Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position pos: a list of positions to be encoded: size (M,) out: (M, D)\n    \"\"\"\n    if embed_dim % 2 != 0:\n        raise ValueError(\"embed_dim must be even\")\n\n    omega = np.arange(embed_dim // 2, dtype=float)\n    omega /= embed_dim / 2.0\n    omega = 1.0 / 10000**omega  # (D/2,)\n\n    pos = pos.reshape(-1)  # (M,)\n    out = np.einsum(\"m,d-&gt;md\", pos, omega)  # (M, D/2), outer product\n\n    emb_sin = np.sin(out)  # (M, D/2)\n    emb_cos = np.cos(out)  # (M, D/2)\n\n    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n    return emb\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.prithvi_mae.get_3d_sincos_pos_embed","title":"<code>get_3d_sincos_pos_embed(embed_dim, grid_size, add_cls_token=False)</code>","text":"<p>Create 3D sin/cos positional embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Embedding dimension.</p> required <code>grid_size</code> <code>tuple[int, int, int] | list[int]</code> <p>The grid depth, height and width.</p> required <code>add_cls_token</code> <code>bool, *optional*, defaults to False</code> <p>Whether or not to add a classification (CLS) token.</p> <code>False</code> <p>Returns:</p> Type Description <p>(<code>torch.FloatTensor</code> of shape (grid_size[0]grid_size[1]grid_size[2], embed_dim) or</p> <code>(1 + grid_size[0] * grid_size[1] * grid_size[2], embed_dim)</code> <p>the position embeddings (with or without cls token)</p> Source code in <code>terratorch/models/backbones/prithvi_mae.py</code> <pre><code>def get_3d_sincos_pos_embed(embed_dim, grid_size, add_cls_token=False):\n    \"\"\"\n    Create 3D sin/cos positional embeddings.\n\n    Args:\n        embed_dim (int):\n            Embedding dimension.\n        grid_size (tuple[int, int, int] | list[int]):\n            The grid depth, height and width.\n        add_cls_token (bool, *optional*, defaults to False):\n            Whether or not to add a classification (CLS) token.\n\n    Returns:\n        (`torch.FloatTensor` of shape (grid_size[0]*grid_size[1]*grid_size[2], embed_dim) or\n        (1+grid_size[0]*grid_size[1]*grid_size[2], embed_dim): the position embeddings (with or without cls token)\n    \"\"\"\n\n    assert embed_dim % 16 == 0\n\n    t_size, h_size, w_size = grid_size\n\n    w_embed_dim = embed_dim // 16 * 6\n    h_embed_dim = embed_dim // 16 * 6\n    t_embed_dim = embed_dim // 16 * 4\n\n    w_pos_embed = get_1d_sincos_pos_embed_from_grid(w_embed_dim, np.arange(w_size))\n    h_pos_embed = get_1d_sincos_pos_embed_from_grid(h_embed_dim, np.arange(h_size))\n    t_pos_embed = get_1d_sincos_pos_embed_from_grid(t_embed_dim, np.arange(t_size))\n\n    w_pos_embed = np.tile(w_pos_embed, (t_size * h_size, 1))\n    h_pos_embed = np.tile(np.repeat(h_pos_embed, w_size, axis=0), (t_size, 1))\n    t_pos_embed = np.repeat(t_pos_embed, h_size * w_size, axis=0)\n\n    pos_embed = np.concatenate((w_pos_embed, h_pos_embed, t_pos_embed), axis=1)\n\n    if add_cls_token:\n        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n    return pos_embed\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.unet","title":"<code>terratorch.models.backbones.unet</code>","text":""},{"location":"backbones/#terratorch.models.backbones.unet.UNet","title":"<code>UNet</code>","text":"<p>               Bases: <code>Module</code></p> <p>UNet backbone.</p> <p>This backbone is the implementation of <code>U-Net: Convolutional Networks for Biomedical Image Segmentation &lt;https://arxiv.org/abs/1505.04597&gt;</code>_.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input image channels. Default\" 3.</p> <code>3</code> <code>out_channels</code> <code>int</code> <p>Number of base channels of each stage. The output channels of the first stage. Default: 64.</p> <code>64</code> <code>num_stages</code> <code>int</code> <p>Number of stages in encoder, normally 5. Default: 5.</p> <code>5</code> <code>strides</code> <code>Sequence[int 1 | 2]</code> <p>Strides of each stage in encoder. len(strides) is equal to num_stages. Normally the stride of the first stage in encoder is 1. If strides[i]=2, it uses stride convolution to downsample in the correspondence encoder stage. Default: (1, 1, 1, 1, 1).</p> <code>(1, 1, 1, 1, 1)</code> <code>enc_num_convs</code> <code>Sequence[int]</code> <p>Number of convolutional layers in the convolution block of the correspondence encoder stage. Default: (2, 2, 2, 2, 2).</p> <code>(2, 2, 2, 2, 2)</code> <code>dec_num_convs</code> <code>Sequence[int]</code> <p>Number of convolutional layers in the convolution block of the correspondence decoder stage. Default: (2, 2, 2, 2).</p> <code>(2, 2, 2, 2)</code> <code>downsamples</code> <code>Sequence[int]</code> <p>Whether use MaxPool to downsample the feature map after the first stage of encoder (stages: [1, num_stages)). If the correspondence encoder stage use stride convolution (strides[i]=2), it will never use MaxPool to downsample, even downsamples[i-1]=True. Default: (True, True, True, True).</p> <code>(True, True, True, True)</code> <code>enc_dilations</code> <code>Sequence[int]</code> <p>Dilation rate of each stage in encoder. Default: (1, 1, 1, 1, 1).</p> <code>(1, 1, 1, 1, 1)</code> <code>dec_dilations</code> <code>Sequence[int]</code> <p>Dilation rate of each stage in decoder. Default: (1, 1, 1, 1).</p> <code>(1, 1, 1, 1)</code> <code>with_cp</code> <code>bool</code> <p>Use checkpoint or not. Using checkpoint will save some memory while slowing down the training speed. Default: False.</p> <code>False</code> <code>conv_cfg</code> <code>dict | None</code> <p>Config dict for convolution layer. Default: None.</p> <code>None</code> <code>norm_cfg</code> <code>dict | None</code> <p>Config dict for normalization layer. Default: dict(type='BN').</p> <code>dict(type='BN')</code> <code>act_cfg</code> <code>dict | None</code> <p>Config dict for activation layer in ConvModule. Default: dict(type='ReLU').</p> <code>dict(type='ReLU')</code> <code>upsample_cfg</code> <code>dict</code> <p>The upsample config of the upsample module in decoder. Default: dict(type='InterpConv').</p> <code>None</code> <code>norm_eval</code> <code>bool</code> <p>Whether to set norm layers to eval mode, namely, freeze running stats (mean and var). Note: Effect on Batch Norm and its variants only. Default: False.</p> <code>False</code> <code>dcn</code> <code>bool</code> <p>Use deformable convolution in convolutional layer or not. Default: None.</p> <code>None</code> <code>plugins</code> <code>dict</code> <p>plugins for convolutional layers. Default: None.</p> <code>None</code> <code>pretrained</code> <code>str</code> <p>model pretrained path. Default: None</p> <code>None</code> <code>init_cfg</code> <code>dict or list[dict]</code> <p>Initialization config dict. Default: None</p> <code>None</code> Notice <p>The input image size should be divisible by the whole downsample rate of the encoder. More detail of the whole downsample rate can be found in UNet._check_input_divisible.</p> Source code in <code>terratorch/models/backbones/unet.py</code> <pre><code>@TERRATORCH_BACKBONE_REGISTRY.register\nclass UNet(nn.Module):\n    \"\"\"UNet backbone.\n\n    This backbone is the implementation of `U-Net: Convolutional Networks\n    for Biomedical Image Segmentation &lt;https://arxiv.org/abs/1505.04597&gt;`_.\n\n    Args:\n        in_channels (int): Number of input image channels. Default\" 3.\n        out_channels (int): Number of base channels of each stage.\n            The output channels of the first stage. Default: 64.\n        num_stages (int): Number of stages in encoder, normally 5. Default: 5.\n        strides (Sequence[int 1 | 2]): Strides of each stage in encoder.\n            len(strides) is equal to num_stages. Normally the stride of the\n            first stage in encoder is 1. If strides[i]=2, it uses stride\n            convolution to downsample in the correspondence encoder stage.\n            Default: (1, 1, 1, 1, 1).\n        enc_num_convs (Sequence[int]): Number of convolutional layers in the\n            convolution block of the correspondence encoder stage.\n            Default: (2, 2, 2, 2, 2).\n        dec_num_convs (Sequence[int]): Number of convolutional layers in the\n            convolution block of the correspondence decoder stage.\n            Default: (2, 2, 2, 2).\n        downsamples (Sequence[int]): Whether use MaxPool to downsample the\n            feature map after the first stage of encoder\n            (stages: [1, num_stages)). If the correspondence encoder stage use\n            stride convolution (strides[i]=2), it will never use MaxPool to\n            downsample, even downsamples[i-1]=True.\n            Default: (True, True, True, True).\n        enc_dilations (Sequence[int]): Dilation rate of each stage in encoder.\n            Default: (1, 1, 1, 1, 1).\n        dec_dilations (Sequence[int]): Dilation rate of each stage in decoder.\n            Default: (1, 1, 1, 1).\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed. Default: False.\n        conv_cfg (dict | None): Config dict for convolution layer.\n            Default: None.\n        norm_cfg (dict | None): Config dict for normalization layer.\n            Default: dict(type='BN').\n        act_cfg (dict | None): Config dict for activation layer in ConvModule.\n            Default: dict(type='ReLU').\n        upsample_cfg (dict): The upsample config of the upsample module in\n            decoder. Default: dict(type='InterpConv').\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only. Default: False.\n        dcn (bool): Use deformable convolution in convolutional layer or not.\n            Default: None.\n        plugins (dict): plugins for convolutional layers. Default: None.\n        pretrained (str, optional): model pretrained path. Default: None\n        init_cfg (dict or list[dict], optional): Initialization config dict.\n            Default: None\n\n    Notice:\n        The input image size should be divisible by the whole downsample rate\n        of the encoder. More detail of the whole downsample rate can be found\n        in UNet._check_input_divisible.\n    \"\"\"\n\n    def __init__(self,\n                 in_channels=3,\n                 out_channels=64,\n                 num_stages=5,\n                 strides=(1, 1, 1, 1, 1),\n                 enc_num_convs=(2, 2, 2, 2, 2),\n                 dec_num_convs=(2, 2, 2, 2),\n                 downsamples=(True, True, True, True),\n                 enc_dilations=(1, 1, 1, 1, 1),\n                 dec_dilations=(1, 1, 1, 1),\n                 with_cp=False,\n                 conv_cfg=None,\n                 norm_cfg=dict(type='BN'),\n                 act_cfg=dict(type='ReLU'),\n                 upsample_cfg=None,\n                 norm_eval=False,\n                 dcn=None,\n                 plugins=None,\n                 pretrained=None,\n                 init_cfg=None):\n        super(UNet, self).__init__()\n\n        self.pretrained = pretrained\n        assert not (init_cfg and pretrained), \\\n            'init_cfg and pretrained cannot be setting at the same time'\n        if isinstance(pretrained, str):\n            warnings.warn('DeprecationWarning: pretrained is a deprecated, '\n                          'please use \"init_cfg\" instead')\n            self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)\n        elif pretrained is None:\n            if init_cfg is None:\n                self.init_cfg = [\n                    dict(type='Kaiming', layer='Conv2d'),\n                    dict(\n                        type='Constant',\n                        val=1,\n                        layer=['_BatchNorm', 'GroupNorm'])\n                ]\n        else:\n            raise TypeError('pretrained must be a str or None')\n\n        assert dcn is None, 'Not implemented yet.'\n        assert plugins is None, 'Not implemented yet.'\n        assert len(strides) == num_stages, \\\n            'The length of strides should be equal to num_stages, '\\\n            f'while the strides is {strides}, the length of '\\\n            f'strides is {len(strides)}, and the num_stages is '\\\n            f'{num_stages}.'\n        assert len(enc_num_convs) == num_stages, \\\n            'The length of enc_num_convs should be equal to num_stages, '\\\n            f'while the enc_num_convs is {enc_num_convs}, the length of '\\\n            f'enc_num_convs is {len(enc_num_convs)}, and the num_stages is '\\\n            f'{num_stages}.'\n        assert len(dec_num_convs) == (num_stages-1), \\\n            'The length of dec_num_convs should be equal to (num_stages-1), '\\\n            f'while the dec_num_convs is {dec_num_convs}, the length of '\\\n            f'dec_num_convs is {len(dec_num_convs)}, and the num_stages is '\\\n            f'{num_stages}.'\n        assert len(downsamples) == (num_stages-1), \\\n            'The length of downsamples should be equal to (num_stages-1), '\\\n            f'while the downsamples is {downsamples}, the length of '\\\n            f'downsamples is {len(downsamples)}, and the num_stages is '\\\n            f'{num_stages}.'\n        assert len(enc_dilations) == num_stages, \\\n            'The length of enc_dilations should be equal to num_stages, '\\\n            f'while the enc_dilations is {enc_dilations}, the length of '\\\n            f'enc_dilations is {len(enc_dilations)}, and the num_stages is '\\\n            f'{num_stages}.'\n        assert len(dec_dilations) == (num_stages-1), \\\n            'The length of dec_dilations should be equal to (num_stages-1), '\\\n            f'while the dec_dilations is {dec_dilations}, the length of '\\\n            f'dec_dilations is {len(dec_dilations)}, and the num_stages is '\\\n            f'{num_stages}.'\n        self.num_stages = num_stages\n        self.strides = strides\n        self.downsamples = downsamples\n        self.norm_eval = norm_eval\n        self.out_channels = [out_channels * 2**i for i in reversed(range(num_stages))]\n\n        self.encoder = nn.ModuleList()\n        self.decoder = nn.ModuleList()\n\n        for i in range(num_stages):\n            enc_conv_block = []\n            if i != 0:\n                if strides[i] == 1 and downsamples[i - 1]:\n                    enc_conv_block.append(nn.MaxPool2d(kernel_size=2))\n                upsample = (strides[i] != 1 or downsamples[i - 1])\n                self.decoder.append(\n                    UpConvBlock(\n                        conv_block=BasicConvBlock,\n                        in_channels=out_channels * 2**i,\n                        skip_channels=out_channels * 2**(i - 1),\n                        out_channels=out_channels * 2**(i - 1),\n                        num_convs=dec_num_convs[i - 1],\n                        stride=1,\n                        dilation=dec_dilations[i - 1],\n                        with_cp=with_cp,\n                        conv_cfg=conv_cfg,\n                        norm_cfg=norm_cfg,\n                        act_cfg=act_cfg,\n                        upsample_cfg=upsample_cfg if upsample else None,\n                        dcn=None,\n                        plugins=None))\n\n            enc_conv_block.append(\n                BasicConvBlock(\n                    in_channels=in_channels,\n                    out_channels=out_channels * 2**i,\n                    num_convs=enc_num_convs[i],\n                    stride=strides[i],\n                    dilation=enc_dilations[i],\n                    with_cp=with_cp,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    act_cfg=act_cfg,\n                    dcn=None,\n                    plugins=None))\n            self.encoder.append((nn.Sequential(*enc_conv_block)))\n            in_channels = out_channels * 2**i\n\n    def forward(self, x):\n\n        # We can check just the first image, since the batch \n        # already was approved by the stackability test, which means\n        # all images has the same dimensions. \n        self._check_input_divisible(x[0])\n\n        enc_outs = []\n        for enc in self.encoder:\n            x = enc(x)\n            enc_outs.append(x)\n        dec_outs = [x]\n        for i in reversed(range(len(self.decoder))):\n            x = self.decoder[i](enc_outs[i], x)\n            dec_outs.append(x)\n        return dec_outs\n\n    def train(self, mode=True):\n        \"\"\"Convert the model into training mode while keep normalization layer\n        freezed.\"\"\"\n        super(UNet, self).train(mode)\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, _BatchNorm):\n                    m.eval()\n\n    def _check_input_divisible(self, x):\n        h, w = x.shape[-2:]\n        whole_downsample_rate = 1\n        for i in range(1, self.num_stages):\n            if self.strides[i] == 2 or self.downsamples[i - 1]:\n                whole_downsample_rate *= 2\n        assert (h % whole_downsample_rate == 0) \\\n            and (w % whole_downsample_rate == 0),\\\n            f'The input image size {(h, w)} should be divisible by the whole '\\\n            f'downsample rate {whole_downsample_rate}, when num_stages is '\\\n            f'{self.num_stages}, strides is {self.strides}, and downsamples '\\\n            f'is {self.downsamples}.'\n</code></pre>"},{"location":"backbones/#terratorch.models.backbones.unet.UNet.train","title":"<code>train(mode=True)</code>","text":"<p>Convert the model into training mode while keep normalization layer freezed.</p> Source code in <code>terratorch/models/backbones/unet.py</code> <pre><code>def train(self, mode=True):\n    \"\"\"Convert the model into training mode while keep normalization layer\n    freezed.\"\"\"\n    super(UNet, self).train(mode)\n    if mode and self.norm_eval:\n        for m in self.modules():\n            # trick: eval have effect on BatchNorm only\n            if isinstance(m, _BatchNorm):\n                m.eval()\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#0999post1","title":"0.99.9post1","text":""},{"location":"changelog/#whats-changed","title":"What's Changed","text":"<ul> <li>Changes data documentation by @PedroConrado in https://github.com/IBM/terratorch/pull/437)</li> <li>Adding badges for the README by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/484</li> <li>Update WxCTutorialDownscaling.ipynb for terratorch 0.99.9 support by @romeokienzler in https://github.com/IBM/terratorch/pull/491</li> <li>Skipping automatic tests when the modifications are for documentation and other files outside the core.  by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/464</li> <li>add release scripts by @romeokienzler in https://github.com/IBM/terratorch/pull/489</li> <li>Badge for coverage by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/495</li> <li>Periodical synchronization between documentation from the working branch with <code>main</code> by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/482</li> <li>Basic support for visualizing models (as graphs) by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/459</li> <li>Improve/tests by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/498</li> <li>Updating the tests workflows to install granitewxc and prithviwxc from PyPI. by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/494</li> <li>Removing unnecessary module by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/502</li> <li>allow users to run iterate using terratorch command by @leotizzei in https://github.com/IBM/terratorch/pull/483</li> <li>Update cli_tools.py, make subcommand optional by @romeokienzler in https://github.com/IBM/terratorch/pull/501</li> <li>graph break fix by @kaoutar55 in https://github.com/IBM/terratorch/pull/509</li> </ul>"},{"location":"changelog/#new-contributors","title":"New Contributors","text":"<ul> <li>@leotizzei made their first contribution in https://github.com/IBM/terratorch/pull/483</li> <li>@kaoutar55 made their first contribution in https://github.com/IBM/terratorch/pull/509</li> </ul> <p>Full Changelog: https://github.com/IBM/terratorch/compare/0.99.9...0.99.9post1</p>"},{"location":"changelog/#0999","title":"0.99.9","text":""},{"location":"changelog/#whats-changed_1","title":"What's Changed","text":"<ul> <li>unpin versions, update author list pyproject.toml by @romeokienzler in https://github.com/IBM/terratorch/pull/408</li> <li>Trying to solve issue with replicated input arguments during model (ViT) instantiation/loading by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/415</li> <li>fix-376 by @PedroConrado in https://github.com/IBM/terratorch/pull/412</li> <li>fix padding by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/397</li> <li>Allowing the segmentation task to output multiple class labels by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/393</li> <li>chore: Updated logic to update the new model versions value <code>backbone_pretrained</code> to <code>false</code>  by @WanjiruCate in https://github.com/IBM/terratorch/pull/418</li> <li>add requirements_dist.txt for pypi publishing by @romeokienzler in https://github.com/IBM/terratorch/pull/406</li> <li>add pin_requirements.py for release by @romeokienzler in https://github.com/IBM/terratorch/pull/405</li> <li>add requirements_test.txt needed for running tests by @romeokienzler in https://github.com/IBM/terratorch/pull/407</li> <li>multicrop HF version by @PedroConrado in https://github.com/IBM/terratorch/pull/419</li> <li>adds integration tests for datamodules by @PedroConrado in https://github.com/IBM/terratorch/pull/432</li> <li>Fix padding for decoders by @blumenstiel in https://github.com/IBM/terratorch/pull/439</li> <li>weights_only=True for all the occurences of torch.load by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/391</li> <li>Update CONTRIBUTING.md by @romeokienzler in https://github.com/IBM/terratorch/pull/446</li> <li>Invoking gc for all these tests by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/445</li> <li>fix model for backwards compatibility by @jaionet in https://github.com/IBM/terratorch/pull/443</li> <li>Adjusting the weights keys when necessary by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/417</li> <li>Updating README by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/434</li> <li>Pinning albumentations and updating eurosat by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/452</li> <li>Disabling stackability test when requested by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/451</li> <li>timm must be bounded in some way by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/477</li> <li>Fixing links and cleaning not necessary info by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/466</li> <li>pinning albumentations==1.4.6 by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/463</li> <li>Padding as input transform by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/461</li> <li>Removing hardcoded paths.  by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/458</li> <li>Fixing issues with <code>interpolate_pos_encoding</code> in prithvi by @daniszw in https://github.com/IBM/terratorch/pull/471</li> <li>ModelCheckpoint must be defined in the config dict, not during the parsing. by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/454</li> <li>Dealing with encoder outputs with dimension &gt; 3 when using the reshaper neck.  by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/468</li> <li>[WIP] fix 479 by @romeokienzler in https://github.com/IBM/terratorch/pull/480</li> <li>Removing unnecessary steps and passing extra arguments for tiled inference by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/469</li> <li>201 downscaling by @romeokienzler in https://github.com/IBM/terratorch/pull/472</li> </ul>"},{"location":"changelog/#new-contributors_1","title":"New Contributors","text":"<ul> <li>@WanjiruCate made their first contribution in https://github.com/IBM/terratorch/pull/418</li> <li>@daniszw made their first contribution in https://github.com/IBM/terratorch/pull/471</li> </ul> <p>Full Changelog: https://github.com/IBM/terratorch/compare/0.99.8...0.99.9</p>"},{"location":"changelog/#0998","title":"0.99.8","text":""},{"location":"changelog/#whats-changed_2","title":"What's Changed","text":"<ul> <li>Improve/tests by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/300</li> <li>info not debug by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/306</li> <li>PRs send to <code>dev</code> also must be tested by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/308</li> <li>Base task for terratorch  by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/309</li> <li>Fixing by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/311</li> <li>Bump h5py from 3.10.0 to 3.12.1 by @dependabot in https://github.com/IBM/terratorch/pull/175</li> <li>Bump actions/checkout from 3 to 4 by @dependabot in https://github.com/IBM/terratorch/pull/34</li> <li>Bump actions/setup-python from 4 to 5 by @dependabot in https://github.com/IBM/terratorch/pull/35</li> <li>Fix timm pretrained error by @blumenstiel in https://github.com/IBM/terratorch/pull/318</li> <li>Remove fallback by error with pretrained weights by @blumenstiel in https://github.com/IBM/terratorch/pull/320</li> <li>Fix base task <code>on_test_epoch_end</code> by @fmartiescofet in https://github.com/IBM/terratorch/pull/319</li> <li>Testing finetuning for more Prithvi-2 backbones by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/322</li> <li>Update contribution_process.md by @romeokienzler in https://github.com/IBM/terratorch/pull/326</li> <li>[WIP] Add torchgeo models by @paolofraccaro in https://github.com/IBM/terratorch/pull/233</li> <li>quickfix select_patch_embed_weights.py by @romeokienzler in https://github.com/IBM/terratorch/pull/346</li> <li>Update README.md by @biancazadrozny in https://github.com/IBM/terratorch/pull/353</li> <li>increasing timeout for unit tests and fixing issues with tests by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/351</li> <li>Update README.md by @biancazadrozny in https://github.com/IBM/terratorch/pull/354</li> <li>[WIP] Yaml Schema generation added  by @jaionet in https://github.com/IBM/terratorch/pull/296</li> <li>Fix: Remove duplicated methods by @fmartiescofet in https://github.com/IBM/terratorch/pull/356</li> <li>Feat: Implement Terratorch UNet decoder by @fmartiescofet in https://github.com/IBM/terratorch/pull/357</li> <li>Add deprecation warning for <code>scale_modules</code> by @fmartiescofet in https://github.com/IBM/terratorch/pull/358</li> <li>adds predict to datamodules by @PedroConrado in https://github.com/IBM/terratorch/pull/337</li> <li>Info not debug by @fmartiescofet in https://github.com/IBM/terratorch/pull/360</li> <li>Improve/docs by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/324</li> <li>Fix timm config loading for prithvi by @blumenstiel in https://github.com/IBM/terratorch/pull/372</li> <li>201modularize wxc by @romeokienzler in https://github.com/IBM/terratorch/pull/328</li> <li>Feat: Implement option to have multiple learning rates by @fmartiescofet in https://github.com/IBM/terratorch/pull/329</li> <li>Testing the repository using the installation from pyproject for torchgeo models.  by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/365</li> <li>pinning jsonargparse by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/379</li> <li>Update branch by @blumenstiel in https://github.com/IBM/terratorch/pull/381</li> <li>Refactor prithvi by @blumenstiel in https://github.com/IBM/terratorch/pull/377</li> <li>add smoke.py and an integration tests folder getting picked up by the IBM Research GPU cluster atm by @romeokienzler in https://github.com/IBM/terratorch/pull/347</li> <li>add cli support for wxc gravity wave by @romeokienzler in https://github.com/IBM/terratorch/pull/380</li> <li>Feat: Implement multiple test dataloaders in all tasks by @fmartiescofet in https://github.com/IBM/terratorch/pull/330</li> <li>Feat: Add <code>compute_statistics</code> subcommand by @fmartiescofet in https://github.com/IBM/terratorch/pull/336</li> <li>Adding padding at the input when necessary by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/342</li> <li>Freeze/head/decoder by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/378</li> <li>consolidate requirements by @romeokienzler in https://github.com/IBM/terratorch/pull/389</li> <li>Unpin exact <code>jsonargparse</code> version by @fmartiescofet in https://github.com/IBM/terratorch/pull/394</li> <li>Fix prithvi by @blumenstiel in https://github.com/IBM/terratorch/pull/398</li> <li>Fix/pyproject by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/396</li> </ul>"},{"location":"changelog/#new-contributors_2","title":"New Contributors","text":"<ul> <li>@paolofraccaro made their first contribution in https://github.com/IBM/terratorch/pull/233</li> <li>@jaionet made their first contribution in https://github.com/IBM/terratorch/pull/296</li> </ul> <p>Full Changelog: https://github.com/IBM/terratorch/compare/0.99.8rc1...0.99.8</p>"},{"location":"changelog/#0997","title":"0.99.7","text":""},{"location":"changelog/#whats-changed_3","title":"What's Changed","text":"<ul> <li>Fix: Remove filter warnings in clay by @fmartiescofet in https://github.com/IBM/terratorch/pull/238</li> <li>fix 239 by @romeokienzler in https://github.com/IBM/terratorch/pull/240</li> <li>Refactor: Use fused attention for Clay by @fmartiescofet in https://github.com/IBM/terratorch/pull/248</li> <li>[WIP] fix 247 by @romeokienzler in https://github.com/IBM/terratorch/pull/250</li> <li>fixes typo at sen1floods11 dataset by @PedroConrado in https://github.com/IBM/terratorch/pull/254</li> <li>Fix/checkpoint by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/255</li> <li>Other ways to define custom modules. by @Joao-L-S-Almeida in https://github.com/IBM/terratorch/pull/251</li> </ul> <p>Full Changelog: https://github.com/IBM/terratorch/compare/0.99.6...0.99.7</p>"},{"location":"contributing/","title":"Contributing to TerraTorch","text":"<p>Contributions to an open source project can came in different ways, but we could summarize them in three main components: adding code (as new models, tasks and auxiliary algorithms or even addressing the solution of a bug), examples using the software (scripts, yaml files and notebooks showcasing the package) and documentation. All these ways are valid for TerraTorch and the users are welcome to contribute in any of these fronts. However, some recommendations and rules are necessary in order to facilitate and organize his process. And this is the matter of the next paragraphs. </p>"},{"location":"contributing/#contributing-with-code","title":"Contributing with code","text":"<p>It is not a trivial task to determine how a modification in the source code will impact already implemented and established features, in this way, for any modification in the core source code (<code>terratorch/</code>) we automatically execute a pipeline with hundreds of unit and integration tests to verify that the package have not broken after the modification be merged to <code>main</code>. In this way, when an user wants to modify <code>terratorch</code> for adding new features or bufixes, this are the best practices. </p> <ul> <li>If you are an user outside the IBM org, create a fork to add your modifications. If you are inside the IBM     org or have received writing provileges, create a branch for it. </li> <li>If you are adding new features, we ask you to also add tests for it. These tests are defined in the     directory <code>tests/</code> and are fundamental to check if your feature is working as expected and not breaking     anything. If your feature is something more complex, as a new model or auxiliary algorithm, you can also     (optionally) to add a complete example, as a notebook, demonstrating how the feature works.</li> <li>After finishing your modifications, we recommend you to test locally using <code>pytest</code>, for example:     <pre><code>pytest -s -v tests/\n</code></pre></li> <li>If all the tests are passing, you can open a PR to <code>terratorch:main</code> describing what you are adding and why     that is important to be merged. You     do not need to choose a reviewer, since the maintainers will check the new open PR and request review for it by themselves.  </li> <li>The PR will pass through the tests in GitHub Actions and if the reviewer approve it, it will soon be merged. </li> <li>It is recommended to add a label to your PR. For example <code>bug</code>, when it solves some issue or <code>enhancement</code>     when it adds new features. </li> </ul> <p>NOTICE: The PR will not be merged if the automatic tests are failing and the user which has sent the PR is     responsible for fixing it. </p>"},{"location":"contributing/#contributing-with-documentation","title":"Contributing with documentation","text":"<p>Documentation is core for any project, however, most part of the time, the developers do not have the time (or patience) to carefully document all the codebase, in this way, contributions from interested users are always welcome.  To add documentation to TerraTorch, you need to be familiar with Markdown, a clean markup language, and MkDocs, a framework which relies on Markdown in order to create webpages as this you are reading. </p> <ul> <li>Install the MkDocs dependencies. Install as a developer <code>pip install terratorch[dev]</code> to include them or manually using this list.</li> <li>Clone the branch dedicated to documentation to a local branch: <pre><code>    git fetch origin improve/docs\n</code></pre></li> <li>Add your modifications and open a PR to <code>improve/docs</code>. It is recommended to add the label <code>documentation</code> to your PR. </li> <li>The PR will be reviewed and approved if it is considered relevant by the maintainers. </li> </ul>"},{"location":"data/","title":"Data Processing","text":"<p>In our workflow, we leverage TorchGeo to implement datasets and data modules, ensuring robust and flexible data handling. For a deeper dive into working with datasets using TorchGeo, please refer to the TorchGeo tutorials on datasets.</p> <p>In most cases, it\u2019s best to create a custom TorchGeo dataset tailored to your specific data. Doing so gives you complete control over: - Data Loading: Customize how your data is read and organized. - Transforms: Decide which preprocessing or augmentation steps to apply. - Visualization: Define custom plotting methods (for example, when logging with TensorBoard).</p> <p>TorchGeo offers two primary classes to suit different data formats: - <code>NonGeoDataset</code>:   Use this if your dataset is already split into neatly tiled pieces ready for neural network consumption. Essentially, <code>NonGeoDataset</code> is a wrapper around a standard PyTorch dataset, making it straightforward to integrate into your pipeline. - <code>GeoDataset</code>:   Opt for this class if your data comes in the form of large GeoTiff files from which you need to sample during training. <code>GeoDataset</code> automatically aligns your input data with corresponding labels and supports a range of geo-aware sampling techniques.</p> <p>In addition to these specialized TorchGeo datasets, TerraTorch offers generic datasets and data modules designed to work with directory-based data structures, similar to those used in MMLab libraries. These generic tools simplify data loading when your data is organized in conventional file directories: - The Generic Pixel-wise Dataset is ideal for tasks where each pixel represents a sample (e.g., segmentation or dense prediction problems). - The Generic Scalar Label Dataset is best suited for classification tasks where each sample is associated with a single label.</p> <p>TerraTorch also provides corresponding generic data modules that bundle the dataset with training, validation, and testing splits, integrating seamlessly with PyTorch Lightning. This arrangement makes it easy to manage data loading, batching, and preprocessing with minimal configuration.</p> <p>While generic datasets offer a quick start for common data structures, many projects require more tailored solutions. Custom datasets and data modules give you complete control over the entire data handling process\u2014from fine-tuned data loading and specific transformations to enhanced visualization. By developing your own dataset and data module classes, you ensure that every step\u2014from data ingestion to final model input\u2014is optimized for your particular use case. TerraTorch\u2019s examples provide an excellent starting point to build these custom components and integrate them seamlessly into your training pipeline.</p> <p>For additional examples on fine-tuning a TerraTorch model using these components, please refer to the Prithvi EO Examples repository.</p>"},{"location":"data/#data-curation","title":"Data curation","text":"<p>Generally speaking, all the datamodules work by collecting sets of files and concatenating them into batches with a size determined by the user. TerraTorch automatically checks the dimensionality of the files in order to guarantee that they are stackable, otherwise a stackability error will be raised. If you are sure that your data files are in the proper format and do not want to check for stackability, define <code>check_stackability: false</code> in the field <code>data</code> of your yaml file. If you are using the script interface, you just need to pass it as argument to your dataloader class. Alternatively, if you want to fix discrepancies related to dimensionality in your input files at the data loading stage, you can add a pad correction pipeline, as seen in the example <code>tests/resources/configs/manufactured-finetune_prithvi_eo_v2_300_pad_transform.yaml</code>. </p>"},{"location":"data/#using-datasets-already-implemented-in-torchgeo","title":"Using Datasets already implemented in TorchGeo","text":"<p>Using existing TorchGeo DataModules is very easy! Just plug them in! For instance, to use the <code>EuroSATDataModule</code>, in your config file, set the data as: <pre><code>data:\n  class_path: torchgeo.datamodules.EuroSATDataModule\n  init_args:\n    batch_size: 32\n    num_workers: 8\n  dict_kwargs:\n    root: /dccstor/geofm-pre/EuroSat\n    download: True\n    bands:\n      - B02\n      - B03\n      - B04\n      - B08A\n      - B09\n      - B10\n</code></pre> Modifying each parameter as you see fit.</p> <p>You can also do this outside of config files! Simply instantiate the data module as normal and plug it in.</p> <p>Warning</p> <p>To define <code>transforms</code> to be passed to DataModules from TorchGeo from config files, you must use the following format: <pre><code>data:\nclass_path: terratorch.datamodules.TorchNonGeoDataModule\ninit_args:\n  cls: torchgeo.datamodules.EuroSATDataModule\n  transforms:\n    - class_path: albumentations.augmentations.geometric.resize.Resize\n      init_args:\n        height: 224\n        width: 224\n    - class_path: ToTensorV2\n</code></pre> Note the class_path is <code>TorchNonGeoDataModule</code> and the class to be used is passed through <code>cls</code> (there is also a <code>TorchGeoDataModule</code> for geo modules). This has to be done as the <code>transforms</code> argument is passed through <code>**kwargs</code> in TorchGeo, making it difficult to instantiate with LightningCLI. See more details below.</p>"},{"location":"data/#generic-datasets-and-data-modules","title":"Generic datasets and data modules","text":"<p>For the <code>NonGeoDataset</code> case, we also provide \"generic\" datasets and datamodules. These can be used when you would like to load data from given directories, in a style similar to the MMLab libraries.</p>"},{"location":"data/#custom-datasets-and-data-modules","title":"Custom datasets and data modules","text":"<p>Our custom datasets and data modules are crafted to handle specific data, offering enhanced control and flexibility throughout the workflow.  In case you want to use TerraTorch on your specific data, we invite you to develop your own dataset and data module classes by following the examples below. </p>"},{"location":"data/#transforms","title":"Transforms","text":"<p>The transforms module provides a set of specialized image transformations designed to manipulate spatial, temporal, and multimodal data efficiently.  These transformations allow for greater flexibility when working with multi-temporal, multi-channel, and multi-modal datasets, ensuring that data can be formatted appropriately for different model architectures.</p>"},{"location":"datamodules/","title":"Datamodules","text":""},{"location":"datamodules/#terratorch.datamodules.biomassters","title":"<code>terratorch.datamodules.biomassters</code>","text":""},{"location":"datamodules/#terratorch.datamodules.biomassters.BioMasstersNonGeoDataModule","title":"<code>BioMasstersNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for BioMassters datamodule.</p> Source code in <code>terratorch/datamodules/biomassters.py</code> <pre><code>class BioMasstersNonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for BioMassters datamodule.\"\"\"\n\n    default_metadata_filename = \"The_BioMassters_-_features_metadata.csv.csv\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        bands: dict[str, Sequence[str]] | Sequence[str] = BioMasstersNonGeo.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        drop_last: bool = True,\n        sensors: Sequence[str] = [\"S1\", \"S2\"],\n        as_time_series: bool = False,\n        metadata_filename: str = default_metadata_filename,\n        max_cloud_percentage: float | None = None,\n        max_red_mean: float | None = None,\n        include_corrupt: bool = True,\n        subset: float = 1,\n        seed: int = 42,\n        use_four_frames: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the DataModule for the non-geospatial BioMassters datamodule.\n\n        Args:\n            data_root (str): Root directory containing the dataset.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            bands (dict[str, Sequence[str]] | Sequence[str], optional): Band configuration; either a dict mapping sensors to bands or a list for the first sensor.\n                Defaults to BioMasstersNonGeo.all_band_names\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            aug (AugmentationSequential, optional): Augmentation or normalization to apply. Defaults to normalization if not provided.\n            drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True.\n            sensors (Sequence[str], optional): List of sensors to use (e.g., [\"S1\", \"S2\"]). Defaults to [\"S1\", \"S2\"].\n            as_time_series (bool, optional): Whether to treat data as a time series. Defaults to False.\n            metadata_filename (str, optional): Metadata filename. Defaults to \"The_BioMassters_-_features_metadata.csv.csv\".\n            max_cloud_percentage (float | None, optional): Maximum allowed cloud percentage. Defaults to None.\n            max_red_mean (float | None, optional): Maximum allowed red band mean. Defaults to None.\n            include_corrupt (bool, optional): Whether to include corrupt data. Defaults to True.\n            subset (float, optional): Fraction of the dataset to use. Defaults to 1.\n            seed (int, optional): Random seed for reproducibility. Defaults to 42.\n            use_four_frames (bool, optional): Whether to use a four frames configuration. Defaults to False.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            None.\n        \"\"\"\n        super().__init__(BioMasstersNonGeo, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n        self.sensors = sensors\n        if isinstance(bands, dict):\n            self.bands = bands\n        else:\n            sens = sensors[0]\n            self.bands = {sens: bands}\n\n        self.means = {}\n        self.stds = {}\n        for sensor in self.sensors:\n            self.means[sensor] = [MEANS[sensor][band] for band in self.bands[sensor]]\n            self.stds[sensor] = [STDS[sensor][band] for band in self.bands[sensor]]\n\n        self.mask_mean = MEANS[\"AGBM\"]\n        self.mask_std = STDS[\"AGBM\"]\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        if len(sensors) == 1:\n            self.aug = Normalize(self.means[sensors[0]], self.stds[sensors[0]]) if aug is None else aug\n        else:\n            MultimodalNormalize(self.means, self.stds) if aug is None else aug\n        self.drop_last = drop_last\n        self.as_time_series = as_time_series\n        self.metadata_filename = metadata_filename\n        self.max_cloud_percentage = max_cloud_percentage\n        self.max_red_mean = max_red_mean\n        self.include_corrupt = include_corrupt\n        self.subset = subset\n        self.seed = seed\n        self.use_four_frames = use_four_frames\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                mask_mean=self.mask_mean,\n                mask_std=self.mask_std,\n                sensors=self.sensors,\n                as_time_series=self.as_time_series,\n                metadata_filename=self.metadata_filename,\n                max_cloud_percentage=self.max_cloud_percentage,\n                max_red_mean=self.max_red_mean,\n                include_corrupt=self.include_corrupt,\n                subset=self.subset,\n                seed=self.seed,\n                use_four_frames=self.use_four_frames,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"test\",\n                root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                mask_mean=self.mask_mean,\n                mask_std=self.mask_std,\n                sensors=self.sensors,\n                as_time_series=self.as_time_series,\n                metadata_filename=self.metadata_filename,\n                max_cloud_percentage=self.max_cloud_percentage,\n                max_red_mean=self.max_red_mean,\n                include_corrupt=self.include_corrupt,\n                subset=self.subset,\n                seed=self.seed,\n                use_four_frames=self.use_four_frames,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"test\",\n                root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                mask_mean=self.mask_mean,\n                mask_std=self.mask_std,\n                sensors=self.sensors,\n                as_time_series=self.as_time_series,\n                metadata_filename=self.metadata_filename,\n                max_cloud_percentage=self.max_cloud_percentage,\n                max_red_mean=self.max_red_mean,\n                include_corrupt=self.include_corrupt,\n                subset=self.subset,\n                seed=self.seed,\n                use_four_frames=self.use_four_frames,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"test\",\n                root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                mask_mean=self.mask_mean,\n                mask_std=self.mask_std,\n                sensors=self.sensors,\n                as_time_series=self.as_time_series,\n                metadata_filename=self.metadata_filename,\n                max_cloud_percentage=self.max_cloud_percentage,\n                max_red_mean=self.max_red_mean,\n                include_corrupt=self.include_corrupt,\n                subset=self.subset,\n                seed=self.seed,\n                use_four_frames=self.use_four_frames,\n            )\n\n    def _dataloader_factory(self, split: str):\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n        return DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=split == \"train\",\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            drop_last=split ==\"train\" and self.drop_last,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.biomassters.BioMasstersNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=BioMasstersNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, aug=None, drop_last=True, sensors=['S1', 'S2'], as_time_series=False, metadata_filename=default_metadata_filename, max_cloud_percentage=None, max_red_mean=None, include_corrupt=True, subset=1, seed=42, use_four_frames=False, **kwargs)</code>","text":"<p>Initializes the DataModule for the non-geospatial BioMassters datamodule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory containing the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>dict[str, Sequence[str]] | Sequence[str]</code> <p>Band configuration; either a dict mapping sensors to bands or a list for the first sensor. Defaults to BioMasstersNonGeo.all_band_names</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation or normalization to apply. Defaults to normalization if not provided.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch. Defaults to True.</p> <code>True</code> <code>sensors</code> <code>Sequence[str]</code> <p>List of sensors to use (e.g., [\"S1\", \"S2\"]). Defaults to [\"S1\", \"S2\"].</p> <code>['S1', 'S2']</code> <code>as_time_series</code> <code>bool</code> <p>Whether to treat data as a time series. Defaults to False.</p> <code>False</code> <code>metadata_filename</code> <code>str</code> <p>Metadata filename. Defaults to \"The_BioMassters_-_features_metadata.csv.csv\".</p> <code>default_metadata_filename</code> <code>max_cloud_percentage</code> <code>float | None</code> <p>Maximum allowed cloud percentage. Defaults to None.</p> <code>None</code> <code>max_red_mean</code> <code>float | None</code> <p>Maximum allowed red band mean. Defaults to None.</p> <code>None</code> <code>include_corrupt</code> <code>bool</code> <p>Whether to include corrupt data. Defaults to True.</p> <code>True</code> <code>subset</code> <code>float</code> <p>Fraction of the dataset to use. Defaults to 1.</p> <code>1</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>use_four_frames</code> <code>bool</code> <p>Whether to use a four frames configuration. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>terratorch/datamodules/biomassters.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    bands: dict[str, Sequence[str]] | Sequence[str] = BioMasstersNonGeo.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    drop_last: bool = True,\n    sensors: Sequence[str] = [\"S1\", \"S2\"],\n    as_time_series: bool = False,\n    metadata_filename: str = default_metadata_filename,\n    max_cloud_percentage: float | None = None,\n    max_red_mean: float | None = None,\n    include_corrupt: bool = True,\n    subset: float = 1,\n    seed: int = 42,\n    use_four_frames: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the DataModule for the non-geospatial BioMassters datamodule.\n\n    Args:\n        data_root (str): Root directory containing the dataset.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        bands (dict[str, Sequence[str]] | Sequence[str], optional): Band configuration; either a dict mapping sensors to bands or a list for the first sensor.\n            Defaults to BioMasstersNonGeo.all_band_names\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        aug (AugmentationSequential, optional): Augmentation or normalization to apply. Defaults to normalization if not provided.\n        drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True.\n        sensors (Sequence[str], optional): List of sensors to use (e.g., [\"S1\", \"S2\"]). Defaults to [\"S1\", \"S2\"].\n        as_time_series (bool, optional): Whether to treat data as a time series. Defaults to False.\n        metadata_filename (str, optional): Metadata filename. Defaults to \"The_BioMassters_-_features_metadata.csv.csv\".\n        max_cloud_percentage (float | None, optional): Maximum allowed cloud percentage. Defaults to None.\n        max_red_mean (float | None, optional): Maximum allowed red band mean. Defaults to None.\n        include_corrupt (bool, optional): Whether to include corrupt data. Defaults to True.\n        subset (float, optional): Fraction of the dataset to use. Defaults to 1.\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n        use_four_frames (bool, optional): Whether to use a four frames configuration. Defaults to False.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        None.\n    \"\"\"\n    super().__init__(BioMasstersNonGeo, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n    self.sensors = sensors\n    if isinstance(bands, dict):\n        self.bands = bands\n    else:\n        sens = sensors[0]\n        self.bands = {sens: bands}\n\n    self.means = {}\n    self.stds = {}\n    for sensor in self.sensors:\n        self.means[sensor] = [MEANS[sensor][band] for band in self.bands[sensor]]\n        self.stds[sensor] = [STDS[sensor][band] for band in self.bands[sensor]]\n\n    self.mask_mean = MEANS[\"AGBM\"]\n    self.mask_std = STDS[\"AGBM\"]\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    if len(sensors) == 1:\n        self.aug = Normalize(self.means[sensors[0]], self.stds[sensors[0]]) if aug is None else aug\n    else:\n        MultimodalNormalize(self.means, self.stds) if aug is None else aug\n    self.drop_last = drop_last\n    self.as_time_series = as_time_series\n    self.metadata_filename = metadata_filename\n    self.max_cloud_percentage = max_cloud_percentage\n    self.max_red_mean = max_red_mean\n    self.include_corrupt = include_corrupt\n    self.subset = subset\n    self.seed = seed\n    self.use_four_frames = use_four_frames\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.biomassters.BioMasstersNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/biomassters.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            mask_mean=self.mask_mean,\n            mask_std=self.mask_std,\n            sensors=self.sensors,\n            as_time_series=self.as_time_series,\n            metadata_filename=self.metadata_filename,\n            max_cloud_percentage=self.max_cloud_percentage,\n            max_red_mean=self.max_red_mean,\n            include_corrupt=self.include_corrupt,\n            subset=self.subset,\n            seed=self.seed,\n            use_four_frames=self.use_four_frames,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"test\",\n            root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            mask_mean=self.mask_mean,\n            mask_std=self.mask_std,\n            sensors=self.sensors,\n            as_time_series=self.as_time_series,\n            metadata_filename=self.metadata_filename,\n            max_cloud_percentage=self.max_cloud_percentage,\n            max_red_mean=self.max_red_mean,\n            include_corrupt=self.include_corrupt,\n            subset=self.subset,\n            seed=self.seed,\n            use_four_frames=self.use_four_frames,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"test\",\n            root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            mask_mean=self.mask_mean,\n            mask_std=self.mask_std,\n            sensors=self.sensors,\n            as_time_series=self.as_time_series,\n            metadata_filename=self.metadata_filename,\n            max_cloud_percentage=self.max_cloud_percentage,\n            max_red_mean=self.max_red_mean,\n            include_corrupt=self.include_corrupt,\n            subset=self.subset,\n            seed=self.seed,\n            use_four_frames=self.use_four_frames,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"test\",\n            root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            mask_mean=self.mask_mean,\n            mask_std=self.mask_std,\n            sensors=self.sensors,\n            as_time_series=self.as_time_series,\n            metadata_filename=self.metadata_filename,\n            max_cloud_percentage=self.max_cloud_percentage,\n            max_red_mean=self.max_red_mean,\n            include_corrupt=self.include_corrupt,\n            subset=self.subset,\n            seed=self.seed,\n            use_four_frames=self.use_four_frames,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.burn_intensity","title":"<code>terratorch.datamodules.burn_intensity</code>","text":""},{"location":"datamodules/#terratorch.datamodules.burn_intensity.BurnIntensityNonGeoDataModule","title":"<code>BurnIntensityNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for BurnIntensity datamodule.</p> Source code in <code>terratorch/datamodules/burn_intensity.py</code> <pre><code>class BurnIntensityNonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for BurnIntensity datamodule.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        bands: Sequence[str] = BurnIntensityNonGeo.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        use_full_data: bool = True,\n        no_data_replace: float | None = 0.0001,\n        no_label_replace: int | None = -1,\n        use_metadata: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the DataModule for the BurnIntensity non-geospatial datamodule.\n\n        Args:\n            data_root (str): Root directory of the dataset.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            bands (Sequence[str], optional): List of bands to use. Defaults to BurnIntensityNonGeo.all_band_names.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction.\n            use_full_data (bool, optional): Whether to use the full dataset or data with less than 25 percent zeros. Defaults to True.\n            no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001.\n            no_label_replace (int | None, optional): Value to replace missing labels. Defaults to -1.\n            use_metadata (bool): Whether to return metadata info (time and location).\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(BurnIntensityNonGeo, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n\n        means = [MEANS[b] for b in bands]\n        stds = [STDS[b] for b in bands]\n        self.bands = bands\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.aug = NormalizeWithTimesteps(means, stds)\n        self.use_full_data = use_full_data\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.use_metadata = use_metadata\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                use_full_data=self.use_full_data,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                use_full_data=self.use_full_data,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                use_full_data=self.use_full_data,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                use_full_data=self.use_full_data,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.burn_intensity.BurnIntensityNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=BurnIntensityNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, use_full_data=True, no_data_replace=0.0001, no_label_replace=-1, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the DataModule for the BurnIntensity non-geospatial datamodule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of bands to use. Defaults to BurnIntensityNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction.</p> <code>None</code> <code>use_full_data</code> <code>bool</code> <p>Whether to use the full dataset or data with less than 25 percent zeros. Defaults to True.</p> <code>True</code> <code>no_data_replace</code> <code>float | None</code> <p>Value to replace missing data. Defaults to 0.0001.</p> <code>0.0001</code> <code>no_label_replace</code> <code>int | None</code> <p>Value to replace missing labels. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/burn_intensity.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    bands: Sequence[str] = BurnIntensityNonGeo.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    use_full_data: bool = True,\n    no_data_replace: float | None = 0.0001,\n    no_label_replace: int | None = -1,\n    use_metadata: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the DataModule for the BurnIntensity non-geospatial datamodule.\n\n    Args:\n        data_root (str): Root directory of the dataset.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        bands (Sequence[str], optional): List of bands to use. Defaults to BurnIntensityNonGeo.all_band_names.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction.\n        use_full_data (bool, optional): Whether to use the full dataset or data with less than 25 percent zeros. Defaults to True.\n        no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001.\n        no_label_replace (int | None, optional): Value to replace missing labels. Defaults to -1.\n        use_metadata (bool): Whether to return metadata info (time and location).\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(BurnIntensityNonGeo, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n\n    means = [MEANS[b] for b in bands]\n    stds = [STDS[b] for b in bands]\n    self.bands = bands\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.aug = NormalizeWithTimesteps(means, stds)\n    self.use_full_data = use_full_data\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.use_metadata = use_metadata\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.burn_intensity.BurnIntensityNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/burn_intensity.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            use_full_data=self.use_full_data,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            use_full_data=self.use_full_data,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            use_full_data=self.use_full_data,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            use_full_data=self.use_full_data,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.carbonflux","title":"<code>terratorch.datamodules.carbonflux</code>","text":""},{"location":"datamodules/#terratorch.datamodules.carbonflux.CarbonFluxNonGeoDataModule","title":"<code>CarbonFluxNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Carbon FLux dataset.</p> Source code in <code>terratorch/datamodules/carbonflux.py</code> <pre><code>class CarbonFluxNonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Carbon FLux dataset.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        bands: Sequence[str] = CarbonFluxNonGeo.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        no_data_replace: float | None = 0.0001,\n        use_metadata: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the CarbonFluxNonGeoDataModule.\n\n        Args:\n            data_root (str): Root directory of the dataset.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            bands (Sequence[str], optional): List of bands to use. Defaults to CarbonFluxNonGeo.all_band_names.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            aug (AugmentationSequential, optional): Augmentation sequence; if None, applies multimodal normalization.\n            no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001.\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(CarbonFluxNonGeo, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n\n        means = {\n            m: ([MEANS[m][band] for band in bands] if m == \"image\" else MEANS[m])\n            for m in MEANS.keys()\n        }\n        stds = {\n            m: ([STDS[m][band] for band in bands] if m == \"image\" else STDS[m])\n            for m in STDS.keys()\n        }\n        self.mask_means = MEANS[\"mask\"]\n        self.mask_std = STDS[\"mask\"]\n        self.bands = bands\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.aug = MultimodalNormalize(means, stds) if aug is None else aug\n        self.no_data_replace = no_data_replace\n        self.use_metadata = use_metadata\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                gpp_mean=self.mask_means,\n                gpp_std=self.mask_std,\n                no_data_replace=self.no_data_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                gpp_mean=self.mask_means,\n                gpp_std=self.mask_std,\n                no_data_replace=self.no_data_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                gpp_mean=self.mask_means,\n                gpp_std=self.mask_std,\n                no_data_replace=self.no_data_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                gpp_mean=self.mask_means,\n                gpp_std=self.mask_std,\n                no_data_replace=self.no_data_replace,\n                use_metadata=self.use_metadata,\n            )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.carbonflux.CarbonFluxNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=CarbonFluxNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, aug=None, no_data_replace=0.0001, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the CarbonFluxNonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of bands to use. Defaults to CarbonFluxNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation sequence; if None, applies multimodal normalization.</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Value to replace missing data. Defaults to 0.0001.</p> <code>0.0001</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/carbonflux.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    bands: Sequence[str] = CarbonFluxNonGeo.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    no_data_replace: float | None = 0.0001,\n    use_metadata: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the CarbonFluxNonGeoDataModule.\n\n    Args:\n        data_root (str): Root directory of the dataset.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        bands (Sequence[str], optional): List of bands to use. Defaults to CarbonFluxNonGeo.all_band_names.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        aug (AugmentationSequential, optional): Augmentation sequence; if None, applies multimodal normalization.\n        no_data_replace (float | None, optional): Value to replace missing data. Defaults to 0.0001.\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(CarbonFluxNonGeo, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n\n    means = {\n        m: ([MEANS[m][band] for band in bands] if m == \"image\" else MEANS[m])\n        for m in MEANS.keys()\n    }\n    stds = {\n        m: ([STDS[m][band] for band in bands] if m == \"image\" else STDS[m])\n        for m in STDS.keys()\n    }\n    self.mask_means = MEANS[\"mask\"]\n    self.mask_std = STDS[\"mask\"]\n    self.bands = bands\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.aug = MultimodalNormalize(means, stds) if aug is None else aug\n    self.no_data_replace = no_data_replace\n    self.use_metadata = use_metadata\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.carbonflux.CarbonFluxNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/carbonflux.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            gpp_mean=self.mask_means,\n            gpp_std=self.mask_std,\n            no_data_replace=self.no_data_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            gpp_mean=self.mask_means,\n            gpp_std=self.mask_std,\n            no_data_replace=self.no_data_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            gpp_mean=self.mask_means,\n            gpp_std=self.mask_std,\n            no_data_replace=self.no_data_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            gpp_mean=self.mask_means,\n            gpp_std=self.mask_std,\n            no_data_replace=self.no_data_replace,\n            use_metadata=self.use_metadata,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.forestnet","title":"<code>terratorch.datamodules.forestnet</code>","text":""},{"location":"datamodules/#terratorch.datamodules.forestnet.ForestNetNonGeoDataModule","title":"<code>ForestNetNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Landslide4Sense dataset.</p> Source code in <code>terratorch/datamodules/forestnet.py</code> <pre><code>class ForestNetNonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Landslide4Sense dataset.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        label_map: dict[str, int] = ForestNetNonGeo.default_label_map,\n        bands: Sequence[str] = ForestNetNonGeo.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        fraction: float = 1.0,\n        aug: AugmentationSequential = None,\n        use_metadata: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the ForestNetNonGeoDataModule.\n\n        Args:\n            data_root (str): Directory containing the dataset.\n            batch_size (int, optional): Batch size for data loaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            label_map (dict[str, int], optional): Mapping of labels to integers. Defaults to ForestNetNonGeo.default_label_map.\n            bands (Sequence[str], optional): List of band names to use. Defaults to ForestNetNonGeo.all_band_names.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction.\n            fraction (float, optional): Fraction of data to use. Defaults to 1.0.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline; if None, uses Normalize.\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(ForestNetNonGeo, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n\n        self.means = [MEANS[b] for b in bands]\n        self.stds = [STDS[b] for b in bands]\n        self.label_map = label_map\n        self.bands = bands\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.aug = Normalize(self.means, self.stds) if aug is None else aug\n        self.fraction = fraction\n        self.use_metadata = use_metadata\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                data_root=self.data_root,\n                label_map=self.label_map,\n                transform=self.train_transform,\n                bands=self.bands,\n                fraction=self.fraction,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                label_map=self.label_map,\n                transform=self.val_transform,\n                bands=self.bands,\n                fraction=self.fraction,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                label_map=self.label_map,\n                transform=self.test_transform,\n                bands=self.bands,\n                fraction=self.fraction,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                label_map=self.label_map,\n                transform=self.predict_transform,\n                bands=self.bands,\n                fraction=self.fraction,\n                use_metadata=self.use_metadata,\n            )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.forestnet.ForestNetNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, label_map=ForestNetNonGeo.default_label_map, bands=ForestNetNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, fraction=1.0, aug=None, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the ForestNetNonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Directory containing the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for data loaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>label_map</code> <code>dict[str, int]</code> <p>Mapping of labels to integers. Defaults to ForestNetNonGeo.default_label_map.</p> <code>default_label_map</code> <code>bands</code> <code>Sequence[str]</code> <p>List of band names to use. Defaults to ForestNetNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction.</p> <code>None</code> <code>fraction</code> <code>float</code> <p>Fraction of data to use. Defaults to 1.0.</p> <code>1.0</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline; if None, uses Normalize.</p> <code>None</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/forestnet.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    label_map: dict[str, int] = ForestNetNonGeo.default_label_map,\n    bands: Sequence[str] = ForestNetNonGeo.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    fraction: float = 1.0,\n    aug: AugmentationSequential = None,\n    use_metadata: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the ForestNetNonGeoDataModule.\n\n    Args:\n        data_root (str): Directory containing the dataset.\n        batch_size (int, optional): Batch size for data loaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        label_map (dict[str, int], optional): Mapping of labels to integers. Defaults to ForestNetNonGeo.default_label_map.\n        bands (Sequence[str], optional): List of band names to use. Defaults to ForestNetNonGeo.all_band_names.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction.\n        fraction (float, optional): Fraction of data to use. Defaults to 1.0.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline; if None, uses Normalize.\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(ForestNetNonGeo, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n\n    self.means = [MEANS[b] for b in bands]\n    self.stds = [STDS[b] for b in bands]\n    self.label_map = label_map\n    self.bands = bands\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.aug = Normalize(self.means, self.stds) if aug is None else aug\n    self.fraction = fraction\n    self.use_metadata = use_metadata\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.forestnet.ForestNetNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/forestnet.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            data_root=self.data_root,\n            label_map=self.label_map,\n            transform=self.train_transform,\n            bands=self.bands,\n            fraction=self.fraction,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            label_map=self.label_map,\n            transform=self.val_transform,\n            bands=self.bands,\n            fraction=self.fraction,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            label_map=self.label_map,\n            transform=self.test_transform,\n            bands=self.bands,\n            fraction=self.fraction,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            label_map=self.label_map,\n            transform=self.predict_transform,\n            bands=self.bands,\n            fraction=self.fraction,\n            use_metadata=self.use_metadata,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.fire_scars","title":"<code>terratorch.datamodules.fire_scars</code>","text":""},{"location":"datamodules/#terratorch.datamodules.fire_scars.FireScarsDataModule","title":"<code>FireScarsDataModule</code>","text":"<p>               Bases: <code>GeoDataModule</code></p> <p>Geo Fire Scars data module implementation that merges input data with ground truth segmentation masks.</p> Source code in <code>terratorch/datamodules/fire_scars.py</code> <pre><code>class FireScarsDataModule(GeoDataModule):\n    \"\"\"Geo Fire Scars data module implementation that merges input data with ground truth segmentation masks.\"\"\"\n\n    def __init__(self, data_root: str, **kwargs: Any) -&gt; None:\n        super().__init__(FireScarsSegmentationMask, 4, 224, 100, 0, **kwargs)\n        means = list(MEANS.values())\n        stds = list(STDS.values())\n        self.train_aug = AugmentationSequential(K.RandomCrop(224, 224), K.Normalize(means, stds))\n        self.aug = AugmentationSequential(K.Normalize(means, stds))\n        self.data_root = data_root\n\n    def setup(self, stage: str) -&gt; None:\n        self.images = FireScarsHLS(\n            os.path.join(self.data_root, \"training/\")\n        )\n        self.labels = FireScarsSegmentationMask(\n            os.path.join(self.data_root, \"training/\")\n        )\n        self.dataset = self.images &amp; self.labels\n        self.train_aug = AugmentationSequential(K.RandomCrop(224, 224), K.normalize())\n\n        self.images_test = FireScarsHLS(\n            os.path.join(self.data_root, \"validation/\")\n        )\n        self.labels_test = FireScarsSegmentationMask(\n            os.path.join(self.data_root, \"validation/\")\n        )\n        self.val_dataset = self.images_test &amp; self.labels_test\n\n        if stage in [\"fit\"]:\n            self.train_batch_sampler = RandomBatchGeoSampler(self.dataset, self.patch_size, self.batch_size, None)\n        if stage in [\"fit\", \"validate\"]:\n            self.val_sampler = GridGeoSampler(self.val_dataset, self.patch_size, self.patch_size)\n        if stage in [\"test\"]:\n            self.test_sampler = GridGeoSampler(self.val_dataset, self.patch_size, self.patch_size)\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.fire_scars.FireScarsNonGeoDataModule","title":"<code>FireScarsNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Fire Scars dataset.</p> Source code in <code>terratorch/datamodules/fire_scars.py</code> <pre><code>class FireScarsNonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Fire Scars dataset.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        bands: Sequence[str] = FireScarsNonGeo.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        drop_last: bool = True,\n        no_data_replace: float | None = 0,\n        no_label_replace: int | None = -1,\n        use_metadata: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the FireScarsNonGeoDataModule.\n\n        Args:\n            data_root (str): Root directory of the dataset.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            bands (Sequence[str], optional): List of band names. Defaults to FireScarsNonGeo.all_band_names.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction.\n            drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True.\n            no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0.\n            no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1.\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(FireScarsNonGeo, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n\n        means = [MEANS[b] for b in bands]\n        stds = [STDS[b] for b in bands]\n        self.bands = bands\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.aug = AugmentationSequential(K.Normalize(means, stds), data_keys=[\"image\"])\n        self.drop_last = drop_last\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.use_metadata = use_metadata\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n\n    def _dataloader_factory(self, split: str) -&gt; DataLoader[dict[str, Tensor]]:\n        \"\"\"Implement one or more PyTorch DataLoaders.\n\n        Args:\n            split: Either 'train', 'val', 'test', or 'predict'.\n\n        Returns:\n            A collection of data loaders specifying samples.\n\n        Raises:\n            MisconfigurationException: If :meth:`setup` does not define a\n                dataset or sampler, or if the dataset or sampler has length 0.\n        \"\"\"\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n        return DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=split == \"train\",\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            drop_last=split == \"train\" and self.drop_last,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.fire_scars.FireScarsNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=FireScarsNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, drop_last=True, no_data_replace=0, no_label_replace=-1, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the FireScarsNonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of band names. Defaults to FireScarsNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch. Defaults to True.</p> <code>True</code> <code>no_data_replace</code> <code>float | None</code> <p>Replacement value for missing data. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replacement value for missing labels. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/fire_scars.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    bands: Sequence[str] = FireScarsNonGeo.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    drop_last: bool = True,\n    no_data_replace: float | None = 0,\n    no_label_replace: int | None = -1,\n    use_metadata: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the FireScarsNonGeoDataModule.\n\n    Args:\n        data_root (str): Root directory of the dataset.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        bands (Sequence[str], optional): List of band names. Defaults to FireScarsNonGeo.all_band_names.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction.\n        drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True.\n        no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0.\n        no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1.\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(FireScarsNonGeo, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n\n    means = [MEANS[b] for b in bands]\n    stds = [STDS[b] for b in bands]\n    self.bands = bands\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.aug = AugmentationSequential(K.Normalize(means, stds), data_keys=[\"image\"])\n    self.drop_last = drop_last\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.use_metadata = use_metadata\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.fire_scars.FireScarsNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/fire_scars.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.landslide4sense","title":"<code>terratorch.datamodules.landslide4sense</code>","text":""},{"location":"datamodules/#terratorch.datamodules.landslide4sense.Landslide4SenseNonGeoDataModule","title":"<code>Landslide4SenseNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Landslide4Sense dataset.</p> Source code in <code>terratorch/datamodules/landslide4sense.py</code> <pre><code>class Landslide4SenseNonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Landslide4Sense dataset.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        bands: Sequence[str] = Landslide4SenseNonGeo.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the Landslide4SenseNonGeoDataModule.\n\n        Args:\n            data_root (str): Root directory of the dataset.\n            batch_size (int, optional): Batch size for data loaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            bands (Sequence[str], optional): List of band names to use. Defaults to Landslide4SenseNonGeo.all_band_names.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            aug (AugmentationSequential, optional): Augmentation pipeline; if None, applies normalization using computed means and stds.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(Landslide4SenseNonGeo, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n\n        self.means = [MEANS[b] for b in bands]\n        self.stds = [STDS[b] for b in bands]\n        self.bands = bands\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.aug = (\n            AugmentationSequential(K.Normalize(self.means, self.stds), data_keys=[\"image\"]) if aug is None else aug\n        )\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands\n            )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.landslide4sense.Landslide4SenseNonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=Landslide4SenseNonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, aug=None, **kwargs)</code>","text":"<p>Initializes the Landslide4SenseNonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for data loaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of band names to use. Defaults to Landslide4SenseNonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation pipeline; if None, applies normalization using computed means and stds.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/landslide4sense.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    bands: Sequence[str] = Landslide4SenseNonGeo.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Landslide4SenseNonGeoDataModule.\n\n    Args:\n        data_root (str): Root directory of the dataset.\n        batch_size (int, optional): Batch size for data loaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        bands (Sequence[str], optional): List of band names to use. Defaults to Landslide4SenseNonGeo.all_band_names.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        aug (AugmentationSequential, optional): Augmentation pipeline; if None, applies normalization using computed means and stds.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(Landslide4SenseNonGeo, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n\n    self.means = [MEANS[b] for b in bands]\n    self.stds = [STDS[b] for b in bands]\n    self.bands = bands\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.aug = (\n        AugmentationSequential(K.Normalize(self.means, self.stds), data_keys=[\"image\"]) if aug is None else aug\n    )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.landslide4sense.Landslide4SenseNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/landslide4sense.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_eurosat","title":"<code>terratorch.datamodules.m_eurosat</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_eurosat.MEuroSATNonGeoDataModule","title":"<code>MEuroSATNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-EuroSAT dataset.</p> Source code in <code>terratorch/datamodules/m_eurosat.py</code> <pre><code>class MEuroSATNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-EuroSAT dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MEuroSATNonGeoDataModule for the MEuroSATNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MEuroSATNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            **kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_eurosat.MEuroSATNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MEuroSATNonGeoDataModule for the MEuroSATNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_eurosat.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MEuroSATNonGeoDataModule for the MEuroSATNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MEuroSATNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        **kwargs,\n    )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_bigearthnet","title":"<code>terratorch.datamodules.m_bigearthnet</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_bigearthnet.MBigEarthNonGeoDataModule","title":"<code>MBigEarthNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-BigEarthNet dataset.</p> Source code in <code>terratorch/datamodules/m_bigearthnet.py</code> <pre><code>class MBigEarthNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-BigEarthNet dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MBigEarthNonGeoDataModule for the M-BigEarthNet dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MBigEarthNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            **kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_bigearthnet.MBigEarthNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MBigEarthNonGeoDataModule for the M-BigEarthNet dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_bigearthnet.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MBigEarthNonGeoDataModule for the M-BigEarthNet dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MBigEarthNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        **kwargs,\n    )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_brick_kiln","title":"<code>terratorch.datamodules.m_brick_kiln</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_brick_kiln.MBrickKilnNonGeoDataModule","title":"<code>MBrickKilnNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-BrickKiln dataset.</p> Source code in <code>terratorch/datamodules/m_brick_kiln.py</code> <pre><code>class MBrickKilnNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-BrickKiln dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MBrickKilnNonGeoDataModule for the M-BrickKilnNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MBrickKilnNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            **kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_brick_kiln.MBrickKilnNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MBrickKilnNonGeoDataModule for the M-BrickKilnNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_brick_kiln.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MBrickKilnNonGeoDataModule for the M-BrickKilnNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MBrickKilnNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        **kwargs,\n    )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_forestnet","title":"<code>terratorch.datamodules.m_forestnet</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_forestnet.MForestNetNonGeoDataModule","title":"<code>MForestNetNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-ForestNet dataset.</p> Source code in <code>terratorch/datamodules/m_forestnet.py</code> <pre><code>class MForestNetNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-ForestNet dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,  # noqa: FBT002, FBT001\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MForestNetNonGeoDataModule for the MForestNetNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MForestNetNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            use_metadata=use_metadata,\n            **kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_forestnet.MForestNetNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MForestNetNonGeoDataModule for the MForestNetNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_forestnet.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,  # noqa: FBT002, FBT001\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MForestNetNonGeoDataModule for the MForestNetNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MForestNetNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        use_metadata=use_metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_so2sat","title":"<code>terratorch.datamodules.m_so2sat</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_so2sat.MSo2SatNonGeoDataModule","title":"<code>MSo2SatNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-So2Sat dataset.</p> Source code in <code>terratorch/datamodules/m_so2sat.py</code> <pre><code>class MSo2SatNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-So2Sat dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MSo2SatNonGeoDataModule for the MSo2SatNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MSo2SatNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            **kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_so2sat.MSo2SatNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MSo2SatNonGeoDataModule for the MSo2SatNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_so2sat.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MSo2SatNonGeoDataModule for the MSo2SatNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MSo2SatNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        **kwargs,\n    )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_pv4ger","title":"<code>terratorch.datamodules.m_pv4ger</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_pv4ger.MPv4gerNonGeoDataModule","title":"<code>MPv4gerNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-Pv4ger dataset.</p> Source code in <code>terratorch/datamodules/m_pv4ger.py</code> <pre><code>class MPv4gerNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-Pv4ger dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,  # noqa: FBT002, FBT001\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MPv4gerNonGeoDataModule for the MPv4gerNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MPv4gerNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            use_metadata=use_metadata,\n            **kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_pv4ger.MPv4gerNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MPv4gerNonGeoDataModule for the MPv4gerNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_pv4ger.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,  # noqa: FBT002, FBT001\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MPv4gerNonGeoDataModule for the MPv4gerNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MPv4gerNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        use_metadata=use_metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_cashew_plantation","title":"<code>terratorch.datamodules.m_cashew_plantation</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_cashew_plantation.MBeninSmallHolderCashewsNonGeoDataModule","title":"<code>MBeninSmallHolderCashewsNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-Cashew Plantation dataset.</p> Source code in <code>terratorch/datamodules/m_cashew_plantation.py</code> <pre><code>class MBeninSmallHolderCashewsNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-Cashew Plantation dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,  # noqa: FBT002, FBT001\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MBeninSmallHolderCashewsNonGeoDataModule for the M-BeninSmallHolderCashewsNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MBeninSmallHolderCashewsNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            use_metadata=use_metadata,\n            **kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_cashew_plantation.MBeninSmallHolderCashewsNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MBeninSmallHolderCashewsNonGeoDataModule for the M-BeninSmallHolderCashewsNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_cashew_plantation.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,  # noqa: FBT002, FBT001\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MBeninSmallHolderCashewsNonGeoDataModule for the M-BeninSmallHolderCashewsNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MBeninSmallHolderCashewsNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        use_metadata=use_metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_nz_cattle","title":"<code>terratorch.datamodules.m_nz_cattle</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_nz_cattle.MNzCattleNonGeoDataModule","title":"<code>MNzCattleNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-NZCattle dataset.</p> Source code in <code>terratorch/datamodules/m_nz_cattle.py</code> <pre><code>class MNzCattleNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-NZCattle dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,  # noqa: FBT002, FBT001\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MNzCattleNonGeoDataModule for the MNzCattleNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MNzCattleNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            use_metadata=use_metadata,\n            **kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_nz_cattle.MNzCattleNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MNzCattleNonGeoDataModule for the MNzCattleNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_nz_cattle.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,  # noqa: FBT002, FBT001\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MNzCattleNonGeoDataModule for the MNzCattleNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MNzCattleNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        use_metadata=use_metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_chesapeake_landcover","title":"<code>terratorch.datamodules.m_chesapeake_landcover</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_chesapeake_landcover.MChesapeakeLandcoverNonGeoDataModule","title":"<code>MChesapeakeLandcoverNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-ChesapeakeLandcover dataset.</p> Source code in <code>terratorch/datamodules/m_chesapeake_landcover.py</code> <pre><code>class MChesapeakeLandcoverNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-ChesapeakeLandcover dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MChesapeakeLandcoverNonGeoDataModule for the M-BigEarthNet dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MChesapeakeLandcoverNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            **kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_chesapeake_landcover.MChesapeakeLandcoverNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MChesapeakeLandcoverNonGeoDataModule for the M-BigEarthNet dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_chesapeake_landcover.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MChesapeakeLandcoverNonGeoDataModule for the M-BigEarthNet dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MChesapeakeLandcoverNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        **kwargs,\n    )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_pv4ger_seg","title":"<code>terratorch.datamodules.m_pv4ger_seg</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_pv4ger_seg.MPv4gerSegNonGeoDataModule","title":"<code>MPv4gerSegNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-Pv4gerSeg dataset.</p> Source code in <code>terratorch/datamodules/m_pv4ger_seg.py</code> <pre><code>class MPv4gerSegNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-Pv4gerSeg dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,  # noqa: FBT002, FBT001\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MPv4gerNonGeoDataModule for the MPv4gerSegNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            use_metadata (bool): Whether to return metadata info.\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MPv4gerSegNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            use_metadata=use_metadata,\n            **kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_pv4ger_seg.MPv4gerSegNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MPv4gerNonGeoDataModule for the MPv4gerSegNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_pv4ger_seg.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,  # noqa: FBT002, FBT001\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MPv4gerNonGeoDataModule for the MPv4gerSegNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        use_metadata (bool): Whether to return metadata info.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MPv4gerSegNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        use_metadata=use_metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_SA_crop_type","title":"<code>terratorch.datamodules.m_SA_crop_type</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_SA_crop_type.MSACropTypeNonGeoDataModule","title":"<code>MSACropTypeNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-SA-CropType dataset.</p> Source code in <code>terratorch/datamodules/m_SA_crop_type.py</code> <pre><code>class MSACropTypeNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-SA-CropType dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MSACropTypeNonGeoDataModule for the MSACropTypeNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MSACropTypeNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            **kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_SA_crop_type.MSACropTypeNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MSACropTypeNonGeoDataModule for the MSACropTypeNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_SA_crop_type.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MSACropTypeNonGeoDataModule for the MSACropTypeNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MSACropTypeNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        **kwargs,\n    )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_neontree","title":"<code>terratorch.datamodules.m_neontree</code>","text":""},{"location":"datamodules/#terratorch.datamodules.m_neontree.MNeonTreeNonGeoDataModule","title":"<code>MNeonTreeNonGeoDataModule</code>","text":"<p>               Bases: <code>GeobenchDataModule</code></p> <p>NonGeo LightningDataModule implementation for M-NeonTree dataset.</p> Source code in <code>terratorch/datamodules/m_neontree.py</code> <pre><code>class MNeonTreeNonGeoDataModule(GeobenchDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for M-NeonTree dataset.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        bands: Sequence[str] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        partition: str = \"default\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MNeonTreeNonGeoDataModule for the MNeonTreeNonGeo dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n            aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n            partition (str, optional): Partition size. Defaults to \"default\".\n            **kwargs (Any): Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            MNeonTreeNonGeo,\n            MEANS,\n            STDS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            data_root=data_root,\n            bands=bands,\n            train_transform=train_transform,\n            val_transform=val_transform,\n            test_transform=test_transform,\n            aug=aug,\n            partition=partition,\n            **kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.m_neontree.MNeonTreeNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', bands=None, train_transform=None, val_transform=None, test_transform=None, aug=None, partition='default', **kwargs)</code>","text":"<p>Initializes the MNeonTreeNonGeoDataModule for the MNeonTreeNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>bands</code> <code>Sequence[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation/normalization pipeline. Defaults to None.</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition size. Defaults to \"default\".</p> <code>'default'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/m_neontree.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    bands: Sequence[str] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    partition: str = \"default\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MNeonTreeNonGeoDataModule for the MNeonTreeNonGeo dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        bands (Sequence[str] | None, optional): List of bands to use. Defaults to None.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing.\n        aug (AugmentationSequential, optional): Augmentation/normalization pipeline. Defaults to None.\n        partition (str, optional): Partition size. Defaults to \"default\".\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        MNeonTreeNonGeo,\n        MEANS,\n        STDS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        data_root=data_root,\n        bands=bands,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        aug=aug,\n        partition=partition,\n        **kwargs,\n    )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.multi_temporal_crop_classification","title":"<code>terratorch.datamodules.multi_temporal_crop_classification</code>","text":""},{"location":"datamodules/#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule","title":"<code>MultiTemporalCropClassificationDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for multi-temporal crop classification.</p> Source code in <code>terratorch/datamodules/multi_temporal_crop_classification.py</code> <pre><code>class MultiTemporalCropClassificationDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for multi-temporal crop classification.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        bands: Sequence[str] = MultiTemporalCropClassification.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        drop_last: bool = True,\n        no_data_replace: float | None = 0,\n        no_label_replace: int | None = -1,\n        expand_temporal_dimension: bool = True,\n        reduce_zero_label: bool = True,\n        use_metadata: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the MultiTemporalCropClassificationDataModule for multi-temporal crop classification.\n\n        Args:\n            data_root (str): Directory containing the dataset.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            bands (Sequence[str], optional): List of bands to use. Defaults to MultiTemporalCropClassification.all_band_names.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            drop_last (bool, optional): Whether to drop the last incomplete batch during training. Defaults to True.\n            no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0.\n            no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1.\n            expand_temporal_dimension (bool, optional): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to True.\n            reduce_zero_label (bool, optional): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to True.\n            use_metadata (bool): Whether to return metadata info (time and location).\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(MultiTemporalCropClassification, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n\n        self.means = [MEANS[b] for b in bands]\n        self.stds = [STDS[b] for b in bands]\n        self.bands = bands\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.aug = Normalize(self.means, self.stds)\n        self.drop_last = drop_last\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.expand_temporal_dimension = expand_temporal_dimension\n        self.reduce_zero_label = reduce_zero_label\n        self.use_metadata = use_metadata\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension = self.expand_temporal_dimension,\n                reduce_zero_label = self.reduce_zero_label,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension = self.expand_temporal_dimension,\n                reduce_zero_label = self.reduce_zero_label,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension = self.expand_temporal_dimension,\n                reduce_zero_label = self.reduce_zero_label,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension = self.expand_temporal_dimension,\n                reduce_zero_label = self.reduce_zero_label,\n                use_metadata=self.use_metadata,\n            )\n\n    def _dataloader_factory(self, split: str) -&gt; DataLoader[dict[str, Tensor]]:\n        \"\"\"Implement one or more PyTorch DataLoaders.\n\n        Args:\n            split: Either 'train', 'val', 'test', or 'predict'.\n\n        Returns:\n            A collection of data loaders specifying samples.\n\n        Raises:\n            MisconfigurationException: If :meth:`setup` does not define a\n                dataset or sampler, or if the dataset or sampler has length 0.\n        \"\"\"\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n        return DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=split == \"train\",\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            drop_last=split == \"train\" and self.drop_last,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=MultiTemporalCropClassification.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, drop_last=True, no_data_replace=0, no_label_replace=-1, expand_temporal_dimension=True, reduce_zero_label=True, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the MultiTemporalCropClassificationDataModule for multi-temporal crop classification.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Directory containing the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of bands to use. Defaults to MultiTemporalCropClassification.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch during training. Defaults to True.</p> <code>True</code> <code>no_data_replace</code> <code>float | None</code> <p>Replacement value for missing data. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replacement value for missing labels. Defaults to -1.</p> <code>-1</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True.</p> <code>True</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True.</p> <code>True</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/multi_temporal_crop_classification.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    bands: Sequence[str] = MultiTemporalCropClassification.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    drop_last: bool = True,\n    no_data_replace: float | None = 0,\n    no_label_replace: int | None = -1,\n    expand_temporal_dimension: bool = True,\n    reduce_zero_label: bool = True,\n    use_metadata: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the MultiTemporalCropClassificationDataModule for multi-temporal crop classification.\n\n    Args:\n        data_root (str): Directory containing the dataset.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        bands (Sequence[str], optional): List of bands to use. Defaults to MultiTemporalCropClassification.all_band_names.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        drop_last (bool, optional): Whether to drop the last incomplete batch during training. Defaults to True.\n        no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0.\n        no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1.\n        expand_temporal_dimension (bool, optional): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to True.\n        reduce_zero_label (bool, optional): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to True.\n        use_metadata (bool): Whether to return metadata info (time and location).\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(MultiTemporalCropClassification, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n\n    self.means = [MEANS[b] for b in bands]\n    self.stds = [STDS[b] for b in bands]\n    self.bands = bands\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.aug = Normalize(self.means, self.stds)\n    self.drop_last = drop_last\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.expand_temporal_dimension = expand_temporal_dimension\n    self.reduce_zero_label = reduce_zero_label\n    self.use_metadata = use_metadata\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/multi_temporal_crop_classification.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            expand_temporal_dimension = self.expand_temporal_dimension,\n            reduce_zero_label = self.reduce_zero_label,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            expand_temporal_dimension = self.expand_temporal_dimension,\n            reduce_zero_label = self.reduce_zero_label,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            expand_temporal_dimension = self.expand_temporal_dimension,\n            reduce_zero_label = self.reduce_zero_label,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            expand_temporal_dimension = self.expand_temporal_dimension,\n            reduce_zero_label = self.reduce_zero_label,\n            use_metadata=self.use_metadata,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.open_sentinel_map","title":"<code>terratorch.datamodules.open_sentinel_map</code>","text":""},{"location":"datamodules/#terratorch.datamodules.open_sentinel_map.OpenSentinelMapDataModule","title":"<code>OpenSentinelMapDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Open Sentinel Map.</p> Source code in <code>terratorch/datamodules/open_sentinel_map.py</code> <pre><code>class OpenSentinelMapDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Open Sentinel Map.\"\"\"\n\n    def __init__(\n        self,\n        bands: list[str] | None = None,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        spatial_interpolate_and_stack_temporally: bool = True,  # noqa: FBT001, FBT002\n        pad_image: int | None = None,\n        truncate_image: int | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the OpenSentinelMapDataModule for the Open Sentinel Map dataset.\n\n        Args:\n            bands (list[str] | None, optional): List of bands to use. Defaults to None.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            spatial_interpolate_and_stack_temporally (bool, optional): If True, the bands are interpolated and concatenated over time.\n                Default is True.\n            pad_image (int | None, optional): Number of timesteps to pad the time dimension of the image.\n                If None, no padding is applied.\n            truncate_image (int | None, optional):  Number of timesteps to truncate the time dimension of the image.\n                If None, no truncation is performed.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            OpenSentinelMap,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            **kwargs,\n        )\n        self.bands = bands\n        self.spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally\n        self.pad_image = pad_image\n        self.truncate_image = truncate_image\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.data_root = data_root\n        self.kwargs = kwargs\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = OpenSentinelMap(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n                pad_image = self.pad_image,\n                truncate_image = self.truncate_image,\n                **self.kwargs,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = OpenSentinelMap(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n                pad_image = self.pad_image,\n                truncate_image = self.truncate_image,\n                **self.kwargs,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = OpenSentinelMap(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n                pad_image = self.pad_image,\n                truncate_image = self.truncate_image,\n                **self.kwargs,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = OpenSentinelMap(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n                pad_image = self.pad_image,\n                truncate_image = self.truncate_image,\n                **self.kwargs,\n            )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.open_sentinel_map.OpenSentinelMapDataModule.__init__","title":"<code>__init__(bands=None, batch_size=8, num_workers=0, data_root='./', train_transform=None, val_transform=None, test_transform=None, predict_transform=None, spatial_interpolate_and_stack_temporally=True, pad_image=None, truncate_image=None, **kwargs)</code>","text":"<p>Initializes the OpenSentinelMapDataModule for the Open Sentinel Map dataset.</p> <p>Parameters:</p> Name Type Description Default <code>bands</code> <code>list[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>spatial_interpolate_and_stack_temporally</code> <code>bool</code> <p>If True, the bands are interpolated and concatenated over time. Default is True.</p> <code>True</code> <code>pad_image</code> <code>int | None</code> <p>Number of timesteps to pad the time dimension of the image. If None, no padding is applied.</p> <code>None</code> <code>truncate_image</code> <code>int | None</code> <p>Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/open_sentinel_map.py</code> <pre><code>def __init__(\n    self,\n    bands: list[str] | None = None,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    spatial_interpolate_and_stack_temporally: bool = True,  # noqa: FBT001, FBT002\n    pad_image: int | None = None,\n    truncate_image: int | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the OpenSentinelMapDataModule for the Open Sentinel Map dataset.\n\n    Args:\n        bands (list[str] | None, optional): List of bands to use. Defaults to None.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        spatial_interpolate_and_stack_temporally (bool, optional): If True, the bands are interpolated and concatenated over time.\n            Default is True.\n        pad_image (int | None, optional): Number of timesteps to pad the time dimension of the image.\n            If None, no padding is applied.\n        truncate_image (int | None, optional):  Number of timesteps to truncate the time dimension of the image.\n            If None, no truncation is performed.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        OpenSentinelMap,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        **kwargs,\n    )\n    self.bands = bands\n    self.spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally\n    self.pad_image = pad_image\n    self.truncate_image = truncate_image\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.data_root = data_root\n    self.kwargs = kwargs\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.open_sentinel_map.OpenSentinelMapDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/open_sentinel_map.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = OpenSentinelMap(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n            pad_image = self.pad_image,\n            truncate_image = self.truncate_image,\n            **self.kwargs,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = OpenSentinelMap(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n            pad_image = self.pad_image,\n            truncate_image = self.truncate_image,\n            **self.kwargs,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = OpenSentinelMap(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n            pad_image = self.pad_image,\n            truncate_image = self.truncate_image,\n            **self.kwargs,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = OpenSentinelMap(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            spatial_interpolate_and_stack_temporally = self.spatial_interpolate_and_stack_temporally,\n            pad_image = self.pad_image,\n            truncate_image = self.truncate_image,\n            **self.kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.openearthmap","title":"<code>terratorch.datamodules.openearthmap</code>","text":""},{"location":"datamodules/#terratorch.datamodules.openearthmap.OpenEarthMapNonGeoDataModule","title":"<code>OpenEarthMapNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Open Earth Map.</p> Source code in <code>terratorch/datamodules/openearthmap.py</code> <pre><code>class OpenEarthMapNonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Open Earth Map.\"\"\"\n\n    def __init__(\n        self, \n        batch_size: int = 8, \n        num_workers: int = 0, \n        data_root: str = \"./\",\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        aug: AugmentationSequential = None,\n        **kwargs: Any\n    ) -&gt; None:\n        \"\"\"\n        Initializes the OpenEarthMapNonGeoDataModule for the Open Earth Map dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            aug (AugmentationSequential, optional): Augmentation pipeline; if None, defaults to normalization using computed means and stds.\n            **kwargs: Additional keyword arguments. Can include 'bands' (list[str]) to specify the bands; defaults to OpenEarthMapNonGeo.all_band_names if not provided.\n        \"\"\"\n        super().__init__(OpenEarthMapNonGeo, batch_size, num_workers, **kwargs)\n\n        bands = kwargs.get(\"bands\", OpenEarthMapNonGeo.all_band_names)\n        self.means = torch.tensor([MEANS[b] for b in bands])\n        self.stds = torch.tensor([STDS[b] for b in bands])\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.data_root = data_root\n        self.aug = AugmentationSequential(K.Normalize(self.means, self.stds), data_keys=[\"image\"]) if aug is None else aug\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(  \n                split=\"train\", data_root=self.data_root, transform=self.train_transform, **self.kwargs\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"val\", data_root=self.data_root, transform=self.val_transform, **self.kwargs\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"test\",data_root=self.data_root, transform=self.test_transform, **self.kwargs\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"test\",data_root=self.data_root, transform=self.predict_transform, **self.kwargs\n            )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.openearthmap.OpenEarthMapNonGeoDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', train_transform=None, val_transform=None, test_transform=None, predict_transform=None, aug=None, **kwargs)</code>","text":"<p>Initializes the OpenEarthMapNonGeoDataModule for the Open Earth Map dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for test data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>aug</code> <code>AugmentationSequential</code> <p>Augmentation pipeline; if None, defaults to normalization using computed means and stds.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Can include 'bands' (list[str]) to specify the bands; defaults to OpenEarthMapNonGeo.all_band_names if not provided.</p> <code>{}</code> Source code in <code>terratorch/datamodules/openearthmap.py</code> <pre><code>def __init__(\n    self, \n    batch_size: int = 8, \n    num_workers: int = 0, \n    data_root: str = \"./\",\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    aug: AugmentationSequential = None,\n    **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Initializes the OpenEarthMapNonGeoDataModule for the Open Earth Map dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        aug (AugmentationSequential, optional): Augmentation pipeline; if None, defaults to normalization using computed means and stds.\n        **kwargs: Additional keyword arguments. Can include 'bands' (list[str]) to specify the bands; defaults to OpenEarthMapNonGeo.all_band_names if not provided.\n    \"\"\"\n    super().__init__(OpenEarthMapNonGeo, batch_size, num_workers, **kwargs)\n\n    bands = kwargs.get(\"bands\", OpenEarthMapNonGeo.all_band_names)\n    self.means = torch.tensor([MEANS[b] for b in bands])\n    self.stds = torch.tensor([STDS[b] for b in bands])\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.data_root = data_root\n    self.aug = AugmentationSequential(K.Normalize(self.means, self.stds), data_keys=[\"image\"]) if aug is None else aug\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.openearthmap.OpenEarthMapNonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/openearthmap.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(  \n            split=\"train\", data_root=self.data_root, transform=self.train_transform, **self.kwargs\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"val\", data_root=self.data_root, transform=self.val_transform, **self.kwargs\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"test\",data_root=self.data_root, transform=self.test_transform, **self.kwargs\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"test\",data_root=self.data_root, transform=self.predict_transform, **self.kwargs\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.pastis","title":"<code>terratorch.datamodules.pastis</code>","text":""},{"location":"datamodules/#terratorch.datamodules.pastis.PASTISDataModule","title":"<code>PASTISDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for PASTIS.</p> Source code in <code>terratorch/datamodules/pastis.py</code> <pre><code>class PASTISDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for PASTIS.\"\"\"\n\n    def __init__(\n        self,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        truncate_image: int | None = None,\n        pad_image: int | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the PASTISDataModule for the PASTIS dataset.\n\n        Args:\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Directory containing the dataset. Defaults to \"./\".\n            truncate_image (int, optional): Truncate the time dimension of the image to \n                a specified number of timesteps. If None, no truncation is performed.\n            pad_image (int, optional): Pad the time dimension of the image to a specified \n                number of timesteps. If None, no padding is applied.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            PASTIS,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            **kwargs,\n        )\n        self.truncate_image = truncate_image\n        self.pad_image = pad_image\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.data_root = data_root\n        self.kwargs = kwargs\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = PASTIS(\n                folds=[1, 2, 3],\n                data_root=self.data_root,\n                transform=self.train_transform,\n                truncate_image=self.truncate_image,\n                pad_image=self.pad_image,\n                **self.kwargs,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = PASTIS(\n                folds=[4],\n                data_root=self.data_root,\n                transform=self.val_transform,\n                truncate_image=self.truncate_image,\n                pad_image=self.pad_image,\n                **self.kwargs,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = PASTIS(\n                folds=[5],\n                data_root=self.data_root,\n                transform=self.test_transform,\n                truncate_image=self.truncate_image,\n                pad_image=self.pad_image,\n                **self.kwargs,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = PASTIS(\n                folds=[5],\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                truncate_image=self.truncate_image,\n                pad_image=self.pad_image,\n                **self.kwargs,\n            )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.pastis.PASTISDataModule.__init__","title":"<code>__init__(batch_size=8, num_workers=0, data_root='./', truncate_image=None, pad_image=None, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, **kwargs)</code>","text":"<p>Initializes the PASTISDataModule for the PASTIS dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Directory containing the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>truncate_image</code> <code>int</code> <p>Truncate the time dimension of the image to  a specified number of timesteps. If None, no truncation is performed.</p> <code>None</code> <code>pad_image</code> <code>int</code> <p>Pad the time dimension of the image to a specified  number of timesteps. If None, no padding is applied.</p> <code>None</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for testing data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/pastis.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    truncate_image: int | None = None,\n    pad_image: int | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the PASTISDataModule for the PASTIS dataset.\n\n    Args:\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Directory containing the dataset. Defaults to \"./\".\n        truncate_image (int, optional): Truncate the time dimension of the image to \n            a specified number of timesteps. If None, no truncation is performed.\n        pad_image (int, optional): Pad the time dimension of the image to a specified \n            number of timesteps. If None, no padding is applied.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for testing data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        PASTIS,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        **kwargs,\n    )\n    self.truncate_image = truncate_image\n    self.pad_image = pad_image\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.data_root = data_root\n    self.kwargs = kwargs\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.pastis.PASTISDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/pastis.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = PASTIS(\n            folds=[1, 2, 3],\n            data_root=self.data_root,\n            transform=self.train_transform,\n            truncate_image=self.truncate_image,\n            pad_image=self.pad_image,\n            **self.kwargs,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = PASTIS(\n            folds=[4],\n            data_root=self.data_root,\n            transform=self.val_transform,\n            truncate_image=self.truncate_image,\n            pad_image=self.pad_image,\n            **self.kwargs,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = PASTIS(\n            folds=[5],\n            data_root=self.data_root,\n            transform=self.test_transform,\n            truncate_image=self.truncate_image,\n            pad_image=self.pad_image,\n            **self.kwargs,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = PASTIS(\n            folds=[5],\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            truncate_image=self.truncate_image,\n            pad_image=self.pad_image,\n            **self.kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.sen1floods11","title":"<code>terratorch.datamodules.sen1floods11</code>","text":""},{"location":"datamodules/#terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule","title":"<code>Sen1Floods11NonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Fire Scars.</p> Source code in <code>terratorch/datamodules/sen1floods11.py</code> <pre><code>class Sen1Floods11NonGeoDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Fire Scars.\"\"\"\n\n    def __init__(\n        self,\n        data_root: str,\n        batch_size: int = 4,\n        num_workers: int = 0,\n        bands: Sequence[str] = Sen1Floods11NonGeo.all_band_names,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        drop_last: bool = True,\n        constant_scale: float = 0.0001,\n        no_data_replace: float | None = 0,\n        no_label_replace: int | None = -1,\n        use_metadata: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the Sen1Floods11NonGeoDataModule.\n\n        Args:\n            data_root (str): Root directory of the dataset.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            bands (Sequence[str], optional): List of bands to use. Defaults to Sen1Floods11NonGeo.all_band_names.\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True.\n            constant_scale (float, optional): Scale constant applied to the dataset. Defaults to 0.0001.\n            no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0.\n            no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1.\n            use_metadata (bool): Whether to return metadata info (time and location).\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(Sen1Floods11NonGeo, batch_size, num_workers, **kwargs)\n        self.data_root = data_root\n\n        means = [MEANS[b] for b in bands]\n        stds = [STDS[b] for b in bands]\n        self.bands = bands\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.aug = AugmentationSequential(K.Normalize(means, stds), data_keys=[\"image\"])\n        self.drop_last = drop_last\n        self.constant_scale = constant_scale\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.use_metadata = use_metadata\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                constant_scale=self.constant_scale,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                constant_scale=self.constant_scale,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                constant_scale=self.constant_scale,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = self.dataset_class(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                constant_scale=self.constant_scale,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                use_metadata=self.use_metadata,\n            )\n\n    def _dataloader_factory(self, split: str) -&gt; DataLoader[dict[str, Tensor]]:\n        \"\"\"Implement one or more PyTorch DataLoaders.\n\n        Args:\n            split: Either 'train', 'val', 'test', or 'predict'.\n\n        Returns:\n            A collection of data loaders specifying samples.\n\n        Raises:\n            MisconfigurationException: If :meth:`setup` does not define a\n                dataset or sampler, or if the dataset or sampler has length 0.\n        \"\"\"\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n        return DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=split == \"train\",\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            drop_last=split == \"train\" and self.drop_last,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule.__init__","title":"<code>__init__(data_root, batch_size=4, num_workers=0, bands=Sen1Floods11NonGeo.all_band_names, train_transform=None, val_transform=None, test_transform=None, predict_transform=None, drop_last=True, constant_scale=0.0001, no_data_replace=0, no_label_replace=-1, use_metadata=False, **kwargs)</code>","text":"<p>Initializes the Sen1Floods11NonGeoDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 4.</p> <code>4</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>bands</code> <code>Sequence[str]</code> <p>List of bands to use. Defaults to Sen1Floods11NonGeo.all_band_names.</p> <code>all_band_names</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for test data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>drop_last</code> <code>bool</code> <p>Whether to drop the last incomplete batch. Defaults to True.</p> <code>True</code> <code>constant_scale</code> <code>float</code> <p>Scale constant applied to the dataset. Defaults to 0.0001.</p> <code>0.0001</code> <code>no_data_replace</code> <code>float | None</code> <p>Replacement value for missing data. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replacement value for missing labels. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/sen1floods11.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    batch_size: int = 4,\n    num_workers: int = 0,\n    bands: Sequence[str] = Sen1Floods11NonGeo.all_band_names,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    drop_last: bool = True,\n    constant_scale: float = 0.0001,\n    no_data_replace: float | None = 0,\n    no_label_replace: int | None = -1,\n    use_metadata: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Sen1Floods11NonGeoDataModule.\n\n    Args:\n        data_root (str): Root directory of the dataset.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 4.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        bands (Sequence[str], optional): List of bands to use. Defaults to Sen1Floods11NonGeo.all_band_names.\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        drop_last (bool, optional): Whether to drop the last incomplete batch. Defaults to True.\n        constant_scale (float, optional): Scale constant applied to the dataset. Defaults to 0.0001.\n        no_data_replace (float | None, optional): Replacement value for missing data. Defaults to 0.\n        no_label_replace (int | None, optional): Replacement value for missing labels. Defaults to -1.\n        use_metadata (bool): Whether to return metadata info (time and location).\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(Sen1Floods11NonGeo, batch_size, num_workers, **kwargs)\n    self.data_root = data_root\n\n    means = [MEANS[b] for b in bands]\n    stds = [STDS[b] for b in bands]\n    self.bands = bands\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.aug = AugmentationSequential(K.Normalize(means, stds), data_keys=[\"image\"])\n    self.drop_last = drop_last\n    self.constant_scale = constant_scale\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.use_metadata = use_metadata\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/sen1floods11.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = self.dataset_class(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            constant_scale=self.constant_scale,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = self.dataset_class(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            constant_scale=self.constant_scale,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            constant_scale=self.constant_scale,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = self.dataset_class(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            constant_scale=self.constant_scale,\n            no_data_replace=self.no_data_replace,\n            no_label_replace=self.no_label_replace,\n            use_metadata=self.use_metadata,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.sen4agrinet","title":"<code>terratorch.datamodules.sen4agrinet</code>","text":""},{"location":"datamodules/#terratorch.datamodules.sen4agrinet.Sen4AgriNetDataModule","title":"<code>Sen4AgriNetDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>NonGeo LightningDataModule implementation for Sen4AgriNet.</p> Source code in <code>terratorch/datamodules/sen4agrinet.py</code> <pre><code>class Sen4AgriNetDataModule(NonGeoDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Sen4AgriNet.\"\"\"\n\n    def __init__(\n        self,\n        bands: list[str] | None = None,\n        batch_size: int = 8,\n        num_workers: int = 0,\n        data_root: str = \"./\",\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n        seed: int = 42,\n        scenario: str = \"random\",\n        requires_norm: bool = True,\n        binary_labels: bool = False,\n        linear_encoder: dict = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the Sen4AgriNetDataModule for the Sen4AgriNet dataset.\n\n        Args:\n            bands (list[str] | None, optional): List of bands to use. Defaults to None.\n            batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n            num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n            data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n            train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n            val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n            test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data.\n            predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n            seed (int, optional): Random seed for reproducibility. Defaults to 42.\n            scenario (str): Defines the splitting scenario to use. Options are:\n                - 'random': Random split of the data.\n                - 'spatial': Split by geographical regions (Catalonia and France).\n                - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020).\n            requires_norm (bool, optional): Whether normalization is required. Defaults to True.\n            binary_labels (bool, optional): Whether to use binary labels. Defaults to False.\n            linear_encoder (dict, optional): Mapping for label encoding. Defaults to None.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        super().__init__(\n            Sen4AgriNet,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            **kwargs,\n        )\n        self.bands = bands\n        self.seed = seed\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n        self.predict_transform = wrap_in_compose_is_list(predict_transform)\n        self.data_root = data_root\n        self.scenario = scenario\n        self.requires_norm = requires_norm\n        self.binary_labels = binary_labels\n        self.linear_encoder = linear_encoder\n        self.kwargs = kwargs\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, validate, test, or predict.\n        \"\"\"\n        if stage in [\"fit\"]:\n            self.train_dataset = Sen4AgriNet(\n                split=\"train\",\n                data_root=self.data_root,\n                transform=self.train_transform,\n                bands=self.bands,\n                seed=self.seed,\n                scenario=self.scenario,\n                requires_norm=self.requires_norm,\n                binary_labels=self.binary_labels,\n                linear_encoder=self.linear_encoder,\n                **self.kwargs,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = Sen4AgriNet(\n                split=\"val\",\n                data_root=self.data_root,\n                transform=self.val_transform,\n                bands=self.bands,\n                seed=self.seed,\n                scenario=self.scenario,\n                requires_norm=self.requires_norm,\n                binary_labels=self.binary_labels,\n                linear_encoder=self.linear_encoder,\n                **self.kwargs,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = Sen4AgriNet(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.test_transform,\n                bands=self.bands,\n                seed=self.seed,\n                scenario=self.scenario,\n                requires_norm=self.requires_norm,\n                binary_labels=self.binary_labels,\n                linear_encoder=self.linear_encoder,\n                **self.kwargs,\n            )\n        if stage in [\"predict\"]:\n            self.predict_dataset = Sen4AgriNet(\n                split=\"test\",\n                data_root=self.data_root,\n                transform=self.predict_transform,\n                bands=self.bands,\n                seed=self.seed,\n                scenario=self.scenario,\n                requires_norm=self.requires_norm,\n                binary_labels=self.binary_labels,\n                linear_encoder=self.linear_encoder,\n                **self.kwargs,\n            )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.sen4agrinet.Sen4AgriNetDataModule.__init__","title":"<code>__init__(bands=None, batch_size=8, num_workers=0, data_root='./', train_transform=None, val_transform=None, test_transform=None, predict_transform=None, seed=42, scenario='random', requires_norm=True, binary_labels=False, linear_encoder=None, **kwargs)</code>","text":"<p>Initializes the Sen4AgriNetDataModule for the Sen4AgriNet dataset.</p> <p>Parameters:</p> Name Type Description Default <code>bands</code> <code>list[str] | None</code> <p>List of bands to use. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders. Defaults to 8.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to 0.</p> <code>0</code> <code>data_root</code> <code>str</code> <p>Root directory of the dataset. Defaults to \"./\".</p> <code>'./'</code> <code>train_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for training data.</p> <code>None</code> <code>val_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for validation data.</p> <code>None</code> <code>test_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for test data.</p> <code>None</code> <code>predict_transform</code> <code>Compose | None | list[BasicTransform]</code> <p>Transformations for prediction data.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>scenario</code> <code>str</code> <p>Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020).</p> <code>'random'</code> <code>requires_norm</code> <code>bool</code> <p>Whether normalization is required. Defaults to True.</p> <code>True</code> <code>binary_labels</code> <code>bool</code> <p>Whether to use binary labels. Defaults to False.</p> <code>False</code> <code>linear_encoder</code> <code>dict</code> <p>Mapping for label encoding. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/sen4agrinet.py</code> <pre><code>def __init__(\n    self,\n    bands: list[str] | None = None,\n    batch_size: int = 8,\n    num_workers: int = 0,\n    data_root: str = \"./\",\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    predict_transform: A.Compose | None | list[A.BasicTransform] = None,\n    seed: int = 42,\n    scenario: str = \"random\",\n    requires_norm: bool = True,\n    binary_labels: bool = False,\n    linear_encoder: dict = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initializes the Sen4AgriNetDataModule for the Sen4AgriNet dataset.\n\n    Args:\n        bands (list[str] | None, optional): List of bands to use. Defaults to None.\n        batch_size (int, optional): Batch size for DataLoaders. Defaults to 8.\n        num_workers (int, optional): Number of workers for data loading. Defaults to 0.\n        data_root (str, optional): Root directory of the dataset. Defaults to \"./\".\n        train_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for training data.\n        val_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for validation data.\n        test_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for test data.\n        predict_transform (A.Compose | None | list[A.BasicTransform], optional): Transformations for prediction data.\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n        scenario (str): Defines the splitting scenario to use. Options are:\n            - 'random': Random split of the data.\n            - 'spatial': Split by geographical regions (Catalonia and France).\n            - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020).\n        requires_norm (bool, optional): Whether normalization is required. Defaults to True.\n        binary_labels (bool, optional): Whether to use binary labels. Defaults to False.\n        linear_encoder (dict, optional): Mapping for label encoding. Defaults to None.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    super().__init__(\n        Sen4AgriNet,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        **kwargs,\n    )\n    self.bands = bands\n    self.seed = seed\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n    self.predict_transform = wrap_in_compose_is_list(predict_transform)\n    self.data_root = data_root\n    self.scenario = scenario\n    self.requires_norm = requires_norm\n    self.binary_labels = binary_labels\n    self.linear_encoder = linear_encoder\n    self.kwargs = kwargs\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.sen4agrinet.Sen4AgriNetDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, validate, test, or predict.</p> required Source code in <code>terratorch/datamodules/sen4agrinet.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, validate, test, or predict.\n    \"\"\"\n    if stage in [\"fit\"]:\n        self.train_dataset = Sen4AgriNet(\n            split=\"train\",\n            data_root=self.data_root,\n            transform=self.train_transform,\n            bands=self.bands,\n            seed=self.seed,\n            scenario=self.scenario,\n            requires_norm=self.requires_norm,\n            binary_labels=self.binary_labels,\n            linear_encoder=self.linear_encoder,\n            **self.kwargs,\n        )\n    if stage in [\"fit\", \"validate\"]:\n        self.val_dataset = Sen4AgriNet(\n            split=\"val\",\n            data_root=self.data_root,\n            transform=self.val_transform,\n            bands=self.bands,\n            seed=self.seed,\n            scenario=self.scenario,\n            requires_norm=self.requires_norm,\n            binary_labels=self.binary_labels,\n            linear_encoder=self.linear_encoder,\n            **self.kwargs,\n        )\n    if stage in [\"test\"]:\n        self.test_dataset = Sen4AgriNet(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.test_transform,\n            bands=self.bands,\n            seed=self.seed,\n            scenario=self.scenario,\n            requires_norm=self.requires_norm,\n            binary_labels=self.binary_labels,\n            linear_encoder=self.linear_encoder,\n            **self.kwargs,\n        )\n    if stage in [\"predict\"]:\n        self.predict_dataset = Sen4AgriNet(\n            split=\"test\",\n            data_root=self.data_root,\n            transform=self.predict_transform,\n            bands=self.bands,\n            seed=self.seed,\n            scenario=self.scenario,\n            requires_norm=self.requires_norm,\n            binary_labels=self.binary_labels,\n            linear_encoder=self.linear_encoder,\n            **self.kwargs,\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.sen4map","title":"<code>terratorch.datamodules.sen4map</code>","text":""},{"location":"datamodules/#terratorch.datamodules.sen4map.Sen4MapLucasDataModule","title":"<code>Sen4MapLucasDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>NonGeo LightningDataModule implementation for Sen4map.</p> Source code in <code>terratorch/datamodules/sen4map.py</code> <pre><code>class Sen4MapLucasDataModule(pl.LightningDataModule):\n    \"\"\"NonGeo LightningDataModule implementation for Sen4map.\"\"\"\n\n    def __init__(\n            self, \n            batch_size,\n            num_workers,\n            prefetch_factor = 0,\n            # dataset_bands:list[HLSBands|int] = None,\n            # input_bands:list[HLSBands|int] = None,\n            train_hdf5_path = None,\n            train_hdf5_keys_path = None,\n            test_hdf5_path = None,\n            test_hdf5_keys_path = None,\n            val_hdf5_path = None,\n            val_hdf5_keys_path = None,\n            **kwargs\n            ):\n        \"\"\"\n        Initializes the Sen4MapLucasDataModule for handling Sen4Map monthly composites.\n\n        Args:\n            batch_size (int): Batch size for DataLoaders.\n            num_workers (int): Number of worker processes for data loading.\n            prefetch_factor (int, optional): Number of samples to prefetch per worker. Defaults to 0.\n            train_hdf5_path (str, optional): Path to the training HDF5 file.\n            train_hdf5_keys_path (str, optional): Path to the training HDF5 keys file.\n            test_hdf5_path (str, optional): Path to the testing HDF5 file.\n            test_hdf5_keys_path (str, optional): Path to the testing HDF5 keys file.\n            val_hdf5_path (str, optional): Path to the validation HDF5 file.\n            val_hdf5_keys_path (str, optional): Path to the validation HDF5 keys file.\n            train_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated train keys.\n            test_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated test keys.\n            val_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated validation keys.\n            shuffle (bool, optional): Global shuffle flag.\n            train_shuffle (bool, optional): Shuffle flag for training data; defaults to global shuffle if unset.\n            val_shuffle (bool, optional): Shuffle flag for validation data.\n            test_shuffle (bool, optional): Shuffle flag for test data.\n            train_data_fraction (float, optional): Fraction of training data to use. Defaults to 1.0.\n            val_data_fraction (float, optional): Fraction of validation data to use. Defaults to 1.0.\n            test_data_fraction (float, optional): Fraction of test data to use. Defaults to 1.0.\n            all_hdf5_data_path (str, optional): General HDF5 data path for all splits. If provided, overrides specific paths.\n            resize (bool, optional): Whether to resize images. Defaults to False.\n            resize_to (int or tuple, optional): Target size for resizing images.\n            resize_interpolation (str, optional): Interpolation mode for resizing ('bilinear', 'bicubic', etc.).\n            resize_antialiasing (bool, optional): Whether to apply antialiasing during resizing. Defaults to True.\n            **kwargs: Additional keyword arguments.\n        \"\"\"\n        self.prepare_data_per_node = False\n        self._log_hyperparams = None\n        self.allow_zero_length_dataloader_with_multiple_devices = False\n\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.prefetch_factor = prefetch_factor\n\n        self.train_hdf5_path = train_hdf5_path\n        self.test_hdf5_path = test_hdf5_path\n        self.val_hdf5_path = val_hdf5_path\n\n        self.train_hdf5_keys_path = train_hdf5_keys_path\n        self.test_hdf5_keys_path = test_hdf5_keys_path\n        self.val_hdf5_keys_path = val_hdf5_keys_path\n\n        if train_hdf5_path and not train_hdf5_keys_path: print(f\"Train dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\")\n        if test_hdf5_path and not test_hdf5_keys_path: print(f\"Test dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\")\n        if val_hdf5_path and not val_hdf5_keys_path: print(f\"Val dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\")\n\n        self.train_hdf5_keys_save_path = kwargs.pop(\"train_hdf5_keys_save_path\", None)\n        self.test_hdf5_keys_save_path = kwargs.pop(\"test_hdf5_keys_save_path\", None)\n        self.val_hdf5_keys_save_path = kwargs.pop(\"val_hdf5_keys_save_path\", None)\n\n        self.shuffle = kwargs.pop(\"shuffle\", None)\n        self.train_shuffle = kwargs.pop(\"train_shuffle\", None) or self.shuffle\n        self.val_shuffle = kwargs.pop(\"val_shuffle\", None)\n        self.test_shuffle = kwargs.pop(\"test_shuffle\", None)\n\n        self.train_data_fraction = kwargs.pop(\"train_data_fraction\", 1.0)\n        self.val_data_fraction = kwargs.pop(\"val_data_fraction\", 1.0)\n        self.test_data_fraction = kwargs.pop(\"test_data_fraction\", 1.0)\n\n        if self.train_data_fraction != 1.0  and  not train_hdf5_keys_path: raise ValueError(f\"train_data_fraction provided as non-unity but train_hdf5_keys_path is unset.\")\n        if self.val_data_fraction != 1.0  and  not val_hdf5_keys_path: raise ValueError(f\"val_data_fraction provided as non-unity but val_hdf5_keys_path is unset.\")\n        if self.test_data_fraction != 1.0  and  not test_hdf5_keys_path: raise ValueError(f\"test_data_fraction provided as non-unity but test_hdf5_keys_path is unset.\")\n\n        all_hdf5_data_path = kwargs.pop(\"all_hdf5_data_path\", None)\n        if all_hdf5_data_path is not None:\n            print(f\"all_hdf5_data_path provided, will be interpreted as the general data path for all splits.\\nKeys in provided train_hdf5_keys_path assumed to encompass all keys for entire data. Validation and Test keys will be subtracted from Train keys.\")\n            if self.train_hdf5_path: raise ValueError(f\"Both general all_hdf5_data_path provided and a specific train_hdf5_path, remove the train_hdf5_path\")\n            if self.val_hdf5_path: raise ValueError(f\"Both general all_hdf5_data_path provided and a specific val_hdf5_path, remove the val_hdf5_path\")\n            if self.test_hdf5_path: raise ValueError(f\"Both general all_hdf5_data_path provided and a specific test_hdf5_path, remove the test_hdf5_path\")\n            self.train_hdf5_path = all_hdf5_data_path\n            self.val_hdf5_path = all_hdf5_data_path\n            self.test_hdf5_path = all_hdf5_data_path\n            self.reduce_train_keys = True\n        else:\n            self.reduce_train_keys = False\n\n        self.resize = kwargs.pop(\"resize\", False)\n        self.resize_to = kwargs.pop(\"resize_to\", None)\n        if self.resize and self.resize_to is None:\n            raise ValueError(f\"Config provided resize as True, but resize_to parameter not given\")\n        self.resize_interpolation = kwargs.pop(\"resize_interpolation\", None)\n        if self.resize and self.resize_interpolation is None:\n            print(f\"Config provided resize as True, but resize_interpolation mode not given. Will assume default bilinear\")\n            self.resize_interpolation = \"bilinear\"\n        interpolation_dict = {\n            \"bilinear\": InterpolationMode.BILINEAR,\n            \"bicubic\": InterpolationMode.BICUBIC,\n            \"nearest\": InterpolationMode.NEAREST,\n            \"nearest_exact\": InterpolationMode.NEAREST_EXACT\n        }\n        if self.resize:\n            if self.resize_interpolation not in interpolation_dict.keys():\n                raise ValueError(f\"resize_interpolation provided as {self.resize_interpolation}, but valid options are: {interpolation_dict.keys()}\")\n            self.resize_interpolation = interpolation_dict[self.resize_interpolation]\n        self.resize_antialiasing = kwargs.pop(\"resize_antialiasing\", True)\n\n        self.kwargs = kwargs\n\n    def _load_hdf5_keys_from_path(self, path, fraction=1.0):\n        if path is None: return None\n        with open(path, \"rb\") as f:\n            keys = pickle.load(f)\n            return keys[:int(fraction*len(keys))]\n\n    def setup(self, stage: str):\n        \"\"\"Set up datasets.\n\n        Args:\n            stage: Either fit, test.\n        \"\"\"\n        if stage == \"fit\":\n            train_keys = self._load_hdf5_keys_from_path(self.train_hdf5_keys_path, fraction=self.train_data_fraction)\n            val_keys = self._load_hdf5_keys_from_path(self.val_hdf5_keys_path, fraction=self.val_data_fraction)\n            if self.reduce_train_keys:\n                test_keys = self._load_hdf5_keys_from_path(self.test_hdf5_keys_path, fraction=self.test_data_fraction)\n                train_keys = list(set(train_keys) - set(val_keys) - set(test_keys))\n            train_file = h5py.File(self.train_hdf5_path, 'r')\n            self.lucasS2_train = Sen4MapDatasetMonthlyComposites(\n                train_file, \n                h5data_keys = train_keys, \n                resize = self.resize,\n                resize_to = self.resize_to,\n                resize_interpolation = self.resize_interpolation,\n                resize_antialiasing = self.resize_antialiasing,\n                save_keys_path = self.train_hdf5_keys_save_path,\n                **self.kwargs\n            )\n            val_file = h5py.File(self.val_hdf5_path, 'r')\n            self.lucasS2_val = Sen4MapDatasetMonthlyComposites(\n                val_file, \n                h5data_keys=val_keys, \n                resize = self.resize,\n                resize_to = self.resize_to,\n                resize_interpolation = self.resize_interpolation,\n                resize_antialiasing = self.resize_antialiasing,\n                save_keys_path = self.val_hdf5_keys_save_path,\n                **self.kwargs\n            )\n        if stage == \"test\":\n            test_file = h5py.File(self.test_hdf5_path, 'r')\n            test_keys = self._load_hdf5_keys_from_path(self.test_hdf5_keys_path, fraction=self.test_data_fraction)\n            self.lucasS2_test = Sen4MapDatasetMonthlyComposites(\n                test_file, \n                h5data_keys=test_keys, \n                resize = self.resize,\n                resize_to = self.resize_to,\n                resize_interpolation = self.resize_interpolation,\n                resize_antialiasing = self.resize_antialiasing,\n                save_keys_path = self.test_hdf5_keys_save_path,\n                **self.kwargs\n            )\n\n    def train_dataloader(self):\n        return DataLoader(self.lucasS2_train, batch_size=self.batch_size, num_workers=self.num_workers, prefetch_factor=self.prefetch_factor, shuffle=self.train_shuffle)\n\n    def val_dataloader(self):\n        return DataLoader(self.lucasS2_val, batch_size=self.batch_size, num_workers=self.num_workers, prefetch_factor=self.prefetch_factor, shuffle=self.val_shuffle)\n\n    def test_dataloader(self):\n        return DataLoader(self.lucasS2_test, batch_size=self.batch_size, num_workers=self.num_workers, prefetch_factor=self.prefetch_factor, shuffle=self.test_shuffle)\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.sen4map.Sen4MapLucasDataModule.__init__","title":"<code>__init__(batch_size, num_workers, prefetch_factor=0, train_hdf5_path=None, train_hdf5_keys_path=None, test_hdf5_path=None, test_hdf5_keys_path=None, val_hdf5_path=None, val_hdf5_keys_path=None, **kwargs)</code>","text":"<p>Initializes the Sen4MapLucasDataModule for handling Sen4Map monthly composites.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size for DataLoaders.</p> required <code>num_workers</code> <code>int</code> <p>Number of worker processes for data loading.</p> required <code>prefetch_factor</code> <code>int</code> <p>Number of samples to prefetch per worker. Defaults to 0.</p> <code>0</code> <code>train_hdf5_path</code> <code>str</code> <p>Path to the training HDF5 file.</p> <code>None</code> <code>train_hdf5_keys_path</code> <code>str</code> <p>Path to the training HDF5 keys file.</p> <code>None</code> <code>test_hdf5_path</code> <code>str</code> <p>Path to the testing HDF5 file.</p> <code>None</code> <code>test_hdf5_keys_path</code> <code>str</code> <p>Path to the testing HDF5 keys file.</p> <code>None</code> <code>val_hdf5_path</code> <code>str</code> <p>Path to the validation HDF5 file.</p> <code>None</code> <code>val_hdf5_keys_path</code> <code>str</code> <p>Path to the validation HDF5 keys file.</p> <code>None</code> <code>train_hdf5_keys_save_path</code> <code>str</code> <p>(from kwargs) Path to save generated train keys.</p> required <code>test_hdf5_keys_save_path</code> <code>str</code> <p>(from kwargs) Path to save generated test keys.</p> required <code>val_hdf5_keys_save_path</code> <code>str</code> <p>(from kwargs) Path to save generated validation keys.</p> required <code>shuffle</code> <code>bool</code> <p>Global shuffle flag.</p> required <code>train_shuffle</code> <code>bool</code> <p>Shuffle flag for training data; defaults to global shuffle if unset.</p> required <code>val_shuffle</code> <code>bool</code> <p>Shuffle flag for validation data.</p> required <code>test_shuffle</code> <code>bool</code> <p>Shuffle flag for test data.</p> required <code>train_data_fraction</code> <code>float</code> <p>Fraction of training data to use. Defaults to 1.0.</p> required <code>val_data_fraction</code> <code>float</code> <p>Fraction of validation data to use. Defaults to 1.0.</p> required <code>test_data_fraction</code> <code>float</code> <p>Fraction of test data to use. Defaults to 1.0.</p> required <code>all_hdf5_data_path</code> <code>str</code> <p>General HDF5 data path for all splits. If provided, overrides specific paths.</p> required <code>resize</code> <code>bool</code> <p>Whether to resize images. Defaults to False.</p> required <code>resize_to</code> <code>int or tuple</code> <p>Target size for resizing images.</p> required <code>resize_interpolation</code> <code>str</code> <p>Interpolation mode for resizing ('bilinear', 'bicubic', etc.).</p> required <code>resize_antialiasing</code> <code>bool</code> <p>Whether to apply antialiasing during resizing. Defaults to True.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>terratorch/datamodules/sen4map.py</code> <pre><code>def __init__(\n        self, \n        batch_size,\n        num_workers,\n        prefetch_factor = 0,\n        # dataset_bands:list[HLSBands|int] = None,\n        # input_bands:list[HLSBands|int] = None,\n        train_hdf5_path = None,\n        train_hdf5_keys_path = None,\n        test_hdf5_path = None,\n        test_hdf5_keys_path = None,\n        val_hdf5_path = None,\n        val_hdf5_keys_path = None,\n        **kwargs\n        ):\n    \"\"\"\n    Initializes the Sen4MapLucasDataModule for handling Sen4Map monthly composites.\n\n    Args:\n        batch_size (int): Batch size for DataLoaders.\n        num_workers (int): Number of worker processes for data loading.\n        prefetch_factor (int, optional): Number of samples to prefetch per worker. Defaults to 0.\n        train_hdf5_path (str, optional): Path to the training HDF5 file.\n        train_hdf5_keys_path (str, optional): Path to the training HDF5 keys file.\n        test_hdf5_path (str, optional): Path to the testing HDF5 file.\n        test_hdf5_keys_path (str, optional): Path to the testing HDF5 keys file.\n        val_hdf5_path (str, optional): Path to the validation HDF5 file.\n        val_hdf5_keys_path (str, optional): Path to the validation HDF5 keys file.\n        train_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated train keys.\n        test_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated test keys.\n        val_hdf5_keys_save_path (str, optional): (from kwargs) Path to save generated validation keys.\n        shuffle (bool, optional): Global shuffle flag.\n        train_shuffle (bool, optional): Shuffle flag for training data; defaults to global shuffle if unset.\n        val_shuffle (bool, optional): Shuffle flag for validation data.\n        test_shuffle (bool, optional): Shuffle flag for test data.\n        train_data_fraction (float, optional): Fraction of training data to use. Defaults to 1.0.\n        val_data_fraction (float, optional): Fraction of validation data to use. Defaults to 1.0.\n        test_data_fraction (float, optional): Fraction of test data to use. Defaults to 1.0.\n        all_hdf5_data_path (str, optional): General HDF5 data path for all splits. If provided, overrides specific paths.\n        resize (bool, optional): Whether to resize images. Defaults to False.\n        resize_to (int or tuple, optional): Target size for resizing images.\n        resize_interpolation (str, optional): Interpolation mode for resizing ('bilinear', 'bicubic', etc.).\n        resize_antialiasing (bool, optional): Whether to apply antialiasing during resizing. Defaults to True.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    self.prepare_data_per_node = False\n    self._log_hyperparams = None\n    self.allow_zero_length_dataloader_with_multiple_devices = False\n\n    self.batch_size = batch_size\n    self.num_workers = num_workers\n    self.prefetch_factor = prefetch_factor\n\n    self.train_hdf5_path = train_hdf5_path\n    self.test_hdf5_path = test_hdf5_path\n    self.val_hdf5_path = val_hdf5_path\n\n    self.train_hdf5_keys_path = train_hdf5_keys_path\n    self.test_hdf5_keys_path = test_hdf5_keys_path\n    self.val_hdf5_keys_path = val_hdf5_keys_path\n\n    if train_hdf5_path and not train_hdf5_keys_path: print(f\"Train dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\")\n    if test_hdf5_path and not test_hdf5_keys_path: print(f\"Test dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\")\n    if val_hdf5_path and not val_hdf5_keys_path: print(f\"Val dataset path provided but not the path to the dataset keys. Generating the keys might take a few minutes.\")\n\n    self.train_hdf5_keys_save_path = kwargs.pop(\"train_hdf5_keys_save_path\", None)\n    self.test_hdf5_keys_save_path = kwargs.pop(\"test_hdf5_keys_save_path\", None)\n    self.val_hdf5_keys_save_path = kwargs.pop(\"val_hdf5_keys_save_path\", None)\n\n    self.shuffle = kwargs.pop(\"shuffle\", None)\n    self.train_shuffle = kwargs.pop(\"train_shuffle\", None) or self.shuffle\n    self.val_shuffle = kwargs.pop(\"val_shuffle\", None)\n    self.test_shuffle = kwargs.pop(\"test_shuffle\", None)\n\n    self.train_data_fraction = kwargs.pop(\"train_data_fraction\", 1.0)\n    self.val_data_fraction = kwargs.pop(\"val_data_fraction\", 1.0)\n    self.test_data_fraction = kwargs.pop(\"test_data_fraction\", 1.0)\n\n    if self.train_data_fraction != 1.0  and  not train_hdf5_keys_path: raise ValueError(f\"train_data_fraction provided as non-unity but train_hdf5_keys_path is unset.\")\n    if self.val_data_fraction != 1.0  and  not val_hdf5_keys_path: raise ValueError(f\"val_data_fraction provided as non-unity but val_hdf5_keys_path is unset.\")\n    if self.test_data_fraction != 1.0  and  not test_hdf5_keys_path: raise ValueError(f\"test_data_fraction provided as non-unity but test_hdf5_keys_path is unset.\")\n\n    all_hdf5_data_path = kwargs.pop(\"all_hdf5_data_path\", None)\n    if all_hdf5_data_path is not None:\n        print(f\"all_hdf5_data_path provided, will be interpreted as the general data path for all splits.\\nKeys in provided train_hdf5_keys_path assumed to encompass all keys for entire data. Validation and Test keys will be subtracted from Train keys.\")\n        if self.train_hdf5_path: raise ValueError(f\"Both general all_hdf5_data_path provided and a specific train_hdf5_path, remove the train_hdf5_path\")\n        if self.val_hdf5_path: raise ValueError(f\"Both general all_hdf5_data_path provided and a specific val_hdf5_path, remove the val_hdf5_path\")\n        if self.test_hdf5_path: raise ValueError(f\"Both general all_hdf5_data_path provided and a specific test_hdf5_path, remove the test_hdf5_path\")\n        self.train_hdf5_path = all_hdf5_data_path\n        self.val_hdf5_path = all_hdf5_data_path\n        self.test_hdf5_path = all_hdf5_data_path\n        self.reduce_train_keys = True\n    else:\n        self.reduce_train_keys = False\n\n    self.resize = kwargs.pop(\"resize\", False)\n    self.resize_to = kwargs.pop(\"resize_to\", None)\n    if self.resize and self.resize_to is None:\n        raise ValueError(f\"Config provided resize as True, but resize_to parameter not given\")\n    self.resize_interpolation = kwargs.pop(\"resize_interpolation\", None)\n    if self.resize and self.resize_interpolation is None:\n        print(f\"Config provided resize as True, but resize_interpolation mode not given. Will assume default bilinear\")\n        self.resize_interpolation = \"bilinear\"\n    interpolation_dict = {\n        \"bilinear\": InterpolationMode.BILINEAR,\n        \"bicubic\": InterpolationMode.BICUBIC,\n        \"nearest\": InterpolationMode.NEAREST,\n        \"nearest_exact\": InterpolationMode.NEAREST_EXACT\n    }\n    if self.resize:\n        if self.resize_interpolation not in interpolation_dict.keys():\n            raise ValueError(f\"resize_interpolation provided as {self.resize_interpolation}, but valid options are: {interpolation_dict.keys()}\")\n        self.resize_interpolation = interpolation_dict[self.resize_interpolation]\n    self.resize_antialiasing = kwargs.pop(\"resize_antialiasing\", True)\n\n    self.kwargs = kwargs\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.sen4map.Sen4MapLucasDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Set up datasets.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>Either fit, test.</p> required Source code in <code>terratorch/datamodules/sen4map.py</code> <pre><code>def setup(self, stage: str):\n    \"\"\"Set up datasets.\n\n    Args:\n        stage: Either fit, test.\n    \"\"\"\n    if stage == \"fit\":\n        train_keys = self._load_hdf5_keys_from_path(self.train_hdf5_keys_path, fraction=self.train_data_fraction)\n        val_keys = self._load_hdf5_keys_from_path(self.val_hdf5_keys_path, fraction=self.val_data_fraction)\n        if self.reduce_train_keys:\n            test_keys = self._load_hdf5_keys_from_path(self.test_hdf5_keys_path, fraction=self.test_data_fraction)\n            train_keys = list(set(train_keys) - set(val_keys) - set(test_keys))\n        train_file = h5py.File(self.train_hdf5_path, 'r')\n        self.lucasS2_train = Sen4MapDatasetMonthlyComposites(\n            train_file, \n            h5data_keys = train_keys, \n            resize = self.resize,\n            resize_to = self.resize_to,\n            resize_interpolation = self.resize_interpolation,\n            resize_antialiasing = self.resize_antialiasing,\n            save_keys_path = self.train_hdf5_keys_save_path,\n            **self.kwargs\n        )\n        val_file = h5py.File(self.val_hdf5_path, 'r')\n        self.lucasS2_val = Sen4MapDatasetMonthlyComposites(\n            val_file, \n            h5data_keys=val_keys, \n            resize = self.resize,\n            resize_to = self.resize_to,\n            resize_interpolation = self.resize_interpolation,\n            resize_antialiasing = self.resize_antialiasing,\n            save_keys_path = self.val_hdf5_keys_save_path,\n            **self.kwargs\n        )\n    if stage == \"test\":\n        test_file = h5py.File(self.test_hdf5_path, 'r')\n        test_keys = self._load_hdf5_keys_from_path(self.test_hdf5_keys_path, fraction=self.test_data_fraction)\n        self.lucasS2_test = Sen4MapDatasetMonthlyComposites(\n            test_file, \n            h5data_keys=test_keys, \n            resize = self.resize,\n            resize_to = self.resize_to,\n            resize_interpolation = self.resize_interpolation,\n            resize_antialiasing = self.resize_antialiasing,\n            save_keys_path = self.test_hdf5_keys_save_path,\n            **self.kwargs\n        )\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.torchgeo_data_module","title":"<code>terratorch.datamodules.torchgeo_data_module</code>","text":"<p>Ugly proxy objects so parsing config file works with transforms.</p> <p>These are necessary since, for LightningCLI to instantiate arguments as objects from the config, they must have type annotations</p> <p>In TorchGeo, <code>transforms</code> is passed in **kwargs, so it has no type annotations! To get around that, we create these wrappers that have transforms type annotated. They create the transforms and forward all method and attribute calls to the original TorchGeo datamodule.</p> <p>Additionally, TorchGeo datasets pass the data to the transforms callable as a dict, and as a tensor.</p> <p>Albumentations expects this data not as a dict but as different key-value arguments, and as numpy. We handle that conversion here.</p>"},{"location":"datamodules/#terratorch.datamodules.torchgeo_data_module.TorchGeoDataModule","title":"<code>TorchGeoDataModule</code>","text":"<p>               Bases: <code>GeoDataModule</code></p> <p>Proxy object for using Geo data modules defined by TorchGeo.</p> <p>Allows for transforms to be defined and passed using config files. The only reason this class exists is so that we can annotate the transforms argument with a type. This is required for lightningcli and config files. As such, all getattr and setattr will be redirected to the underlying class.</p> Source code in <code>terratorch/datamodules/torchgeo_data_module.py</code> <pre><code>class TorchGeoDataModule(GeoDataModule):\n    \"\"\"Proxy object for using Geo data modules defined by TorchGeo.\n\n    Allows for transforms to be defined and passed using config files.\n    The only reason this class exists is so that we can annotate the transforms argument with a type.\n    This is required for lightningcli and config files.\n    As such, all getattr and setattr will be redirected to the underlying class.\n    \"\"\"\n\n    def __init__(\n        self,\n        cls: type[GeoDataModule],\n        batch_size: int | None = None,\n        num_workers: int = 0,\n        transforms: None | list[BasicTransform] = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Constructor\n\n        Args:\n            cls (type[GeoDataModule]): TorchGeo DataModule class to be instantiated\n            batch_size (int | None, optional): batch_size. Defaults to None.\n            num_workers (int, optional): num_workers. Defaults to 0.\n            transforms (None | list[BasicTransform], optional): List of Albumentations Transforms.\n                Should enc with ToTensorV2. Defaults to None.\n            **kwargs (Any): Arguments passed to instantiate `cls`.\n        \"\"\"\n        if batch_size is not None:\n            kwargs[\"batch_size\"] = batch_size\n        if transforms is not None:\n            transforms_as_callable = albumentations_to_callable_with_dict(transforms)\n            kwargs[\"transforms\"] = build_callable_transform_from_torch_tensor(transforms_as_callable)\n        # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs)\n        self._proxy = cls(num_workers=num_workers, **kwargs)\n        super().__init__(self._proxy.dataset_class)  # dummy arg\n\n    @property\n    def collate_fn(self):\n        return self._proxy.collate_fn\n\n    @collate_fn.setter\n    def collate_fn(self, value):\n        self._proxy.collate_fn = value\n\n    @property\n    def patch_size(self):\n        return self._proxy.patch_size\n\n    @property\n    def length(self):\n        return self._proxy.length\n\n    def setup(self, stage: str):\n        return self._proxy.setup(stage)\n\n    def train_dataloader(self):\n        return self._proxy.train_dataloader()\n\n    def val_dataloader(self):\n        return self._proxy.val_dataloader()\n\n    def test_dataloader(self):\n        return self._proxy.test_dataloader()\n\n    def predict_dataloader(self):\n        return self._proxy.predict_dataloader()\n\n    def transfer_batch_to_device(self, batch, device, dataloader_idx):\n        return self._proxy.predict_dataloader(batch, device, dataloader_idx)\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.torchgeo_data_module.TorchGeoDataModule.__init__","title":"<code>__init__(cls, batch_size=None, num_workers=0, transforms=None, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[GeoDataModule]</code> <p>TorchGeo DataModule class to be instantiated</p> required <code>batch_size</code> <code>int | None</code> <p>batch_size. Defaults to None.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>num_workers. Defaults to 0.</p> <code>0</code> <code>transforms</code> <code>None | list[BasicTransform]</code> <p>List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Arguments passed to instantiate <code>cls</code>.</p> <code>{}</code> Source code in <code>terratorch/datamodules/torchgeo_data_module.py</code> <pre><code>def __init__(\n    self,\n    cls: type[GeoDataModule],\n    batch_size: int | None = None,\n    num_workers: int = 0,\n    transforms: None | list[BasicTransform] = None,\n    **kwargs: Any,\n):\n    \"\"\"Constructor\n\n    Args:\n        cls (type[GeoDataModule]): TorchGeo DataModule class to be instantiated\n        batch_size (int | None, optional): batch_size. Defaults to None.\n        num_workers (int, optional): num_workers. Defaults to 0.\n        transforms (None | list[BasicTransform], optional): List of Albumentations Transforms.\n            Should enc with ToTensorV2. Defaults to None.\n        **kwargs (Any): Arguments passed to instantiate `cls`.\n    \"\"\"\n    if batch_size is not None:\n        kwargs[\"batch_size\"] = batch_size\n    if transforms is not None:\n        transforms_as_callable = albumentations_to_callable_with_dict(transforms)\n        kwargs[\"transforms\"] = build_callable_transform_from_torch_tensor(transforms_as_callable)\n    # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs)\n    self._proxy = cls(num_workers=num_workers, **kwargs)\n    super().__init__(self._proxy.dataset_class)  # dummy arg\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.torchgeo_data_module.TorchNonGeoDataModule","title":"<code>TorchNonGeoDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>Proxy object for using NonGeo data modules defined by TorchGeo.</p> <p>Allows for transforms to be defined and passed using config files. The only reason this class exists is so that we can annotate the transforms argument with a type. This is required for lightningcli and config files. As such, all getattr and setattr will be redirected to the underlying class.</p> Source code in <code>terratorch/datamodules/torchgeo_data_module.py</code> <pre><code>class TorchNonGeoDataModule(NonGeoDataModule):\n    \"\"\"Proxy object for using NonGeo data modules defined by TorchGeo.\n\n    Allows for transforms to be defined and passed using config files.\n    The only reason this class exists is so that we can annotate the transforms argument with a type.\n    This is required for lightningcli and config files.\n    As such, all getattr and setattr will be redirected to the underlying class.\n    \"\"\"\n\n    def __init__(\n        self,\n        cls: type[NonGeoDataModule],\n        batch_size: int | None = None,\n        num_workers: int = 0,\n        transforms: None | list[BasicTransform] = None,\n        **kwargs: Any,\n    ):\n        \"\"\"Constructor\n\n        Args:\n            cls (type[NonGeoDataModule]): TorchGeo DataModule class to be instantiated\n            batch_size (int | None, optional): batch_size. Defaults to None.\n            num_workers (int, optional): num_workers. Defaults to 0.\n            transforms (None | list[BasicTransform], optional): List of Albumentations Transforms.\n                Should enc with ToTensorV2. Defaults to None.\n            **kwargs (Any): Arguments passed to instantiate `cls`.\n        \"\"\"\n        if batch_size is not None:\n            kwargs[\"batch_size\"] = batch_size\n        if transforms is not None:\n            transforms_as_callable = albumentations_to_callable_with_dict(transforms)\n            kwargs[\"transforms\"] = build_callable_transform_from_torch_tensor(transforms_as_callable)\n        # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs)\n        self._proxy = cls(num_workers=num_workers, **kwargs)\n        super().__init__(self._proxy.dataset_class)  # dummy arg\n\n    @property\n    def collate_fn(self):\n        return self._proxy.collate_fn\n\n    @collate_fn.setter\n    def collate_fn(self, value):\n        self._proxy.collate_fn = value\n\n    def setup(self, stage: str):\n        return self._proxy.setup(stage)\n\n    def train_dataloader(self):\n        return self._proxy.train_dataloader()\n\n    def val_dataloader(self):\n        return self._proxy.val_dataloader()\n\n    def test_dataloader(self):\n        return self._proxy.test_dataloader()\n\n    def predict_dataloader(self):\n        return self._proxy.predict_dataloader()\n</code></pre>"},{"location":"datamodules/#terratorch.datamodules.torchgeo_data_module.TorchNonGeoDataModule.__init__","title":"<code>__init__(cls, batch_size=None, num_workers=0, transforms=None, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type[NonGeoDataModule]</code> <p>TorchGeo DataModule class to be instantiated</p> required <code>batch_size</code> <code>int | None</code> <p>batch_size. Defaults to None.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>num_workers. Defaults to 0.</p> <code>0</code> <code>transforms</code> <code>None | list[BasicTransform]</code> <p>List of Albumentations Transforms. Should enc with ToTensorV2. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Arguments passed to instantiate <code>cls</code>.</p> <code>{}</code> Source code in <code>terratorch/datamodules/torchgeo_data_module.py</code> <pre><code>def __init__(\n    self,\n    cls: type[NonGeoDataModule],\n    batch_size: int | None = None,\n    num_workers: int = 0,\n    transforms: None | list[BasicTransform] = None,\n    **kwargs: Any,\n):\n    \"\"\"Constructor\n\n    Args:\n        cls (type[NonGeoDataModule]): TorchGeo DataModule class to be instantiated\n        batch_size (int | None, optional): batch_size. Defaults to None.\n        num_workers (int, optional): num_workers. Defaults to 0.\n        transforms (None | list[BasicTransform], optional): List of Albumentations Transforms.\n            Should enc with ToTensorV2. Defaults to None.\n        **kwargs (Any): Arguments passed to instantiate `cls`.\n    \"\"\"\n    if batch_size is not None:\n        kwargs[\"batch_size\"] = batch_size\n    if transforms is not None:\n        transforms_as_callable = albumentations_to_callable_with_dict(transforms)\n        kwargs[\"transforms\"] = build_callable_transform_from_torch_tensor(transforms_as_callable)\n    # self.__dict__[\"datamodule\"] = cls(num_workers=num_workers, **kwargs)\n    self._proxy = cls(num_workers=num_workers, **kwargs)\n    super().__init__(self._proxy.dataset_class)  # dummy arg\n</code></pre>"},{"location":"datasets/","title":"Datasets","text":""},{"location":"datasets/#terratorch.datasets.biomassters","title":"<code>terratorch.datasets.biomassters</code>","text":""},{"location":"datasets/#terratorch.datasets.biomassters.BioMasstersNonGeo","title":"<code>BioMasstersNonGeo</code>","text":"<p>               Bases: <code>BioMassters</code></p> <p>BioMassters Dataset for Aboveground Biomass prediction.</p> <p>Dataset intended for Aboveground Biomass (AGB) prediction over Finnish forests based on Sentinel 1 and 2 data with corresponding target AGB mask values generated by Light Detection and Ranging (LiDAR).</p> <p>Dataset Format:</p> <ul> <li>.tif files for Sentinel 1 and 2 data</li> <li>.tif file for pixel wise AGB target mask</li> <li>.csv files for metadata regarding features and targets</li> </ul> <p>Dataset Features:</p> <ul> <li>13,000 target AGB masks of size (256x256px)</li> <li>12 months of data per target mask</li> <li>Sentinel 1 and Sentinel 2 data for each location</li> <li>Sentinel 1 available for every month</li> <li>Sentinel 2 available for almost every month   (not available for every month due to ESA acquisition halt over the region   during particular periods)</li> </ul> <p>If you use this dataset in your research, please cite the following paper:</p> <ul> <li>https://nascetti-a.github.io/BioMasster/</li> </ul> <p>.. versionadded:: 0.5</p> Source code in <code>terratorch/datasets/biomassters.py</code> <pre><code>class BioMasstersNonGeo(BioMassters):\n    \"\"\"[BioMassters Dataset](https://huggingface.co/datasets/ibm-nasa-geospatial/BioMassters) for Aboveground Biomass prediction.\n\n    Dataset intended for Aboveground Biomass (AGB) prediction\n    over Finnish forests based on Sentinel 1 and 2 data with\n    corresponding target AGB mask values generated by Light Detection\n    and Ranging (LiDAR).\n\n    Dataset Format:\n\n    * .tif files for Sentinel 1 and 2 data\n    * .tif file for pixel wise AGB target mask\n    * .csv files for metadata regarding features and targets\n\n    Dataset Features:\n\n    * 13,000 target AGB masks of size (256x256px)\n    * 12 months of data per target mask\n    * Sentinel 1 and Sentinel 2 data for each location\n    * Sentinel 1 available for every month\n    * Sentinel 2 available for almost every month\n      (not available for every month due to ESA acquisition halt over the region\n      during particular periods)\n\n    If you use this dataset in your research, please cite the following paper:\n\n    * https://nascetti-a.github.io/BioMasster/\n\n    .. versionadded:: 0.5\n    \"\"\"\n\n    S1_BAND_NAMES = [\"VV_Asc\", \"VH_Asc\", \"VV_Desc\", \"VH_Desc\", \"RVI_Asc\", \"RVI_Desc\"]\n    S2_BAND_NAMES = [\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"RED_EDGE_1\",\n        \"RED_EDGE_2\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"NIR_NARROW\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n        \"CLOUD_PROBABILITY\",\n    ]\n\n    all_band_names = {\n        \"S1\": S1_BAND_NAMES,\n        \"S2\": S2_BAND_NAMES,\n    }\n\n    rgb_bands = {\n        \"S1\": [],\n        \"S2\": [\"RED\", \"GREEN\", \"BLUE\"],\n    }\n\n    valid_splits = (\"train\", \"test\")\n    valid_sensors = (\"S1\", \"S2\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    default_metadata_filename = \"The_BioMassters_-_features_metadata.csv.csv\"\n\n    def __init__(\n        self,\n        root = \"data\",\n        split: str = \"train\",\n        bands: dict[str, Sequence[str]] | Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        mask_mean: float | None = 63.4584,\n        mask_std: float | None = 72.21242,\n        sensors: Sequence[str] = [\"S1\", \"S2\"],\n        as_time_series: bool = False,\n        metadata_filename: str = default_metadata_filename,\n        max_cloud_percentage: float | None = None,\n        max_red_mean: float | None = None,\n        include_corrupt: bool = True,\n        subset: float = 1,\n        seed: int = 42,\n        use_four_frames: bool = False\n    ) -&gt; None:\n        \"\"\"Initialize a new instance of BioMassters dataset.\n\n        If ``as_time_series=False`` (the default), each time step becomes its own\n        sample with the target being shared across multiple samples.\n\n        Args:\n            root: root directory where dataset can be found\n            split: train or test split\n            sensors: which sensors to consider for the sample, Sentinel 1 and/or\n                Sentinel 2 ('S1', 'S2')\n            as_time_series: whether or not to return all available\n                time-steps or just a single one for a given target location\n            metadata_filename: metadata file to be used\n            max_cloud_percentage: maximum allowed cloud percentage for images\n            max_red_mean: maximum allowed red_mean value for images\n            include_corrupt: whether to include images marked as corrupted\n\n        Raises:\n            AssertionError: if ``split`` or ``sensors`` is invalid\n            DatasetNotFoundError: If dataset is not found.\n        \"\"\"\n        self.root = root\n        self.sensors = sensors\n        self.bands = bands\n        assert (\n            split in self.valid_splits\n        ), f\"Please choose one of the valid splits: {self.valid_splits}.\"\n        self.split = split\n\n        assert set(sensors).issubset(\n            set(self.valid_sensors)\n        ), f\"Please choose a subset of valid sensors: {self.valid_sensors}.\"\n\n        if len(self.sensors) == 1:\n            sens = self.sensors[0]\n            self.band_indices = [\n                self.all_band_names[sens].index(band) for band in self.bands[sens]\n            ]\n        else:\n            self.band_indices = {\n                sens: [self.all_band_names[sens].index(band) for band in self.bands[sens]]\n                for sens in self.sensors\n            }\n\n        self.mask_mean = mask_mean\n        self.mask_std = mask_std\n        self.as_time_series = as_time_series\n        self.metadata_filename = metadata_filename\n        self.max_cloud_percentage = max_cloud_percentage\n        self.max_red_mean = max_red_mean\n        self.include_corrupt = include_corrupt\n        self.subset = subset\n        self.seed = seed\n        self.use_four_frames = use_four_frames\n\n        self._verify()\n\n        # open metadata csv files\n        self.df = pd.read_csv(os.path.join(self.root, self.metadata_filename))\n\n        # Filter sensors\n        self.df = self.df[self.df[\"satellite\"].isin(self.sensors)]\n\n        # Filter split\n        self.df = self.df[self.df[\"split\"] == self.split]\n\n        # Optional filtering\n        self._filter_and_select_data()\n\n        # Optional subsampling\n        self._random_subsample()\n\n        # generate numerical month from filename since first month is September\n        # and has numerical index of 0\n        self.df[\"num_month\"] = (\n            self.df[\"filename\"]\n            .str.split(\"_\", expand=True)[2]\n            .str.split(\".\", expand=True)[0]\n            .astype(int)\n        )\n\n        # Set dataframe index depending on the task for easier indexing\n        if self.as_time_series:\n            self.df[\"num_index\"] = self.df.groupby([\"chip_id\"]).ngroup()\n        else:\n            filter_df = (\n                self.df.groupby([\"chip_id\", \"month\"])[\"satellite\"].count().reset_index()\n            )\n            filter_df = filter_df[\n                filter_df[\"satellite\"] == len(self.sensors)\n            ].drop(\"satellite\", axis=1)\n            # Guarantee that each sample has corresponding number of images available\n            self.df = self.df.merge(filter_df, on=[\"chip_id\", \"month\"], how=\"inner\")\n\n            self.df[\"num_index\"] = self.df.groupby([\"chip_id\", \"month\"]).ngroup()\n\n        # Adjust transforms based on the number of sensors\n        if len(self.sensors) == 1:\n            self.transform = transform if transform else default_transform\n        elif transform is None:\n            self.transform = MultimodalToTensor(self.sensors)\n        else:\n            transform = {\n                s: transform[s] if s in transform else default_transform\n                for s in self.sensors\n            }\n            self.transform = MultimodalTransforms(transform, shared=False)\n\n        if self.use_four_frames:\n            self._select_4_frames()\n\n    def __len__(self) -&gt; int:\n        return len(self.df[\"num_index\"].unique())\n\n    def _load_input(self, filenames: list[Path]) -&gt; Tensor:\n        \"\"\"Load the input imagery at the index.\n\n        Args:\n            filenames: list of filenames corresponding to input\n\n        Returns:\n            input image\n        \"\"\"\n        filepaths = [\n            os.path.join(self.root, f\"{self.split}_features\", f) for f in filenames\n        ]\n        arr_list = [rasterio.open(fp).read() for fp in filepaths]\n\n        if self.as_time_series:\n            arr = np.stack(arr_list, axis=0) # (T, C, H, W)\n        else:\n            arr = np.concatenate(arr_list, axis=0)\n        return arr.astype(np.int32)\n\n    def _load_target(self, filename: Path) -&gt; Tensor:\n        \"\"\"Load the target mask at the index.\n\n        Args:\n            filename: filename of target to index\n\n        Returns:\n            target mask\n        \"\"\"\n        with rasterio.open(os.path.join(self.root, f\"{self.split}_agbm\", filename), \"r\") as src:\n            arr: np.typing.NDArray[np.float64] = src.read()\n\n        return arr\n\n    def _compute_rvi(self, img: np.ndarray, linear: np.ndarray, sens: str) -&gt; np.ndarray:\n        \"\"\"Compute the RVI indices for S1 data.\"\"\"\n        rvi_channels = []\n        if self.as_time_series:\n            if \"RVI_Asc\" in self.bands[sens]:\n                try:\n                    vv_asc_index = self.all_band_names[\"S1\"].index(\"VV_Asc\")\n                    vh_asc_index = self.all_band_names[\"S1\"].index(\"VH_Asc\")\n                except ValueError as e:\n                    msg = f\"RVI_Asc needs band: {e}\"\n                    raise ValueError(msg) from e\n\n                VV = linear[:, vv_asc_index, :, :]\n                VH = linear[:, vh_asc_index, :, :]\n                rvi_asc = 4 * VH / (VV + VH + 1e-6)\n                rvi_asc = np.expand_dims(rvi_asc, axis=1)\n                rvi_channels.append(rvi_asc)\n            if \"RVI_Desc\" in self.bands[sens]:\n                try:\n                    vv_desc_index = self.all_band_names[\"S1\"].index(\"VV_Desc\")\n                    vh_desc_index = self.all_band_names[\"S1\"].index(\"VH_Desc\")\n                except ValueError as e:\n                    msg = f\"RVI_Desc needs band: {e}\"\n                    raise ValueError(msg) from e\n\n                VV_desc = linear[:, vv_desc_index, :, :]\n                VH_desc = linear[:, vh_desc_index, :, :]\n                rvi_desc = 4 * VH_desc / (VV_desc + VH_desc + 1e-6)\n                rvi_desc = np.expand_dims(rvi_desc, axis=1)\n                rvi_channels.append(rvi_desc)\n            if rvi_channels:\n                rvi_concat = np.concatenate(rvi_channels, axis=1)\n                img = np.concatenate([img, rvi_concat], axis=1)\n        else:\n            if \"RVI_Asc\" in self.bands[sens]:\n                if linear.shape[0] &lt; 2:\n                    msg = f\"Not enough bands to calculate RVI_Asc. Available bands: {linear.shape[0]}\"\n                    raise ValueError(msg)\n                VV = linear[0]\n                VH = linear[1]\n                rvi_asc = 4 * VH / (VV + VH + 1e-6)\n                rvi_asc = np.expand_dims(rvi_asc, axis=0)\n                rvi_channels.append(rvi_asc)\n            if \"RVI_Desc\" in self.bands[sens]:\n                if linear.shape[0] &lt; 4:\n                    msg = f\"Not enough bands to calculate RVI_Desc. Available bands: {linear.shape[0]}\"\n                    raise ValueError(msg)\n                VV_desc = linear[2]\n                VH_desc = linear[3]\n                rvi_desc = 4 * VH_desc / (VV_desc + VH_desc + 1e-6)\n                rvi_desc = np.expand_dims(rvi_desc, axis=0) \n                rvi_channels.append(rvi_desc)\n            if rvi_channels:\n                rvi_concat = np.concatenate(rvi_channels, axis=0)\n                img = np.concatenate([linear, rvi_concat], axis=0)\n        return img\n\n    def _select_4_frames(self):\n        \"\"\"Filter the dataset to select only 4 frames per sample.\"\"\"\n\n        if \"cloud_percentage\" in self.df.columns:\n            self.df = self.df.sort_values(by=[\"chip_id\", \"cloud_percentage\"])\n        else:\n            self.df = self.df.sort_values(by=[\"chip_id\", \"num_month\"])\n\n        self.df = (\n            self.df.groupby(\"chip_id\")\n            .head(4)  # Select the first 4 frames per chip\n            .reset_index(drop=True)\n        )\n\n    def _process_sensor_images(self, sens: str, sens_filepaths: list[str]) -&gt; np.ndarray:\n        \"\"\"Process images for a given sensor.\"\"\"\n        img = self._load_input(sens_filepaths)\n        if sens == \"S1\":\n            img = img.astype(np.float32)\n            linear = 10 ** (img / 10)\n            img = self._compute_rvi(img, linear, sens)\n        if self.as_time_series:\n            img = img.transpose(0, 2, 3, 1)  # (T, H, W, C)\n        else:\n            img = img.transpose(1, 2, 0)  # (H, W, C)\n        if len(self.sensors) == 1:\n            img = img[..., self.band_indices]\n        else:\n            img = img[..., self.band_indices[sens]]\n        return img\n\n    def __getitem__(self, index: int) -&gt; dict:\n        sample_df = self.df[self.df[\"num_index\"] == index].copy()\n        # Sort by satellite and month\n        sample_df.sort_values(\n            by=[\"satellite\", \"num_month\"], inplace=True, ascending=True\n        )\n\n        filepaths = sample_df[\"filename\"].tolist()\n        output = {}\n\n        if len(self.sensors) == 1:\n            sens = self.sensors[0]\n            sens_filepaths = [fp for fp in filepaths if sens in fp]\n            img = self._process_sensor_images(sens, sens_filepaths)\n            output[\"image\"] = img.astype(np.float32)\n        else:\n            for sens in self.sensors:\n                sens_filepaths = [fp for fp in filepaths if sens in fp]\n                img = self._process_sensor_images(sens, sens_filepaths)\n                output[sens] = img.astype(np.float32)\n\n        # Load target\n        target_filename = sample_df[\"corresponding_agbm\"].unique()[0]\n        target = np.array(self._load_target(Path(target_filename)))\n        target = target.transpose(1, 2, 0)\n        output[\"mask\"] = target\n        if self.transform:\n            if len(self.sensors) == 1:\n                output = self.transform(**output)\n            else:\n                output = self.transform(output)\n        output[\"mask\"] = output[\"mask\"].squeeze().float()\n        return output\n\n    def _filter_and_select_data(self):\n        if (\n            self.max_cloud_percentage is not None\n            and \"cloud_percentage\" in self.df.columns\n        ):\n            self.df = self.df[self.df[\"cloud_percentage\"] &lt;= self.max_cloud_percentage]\n\n        if self.max_red_mean is not None and \"red_mean\" in self.df.columns:\n            self.df = self.df[self.df[\"red_mean\"] &lt;= self.max_red_mean]\n\n        if not self.include_corrupt and \"corrupt_values\" in self.df.columns:\n            self.df = self.df[self.df[\"corrupt_values\"] is False]\n\n    def _random_subsample(self):\n        if self.split == \"train\" and self.subset &lt; 1.0:\n            num_samples = int(len(self.df[\"num_index\"].unique()) * self.subset)\n            if self.seed is not None:\n                random.seed(self.seed)\n            selected_indices = random.sample(\n                list(self.df[\"num_index\"].unique()), num_samples\n            )\n            self.df = self.df[self.df[\"num_index\"].isin(selected_indices)]\n            self.df.reset_index(drop=True, inplace=True)\n\n    def plot(\n        self,\n        sample: dict[str, Tensor],\n        show_titles: bool = True,\n        suptitle: str | None = None,\n    ) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            show_titles: flag indicating whether to show titles above each panel\n            suptitle: optional suptitle to use for figure\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n        \"\"\"\n        # Determine if the sample contains multiple sensors or a single sensor\n        if isinstance(sample[\"image\"], dict):\n            ncols = len(self.sensors) + 1\n        else:\n            ncols = 2  # One for the image and one for the mask\n\n        showing_predictions = \"prediction\" in sample\n        if showing_predictions:\n            ncols += 1\n\n        fig, axs = plt.subplots(1, ncols=ncols, figsize=(5 * ncols, 10))\n\n        if isinstance(sample[\"image\"], dict):\n            # Multiple sensors case\n            for idx, sens in enumerate(self.sensors):\n                img = sample[\"image\"][sens].numpy()\n                if self.as_time_series:\n                    # Plot last time step\n                    img = img[:, -1, ...]\n                if sens == \"S2\":\n                    img = img[[2, 1, 0], ...].transpose(1, 2, 0)\n                    img = percentile_normalization(img)\n                else:\n                    co_polarization = img[0]  # transmit == receive\n                    cross_polarization = img[1]  # transmit != receive\n                    ratio = co_polarization / (cross_polarization + 1e-6)\n\n                    co_polarization = np.clip(co_polarization / 0.3, 0, 1)\n                    cross_polarization = np.clip(cross_polarization / 0.05, 0, 1)\n                    ratio = np.clip(ratio / 25, 0, 1)\n\n                    img = np.stack(\n                        (co_polarization, cross_polarization, ratio), axis=0\n                    )\n                    img = img.transpose(1, 2, 0)  # Convert to (H, W, 3)\n\n                axs[idx].imshow(img)\n                axs[idx].axis(\"off\")\n                if show_titles:\n                    axs[idx].set_title(sens)\n            mask_idx = len(self.sensors)\n        else:\n            # Single sensor case\n            sens = self.sensors[0]\n            img = sample[\"image\"].numpy()\n            if self.as_time_series:\n                # Plot last time step\n                img = img[:, -1, ...]\n            if sens == \"S2\":\n                img = img[[2, 1, 0], ...].transpose(1, 2, 0)\n                img = percentile_normalization(img)\n            else:\n                co_polarization = img[0]  # transmit == receive\n                cross_polarization = img[1]  # transmit != receive\n                ratio = co_polarization / (cross_polarization + 1e-6)\n\n                co_polarization = np.clip(co_polarization / 0.3, 0, 1)\n                cross_polarization = np.clip(cross_polarization / 0.05, 0, 1)\n                ratio = np.clip(ratio / 25, 0, 1)\n\n                img = np.stack(\n                    (co_polarization, cross_polarization, ratio), axis=0\n                )\n                img = img.transpose(1, 2, 0)  # Convert to (H, W, 3)\n\n            axs[0].imshow(img)\n            axs[0].axis(\"off\")\n            if show_titles:\n                axs[0].set_title(sens)\n            mask_idx = 1\n\n        # Plot target mask\n        if \"mask\" in sample:\n            target = sample[\"mask\"].squeeze()\n            target_im = axs[mask_idx].imshow(target, cmap=\"YlGn\")\n            plt.colorbar(target_im, ax=axs[mask_idx], fraction=0.046, pad=0.04)\n            axs[mask_idx].axis(\"off\")\n            if show_titles:\n                axs[mask_idx].set_title(\"Target\")\n\n        # Plot prediction if available\n        if showing_predictions:\n            pred_idx = mask_idx + 1\n            prediction = sample[\"prediction\"].squeeze()\n            pred_im = axs[pred_idx].imshow(prediction, cmap=\"YlGn\")\n            plt.colorbar(pred_im, ax=axs[pred_idx], fraction=0.046, pad=0.04)\n            axs[pred_idx].axis(\"off\")\n            if show_titles:\n                axs[pred_idx].set_title(\"Prediction\")\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.biomassters.BioMasstersNonGeo.__init__","title":"<code>__init__(root='data', split='train', bands=BAND_SETS['all'], transform=None, mask_mean=63.4584, mask_std=72.21242, sensors=['S1', 'S2'], as_time_series=False, metadata_filename=default_metadata_filename, max_cloud_percentage=None, max_red_mean=None, include_corrupt=True, subset=1, seed=42, use_four_frames=False)</code>","text":"<p>Initialize a new instance of BioMassters dataset.</p> <p>If <code>as_time_series=False</code> (the default), each time step becomes its own sample with the target being shared across multiple samples.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <p>root directory where dataset can be found</p> <code>'data'</code> <code>split</code> <code>str</code> <p>train or test split</p> <code>'train'</code> <code>sensors</code> <code>Sequence[str]</code> <p>which sensors to consider for the sample, Sentinel 1 and/or Sentinel 2 ('S1', 'S2')</p> <code>['S1', 'S2']</code> <code>as_time_series</code> <code>bool</code> <p>whether or not to return all available time-steps or just a single one for a given target location</p> <code>False</code> <code>metadata_filename</code> <code>str</code> <p>metadata file to be used</p> <code>default_metadata_filename</code> <code>max_cloud_percentage</code> <code>float | None</code> <p>maximum allowed cloud percentage for images</p> <code>None</code> <code>max_red_mean</code> <code>float | None</code> <p>maximum allowed red_mean value for images</p> <code>None</code> <code>include_corrupt</code> <code>bool</code> <p>whether to include images marked as corrupted</p> <code>True</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if <code>split</code> or <code>sensors</code> is invalid</p> <code>DatasetNotFoundError</code> <p>If dataset is not found.</p> Source code in <code>terratorch/datasets/biomassters.py</code> <pre><code>def __init__(\n    self,\n    root = \"data\",\n    split: str = \"train\",\n    bands: dict[str, Sequence[str]] | Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    mask_mean: float | None = 63.4584,\n    mask_std: float | None = 72.21242,\n    sensors: Sequence[str] = [\"S1\", \"S2\"],\n    as_time_series: bool = False,\n    metadata_filename: str = default_metadata_filename,\n    max_cloud_percentage: float | None = None,\n    max_red_mean: float | None = None,\n    include_corrupt: bool = True,\n    subset: float = 1,\n    seed: int = 42,\n    use_four_frames: bool = False\n) -&gt; None:\n    \"\"\"Initialize a new instance of BioMassters dataset.\n\n    If ``as_time_series=False`` (the default), each time step becomes its own\n    sample with the target being shared across multiple samples.\n\n    Args:\n        root: root directory where dataset can be found\n        split: train or test split\n        sensors: which sensors to consider for the sample, Sentinel 1 and/or\n            Sentinel 2 ('S1', 'S2')\n        as_time_series: whether or not to return all available\n            time-steps or just a single one for a given target location\n        metadata_filename: metadata file to be used\n        max_cloud_percentage: maximum allowed cloud percentage for images\n        max_red_mean: maximum allowed red_mean value for images\n        include_corrupt: whether to include images marked as corrupted\n\n    Raises:\n        AssertionError: if ``split`` or ``sensors`` is invalid\n        DatasetNotFoundError: If dataset is not found.\n    \"\"\"\n    self.root = root\n    self.sensors = sensors\n    self.bands = bands\n    assert (\n        split in self.valid_splits\n    ), f\"Please choose one of the valid splits: {self.valid_splits}.\"\n    self.split = split\n\n    assert set(sensors).issubset(\n        set(self.valid_sensors)\n    ), f\"Please choose a subset of valid sensors: {self.valid_sensors}.\"\n\n    if len(self.sensors) == 1:\n        sens = self.sensors[0]\n        self.band_indices = [\n            self.all_band_names[sens].index(band) for band in self.bands[sens]\n        ]\n    else:\n        self.band_indices = {\n            sens: [self.all_band_names[sens].index(band) for band in self.bands[sens]]\n            for sens in self.sensors\n        }\n\n    self.mask_mean = mask_mean\n    self.mask_std = mask_std\n    self.as_time_series = as_time_series\n    self.metadata_filename = metadata_filename\n    self.max_cloud_percentage = max_cloud_percentage\n    self.max_red_mean = max_red_mean\n    self.include_corrupt = include_corrupt\n    self.subset = subset\n    self.seed = seed\n    self.use_four_frames = use_four_frames\n\n    self._verify()\n\n    # open metadata csv files\n    self.df = pd.read_csv(os.path.join(self.root, self.metadata_filename))\n\n    # Filter sensors\n    self.df = self.df[self.df[\"satellite\"].isin(self.sensors)]\n\n    # Filter split\n    self.df = self.df[self.df[\"split\"] == self.split]\n\n    # Optional filtering\n    self._filter_and_select_data()\n\n    # Optional subsampling\n    self._random_subsample()\n\n    # generate numerical month from filename since first month is September\n    # and has numerical index of 0\n    self.df[\"num_month\"] = (\n        self.df[\"filename\"]\n        .str.split(\"_\", expand=True)[2]\n        .str.split(\".\", expand=True)[0]\n        .astype(int)\n    )\n\n    # Set dataframe index depending on the task for easier indexing\n    if self.as_time_series:\n        self.df[\"num_index\"] = self.df.groupby([\"chip_id\"]).ngroup()\n    else:\n        filter_df = (\n            self.df.groupby([\"chip_id\", \"month\"])[\"satellite\"].count().reset_index()\n        )\n        filter_df = filter_df[\n            filter_df[\"satellite\"] == len(self.sensors)\n        ].drop(\"satellite\", axis=1)\n        # Guarantee that each sample has corresponding number of images available\n        self.df = self.df.merge(filter_df, on=[\"chip_id\", \"month\"], how=\"inner\")\n\n        self.df[\"num_index\"] = self.df.groupby([\"chip_id\", \"month\"]).ngroup()\n\n    # Adjust transforms based on the number of sensors\n    if len(self.sensors) == 1:\n        self.transform = transform if transform else default_transform\n    elif transform is None:\n        self.transform = MultimodalToTensor(self.sensors)\n    else:\n        transform = {\n            s: transform[s] if s in transform else default_transform\n            for s in self.sensors\n        }\n        self.transform = MultimodalTransforms(transform, shared=False)\n\n    if self.use_four_frames:\n        self._select_4_frames()\n</code></pre>"},{"location":"datasets/#terratorch.datasets.biomassters.BioMasstersNonGeo.plot","title":"<code>plot(sample, show_titles=True, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>show_titles</code> <code>bool</code> <p>flag indicating whether to show titles above each panel</p> <code>True</code> <code>suptitle</code> <code>str | None</code> <p>optional suptitle to use for figure</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> Source code in <code>terratorch/datasets/biomassters.py</code> <pre><code>def plot(\n    self,\n    sample: dict[str, Tensor],\n    show_titles: bool = True,\n    suptitle: str | None = None,\n) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        show_titles: flag indicating whether to show titles above each panel\n        suptitle: optional suptitle to use for figure\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n    \"\"\"\n    # Determine if the sample contains multiple sensors or a single sensor\n    if isinstance(sample[\"image\"], dict):\n        ncols = len(self.sensors) + 1\n    else:\n        ncols = 2  # One for the image and one for the mask\n\n    showing_predictions = \"prediction\" in sample\n    if showing_predictions:\n        ncols += 1\n\n    fig, axs = plt.subplots(1, ncols=ncols, figsize=(5 * ncols, 10))\n\n    if isinstance(sample[\"image\"], dict):\n        # Multiple sensors case\n        for idx, sens in enumerate(self.sensors):\n            img = sample[\"image\"][sens].numpy()\n            if self.as_time_series:\n                # Plot last time step\n                img = img[:, -1, ...]\n            if sens == \"S2\":\n                img = img[[2, 1, 0], ...].transpose(1, 2, 0)\n                img = percentile_normalization(img)\n            else:\n                co_polarization = img[0]  # transmit == receive\n                cross_polarization = img[1]  # transmit != receive\n                ratio = co_polarization / (cross_polarization + 1e-6)\n\n                co_polarization = np.clip(co_polarization / 0.3, 0, 1)\n                cross_polarization = np.clip(cross_polarization / 0.05, 0, 1)\n                ratio = np.clip(ratio / 25, 0, 1)\n\n                img = np.stack(\n                    (co_polarization, cross_polarization, ratio), axis=0\n                )\n                img = img.transpose(1, 2, 0)  # Convert to (H, W, 3)\n\n            axs[idx].imshow(img)\n            axs[idx].axis(\"off\")\n            if show_titles:\n                axs[idx].set_title(sens)\n        mask_idx = len(self.sensors)\n    else:\n        # Single sensor case\n        sens = self.sensors[0]\n        img = sample[\"image\"].numpy()\n        if self.as_time_series:\n            # Plot last time step\n            img = img[:, -1, ...]\n        if sens == \"S2\":\n            img = img[[2, 1, 0], ...].transpose(1, 2, 0)\n            img = percentile_normalization(img)\n        else:\n            co_polarization = img[0]  # transmit == receive\n            cross_polarization = img[1]  # transmit != receive\n            ratio = co_polarization / (cross_polarization + 1e-6)\n\n            co_polarization = np.clip(co_polarization / 0.3, 0, 1)\n            cross_polarization = np.clip(cross_polarization / 0.05, 0, 1)\n            ratio = np.clip(ratio / 25, 0, 1)\n\n            img = np.stack(\n                (co_polarization, cross_polarization, ratio), axis=0\n            )\n            img = img.transpose(1, 2, 0)  # Convert to (H, W, 3)\n\n        axs[0].imshow(img)\n        axs[0].axis(\"off\")\n        if show_titles:\n            axs[0].set_title(sens)\n        mask_idx = 1\n\n    # Plot target mask\n    if \"mask\" in sample:\n        target = sample[\"mask\"].squeeze()\n        target_im = axs[mask_idx].imshow(target, cmap=\"YlGn\")\n        plt.colorbar(target_im, ax=axs[mask_idx], fraction=0.046, pad=0.04)\n        axs[mask_idx].axis(\"off\")\n        if show_titles:\n            axs[mask_idx].set_title(\"Target\")\n\n    # Plot prediction if available\n    if showing_predictions:\n        pred_idx = mask_idx + 1\n        prediction = sample[\"prediction\"].squeeze()\n        pred_im = axs[pred_idx].imshow(prediction, cmap=\"YlGn\")\n        plt.colorbar(pred_im, ax=axs[pred_idx], fraction=0.046, pad=0.04)\n        axs[pred_idx].axis(\"off\")\n        if show_titles:\n            axs[pred_idx].set_title(\"Prediction\")\n\n    if suptitle is not None:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.burn_intensity","title":"<code>terratorch.datasets.burn_intensity</code>","text":""},{"location":"datasets/#terratorch.datasets.burn_intensity.BurnIntensityNonGeo","title":"<code>BurnIntensityNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>Dataset implementation for Burn Intensity classification.</p> Source code in <code>terratorch/datasets/burn_intensity.py</code> <pre><code>class BurnIntensityNonGeo(NonGeoDataset):\n    \"\"\"Dataset implementation for [Burn Intensity classification](https://huggingface.co/datasets/ibm-nasa-geospatial/burn_intensity).\"\"\"\n\n    all_band_names = (\n        \"BLUE\", \"GREEN\", \"RED\", \"NIR\", \"SWIR_1\", \"SWIR_2\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    class_names = (\n        \"No burn\",\n        \"Unburned to Very Low\",\n        \"Low Severity\",\n        \"Moderate Severity\",\n        \"High Severity\"\n    )\n\n    CSV_FILES = {\n        \"limited\": \"BS_files_with_less_than_25_percent_zeros.csv\",\n        \"full\": \"BS_files_raw.csv\",\n    }\n\n    num_classes = 5\n    splits = {\"train\": \"train\", \"val\": \"val\"}\n    time_steps = [\"pre\", \"during\", \"post\"]\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        use_full_data: bool = True,\n        no_data_replace: float | None = 0.0001,\n        no_label_replace: int | None = -1,\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the BurnIntensity dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train' or 'val'.\n            bands (Sequence[str]): Bands to output. Defaults to all bands.\n            transform (Optional[A.Compose]): Albumentations transform to be applied.\n            use_metadata (bool): Whether to return metadata info (location).\n            use_full_data (bool): Wheter to use full data or data with less than 25 percent zeros.\n            no_data_replace (Optional[float]): Value to replace NaNs in images.\n            no_label_replace (Optional[int]): Value to replace NaNs in labels.\n        \"\"\"\n        super().__init__()\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n\n        self.data_root = Path(data_root)\n\n        # Read the CSV file to get the list of cases to include\n        csv_file_key = \"full\" if use_full_data else \"limited\"\n        csv_path = self.data_root / self.CSV_FILES[csv_file_key]\n        df = pd.read_csv(csv_path)\n        casenames = df[\"Case_Name\"].tolist()\n\n        split_file = self.data_root / f\"{split}.txt\"\n        with open(split_file) as f:\n            split_images = [line.strip() for line in f.readlines()]\n\n        split_images = [img for img in split_images if self._extract_casename(img) in casenames]\n\n        # Build the samples list\n        self.samples = []\n        for image_filename in split_images:\n            image_files = []\n            for time_step in self.time_steps:\n                image_file = self.data_root / time_step / image_filename\n                image_files.append(str(image_file))\n            mask_filename = image_filename.replace(\"HLS_\", \"BS_\")\n            mask_file = self.data_root / \"pre\" / mask_filename\n            self.samples.append({\n                \"image_files\": image_files,\n                \"mask_file\": str(mask_file),\n                \"casename\": self._extract_casename(image_filename),\n            })\n\n        self.use_metadata = use_metadata\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n\n        self.transform = transform if transform else default_transform\n\n    def _extract_basename(self, filepath: str) -&gt; str:\n        \"\"\"Extract the base filename without extension.\"\"\"\n        return os.path.splitext(os.path.basename(filepath))[0]\n\n    def _extract_casename(self, filename: str) -&gt; str:\n        \"\"\"Extract the casename from the filename.\"\"\"\n        basename = self._extract_basename(filename)\n        # Remove 'HLS_' or 'BS_' prefix\n        casename = basename.replace(\"HLS_\", \"\").replace(\"BS_\", \"\")\n        return casename\n\n    def __len__(self) -&gt; int:\n        return len(self.samples)\n\n    def _get_coords(self, image: DataArray) -&gt; torch.Tensor:\n        pixel_scale = image.rio.resolution()\n        width, height = image.rio.width, image.rio.height\n\n        left, bottom, right, top = image.rio.bounds()\n        tie_point_x, tie_point_y = left, top\n\n        center_col = width / 2\n        center_row = height / 2\n\n        center_lon = tie_point_x + (center_col * pixel_scale[0])\n        center_lat = tie_point_y - (center_row * pixel_scale[1])\n\n        lat_lon = np.asarray([center_lat, center_lon])\n        return torch.tensor(lat_lon, dtype=torch.float32)\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        sample = self.samples[index]\n        image_files = sample[\"image_files\"]\n        mask_file = sample[\"mask_file\"]\n\n        images = []\n        for idx, image_file in enumerate(image_files):\n            image = self._load_file(Path(image_file), nan_replace=self.no_data_replace)\n            if idx == 0 and self.use_metadata:\n                location_coords = self._get_coords(image)\n            image = image.to_numpy()\n            image = np.moveaxis(image, 0, -1)\n            image = image[..., self.band_indices]\n            images.append(image)\n\n        images = np.stack(images, axis=0)  # (T, H, W, C)\n\n        output = {\n            \"image\": images.astype(np.float32),\n            \"mask\": self._load_file(Path(mask_file), nan_replace=self.no_label_replace).to_numpy()[0]\n        }\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"mask\"] = output[\"mask\"].long()\n        if self.use_metadata:\n            output[\"location_coords\"] = location_coords\n\n        return output\n\n    def _load_file(self, path: Path, nan_replace: float | int | None = None) -&gt; DataArray:\n        data = rioxarray.open_rasterio(path, masked=True)\n        if nan_replace is not None:\n            data = data.fillna(nan_replace)\n        return data\n\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Any:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: A sample returned by `__getitem__`.\n            suptitle: Optional string to use as a suptitle.\n\n        Returns:\n            A matplotlib Figure with the rendered sample.\n        \"\"\"\n        num_images = len(self.time_steps) + 2\n        if \"prediction\" in sample:\n            num_images += 1\n\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        images = sample[\"image\"]  # (C, T, H, W)\n        mask = sample[\"mask\"].numpy()\n        num_classes = len(np.unique(mask))\n\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 5, 5))\n\n        for i in range(len(self.time_steps)):\n            image = images[:, i, :, :]  # (C, H, W)\n            image = np.transpose(image, (1, 2, 0))  # (H, W, C)\n            rgb_image = image[..., rgb_indices]\n            rgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min() + 1e-8)\n            rgb_image = np.clip(rgb_image, 0, 1)\n            ax[i].imshow(rgb_image)\n            ax[i].axis(\"off\")\n            ax[i].set_title(f\"{self.time_steps[i].capitalize()} Image\")\n\n        cmap = plt.get_cmap(\"jet\", num_classes)\n        norm = Normalize(vmin=0, vmax=num_classes - 1)\n\n        mask_ax_index = len(self.time_steps)\n        ax[mask_ax_index].imshow(mask, cmap=cmap, norm=norm)\n        ax[mask_ax_index].axis(\"off\")\n        ax[mask_ax_index].set_title(\"Ground Truth Mask\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"].numpy()\n            pred_ax_index = mask_ax_index + 1\n            ax[pred_ax_index].imshow(prediction, cmap=cmap, norm=norm)\n            ax[pred_ax_index].axis(\"off\")\n            ax[pred_ax_index].set_title(\"Predicted Mask\")\n\n        legend_ax_index = -1\n        class_names = sample.get(\"class_names\", self.class_names)\n        positions = np.linspace(0, 1, num_classes) if num_classes &gt; 1 else [0.5]\n\n        legend_handles = [\n            mpatches.Patch(color=cmap(pos), label=class_names[i])\n            for i, pos in enumerate(positions)\n        ]\n        ax[legend_ax_index].legend(handles=legend_handles, loc=\"center\")\n        ax[legend_ax_index].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        plt.tight_layout()\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.burn_intensity.BurnIntensityNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, use_full_data=True, no_data_replace=0.0001, no_label_replace=-1, use_metadata=False)</code>","text":"<p>Initialize the BurnIntensity dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train' or 'val'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to output. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Optional[Compose]</code> <p>Albumentations transform to be applied.</p> <code>None</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (location).</p> <code>False</code> <code>use_full_data</code> <code>bool</code> <p>Wheter to use full data or data with less than 25 percent zeros.</p> <code>True</code> <code>no_data_replace</code> <code>Optional[float]</code> <p>Value to replace NaNs in images.</p> <code>0.0001</code> <code>no_label_replace</code> <code>Optional[int]</code> <p>Value to replace NaNs in labels.</p> <code>-1</code> Source code in <code>terratorch/datasets/burn_intensity.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    use_full_data: bool = True,\n    no_data_replace: float | None = 0.0001,\n    no_label_replace: int | None = -1,\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the BurnIntensity dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train' or 'val'.\n        bands (Sequence[str]): Bands to output. Defaults to all bands.\n        transform (Optional[A.Compose]): Albumentations transform to be applied.\n        use_metadata (bool): Whether to return metadata info (location).\n        use_full_data (bool): Wheter to use full data or data with less than 25 percent zeros.\n        no_data_replace (Optional[float]): Value to replace NaNs in images.\n        no_label_replace (Optional[int]): Value to replace NaNs in labels.\n    \"\"\"\n    super().__init__()\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n\n    self.data_root = Path(data_root)\n\n    # Read the CSV file to get the list of cases to include\n    csv_file_key = \"full\" if use_full_data else \"limited\"\n    csv_path = self.data_root / self.CSV_FILES[csv_file_key]\n    df = pd.read_csv(csv_path)\n    casenames = df[\"Case_Name\"].tolist()\n\n    split_file = self.data_root / f\"{split}.txt\"\n    with open(split_file) as f:\n        split_images = [line.strip() for line in f.readlines()]\n\n    split_images = [img for img in split_images if self._extract_casename(img) in casenames]\n\n    # Build the samples list\n    self.samples = []\n    for image_filename in split_images:\n        image_files = []\n        for time_step in self.time_steps:\n            image_file = self.data_root / time_step / image_filename\n            image_files.append(str(image_file))\n        mask_filename = image_filename.replace(\"HLS_\", \"BS_\")\n        mask_file = self.data_root / \"pre\" / mask_filename\n        self.samples.append({\n            \"image_files\": image_files,\n            \"mask_file\": str(mask_file),\n            \"casename\": self._extract_casename(image_filename),\n        })\n\n    self.use_metadata = use_metadata\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.burn_intensity.BurnIntensityNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by <code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/burn_intensity.py</code> <pre><code>def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Any:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: A sample returned by `__getitem__`.\n        suptitle: Optional string to use as a suptitle.\n\n    Returns:\n        A matplotlib Figure with the rendered sample.\n    \"\"\"\n    num_images = len(self.time_steps) + 2\n    if \"prediction\" in sample:\n        num_images += 1\n\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    images = sample[\"image\"]  # (C, T, H, W)\n    mask = sample[\"mask\"].numpy()\n    num_classes = len(np.unique(mask))\n\n    fig, ax = plt.subplots(1, num_images, figsize=(num_images * 5, 5))\n\n    for i in range(len(self.time_steps)):\n        image = images[:, i, :, :]  # (C, H, W)\n        image = np.transpose(image, (1, 2, 0))  # (H, W, C)\n        rgb_image = image[..., rgb_indices]\n        rgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min() + 1e-8)\n        rgb_image = np.clip(rgb_image, 0, 1)\n        ax[i].imshow(rgb_image)\n        ax[i].axis(\"off\")\n        ax[i].set_title(f\"{self.time_steps[i].capitalize()} Image\")\n\n    cmap = plt.get_cmap(\"jet\", num_classes)\n    norm = Normalize(vmin=0, vmax=num_classes - 1)\n\n    mask_ax_index = len(self.time_steps)\n    ax[mask_ax_index].imshow(mask, cmap=cmap, norm=norm)\n    ax[mask_ax_index].axis(\"off\")\n    ax[mask_ax_index].set_title(\"Ground Truth Mask\")\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"].numpy()\n        pred_ax_index = mask_ax_index + 1\n        ax[pred_ax_index].imshow(prediction, cmap=cmap, norm=norm)\n        ax[pred_ax_index].axis(\"off\")\n        ax[pred_ax_index].set_title(\"Predicted Mask\")\n\n    legend_ax_index = -1\n    class_names = sample.get(\"class_names\", self.class_names)\n    positions = np.linspace(0, 1, num_classes) if num_classes &gt; 1 else [0.5]\n\n    legend_handles = [\n        mpatches.Patch(color=cmap(pos), label=class_names[i])\n        for i, pos in enumerate(positions)\n    ]\n    ax[legend_ax_index].legend(handles=legend_handles, loc=\"center\")\n    ax[legend_ax_index].axis(\"off\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.carbonflux","title":"<code>terratorch.datasets.carbonflux</code>","text":""},{"location":"datasets/#terratorch.datasets.carbonflux.CarbonFluxNonGeo","title":"<code>CarbonFluxNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>Dataset for Carbon Flux regression from HLS images and MERRA data.</p> Source code in <code>terratorch/datasets/carbonflux.py</code> <pre><code>class CarbonFluxNonGeo(NonGeoDataset):\n    \"\"\"Dataset for [Carbon Flux](https://huggingface.co/datasets/ibm-nasa-geospatial/hls_merra2_gppFlux) regression from HLS images and MERRA data.\"\"\"\n\n    all_band_names = (\n        \"BLUE\", \"GREEN\", \"RED\", \"NIR\", \"SWIR_1\", \"SWIR_2\",\n    )\n\n    rgb_bands = (\n        \"RED\", \"GREEN\", \"BLUE\",\n    )\n\n    merra_var_names = (\n        \"T2MIN\", \"T2MAX\", \"T2MEAN\", \"TSMDEWMEAN\", \"GWETROOT\",\n        \"LHLAND\", \"SHLAND\", \"SWLAND\", \"PARDFLAND\", \"PRECTOTLAND\"\n    )\n\n    splits = {\"train\": \"train\", \"test\": \"test\"}\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    metadata_file = \"data_train_hls_37sites_v0_1.csv\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        gpp_mean: float | None = None,\n        gpp_std: float | None = None,\n        no_data_replace: float | None = 0.0001,\n        use_metadata: bool = False,\n        modalities: Sequence[str] = (\"image\", \"merra_vars\")\n    ) -&gt; None:\n        \"\"\"Initialize the CarbonFluxNonGeo dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): 'train' or 'test'.\n            bands (Sequence[str]): Bands to use. Defaults to all bands.\n            transform (Optional[A.Compose]): Albumentations transform to be applied.\n            use_metadata (bool): Whether to return metadata (coordinates and date).\n            merra_means (Sequence[float]): Means for MERRA data normalization.\n            merra_stds (Sequence[float]): Standard deviations for MERRA data normalization.\n            gpp_mean (float): Mean for GPP normalization.\n            gpp_std (float): Standard deviation for GPP normalization.\n            no_data_replace (Optional[float]): Value to replace NO_DATA values in images.\n        \"\"\"\n        super().__init__()\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = [self.all_band_names.index(band) for band in bands]\n\n        self.data_root = Path(data_root)\n\n        # Load the CSV file with metadata\n        csv_file = self.data_root / self.metadata_file\n        df = pd.read_csv(csv_file)\n\n        # Get list of image filenames in the split directory\n        image_dir = self.data_root / self.split\n        image_files = [f.name for f in image_dir.glob(\"*.tiff\")]\n\n        df[\"Chip\"] = df[\"Chip\"].str.replace(\".tif$\", \".tiff\", regex=True)\n        # Filter the DataFrame to include only rows with 'Chip' in image_files\n        df = df[df[\"Chip\"].isin(image_files)]\n\n        # Build the samples list\n        self.samples = []\n        for _, row in df.iterrows():\n            image_filename = row[\"Chip\"]\n            image_path = image_dir / image_filename\n            # MERRA vectors\n            merra_vars = row[list(self.merra_var_names)].values.astype(np.float32)\n            # GPP target\n            gpp = row[\"GPP\"]\n\n            image_path = image_dir / row[\"Chip\"]\n            merra_vars = row[list(self.merra_var_names)].values.astype(np.float32)\n            gpp = row[\"GPP\"]\n            self.samples.append({\n                \"image_path\": str(image_path),\n                \"merra_vars\": merra_vars,\n                \"gpp\": gpp,\n            })\n\n        if gpp_mean is None or gpp_std is None:\n            msg = \"Mean and standard deviation for GPP must be provided.\"\n            raise ValueError(msg)\n        self.gpp_mean = gpp_mean\n        self.gpp_std = gpp_std\n\n        self.use_metadata = use_metadata\n        self.modalities = modalities\n        self.no_data_replace = no_data_replace\n\n        if transform is None:\n            self.transform = MultimodalToTensor(self.modalities)\n        else:\n            transform = {m: transform[m] if m in transform else default_transform\n                for m in self.modalities}\n            self.transform = MultimodalTransforms(transform, shared=False)\n\n    def __len__(self) -&gt; int:\n        return len(self.samples)\n\n    def _load_file(self, path: str, nan_replace: float | int | None = None):\n        data = rioxarray.open_rasterio(path, masked=True)\n        if nan_replace is not None:\n            data = data.fillna(nan_replace)\n        return data\n\n    def _get_coords(self, image) -&gt; torch.Tensor:\n        \"\"\"Extract the center coordinates from the image geospatial metadata.\"\"\"\n        pixel_scale = image.rio.resolution()\n        width, height = image.rio.width, image.rio.height\n\n        left, bottom, right, top = image.rio.bounds()\n        tie_point_x, tie_point_y = left, top\n\n        center_col = width / 2\n        center_row = height / 2\n\n        center_lon = tie_point_x + (center_col * pixel_scale[0])\n        center_lat = tie_point_y - (center_row * pixel_scale[1])\n\n        src_crs = image.rio.crs\n        dst_crs = \"EPSG:4326\"\n\n        transformer = pyproj.Transformer.from_crs(src_crs, dst_crs, always_xy=True)\n        lon, lat = transformer.transform(center_lon, center_lat)\n\n        coords = np.array([lat, lon], dtype=np.float32)\n        return torch.from_numpy(coords)\n\n    def _get_date(self, filename: str) -&gt; torch.Tensor:\n        \"\"\"Extract the date from the filename.\"\"\"\n        base_filename = os.path.basename(filename)\n        pattern = r\"HLS\\..{3}\\.[A-Z0-9]{6}\\.(?P&lt;date&gt;\\d{7}T\\d{6})\\..*\\.tiff$\"\n        match = re.match(pattern, base_filename)\n        if not match:\n            msg = f\"Filename {filename} does not match expected pattern.\"\n            raise ValueError(msg)\n\n        date_str = match.group(\"date\")\n        year = int(date_str[:4])\n        julian_day = int(date_str[4:7])\n\n        date_tensor = torch.tensor([year, julian_day], dtype=torch.int32)\n        return date_tensor\n\n    def __getitem__(self, idx: int) -&gt; dict[str, Any]:\n        sample = self.samples[idx]\n        image_path = sample[\"image_path\"]\n\n        image = self._load_file(image_path, nan_replace=self.no_data_replace)\n\n        if self.use_metadata:\n            location_coords = self._get_coords(image)\n            temporal_coords = self._get_date(os.path.basename(image_path))\n\n        image = image.to_numpy()  # (C, H, W)\n        image = image[self.band_indices, ...]\n        image = np.moveaxis(image, 0, -1) # (H, W, C)\n\n        merra_vars = np.array(sample[\"merra_vars\"])\n        target = np.array(sample[\"gpp\"])\n        target_norm = (target - self.gpp_mean) / self.gpp_std\n        target_norm = torch.tensor(target_norm, dtype=torch.float32)\n        output = {\n            \"image\": image.astype(np.float32),\n            \"merra_vars\": merra_vars,\n        }\n\n        if self.transform:\n            output = self.transform(output)\n\n        output = {\n            \"image\": {m: output[m] for m in self.modalities if m in output},\n            \"mask\": target_norm\n        }\n        if self.use_metadata:\n            output[\"location_coords\"] = location_coords\n            output[\"temporal_coords\"] = temporal_coords\n\n        return output\n\n    def plot(self, sample: dict[str, Any], suptitle: str | None = None) -&gt; Any:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: A sample returned by `__getitem__`.\n            suptitle: Optional title for the figure.\n\n        Returns:\n            A matplotlib figure with the rendered sample.\n        \"\"\"\n        image = sample[\"image\"].numpy()\n\n        image = np.transpose(image, (1, 2, 0))  # (H, W, C)\n\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        rgb_image = image[..., rgb_indices]\n\n        rgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min() + 1e-8)\n        rgb_image = np.clip(rgb_image, 0, 1)\n\n        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n        ax.imshow(rgb_image)\n        ax.axis(\"off\")\n        ax.set_title(\"Image\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        plt.tight_layout()\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.carbonflux.CarbonFluxNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, gpp_mean=None, gpp_std=None, no_data_replace=0.0001, use_metadata=False, modalities=('image', 'merra_vars'))</code>","text":"<p>Initialize the CarbonFluxNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>'train' or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to use. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Optional[Compose]</code> <p>Albumentations transform to be applied.</p> <code>None</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata (coordinates and date).</p> <code>False</code> <code>merra_means</code> <code>Sequence[float]</code> <p>Means for MERRA data normalization.</p> required <code>merra_stds</code> <code>Sequence[float]</code> <p>Standard deviations for MERRA data normalization.</p> required <code>gpp_mean</code> <code>float</code> <p>Mean for GPP normalization.</p> <code>None</code> <code>gpp_std</code> <code>float</code> <p>Standard deviation for GPP normalization.</p> <code>None</code> <code>no_data_replace</code> <code>Optional[float]</code> <p>Value to replace NO_DATA values in images.</p> <code>0.0001</code> Source code in <code>terratorch/datasets/carbonflux.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    gpp_mean: float | None = None,\n    gpp_std: float | None = None,\n    no_data_replace: float | None = 0.0001,\n    use_metadata: bool = False,\n    modalities: Sequence[str] = (\"image\", \"merra_vars\")\n) -&gt; None:\n    \"\"\"Initialize the CarbonFluxNonGeo dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): 'train' or 'test'.\n        bands (Sequence[str]): Bands to use. Defaults to all bands.\n        transform (Optional[A.Compose]): Albumentations transform to be applied.\n        use_metadata (bool): Whether to return metadata (coordinates and date).\n        merra_means (Sequence[float]): Means for MERRA data normalization.\n        merra_stds (Sequence[float]): Standard deviations for MERRA data normalization.\n        gpp_mean (float): Mean for GPP normalization.\n        gpp_std (float): Standard deviation for GPP normalization.\n        no_data_replace (Optional[float]): Value to replace NO_DATA values in images.\n    \"\"\"\n    super().__init__()\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = [self.all_band_names.index(band) for band in bands]\n\n    self.data_root = Path(data_root)\n\n    # Load the CSV file with metadata\n    csv_file = self.data_root / self.metadata_file\n    df = pd.read_csv(csv_file)\n\n    # Get list of image filenames in the split directory\n    image_dir = self.data_root / self.split\n    image_files = [f.name for f in image_dir.glob(\"*.tiff\")]\n\n    df[\"Chip\"] = df[\"Chip\"].str.replace(\".tif$\", \".tiff\", regex=True)\n    # Filter the DataFrame to include only rows with 'Chip' in image_files\n    df = df[df[\"Chip\"].isin(image_files)]\n\n    # Build the samples list\n    self.samples = []\n    for _, row in df.iterrows():\n        image_filename = row[\"Chip\"]\n        image_path = image_dir / image_filename\n        # MERRA vectors\n        merra_vars = row[list(self.merra_var_names)].values.astype(np.float32)\n        # GPP target\n        gpp = row[\"GPP\"]\n\n        image_path = image_dir / row[\"Chip\"]\n        merra_vars = row[list(self.merra_var_names)].values.astype(np.float32)\n        gpp = row[\"GPP\"]\n        self.samples.append({\n            \"image_path\": str(image_path),\n            \"merra_vars\": merra_vars,\n            \"gpp\": gpp,\n        })\n\n    if gpp_mean is None or gpp_std is None:\n        msg = \"Mean and standard deviation for GPP must be provided.\"\n        raise ValueError(msg)\n    self.gpp_mean = gpp_mean\n    self.gpp_std = gpp_std\n\n    self.use_metadata = use_metadata\n    self.modalities = modalities\n    self.no_data_replace = no_data_replace\n\n    if transform is None:\n        self.transform = MultimodalToTensor(self.modalities)\n    else:\n        transform = {m: transform[m] if m in transform else default_transform\n            for m in self.modalities}\n        self.transform = MultimodalTransforms(transform, shared=False)\n</code></pre>"},{"location":"datasets/#terratorch.datasets.carbonflux.CarbonFluxNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Any]</code> <p>A sample returned by <code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional title for the figure.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A matplotlib figure with the rendered sample.</p> Source code in <code>terratorch/datasets/carbonflux.py</code> <pre><code>def plot(self, sample: dict[str, Any], suptitle: str | None = None) -&gt; Any:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: A sample returned by `__getitem__`.\n        suptitle: Optional title for the figure.\n\n    Returns:\n        A matplotlib figure with the rendered sample.\n    \"\"\"\n    image = sample[\"image\"].numpy()\n\n    image = np.transpose(image, (1, 2, 0))  # (H, W, C)\n\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    rgb_image = image[..., rgb_indices]\n\n    rgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min() + 1e-8)\n    rgb_image = np.clip(rgb_image, 0, 1)\n\n    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n    ax.imshow(rgb_image)\n    ax.axis(\"off\")\n    ax.set_title(\"Image\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.forestnet","title":"<code>terratorch.datasets.forestnet</code>","text":""},{"location":"datasets/#terratorch.datasets.forestnet.ForestNetNonGeo","title":"<code>ForestNetNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for ForestNet.</p> Source code in <code>terratorch/datasets/forestnet.py</code> <pre><code>class ForestNetNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [ForestNet](https://huggingface.co/datasets/ibm-nasa-geospatial/ForestNet).\"\"\"\n\n    all_band_names = (\n        \"RED\", \"GREEN\", \"BLUE\", \"NIR\", \"SWIR_1\", \"SWIR_2\"\n    )\n\n    rgb_bands = (\n        \"RED\", \"GREEN\", \"BLUE\",\n    )\n\n    splits = (\"train\", \"test\", \"val\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    default_label_map = {  # noqa: RUF012\n        \"Plantation\": 0,\n        \"Smallholder agriculture\": 1,\n        \"Grassland shrubland\": 2,\n        \"Other\": 3,\n    }\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        label_map: dict[str, int] = default_label_map,\n        transform: A.Compose | None = None,\n        fraction: float = 1.0,\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ForestNetNonGeo dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            label_map (Dict[str, int]): Mapping from label names to integer labels.\n            transform: Transformations to be applied to the images.\n            fraction (float): Fraction of the dataset to use. Defaults to 1.0 (use all data).\n        \"\"\"\n        super().__init__()\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits)}.\"\n            raise ValueError(msg)\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n        self.use_metadata = use_metadata\n\n        self.data_root = Path(data_root)\n        self.label_map = label_map\n\n        # Load the CSV file corresponding to the split\n        csv_file = self.data_root / f\"{split}_filtered.csv\"\n        original_df = pd.read_csv(csv_file)\n\n        # Apply stratified sampling if fraction &lt; 1.0\n        if fraction &lt; 1.0:\n            sss = StratifiedShuffleSplit(n_splits=1, test_size=1 - fraction, random_state=47)\n            stratified_indices, _ = next(sss.split(original_df, original_df[\"merged_label\"]))\n            self.dataset = original_df.iloc[stratified_indices].reset_index(drop=True)\n        else:\n            self.dataset = original_df\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.dataset)\n\n    def _get_coords(self, event_path: Path) -&gt; torch.Tensor:\n        auxiliary_path = event_path / \"auxiliary\"\n        osm_json_path = auxiliary_path / \"osm.json\"\n\n        with open(osm_json_path) as f:\n            osm_data = json.load(f)\n            lat = float(osm_data[\"closest_city\"][\"lat\"])\n            lon = float(osm_data[\"closest_city\"][\"lon\"])\n            lat_lon = np.asarray([lat, lon])\n\n        return torch.tensor(lat_lon, dtype=torch.float32)\n\n    def _get_dates(self, image_files: list) -&gt; list:\n        dates = []\n        pattern = re.compile(r\"(\\d{4})_(\\d{2})_(\\d{2})_cloud_\\d+\\.(png|npy)\")\n        for img_path in image_files:\n            match = pattern.search(img_path)\n            year, month, day = int(match.group(1)), int(match.group(2)), int(match.group(3))\n            date_obj = datetime.datetime(year, month, day)  # noqa: DTZ001\n            julian_day = date_obj.timetuple().tm_yday\n            date_tensor = torch.tensor([year, julian_day], dtype=torch.int32)\n            dates.append(date_tensor)\n        return torch.stack(dates, dim=0)\n\n    def __getitem__(self, index: int):\n        path = self.data_root / self.dataset[\"example_path\"][index]\n        label = self.map_label(index)\n\n        visible_images, infrared_images, temporal_coords = self._load_images(path)\n\n        visible_images = np.stack(visible_images, axis=0)\n        infrared_images = np.stack(infrared_images, axis=0)\n        merged_images = np.concatenate([visible_images, infrared_images], axis=-1)\n        merged_images = merged_images[..., self.band_indices] # (T, H, W, 2C)\n        output = {\n            \"image\": merged_images.astype(np.float32)\n        }\n\n        if self.transform:\n            output = self.transform(**output)\n\n        if self.use_metadata:\n            location_coords = self._get_coords(path)\n            output[\"location_coords\"] = location_coords\n            output[\"temporal_coords\"] = temporal_coords\n\n        output[\"label\"] = label\n\n        return output\n\n    def _load_images(self, path: str):\n        \"\"\"Load visible and infrared images from the given event path\"\"\"\n        visible_image_files = glob.glob(os.path.join(path, \"images/visible/*_cloud_*.png\"))\n        infra_image_files = glob.glob(os.path.join(path, \"images/infrared/*_cloud_*.npy\"))\n\n        selected_visible_images = self.select_images(visible_image_files)\n        selected_infra_images = self.select_images(infra_image_files)\n\n        dates = None\n        if self.use_metadata:\n            dates = self._get_dates(selected_visible_images)\n\n        vis_images = [np.array(Image.open(img)) for img in selected_visible_images] # (T, H, W, C)\n        inf_images = [np.load(img, allow_pickle=True) for img in selected_infra_images] # (T, H, W, C)\n        return vis_images, inf_images, dates\n\n    def least_cloudy_image(self, image_files):\n        pattern = re.compile(r\"(\\d{4})_\\d{2}_\\d{2}_cloud_(\\d+)\\.(png|npy)\")\n        lowest_cloud_images = defaultdict(lambda: {\"path\": None, \"cloud_value\": float(\"inf\")})\n\n        for path in image_files:\n            match = pattern.search(path)\n            if match:\n                year, cloud_value = match.group(1), int(match.group(2))\n                if cloud_value &lt; lowest_cloud_images[year][\"cloud_value\"]:\n                    lowest_cloud_images[year] = {\"path\": path, \"cloud_value\": cloud_value}\n\n        return [info[\"path\"] for info in lowest_cloud_images.values()]\n\n    def match_timesteps(self, image_files, selected_images):\n        if len(selected_images) &lt; 3:\n            extra_imgs = [img for img in image_files if img not in selected_images]\n            selected_images += extra_imgs[:3 - len(selected_images)]\n\n        while len(selected_images) &lt; 3:\n            selected_images.append(selected_images[-1])\n        return selected_images[:3]\n\n    def select_images(self, image_files):\n        selected = self.least_cloudy_image(image_files)\n        return self.match_timesteps(image_files, selected)\n\n    def map_label(self, index: int) -&gt; torch.Tensor:\n        \"\"\"Map the label name to an integer label.\"\"\"\n        label_name = self.dataset[\"merged_label\"][index]\n        label = self.label_map[label_name]\n        return label\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None):\n\n        num_images = sample[\"image\"].shape[1] + 1\n\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        fig, ax = plt.subplots(1, num_images, figsize=(15, 5))\n\n        for i in range(sample[\"image\"].shape[1]):\n            image = sample[\"image\"][:, i, :, :]\n            if torch.is_tensor(image):\n                image = image.permute(1, 2, 0).numpy()\n            rgb_image = image[..., rgb_indices]\n            rgb_image = (rgb_image - rgb_image.min()) / (rgb_image.max() - rgb_image.min() + 1e-8)\n            rgb_image = np.clip(rgb_image, 0, 1)\n            ax[i].imshow(rgb_image)\n            ax[i].axis(\"off\")\n            ax[i].set_title(f\"Timestep {i + 1}\")\n\n        legend_handles = [Rectangle((0, 0), 1, 1, color=\"blue\")]\n        legend_label = [self.label_map.get(sample[\"label\"], \"Unknown Label\")]\n        ax[-1].legend(legend_handles, legend_label, loc=\"center\")\n        ax[-1].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        plt.tight_layout()\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.forestnet.ForestNetNonGeo.__init__","title":"<code>__init__(data_root, split='train', label_map=default_label_map, transform=None, fraction=1.0, bands=BAND_SETS['all'], use_metadata=False)</code>","text":"<p>Initialize the ForestNetNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>label_map</code> <code>Dict[str, int]</code> <p>Mapping from label names to integer labels.</p> <code>default_label_map</code> <code>transform</code> <code>Compose | None</code> <p>Transformations to be applied to the images.</p> <code>None</code> <code>fraction</code> <code>float</code> <p>Fraction of the dataset to use. Defaults to 1.0 (use all data).</p> <code>1.0</code> Source code in <code>terratorch/datasets/forestnet.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    label_map: dict[str, int] = default_label_map,\n    transform: A.Compose | None = None,\n    fraction: float = 1.0,\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the ForestNetNonGeo dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        label_map (Dict[str, int]): Mapping from label names to integer labels.\n        transform: Transformations to be applied to the images.\n        fraction (float): Fraction of the dataset to use. Defaults to 1.0 (use all data).\n    \"\"\"\n    super().__init__()\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits)}.\"\n        raise ValueError(msg)\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n    self.use_metadata = use_metadata\n\n    self.data_root = Path(data_root)\n    self.label_map = label_map\n\n    # Load the CSV file corresponding to the split\n    csv_file = self.data_root / f\"{split}_filtered.csv\"\n    original_df = pd.read_csv(csv_file)\n\n    # Apply stratified sampling if fraction &lt; 1.0\n    if fraction &lt; 1.0:\n        sss = StratifiedShuffleSplit(n_splits=1, test_size=1 - fraction, random_state=47)\n        stratified_indices, _ = next(sss.split(original_df, original_df[\"merged_label\"]))\n        self.dataset = original_df.iloc[stratified_indices].reset_index(drop=True)\n    else:\n        self.dataset = original_df\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.forestnet.ForestNetNonGeo.map_label","title":"<code>map_label(index)</code>","text":"<p>Map the label name to an integer label.</p> Source code in <code>terratorch/datasets/forestnet.py</code> <pre><code>def map_label(self, index: int) -&gt; torch.Tensor:\n    \"\"\"Map the label name to an integer label.\"\"\"\n    label_name = self.dataset[\"merged_label\"][index]\n    label = self.label_map[label_name]\n    return label\n</code></pre>"},{"location":"datasets/#terratorch.datasets.fire_scars","title":"<code>terratorch.datasets.fire_scars</code>","text":""},{"location":"datasets/#terratorch.datasets.fire_scars.FireScarsHLS","title":"<code>FireScarsHLS</code>","text":"<p>               Bases: <code>RasterDataset</code></p> <p>RasterDataset implementation for fire scars input images.</p> Source code in <code>terratorch/datasets/fire_scars.py</code> <pre><code>class FireScarsHLS(RasterDataset):\n    \"\"\"RasterDataset implementation for fire scars input images.\"\"\"\n\n    filename_glob = \"subsetted*_merged.tif\"\n    filename_regex = r\"subsetted_512x512_HLS\\..30\\..{6}\\.(?P&lt;date&gt;[0-9]*)\\.v1.4_merged.tif\"\n    date_format = \"%Y%j\"\n    is_image = True\n    separate_files = False\n    all_bands = dataclasses.field(default_factory=[\"B02\", \"B03\", \"B04\", \"B8A\", \"B11\", \"B12\"])\n    rgb_bands = dataclasses.field(default_factory=[\"B04\", \"B03\", \"B02\"])\n</code></pre>"},{"location":"datasets/#terratorch.datasets.fire_scars.FireScarsNonGeo","title":"<code>FireScarsNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for fire scars.</p> Source code in <code>terratorch/datasets/fire_scars.py</code> <pre><code>class FireScarsNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [fire scars](https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars).\"\"\"\n    all_band_names = (\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"NIR_NARROW\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    num_classes = 2\n    splits = {\"train\": \"training\", \"val\": \"validation\"}   # Only train and val splits available\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        no_data_replace: float | None = 0,\n        no_label_replace: int | None = -1,\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (str): Path to the data root directory.\n            bands (list[str]): Bands that should be output by the dataset. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). If used through the corresponding data module,\n                should not include normalization. Defaults to None, which applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input images with this value.\n                If None, does no replacement. Defaults to 0.\n            no_label_replace (int | None): Replace nan values in label with this value.\n                If none, does no replacement. Defaults to -1.\n            use_metadata (bool): whether to return metadata info (time and location).\n        \"\"\"\n        super().__init__()\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {self.splits}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n        self.data_root = Path(data_root)\n\n        input_dir = self.data_root / split_name\n        self.image_files = sorted(glob.glob(os.path.join(input_dir, \"*_merged.tif\")))\n        self.segmentation_mask_files = sorted(glob.glob(os.path.join(input_dir, \"*.mask.tif\")))\n\n        self.use_metadata = use_metadata\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n\n        # If no transform is given, apply only to transform to torch tensor\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def _get_date(self, index: int) -&gt; torch.Tensor:\n        file_name = self.image_files[index]\n        base_filename = os.path.basename(file_name)\n\n        filename_regex = r\"subsetted_512x512_HLS\\.S30\\.T[0-9A-Z]{5}\\.(?P&lt;date&gt;[0-9]+)\\.v1\\.4_merged\\.tif\"\n        match = re.match(filename_regex, base_filename)\n        date_str = match.group(\"date\")\n        year = int(date_str[:4])\n        julian_day = int(date_str[4:])\n\n        return torch.tensor([[year, julian_day]], dtype=torch.float32)\n\n    def _get_coords(self, image: DataArray) -&gt; torch.Tensor:\n        px = image.x.shape[0] // 2\n        py = image.y.shape[0] // 2\n\n        # get center point to reproject to lat/lon\n        point = image.isel(band=0, x=slice(px, px + 1), y=slice(py, py + 1))\n        point = point.rio.reproject(\"epsg:4326\")\n\n        lat_lon = np.asarray([point.y[0], point.x[0]])\n\n        return torch.tensor(lat_lon, dtype=torch.float32)\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        image = self._load_file(self.image_files[index], nan_replace=self.no_data_replace)\n\n        location_coords, temporal_coords = None, None\n        if self.use_metadata:\n            location_coords = self._get_coords(image)\n            temporal_coords = self._get_date(index)\n\n        # to channels last\n        image = image.to_numpy()\n        image = np.moveaxis(image, 0, -1)\n\n        # filter bands\n        image = image[..., self.band_indices]\n\n        output = {\n            \"image\": image.astype(np.float32),\n            \"mask\": self._load_file(\n                self.segmentation_mask_files[index], nan_replace=self.no_label_replace).to_numpy()[0],\n        }\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        if self.use_metadata:\n            output[\"location_coords\"] = location_coords\n            output[\"temporal_coords\"] = temporal_coords\n\n        return output\n\n    def _load_file(self, path: Path, nan_replace: int | float | None = None) -&gt; DataArray:\n        data = rioxarray.open_rasterio(path, masked=True)\n        if nan_replace is not None:\n            data = data.fillna(nan_replace)\n        return data\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            suptitle: optional string to use as a suptitle\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n        \"\"\"\n        num_images = 4\n\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        # RGB -&gt; channels-last\n        image = sample[\"image\"][rgb_indices, ...].permute(1, 2, 0).numpy()\n        mask = sample[\"mask\"].numpy()\n\n        image = clip_image_percentile(image)\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"]\n            num_images += 1\n        else:\n            prediction = None\n\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 5), layout=\"compressed\")\n\n        ax[0].axis(\"off\")\n\n        norm = mpl.colors.Normalize(vmin=0, vmax=self.num_classes - 1)\n        ax[1].axis(\"off\")\n        ax[1].title.set_text(\"Image\")\n        ax[1].imshow(image)\n\n        ax[2].axis(\"off\")\n        ax[2].title.set_text(\"Ground Truth Mask\")\n        ax[2].imshow(mask, cmap=\"jet\", norm=norm)\n\n        ax[3].axis(\"off\")\n        ax[3].title.set_text(\"GT Mask on Image\")\n        ax[3].imshow(image)\n        ax[3].imshow(mask, cmap=\"jet\", alpha=0.3, norm=norm)\n\n        if \"prediction\" in sample:\n            ax[4].title.set_text(\"Predicted Mask\")\n            ax[4].imshow(prediction, cmap=\"jet\", norm=norm)\n\n        cmap = plt.get_cmap(\"jet\")\n        legend_data = [[i, cmap(norm(i)), str(i)] for i in range(self.num_classes)]\n        handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n        labels = [n for k, c, n in legend_data]\n        ax[0].legend(handles, labels, loc=\"center\")\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.fire_scars.FireScarsNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, no_data_replace=0, no_label_replace=-1, use_metadata=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>bands</code> <code>list[str]</code> <p>Bands that should be output by the dataset. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If None, does no replacement. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>whether to return metadata info (time and location).</p> <code>False</code> Source code in <code>terratorch/datasets/fire_scars.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    no_data_replace: float | None = 0,\n    no_label_replace: int | None = -1,\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (str): Path to the data root directory.\n        bands (list[str]): Bands that should be output by the dataset. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). If used through the corresponding data module,\n            should not include normalization. Defaults to None, which applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input images with this value.\n            If None, does no replacement. Defaults to 0.\n        no_label_replace (int | None): Replace nan values in label with this value.\n            If none, does no replacement. Defaults to -1.\n        use_metadata (bool): whether to return metadata info (time and location).\n    \"\"\"\n    super().__init__()\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {self.splits}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n    self.data_root = Path(data_root)\n\n    input_dir = self.data_root / split_name\n    self.image_files = sorted(glob.glob(os.path.join(input_dir, \"*_merged.tif\")))\n    self.segmentation_mask_files = sorted(glob.glob(os.path.join(input_dir, \"*.mask.tif\")))\n\n    self.use_metadata = use_metadata\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n\n    # If no transform is given, apply only to transform to torch tensor\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.fire_scars.FireScarsNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> Source code in <code>terratorch/datasets/fire_scars.py</code> <pre><code>def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        suptitle: optional string to use as a suptitle\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n    \"\"\"\n    num_images = 4\n\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    # RGB -&gt; channels-last\n    image = sample[\"image\"][rgb_indices, ...].permute(1, 2, 0).numpy()\n    mask = sample[\"mask\"].numpy()\n\n    image = clip_image_percentile(image)\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"]\n        num_images += 1\n    else:\n        prediction = None\n\n    fig, ax = plt.subplots(1, num_images, figsize=(12, 5), layout=\"compressed\")\n\n    ax[0].axis(\"off\")\n\n    norm = mpl.colors.Normalize(vmin=0, vmax=self.num_classes - 1)\n    ax[1].axis(\"off\")\n    ax[1].title.set_text(\"Image\")\n    ax[1].imshow(image)\n\n    ax[2].axis(\"off\")\n    ax[2].title.set_text(\"Ground Truth Mask\")\n    ax[2].imshow(mask, cmap=\"jet\", norm=norm)\n\n    ax[3].axis(\"off\")\n    ax[3].title.set_text(\"GT Mask on Image\")\n    ax[3].imshow(image)\n    ax[3].imshow(mask, cmap=\"jet\", alpha=0.3, norm=norm)\n\n    if \"prediction\" in sample:\n        ax[4].title.set_text(\"Predicted Mask\")\n        ax[4].imshow(prediction, cmap=\"jet\", norm=norm)\n\n    cmap = plt.get_cmap(\"jet\")\n    legend_data = [[i, cmap(norm(i)), str(i)] for i in range(self.num_classes)]\n    handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n    labels = [n for k, c, n in legend_data]\n    ax[0].legend(handles, labels, loc=\"center\")\n    if suptitle is not None:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.fire_scars.FireScarsSegmentationMask","title":"<code>FireScarsSegmentationMask</code>","text":"<p>               Bases: <code>RasterDataset</code></p> <p>RasterDataset implementation for fire scars segmentation mask. Can be easily merged with input images using the &amp; operator.</p> Source code in <code>terratorch/datasets/fire_scars.py</code> <pre><code>class FireScarsSegmentationMask(RasterDataset):\n    \"\"\"RasterDataset implementation for fire scars segmentation mask.\n    Can be easily merged with input images using the &amp; operator.\n    \"\"\"\n\n    filename_glob = \"subsetted*.mask.tif\"\n    filename_regex = r\"subsetted_512x512_HLS\\..30\\..{6}\\.(?P&lt;date&gt;[0-9]*)\\.v1.4.mask.tif\"\n    date_format = \"%Y%j\"\n    is_image = False\n    separate_files = False\n</code></pre>"},{"location":"datasets/#terratorch.datasets.landslide4sense","title":"<code>terratorch.datasets.landslide4sense</code>","text":""},{"location":"datasets/#terratorch.datasets.landslide4sense.Landslide4SenseNonGeo","title":"<code>Landslide4SenseNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for Landslide4Sense.</p> Source code in <code>terratorch/datasets/landslide4sense.py</code> <pre><code>class Landslide4SenseNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [Landslide4Sense](https://huggingface.co/datasets/ibm-nasa-geospatial/Landslide4sense).\"\"\"\n    all_band_names = (\n        \"COASTAL AEROSOL\",\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"RED_EDGE_1\",\n        \"RED_EDGE_2\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"WATER_VAPOR\",\n        \"CIRRUS\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n        \"SLOPE\",\n        \"DEM\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"validation\", \"test\": \"test\"}\n\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Landslide4Sense dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'validation', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n        self.data_directory = Path(data_root)\n\n        images_dir = self.data_directory / \"images\" / split_name\n        annotations_dir = self.data_directory / \"annotations\" / split_name\n\n        self.image_files = sorted(images_dir.glob(\"image_*.h5\"))\n        self.mask_files = sorted(annotations_dir.glob(\"mask_*.h5\"))\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        image_file = self.image_files[index]\n        mask_file = self.mask_files[index]\n\n        with h5py.File(image_file, \"r\") as h5file:\n            image = np.array(h5file[\"img\"])[..., self.band_indices]\n\n        with h5py.File(mask_file, \"r\") as h5file:\n            mask = np.array(h5file[\"mask\"])\n\n        output = {\"image\": image.astype(np.float32), \"mask\": mask}\n\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        mask = sample[\"mask\"].numpy()\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()\n\n        rgb_image = image[:, :, rgb_indices]\n\n        rgb_image = (rgb_image - rgb_image.min(axis=(0, 1))) * (1 / rgb_image.max(axis=(0, 1)))\n        rgb_image = np.clip(rgb_image, 0, 1)\n\n        num_classes = len(np.unique(mask))\n        cmap = colormaps[\"jet\"]\n        norm = Normalize(vmin=0, vmax=num_classes - 1)\n\n        num_images = 4 if \"prediction\" in sample else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n        ax[0].imshow(rgb_image)\n        ax[0].set_title(\"Image\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(mask, cmap=cmap, norm=norm)\n        ax[1].set_title(\"Ground Truth Mask\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(rgb_image)\n        ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n        ax[2].set_title(\"GT Mask on Image\")\n        ax[2].axis(\"off\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"]\n            ax[3].imshow(prediction, cmap=cmap, norm=norm)\n            ax[3].set_title(\"Predicted Mask\")\n            ax[3].axis(\"off\")\n\n        if sample.get(\"class_names\"):\n            class_names = sample[\"class_names\"]\n            legend_handles = [\n                mpatches.Patch(color=cmap(i), label=class_names[i]) for i in range(num_classes)\n            ]\n            ax[0].legend(handles=legend_handles, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.landslide4sense.Landslide4SenseNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None)</code>","text":"<p>Initialize the Landslide4Sense dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'validation', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> Source code in <code>terratorch/datasets/landslide4sense.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Landslide4Sense dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'validation', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n    self.data_directory = Path(data_root)\n\n    images_dir = self.data_directory / \"images\" / split_name\n    annotations_dir = self.data_directory / \"annotations\" / split_name\n\n    self.image_files = sorted(images_dir.glob(\"image_*.h5\"))\n    self.mask_files = sorted(annotations_dir.glob(\"mask_*.h5\"))\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_eurosat","title":"<code>terratorch.datasets.m_eurosat</code>","text":""},{"location":"datasets/#terratorch.datasets.m_eurosat.MEuroSATNonGeo","title":"<code>MEuroSATNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-EuroSAT.</p> Source code in <code>terratorch/datasets/m_eurosat.py</code> <pre><code>class MEuroSATNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-EuroSAT](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\n        \"COASTAL_AEROSOL\",\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"RED_EDGE_1\",\n        \"RED_EDGE_2\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"NIR_NARROW\",\n        \"WATER_VAPOR\",\n        \"CIRRUS\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-eurosat\"\n    partition_file_template = \"{partition}_partition.json\"\n    label_map_file = \"label_map.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = [self.all_band_names.index(b) for b in bands]\\\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        label_map_path = self.data_directory / self.label_map_file\n        with open(label_map_path) as file:\n            self.label_map = json.load(file)\n\n        self.id_to_class = {img_id: cls for cls, ids in self.label_map.items() for img_id in ids}\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        image_id = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n\n        label_class = self.id_to_class[image_id]\n        label_index = list(self.label_map.keys()).index(label_class)\n\n        output = {\"image\": image.astype(np.float32)}\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"label\"] = label_index\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        label_index = sample[\"label\"]\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        class_names = list(self.label_map.keys())\n        class_name = class_names[label_index]\n\n        fig, ax = plt.subplots(figsize=(6, 6))\n        ax.imshow(rgb_image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Class: {class_name}\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_eurosat.MEuroSATNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> Source code in <code>terratorch/datasets/m_eurosat.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = [self.all_band_names.index(b) for b in bands]\\\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    label_map_path = self.data_directory / self.label_map_file\n    with open(label_map_path) as file:\n        self.label_map = json.load(file)\n\n    self.id_to_class = {img_id: cls for cls, ids in self.label_map.items() for img_id in ids}\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_eurosat.MEuroSATNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_eurosat.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    label_index = sample[\"label\"]\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    class_names = list(self.label_map.keys())\n    class_name = class_names[label_index]\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.imshow(rgb_image)\n    ax.axis(\"off\")\n    ax.set_title(f\"Class: {class_name}\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_bigearthnet","title":"<code>terratorch.datasets.m_bigearthnet</code>","text":""},{"location":"datasets/#terratorch.datasets.m_bigearthnet.MBigEarthNonGeo","title":"<code>MBigEarthNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-BigEarthNet.</p> Source code in <code>terratorch/datasets/m_bigearthnet.py</code> <pre><code>class MBigEarthNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-BigEarthNet](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\n        \"COASTAL_AEROSOL\",\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"RED_EDGE_1\",\n        \"RED_EDGE_2\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"NIR_NARROW\",\n        \"WATER_VAPOR\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-bigearthnet\"\n    label_map_file = \"label_stats.json\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        label_map_path = self.data_directory / self.label_map_file\n        with open(label_map_path) as file:\n            self.label_map = json.load(file)\n\n        self.num_classes = len(next(iter(self.label_map.values())))\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found in partition file.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        image_id = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n\n        labels_vector = self.label_map[image_id]\n        labels_tensor = torch.tensor(labels_vector, dtype=torch.float)\n\n        output = {\"image\": image}\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"label\"] = labels_tensor\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        label = sample[\"label\"].numpy()\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()  # Convert to (H, W, C)\n\n        rgb_image = image[:, :, rgb_indices]\n\n        rgb_image = clip_image(rgb_image)\n\n        active_labels = [i for i, lbl in enumerate(label) if lbl == 1]\n\n        fig, ax = plt.subplots(figsize=(6, 6))\n\n        ax.imshow(rgb_image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Active Labels: {active_labels}\")\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_bigearthnet.MBigEarthNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> Source code in <code>terratorch/datasets/m_bigearthnet.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    label_map_path = self.data_directory / self.label_map_file\n    with open(label_map_path) as file:\n        self.label_map = json.load(file)\n\n    self.num_classes = len(next(iter(self.label_map.values())))\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found in partition file.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_bigearthnet.MBigEarthNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_bigearthnet.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    label = sample[\"label\"].numpy()\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()  # Convert to (H, W, C)\n\n    rgb_image = image[:, :, rgb_indices]\n\n    rgb_image = clip_image(rgb_image)\n\n    active_labels = [i for i, lbl in enumerate(label) if lbl == 1]\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n\n    ax.imshow(rgb_image)\n    ax.axis(\"off\")\n    ax.set_title(f\"Active Labels: {active_labels}\")\n\n    if suptitle is not None:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_brick_kiln","title":"<code>terratorch.datasets.m_brick_kiln</code>","text":""},{"location":"datasets/#terratorch.datasets.m_brick_kiln.MBrickKilnNonGeo","title":"<code>MBrickKilnNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-BrickKiln.</p> Source code in <code>terratorch/datasets/m_brick_kiln.py</code> <pre><code>class MBrickKilnNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-BrickKiln](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\n        \"COASTAL_AEROSOL\",\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"RED_EDGE_1\",\n        \"RED_EDGE_2\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"NIR_NARROW\",\n        \"WATER_VAPOR\",\n        \"CIRRUS\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-brick-kiln\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found in partition file.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        image_id = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            attr_dict = pickle.loads(ast.literal_eval(h5file.attrs[\"pickle\"]))\n            class_index = attr_dict[\"label\"]\n\n        output = {\"image\": image.astype(np.float32)}\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"label\"] = class_index\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        label = sample[\"label\"]\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()  # Convert to (H, W, C)\n\n        rgb_image = image[:, :, rgb_indices]\n\n        rgb_image = clip_image(rgb_image)\n\n        fig, ax = plt.subplots(figsize=(6, 6))\n\n        ax.imshow(rgb_image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Class: {label}\")\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_brick_kiln.MBrickKilnNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> Source code in <code>terratorch/datasets/m_brick_kiln.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found in partition file.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_brick_kiln.MBrickKilnNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_brick_kiln.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    label = sample[\"label\"]\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()  # Convert to (H, W, C)\n\n    rgb_image = image[:, :, rgb_indices]\n\n    rgb_image = clip_image(rgb_image)\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n\n    ax.imshow(rgb_image)\n    ax.axis(\"off\")\n    ax.set_title(f\"Class: {label}\")\n\n    if suptitle is not None:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_forestnet","title":"<code>terratorch.datasets.m_forestnet</code>","text":""},{"location":"datasets/#terratorch.datasets.m_forestnet.MForestNetNonGeo","title":"<code>MForestNetNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-ForestNet.</p> Source code in <code>terratorch/datasets/m_forestnet.py</code> <pre><code>class MForestNetNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-ForestNet](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"NIR\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-forestnet\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n            use_metadata (bool): Whether to return metadata info (time and location).\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.use_metadata = use_metadata\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        image_id = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            attr_dict = pickle.loads(ast.literal_eval(h5file.attrs[\"pickle\"]))  # noqa: S301\n            class_index = attr_dict[\"label\"]\n\n        output = {\"image\": image.astype(np.float32)}\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"label\"] = class_index\n\n        if self.use_metadata:\n            temporal_coords = self._get_date(image_id)\n            location_coords = self._get_coords(image_id)\n\n            output[\"temporal_coords\"] = temporal_coords\n            output[\"location_coords\"] = location_coords\n\n        return output\n\n    def _get_coords(self, image_id: str) -&gt; torch.Tensor:\n        \"\"\"Extract spatial coordinates from the image ID.\n\n        Args:\n            image_id (str): The ID of the image.\n\n        Returns:\n            torch.Tensor: Tensor containing latitude and longitude.\n        \"\"\"\n        lat_str, lon_str, _ = image_id.split(\"_\", 2)\n        latitude = float(lat_str)\n        longitude = float(lon_str)\n        return torch.tensor([latitude, longitude], dtype=torch.float32)\n\n    def _get_date(self, image_id: str) -&gt; torch.Tensor:\n        _, _, date_str = image_id.split(\"_\", 2)\n        date = pd.to_datetime(date_str, format=\"%Y_%m_%d\")\n\n        return torch.tensor([[date.year, date.dayofyear - 1]], dtype=torch.float32)\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        label = sample[\"label\"]\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        fig, ax = plt.subplots(figsize=(6, 6))\n        ax.imshow(rgb_image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Class: {label}\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_forestnet.MForestNetNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code> Source code in <code>terratorch/datasets/m_forestnet.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        use_metadata (bool): Whether to return metadata info (time and location).\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.use_metadata = use_metadata\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_forestnet.MForestNetNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_forestnet.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    label = sample[\"label\"]\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.imshow(rgb_image)\n    ax.axis(\"off\")\n    ax.set_title(f\"Class: {label}\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_so2sat","title":"<code>terratorch.datasets.m_so2sat</code>","text":""},{"location":"datasets/#terratorch.datasets.m_so2sat.MSo2SatNonGeo","title":"<code>MSo2SatNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-So2Sat.</p> Source code in <code>terratorch/datasets/m_so2sat.py</code> <pre><code>class MSo2SatNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-So2Sat](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\n        \"VH_REAL\",\n        \"BLUE\",\n        \"VH_IMAGINARY\",\n        \"GREEN\",\n        \"VV_REAL\",\n        \"RED\",\n        \"VV_IMAGINARY\",\n        \"VH_LEE_FILTERED\",\n        \"RED_EDGE_1\",\n        \"VV_LEE_FILTERED\",\n        \"RED_EDGE_2\",\n        \"VH_LEE_FILTERED_REAL\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"VV_LEE_FILTERED_IMAGINARY\",\n        \"NIR_NARROW\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-so2sat\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        image_id = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            attr_dict = pickle.loads(ast.literal_eval(h5file.attrs[\"pickle\"]))\n            class_index = attr_dict[\"label\"]\n\n        output = {\"image\": image.astype(np.float32)}\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"label\"] = class_index\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        label_index = sample[\"label\"]\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        class_name = str(label_index)\n\n        fig, ax = plt.subplots(figsize=(6, 6))\n        ax.imshow(rgb_image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Class: {class_name}\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_so2sat.MSo2SatNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> Source code in <code>terratorch/datasets/m_so2sat.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_so2sat.MSo2SatNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_so2sat.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    label_index = sample[\"label\"]\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    class_name = str(label_index)\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.imshow(rgb_image)\n    ax.axis(\"off\")\n    ax.set_title(f\"Class: {class_name}\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_pv4ger","title":"<code>terratorch.datasets.m_pv4ger</code>","text":""},{"location":"datasets/#terratorch.datasets.m_pv4ger.MPv4gerNonGeo","title":"<code>MPv4gerNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-PV4GER.</p> Source code in <code>terratorch/datasets/m_pv4ger.py</code> <pre><code>class MPv4gerNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-PV4GER](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\"BLUE\", \"GREEN\", \"RED\")\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-pv4ger\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n            use_metadata (bool): Whether to return metadata info (location coordinates).\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.use_metadata = use_metadata\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        image_id = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            attr_dict = pickle.loads(ast.literal_eval(h5file.attrs[\"pickle\"]))  # noqa: S301\n            class_index = attr_dict[\"label\"]\n\n        output = {\"image\": image.astype(np.float32)}\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"label\"] = class_index\n\n        if self.use_metadata:\n            output[\"location_coords\"] = self._get_coords(image_id)\n\n        return output\n\n    def _get_coords(self, image_id: str) -&gt; torch.Tensor:\n        \"\"\"Extract spatial coordinates from the image ID.\"\"\"\n        lat_str, lon_str = image_id.split(\",\")\n        latitude = float(lat_str)\n        longitude = float(lon_str)\n        return torch.tensor([latitude, longitude], dtype=torch.float32)\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        label = sample[\"label\"]\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        fig, ax = plt.subplots(figsize=(6, 6))\n        ax.imshow(rgb_image)\n        ax.axis(\"off\")\n        ax.set_title(f\"Class: {label}\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_pv4ger.MPv4gerNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (location coordinates).</p> <code>False</code> Source code in <code>terratorch/datasets/m_pv4ger.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        use_metadata (bool): Whether to return metadata info (location coordinates).\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.use_metadata = use_metadata\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_pv4ger.MPv4gerNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_pv4ger.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    label = sample[\"label\"]\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n    ax.imshow(rgb_image)\n    ax.axis(\"off\")\n    ax.set_title(f\"Class: {label}\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_cashew_plantation","title":"<code>terratorch.datasets.m_cashew_plantation</code>","text":""},{"location":"datasets/#terratorch.datasets.m_cashew_plantation.MBeninSmallHolderCashewsNonGeo","title":"<code>MBeninSmallHolderCashewsNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-BeninSmallHolderCashews.</p> Source code in <code>terratorch/datasets/m_cashew_plantation.py</code> <pre><code>class MBeninSmallHolderCashewsNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-BeninSmallHolderCashews](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\n        \"COASTAL_AEROSOL\",\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"RED_EDGE_1\",\n        \"RED_EDGE_2\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"NIR_NARROW\",\n        \"WATER_VAPOR\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n        \"CLOUD_PROBABILITY\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-cashew-plant\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n            use_metadata (bool): Whether to return metadata info (time).\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.use_metadata = use_metadata\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found in partition file.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def _get_date(self, keys) -&gt; torch.Tensor:\n        date_pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}\")\n\n        date_str = None\n        for key in keys:\n            match = date_pattern.search(key)\n            if match:\n                date_str = match.group()\n                break\n\n        date = torch.zeros((1, 2), dtype=torch.float32)\n        if date_str:\n            date = pd.to_datetime(date_str, format=\"%Y-%m-%d\")\n            date = torch.tensor([[date.year, date.dayofyear - 1]], dtype=torch.float32)\n\n        return date\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            temporal_coords = self._get_date(h5file)\n            mask = np.array(h5file[\"label\"])\n\n        output = {\"image\": image.astype(np.float32), \"mask\": mask}\n\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n        if self.use_metadata:\n            output[\"temporal_coords\"] = temporal_coords\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        mask = sample[\"mask\"].numpy()\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        num_classes = len(np.unique(mask))\n        cmap = plt.get_cmap(\"jet\")\n        norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n        num_images = 4 if \"prediction\" in sample else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n        ax[0].imshow(rgb_image)\n        ax[0].set_title(\"Image\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(mask, cmap=cmap, norm=norm)\n        ax[1].set_title(\"Ground Truth Mask\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(rgb_image)\n        ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n        ax[2].set_title(\"GT Mask on Image\")\n        ax[2].axis(\"off\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"].numpy()\n            ax[3].imshow(prediction, cmap=cmap, norm=norm)\n            ax[3].set_title(\"Predicted Mask\")\n            ax[3].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_cashew_plantation.MBeninSmallHolderCashewsNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time).</p> <code>False</code> Source code in <code>terratorch/datasets/m_cashew_plantation.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        use_metadata (bool): Whether to return metadata info (time).\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.use_metadata = use_metadata\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found in partition file.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_cashew_plantation.MBeninSmallHolderCashewsNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_cashew_plantation.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    mask = sample[\"mask\"].numpy()\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    num_classes = len(np.unique(mask))\n    cmap = plt.get_cmap(\"jet\")\n    norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n    num_images = 4 if \"prediction\" in sample else 3\n    fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n    ax[0].imshow(rgb_image)\n    ax[0].set_title(\"Image\")\n    ax[0].axis(\"off\")\n\n    ax[1].imshow(mask, cmap=cmap, norm=norm)\n    ax[1].set_title(\"Ground Truth Mask\")\n    ax[1].axis(\"off\")\n\n    ax[2].imshow(rgb_image)\n    ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n    ax[2].set_title(\"GT Mask on Image\")\n    ax[2].axis(\"off\")\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"].numpy()\n        ax[3].imshow(prediction, cmap=cmap, norm=norm)\n        ax[3].set_title(\"Predicted Mask\")\n        ax[3].axis(\"off\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_nz_cattle","title":"<code>terratorch.datasets.m_nz_cattle</code>","text":""},{"location":"datasets/#terratorch.datasets.m_nz_cattle.MNzCattleNonGeo","title":"<code>MNzCattleNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-NZ-Cattle.</p> Source code in <code>terratorch/datasets/m_nz_cattle.py</code> <pre><code>class MNzCattleNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-NZ-Cattle](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\"BLUE\", \"GREEN\", \"RED\")\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-nz-cattle\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n            use_metadata (bool): Whether to return metadata info (time and location).\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n        self.use_metadata = use_metadata\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        file_name = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n\n            data_keys = [key for key in keys if \"label\" not in key]\n            label_keys = [key for key in keys if \"label\" in key]\n\n            temporal_coords = self._get_date(data_keys[0])\n\n            bands = [np.array(h5file[key]) for key in data_keys]\n            image = np.stack(bands, axis=-1)\n\n            mask = np.array(h5file[label_keys[0]])\n\n        output = {\"image\": image.astype(np.float32), \"mask\": mask}\n\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        if self.use_metadata:\n            location_coords = self._get_coords(file_name)\n            output[\"location_coords\"] = location_coords\n            output[\"temporal_coords\"] = temporal_coords\n\n        return output\n\n    def _get_coords(self, file_name: str) -&gt; torch.Tensor:\n        \"\"\"Extract spatial coordinates from the file name.\"\"\"\n        match = re.search(r\"_(\\-?\\d+\\.\\d+),(\\-?\\d+\\.\\d+)\", file_name)\n        if match:\n            longitude, latitude = map(float, match.groups())\n\n        return torch.tensor([latitude, longitude], dtype=torch.float32)\n\n    def _get_date(self, band_name: str) -&gt; torch.Tensor:\n        date_str = band_name.split(\"_\")[-1]\n        date = pd.to_datetime(date_str, format=\"%Y-%m-%d\")\n\n        return torch.tensor([[date.year, date.dayofyear - 1]], dtype=torch.float32)\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        mask = sample[\"mask\"].numpy()\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        num_classes = len(np.unique(mask))\n        cmap = plt.get_cmap(\"jet\")\n        norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n        num_images = 4 if \"prediction\" in sample else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n        ax[0].imshow(rgb_image)\n        ax[0].set_title(\"Image\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(mask, cmap=cmap, norm=norm)\n        ax[1].set_title(\"Ground Truth Mask\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(rgb_image)\n        ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n        ax[2].set_title(\"GT Mask on Image\")\n        ax[2].axis(\"off\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"].numpy()\n            ax[3].imshow(prediction, cmap=cmap, norm=norm)\n            ax[3].set_title(\"Predicted Mask\")\n            ax[3].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_nz_cattle.MNzCattleNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (time and location).</p> <code>False</code> Source code in <code>terratorch/datasets/m_nz_cattle.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        use_metadata (bool): Whether to return metadata info (time and location).\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n    self.use_metadata = use_metadata\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_nz_cattle.MNzCattleNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_nz_cattle.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    mask = sample[\"mask\"].numpy()\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    num_classes = len(np.unique(mask))\n    cmap = plt.get_cmap(\"jet\")\n    norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n    num_images = 4 if \"prediction\" in sample else 3\n    fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n    ax[0].imshow(rgb_image)\n    ax[0].set_title(\"Image\")\n    ax[0].axis(\"off\")\n\n    ax[1].imshow(mask, cmap=cmap, norm=norm)\n    ax[1].set_title(\"Ground Truth Mask\")\n    ax[1].axis(\"off\")\n\n    ax[2].imshow(rgb_image)\n    ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n    ax[2].set_title(\"GT Mask on Image\")\n    ax[2].axis(\"off\")\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"].numpy()\n        ax[3].imshow(prediction, cmap=cmap, norm=norm)\n        ax[3].set_title(\"Predicted Mask\")\n        ax[3].axis(\"off\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_chesapeake_landcover","title":"<code>terratorch.datasets.m_chesapeake_landcover</code>","text":""},{"location":"datasets/#terratorch.datasets.m_chesapeake_landcover.MChesapeakeLandcoverNonGeo","title":"<code>MChesapeakeLandcoverNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-ChesapeakeLandcover.</p> Source code in <code>terratorch/datasets/m_chesapeake_landcover.py</code> <pre><code>class MChesapeakeLandcoverNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-ChesapeakeLandcover](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\"BLUE\", \"GREEN\", \"NIR\", \"RED\")\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-chesapeake\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found in partition file.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            mask = np.array(h5file[\"label\"])\n\n        output = {\"image\": image.astype(np.float32), \"mask\": mask}\n\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        mask = sample[\"mask\"].numpy()\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        num_classes = len(np.unique(mask))\n        cmap = plt.get_cmap(\"jet\")\n        norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n        num_images = 4 if \"prediction\" in sample else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n        ax[0].imshow(rgb_image)\n        ax[0].set_title(\"Image\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(mask, cmap=cmap, norm=norm)\n        ax[1].set_title(\"Ground Truth Mask\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(rgb_image)\n        ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n        ax[2].set_title(\"GT Mask on Image\")\n        ax[2].axis(\"off\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"].numpy()\n            ax[3].imshow(prediction, cmap=cmap, norm=norm)\n            ax[3].set_title(\"Predicted Mask\")\n            ax[3].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_chesapeake_landcover.MChesapeakeLandcoverNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> Source code in <code>terratorch/datasets/m_chesapeake_landcover.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found in partition file.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_chesapeake_landcover.MChesapeakeLandcoverNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_chesapeake_landcover.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    mask = sample[\"mask\"].numpy()\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    num_classes = len(np.unique(mask))\n    cmap = plt.get_cmap(\"jet\")\n    norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n    num_images = 4 if \"prediction\" in sample else 3\n    fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n    ax[0].imshow(rgb_image)\n    ax[0].set_title(\"Image\")\n    ax[0].axis(\"off\")\n\n    ax[1].imshow(mask, cmap=cmap, norm=norm)\n    ax[1].set_title(\"Ground Truth Mask\")\n    ax[1].axis(\"off\")\n\n    ax[2].imshow(rgb_image)\n    ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n    ax[2].set_title(\"GT Mask on Image\")\n    ax[2].axis(\"off\")\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"].numpy()\n        ax[3].imshow(prediction, cmap=cmap, norm=norm)\n        ax[3].set_title(\"Predicted Mask\")\n        ax[3].axis(\"off\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_pv4ger_seg","title":"<code>terratorch.datasets.m_pv4ger_seg</code>","text":""},{"location":"datasets/#terratorch.datasets.m_pv4ger_seg.MPv4gerSegNonGeo","title":"<code>MPv4gerSegNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-PV4GER-SEG.</p> Source code in <code>terratorch/datasets/m_pv4ger_seg.py</code> <pre><code>class MPv4gerSegNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-PV4GER-SEG](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\"BLUE\", \"GREEN\", \"RED\")\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-pv4ger-seg\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n            use_metadata (bool): Whether to return metadata info (location coordinates).\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.use_metadata = use_metadata\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n        image_id = file_path.stem\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            mask = np.array(h5file[\"label\"])\n\n        output = {\"image\": image.astype(np.float32), \"mask\": mask}\n\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        if self.use_metadata:\n            output[\"location_coords\"] = self._get_coords(image_id)\n\n        return output\n\n    def _get_coords(self, image_id: str) -&gt; torch.Tensor:\n        \"\"\"Extract spatial coordinates from the image ID.\"\"\"\n        lat_str, lon_str = image_id.split(\",\")\n        latitude = float(lat_str)\n        longitude = float(lon_str)\n        return torch.tensor([latitude, longitude], dtype=torch.float32)\n\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        mask = sample[\"mask\"].numpy()\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        num_classes = len(np.unique(mask))\n        cmap = plt.get_cmap(\"jet\")\n        norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n        num_images = 4 if \"prediction\" in sample else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n        ax[0].imshow(rgb_image)\n        ax[0].set_title(\"Image\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(mask, cmap=cmap, norm=norm)\n        ax[1].set_title(\"Ground Truth Mask\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(rgb_image)\n        ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n        ax[2].set_title(\"GT Mask on Image\")\n        ax[2].axis(\"off\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"].numpy()\n            ax[3].imshow(prediction, cmap=cmap, norm=norm)\n            ax[3].set_title(\"Predicted Mask\")\n            ax[3].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_pv4ger_seg.MPv4gerSegNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default', use_metadata=False)</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> <code>use_metadata</code> <code>bool</code> <p>Whether to return metadata info (location coordinates).</p> <code>False</code> Source code in <code>terratorch/datasets/m_pv4ger_seg.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        use_metadata (bool): Whether to return metadata info (location coordinates).\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.use_metadata = use_metadata\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_pv4ger_seg.MPv4gerSegNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_pv4ger_seg.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    mask = sample[\"mask\"].numpy()\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    num_classes = len(np.unique(mask))\n    cmap = plt.get_cmap(\"jet\")\n    norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n    num_images = 4 if \"prediction\" in sample else 3\n    fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n    ax[0].imshow(rgb_image)\n    ax[0].set_title(\"Image\")\n    ax[0].axis(\"off\")\n\n    ax[1].imshow(mask, cmap=cmap, norm=norm)\n    ax[1].set_title(\"Ground Truth Mask\")\n    ax[1].axis(\"off\")\n\n    ax[2].imshow(rgb_image)\n    ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n    ax[2].set_title(\"GT Mask on Image\")\n    ax[2].axis(\"off\")\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"].numpy()\n        ax[3].imshow(prediction, cmap=cmap, norm=norm)\n        ax[3].set_title(\"Predicted Mask\")\n        ax[3].axis(\"off\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_SA_crop_type","title":"<code>terratorch.datasets.m_SA_crop_type</code>","text":""},{"location":"datasets/#terratorch.datasets.m_SA_crop_type.MSACropTypeNonGeo","title":"<code>MSACropTypeNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-SA-Crop-Type.</p> Source code in <code>terratorch/datasets/m_SA_crop_type.py</code> <pre><code>class MSACropTypeNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-SA-Crop-Type](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\n        \"COASTAL_AEROSOL\",\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"RED_EDGE_1\",\n        \"RED_EDGE_2\",\n        \"RED_EDGE_3\",\n        \"NIR_BROAD\",\n        \"NIR_NARROW\",\n        \"WATER_VAPOR\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n        \"CLOUD_PROBABILITY\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-SA-crop-type\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            mask = np.array(h5file[\"label\"])\n\n        output = {\"image\": image.astype(np.float32), \"mask\": mask}\n\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        image = sample[\"image\"]\n        mask = sample[\"mask\"].numpy()\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        num_classes = len(np.unique(mask))\n        cmap = plt.get_cmap(\"jet\")\n        norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n        num_images = 4 if \"prediction\" in sample else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n        ax[0].imshow(rgb_image)\n        ax[0].set_title(\"Image\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(mask, cmap=cmap, norm=norm)\n        ax[1].set_title(\"Ground Truth Mask\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(rgb_image)\n        ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n        ax[2].set_title(\"GT Mask on Image\")\n        ax[2].axis(\"off\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"].numpy()\n            ax[3].imshow(prediction, cmap=cmap, norm=norm)\n            ax[3].set_title(\"Predicted Mask\")\n            ax[3].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_SA_crop_type.MSACropTypeNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> Source code in <code>terratorch/datasets/m_SA_crop_type.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = [self.all_band_names.index(b) for b in bands]\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_SA_crop_type.MSACropTypeNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_SA_crop_type.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    image = sample[\"image\"]\n    mask = sample[\"mask\"].numpy()\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    num_classes = len(np.unique(mask))\n    cmap = plt.get_cmap(\"jet\")\n    norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n    num_images = 4 if \"prediction\" in sample else 3\n    fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n    ax[0].imshow(rgb_image)\n    ax[0].set_title(\"Image\")\n    ax[0].axis(\"off\")\n\n    ax[1].imshow(mask, cmap=cmap, norm=norm)\n    ax[1].set_title(\"Ground Truth Mask\")\n    ax[1].axis(\"off\")\n\n    ax[2].imshow(rgb_image)\n    ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n    ax[2].set_title(\"GT Mask on Image\")\n    ax[2].axis(\"off\")\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"].numpy()\n        ax[3].imshow(prediction, cmap=cmap, norm=norm)\n        ax[3].set_title(\"Predicted Mask\")\n        ax[3].axis(\"off\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_neontree","title":"<code>terratorch.datasets.m_neontree</code>","text":""},{"location":"datasets/#terratorch.datasets.m_neontree.MNeonTreeNonGeo","title":"<code>MNeonTreeNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for M-NeonTree.</p> Source code in <code>terratorch/datasets/m_neontree.py</code> <pre><code>class MNeonTreeNonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [M-NeonTree](https://github.com/ServiceNow/geo-bench?tab=readme-ov-file).\"\"\"\n    all_band_names = (\"BLUE\", \"CANOPY_HEIGHT_MODEL\", \"GREEN\", \"NEON\", \"RED\")\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n\n    data_dir = \"m-NeonTree\"\n    partition_file_template = \"{partition}_partition.json\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = rgb_bands,\n        transform: A.Compose | None = None,\n        partition: str = \"default\",\n    ) -&gt; None:\n        \"\"\"Initialize the dataset.\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): One of 'train', 'val', or 'test'.\n            bands (Sequence[str]): Bands to be used. Defaults to RGB bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Defaults to None, which applies default_transform().\n            partition (str): Partition name for the dataset splits. Defaults to 'default'.\n        \"\"\"\n        super().__init__()\n\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n        self.data_root = Path(data_root)\n        self.data_directory = self.data_root / self.data_dir\n\n        partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n        with open(partition_file) as file:\n            partitions = json.load(file)\n\n        if split_name not in partitions:\n            msg = f\"Split '{split_name}' not found.\"\n            raise ValueError(msg)\n\n        self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        file_path = self.image_files[index]\n\n        with h5py.File(file_path, \"r\") as h5file:\n            keys = sorted(h5file.keys())\n            keys = np.array([key for key in keys if key != \"label\"])[self.band_indices]\n            bands = [np.array(h5file[key]) for key in keys]\n\n            image = np.stack(bands, axis=-1)\n            mask = np.array(h5file[\"label\"])\n\n        output = {\"image\": image.astype(np.float32), \"mask\": mask}\n\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        return output\n\n    def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n            suptitle (str | None): Optional string to use as a suptitle.\n\n        Returns:\n            matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n        \"\"\"\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n        image = sample[\"image\"]\n        mask = sample[\"mask\"].numpy()\n\n        if torch.is_tensor(image):\n            image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n        rgb_image = image[:, :, rgb_indices]\n        rgb_image = clip_image(rgb_image)\n\n        num_classes = len(np.unique(mask))\n        cmap = plt.get_cmap(\"jet\")\n        norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n        num_images = 4 if \"prediction\" in sample else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n        ax[0].imshow(rgb_image)\n        ax[0].set_title(\"Image\")\n        ax[0].axis(\"off\")\n\n        ax[1].imshow(mask, cmap=cmap, norm=norm)\n        ax[1].set_title(\"Ground Truth Mask\")\n        ax[1].axis(\"off\")\n\n        ax[2].imshow(rgb_image)\n        ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n        ax[2].set_title(\"GT Mask on Image\")\n        ax[2].axis(\"off\")\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"].numpy()\n            ax[3].imshow(prediction, cmap=cmap, norm=norm)\n            ax[3].set_title(\"Predicted Mask\")\n            ax[3].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_neontree.MNeonTreeNonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=rgb_bands, transform=None, partition='default')</code>","text":"<p>Initialize the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>One of 'train', 'val', or 'test'.</p> <code>'train'</code> <code>bands</code> <code>Sequence[str]</code> <p>Bands to be used. Defaults to RGB bands.</p> <code>rgb_bands</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Defaults to None, which applies default_transform().</p> <code>None</code> <code>partition</code> <code>str</code> <p>Partition name for the dataset splits. Defaults to 'default'.</p> <code>'default'</code> Source code in <code>terratorch/datasets/m_neontree.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = rgb_bands,\n    transform: A.Compose | None = None,\n    partition: str = \"default\",\n) -&gt; None:\n    \"\"\"Initialize the dataset.\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): One of 'train', 'val', or 'test'.\n        bands (Sequence[str]): Bands to be used. Defaults to RGB bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Defaults to None, which applies default_transform().\n        partition (str): Partition name for the dataset splits. Defaults to 'default'.\n    \"\"\"\n    super().__init__()\n\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {list(self.splits.keys())}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.array([self.all_band_names.index(b) for b in bands])\n\n    self.data_root = Path(data_root)\n    self.data_directory = self.data_root / self.data_dir\n\n    partition_file = self.data_directory / self.partition_file_template.format(partition=partition)\n    with open(partition_file) as file:\n        partitions = json.load(file)\n\n    if split_name not in partitions:\n        msg = f\"Split '{split_name}' not found.\"\n        raise ValueError(msg)\n\n    self.image_files = [self.data_directory / f\"{filename}.hdf5\" for filename in partitions[split_name]]\n\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.m_neontree.MNeonTreeNonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>A sample returned by :meth:<code>__getitem__</code>.</p> required <code>suptitle</code> <code>str | None</code> <p>Optional string to use as a suptitle.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.</p> Source code in <code>terratorch/datasets/m_neontree.py</code> <pre><code>def plot(self, sample: dict[str, torch.Tensor], suptitle: str | None = None) -&gt; plt.Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, torch.Tensor]): A sample returned by :meth:`__getitem__`.\n        suptitle (str | None): Optional string to use as a suptitle.\n\n    Returns:\n        matplotlib.figure.Figure: A matplotlib Figure with the rendered sample.\n    \"\"\"\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands if band in self.bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n    image = sample[\"image\"]\n    mask = sample[\"mask\"].numpy()\n\n    if torch.is_tensor(image):\n        image = image.permute(1, 2, 0).numpy()  # (H, W, C)\n\n    rgb_image = image[:, :, rgb_indices]\n    rgb_image = clip_image(rgb_image)\n\n    num_classes = len(np.unique(mask))\n    cmap = plt.get_cmap(\"jet\")\n    norm = plt.Normalize(vmin=0, vmax=num_classes - 1)\n\n    num_images = 4 if \"prediction\" in sample else 3\n    fig, ax = plt.subplots(1, num_images, figsize=(num_images * 4, 4), tight_layout=True)\n\n    ax[0].imshow(rgb_image)\n    ax[0].set_title(\"Image\")\n    ax[0].axis(\"off\")\n\n    ax[1].imshow(mask, cmap=cmap, norm=norm)\n    ax[1].set_title(\"Ground Truth Mask\")\n    ax[1].axis(\"off\")\n\n    ax[2].imshow(rgb_image)\n    ax[2].imshow(mask, cmap=cmap, alpha=0.3, norm=norm)\n    ax[2].set_title(\"GT Mask on Image\")\n    ax[2].axis(\"off\")\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"].numpy()\n        ax[3].imshow(prediction, cmap=cmap, norm=norm)\n        ax[3].set_title(\"Predicted Mask\")\n        ax[3].axis(\"off\")\n\n    if suptitle:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.multi_temporal_crop_classification","title":"<code>terratorch.datasets.multi_temporal_crop_classification</code>","text":""},{"location":"datasets/#terratorch.datasets.multi_temporal_crop_classification.MultiTemporalCropClassification","title":"<code>MultiTemporalCropClassification</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for multi-temporal crop classification.</p> Source code in <code>terratorch/datasets/multi_temporal_crop_classification.py</code> <pre><code>class MultiTemporalCropClassification(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [multi-temporal crop classification](https://huggingface.co/datasets/ibm-nasa-geospatial/multi-temporal-crop-classification).\"\"\"\n\n    all_band_names = (\n        \"BLUE\",\n        \"GREEN\",\n        \"RED\",\n        \"NIR_NARROW\",\n        \"SWIR_1\",\n        \"SWIR_2\",\n    )\n\n    class_names = (\n        \"Natural Vegetation\",\n        \"Forest\",\n        \"Corn\",\n        \"Soybeans\",\n        \"Wetlands\",\n        \"Developed / Barren\",\n        \"Open Water\",\n        \"Winter Wheat\",\n        \"Alfalfa\",\n        \"Fallow / Idle Cropland\",\n        \"Cotton\",\n        \"Sorghum\",\n        \"Other\",\n    )\n\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    num_classes = 13\n    time_steps = 3\n    splits = {\"train\": \"training\", \"val\": \"validation\"}  # Only train and val splits available\n    metadata_file_name = \"chips_df.csv\"\n    col_name = \"chip_id\"\n    date_columns = [\"first_img_date\", \"middle_img_date\", \"last_img_date\"]\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = None,\n        expand_temporal_dimension: bool = True,\n        reduce_zero_label: bool = True,\n        use_metadata: bool = False,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): one of 'train' or 'val'.\n            bands (list[str]): Bands that should be output by the dataset. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). If used through the corresponding data module,\n                should not include normalization. Defaults to None, which applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input images with this value.\n                If None, does no replacement. Defaults to None.\n            no_label_replace (int | None): Replace nan values in label with this value.\n                If none, does no replacement. Defaults to None.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to True.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to True.\n            use_metadata (bool): whether to return metadata info (time and location).\n        \"\"\"\n        super().__init__()\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {self.splits}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n        self.data_root = Path(data_root)\n\n        data_dir = self.data_root / f\"{split_name}_chips\"\n        self.image_files = sorted(glob.glob(os.path.join(data_dir, \"*_merged.tif\")))\n        self.segmentation_mask_files = sorted(glob.glob(os.path.join(data_dir, \"*.mask.tif\")))\n        split_file = self.data_root / f\"{split_name}_data.txt\"\n\n        with open(split_file) as f:\n            split = f.readlines()\n        valid_files = {rf\"{substring.strip()}\" for substring in split}\n        self.image_files = filter_valid_files(\n            self.image_files,\n            valid_files=valid_files,\n            ignore_extensions=True,\n            allow_substring=True,\n        )\n        self.segmentation_mask_files = filter_valid_files(\n            self.segmentation_mask_files,\n            valid_files=valid_files,\n            ignore_extensions=True,\n            allow_substring=True,\n        )\n\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.reduce_zero_label = reduce_zero_label\n        self.expand_temporal_dimension = expand_temporal_dimension\n        self.use_metadata = use_metadata\n        self.metadata = None\n        if self.use_metadata:\n            metadata_file = self.data_root / self.metadata_file_name\n            self.metadata = pd.read_csv(metadata_file)\n            self._build_image_metadata_mapping()\n\n        # If no transform is given, apply only to transform to torch tensor\n        self.transform = transform if transform else default_transform\n\n    def _build_image_metadata_mapping(self):\n        \"\"\"Build a mapping from image filenames to metadata indices.\"\"\"\n        self.image_to_metadata_index = dict()\n\n        for idx, image_file in enumerate(self.image_files):\n            image_filename = Path(image_file).name\n            image_id = image_filename.replace(\"_merged.tif\", \"\").replace(\".tif\", \"\")\n            metadata_indices = self.metadata.index[self.metadata[self.col_name] == image_id].tolist()\n            self.image_to_metadata_index[idx] = metadata_indices[0]\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def _get_date(self, row: pd.Series) -&gt; torch.Tensor:\n        \"\"\"Extract and format temporal coordinates (T, date) from metadata.\"\"\"\n        temporal_coords = []\n        for col in self.date_columns:\n            date_str = row[col]\n            date = pd.to_datetime(date_str, format=\"%Y-%m-%d\")\n            temporal_coords.append([date.year, date.dayofyear - 1])\n\n        return torch.tensor(temporal_coords, dtype=torch.float32)\n\n    def _get_coords(self, image: DataArray) -&gt; torch.Tensor:\n        px = image.x.shape[0] // 2\n        py = image.y.shape[0] // 2\n\n        # get center point to reproject to lat/lon\n        point = image.isel(band=0, x=slice(px, px + 1), y=slice(py, py + 1))\n        point = point.rio.reproject(\"epsg:4326\")\n\n        lat_lon = np.asarray([point.y[0], point.x[0]])\n\n        return torch.tensor(lat_lon, dtype=torch.float32)\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        image = self._load_file(self.image_files[index], nan_replace=self.no_data_replace)\n\n        location_coords, temporal_coords = None, None\n        if self.use_metadata:\n            location_coords = self._get_coords(image)\n            metadata_idx = self.image_to_metadata_index.get(index, None)\n            if metadata_idx is not None:\n                row = self.metadata.iloc[metadata_idx]\n                temporal_coords = self._get_date(row)\n\n        # to channels last\n        image = image.to_numpy()\n        if self.expand_temporal_dimension:\n            image = rearrange(image, \"(channels time) h w -&gt; channels time h w\", channels=len(self.bands))\n        image = np.moveaxis(image, 0, -1)\n\n        # filter bands\n        image = image[..., self.band_indices]\n\n        output = {\n            \"image\": image.astype(np.float32),\n            \"mask\": self._load_file(\n                self.segmentation_mask_files[index], nan_replace=self.no_label_replace).to_numpy()[0],\n        }\n\n        if self.reduce_zero_label:\n            output[\"mask\"] -= 1\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        if self.use_metadata:\n            output[\"location_coords\"] = location_coords\n            output[\"temporal_coords\"] = temporal_coords\n\n        return output\n\n    def _load_file(self, path: Path, nan_replace: int | float | None = None) -&gt; DataArray:\n        data = rioxarray.open_rasterio(path, masked=True)\n        if nan_replace is not None:\n            data = data.fillna(nan_replace)\n        return data\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            suptitle: optional string to use as a suptitle\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n        \"\"\"\n        num_images = self.time_steps + 2\n\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        images = sample[\"image\"]\n        images = images[rgb_indices, ...]  # Shape: (T, 3, H, W)\n\n        processed_images = []\n        for t in range(self.time_steps):\n            img = images[t]\n            img = img.permute(1, 2, 0)\n            img = img.numpy()\n            img = clip_image(img)\n            processed_images.append(img)\n\n        mask = sample[\"mask\"].numpy()\n        if \"prediction\" in sample:\n            num_images += 1\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 5), layout=\"compressed\")\n        ax[0].axis(\"off\")\n\n        norm = mpl.colors.Normalize(vmin=0, vmax=self.num_classes - 1)\n        for i, img in enumerate(processed_images):\n            ax[i + 1].axis(\"off\")\n            ax[i + 1].title.set_text(f\"T{i}\")\n            ax[i + 1].imshow(img)\n\n        ax[self.time_steps + 1].axis(\"off\")\n        ax[self.time_steps + 1].title.set_text(\"Ground Truth Mask\")\n        ax[self.time_steps + 1].imshow(mask, cmap=\"jet\", norm=norm)\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"]\n            ax[self.time_steps + 2].axis(\"off\")\n            ax[self.time_steps + 2].title.set_text(\"Predicted Mask\")\n            ax[self.time_steps + 2].imshow(prediction, cmap=\"jet\", norm=norm)\n\n        cmap = plt.get_cmap(\"jet\")\n        legend_data = [[i, cmap(norm(i)), self.class_names[i]] for i in range(self.num_classes)]\n        handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n        labels = [n for k, c, n in legend_data]\n        ax[0].legend(handles, labels, loc=\"center\")\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.multi_temporal_crop_classification.MultiTemporalCropClassification.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=True, reduce_zero_label=True, use_metadata=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>one of 'train' or 'val'.</p> <code>'train'</code> <code>bands</code> <code>list[str]</code> <p>Bands that should be output by the dataset. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the corresponding data module, should not include normalization. Defaults to None, which applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If None, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to True.</p> <code>True</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to True.</p> <code>True</code> <code>use_metadata</code> <code>bool</code> <p>whether to return metadata info (time and location).</p> <code>False</code> Source code in <code>terratorch/datasets/multi_temporal_crop_classification.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = None,\n    expand_temporal_dimension: bool = True,\n    reduce_zero_label: bool = True,\n    use_metadata: bool = False,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): one of 'train' or 'val'.\n        bands (list[str]): Bands that should be output by the dataset. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). If used through the corresponding data module,\n            should not include normalization. Defaults to None, which applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input images with this value.\n            If None, does no replacement. Defaults to None.\n        no_label_replace (int | None): Replace nan values in label with this value.\n            If none, does no replacement. Defaults to None.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to True.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to True.\n        use_metadata (bool): whether to return metadata info (time and location).\n    \"\"\"\n    super().__init__()\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {self.splits}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n    self.data_root = Path(data_root)\n\n    data_dir = self.data_root / f\"{split_name}_chips\"\n    self.image_files = sorted(glob.glob(os.path.join(data_dir, \"*_merged.tif\")))\n    self.segmentation_mask_files = sorted(glob.glob(os.path.join(data_dir, \"*.mask.tif\")))\n    split_file = self.data_root / f\"{split_name}_data.txt\"\n\n    with open(split_file) as f:\n        split = f.readlines()\n    valid_files = {rf\"{substring.strip()}\" for substring in split}\n    self.image_files = filter_valid_files(\n        self.image_files,\n        valid_files=valid_files,\n        ignore_extensions=True,\n        allow_substring=True,\n    )\n    self.segmentation_mask_files = filter_valid_files(\n        self.segmentation_mask_files,\n        valid_files=valid_files,\n        ignore_extensions=True,\n        allow_substring=True,\n    )\n\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.reduce_zero_label = reduce_zero_label\n    self.expand_temporal_dimension = expand_temporal_dimension\n    self.use_metadata = use_metadata\n    self.metadata = None\n    if self.use_metadata:\n        metadata_file = self.data_root / self.metadata_file_name\n        self.metadata = pd.read_csv(metadata_file)\n        self._build_image_metadata_mapping()\n\n    # If no transform is given, apply only to transform to torch tensor\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.multi_temporal_crop_classification.MultiTemporalCropClassification.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> Source code in <code>terratorch/datasets/multi_temporal_crop_classification.py</code> <pre><code>def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        suptitle: optional string to use as a suptitle\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n    \"\"\"\n    num_images = self.time_steps + 2\n\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    images = sample[\"image\"]\n    images = images[rgb_indices, ...]  # Shape: (T, 3, H, W)\n\n    processed_images = []\n    for t in range(self.time_steps):\n        img = images[t]\n        img = img.permute(1, 2, 0)\n        img = img.numpy()\n        img = clip_image(img)\n        processed_images.append(img)\n\n    mask = sample[\"mask\"].numpy()\n    if \"prediction\" in sample:\n        num_images += 1\n    fig, ax = plt.subplots(1, num_images, figsize=(12, 5), layout=\"compressed\")\n    ax[0].axis(\"off\")\n\n    norm = mpl.colors.Normalize(vmin=0, vmax=self.num_classes - 1)\n    for i, img in enumerate(processed_images):\n        ax[i + 1].axis(\"off\")\n        ax[i + 1].title.set_text(f\"T{i}\")\n        ax[i + 1].imshow(img)\n\n    ax[self.time_steps + 1].axis(\"off\")\n    ax[self.time_steps + 1].title.set_text(\"Ground Truth Mask\")\n    ax[self.time_steps + 1].imshow(mask, cmap=\"jet\", norm=norm)\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"]\n        ax[self.time_steps + 2].axis(\"off\")\n        ax[self.time_steps + 2].title.set_text(\"Predicted Mask\")\n        ax[self.time_steps + 2].imshow(prediction, cmap=\"jet\", norm=norm)\n\n    cmap = plt.get_cmap(\"jet\")\n    legend_data = [[i, cmap(norm(i)), self.class_names[i]] for i in range(self.num_classes)]\n    handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n    labels = [n for k, c, n in legend_data]\n    ax[0].legend(handles, labels, loc=\"center\")\n\n    if suptitle is not None:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.open_sentinel_map","title":"<code>terratorch.datasets.open_sentinel_map</code>","text":""},{"location":"datasets/#terratorch.datasets.open_sentinel_map.OpenSentinelMap","title":"<code>OpenSentinelMap</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>Pytorch Dataset class to load samples from the OpenSentinelMap dataset, supporting multiple bands and temporal sampling strategies.</p> Source code in <code>terratorch/datasets/open_sentinel_map.py</code> <pre><code>class OpenSentinelMap(NonGeoDataset):\n    \"\"\"\n        Pytorch Dataset class to load samples from the [OpenSentinelMap](https://visionsystemsinc.github.io/open-sentinel-map/) dataset, supporting\n        multiple bands and temporal sampling strategies.\n    \"\"\"\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: list[str] | None = None,\n        transform: A.Compose | None = None,\n        spatial_interpolate_and_stack_temporally: bool = True,  # noqa: FBT001, FBT002\n        pad_image: int | None = None,\n        truncate_image: int | None = None,\n        target: int = 0,\n        pick_random_pair: bool = True,  # noqa: FBT002, FBT001\n    ) -&gt; None:\n        \"\"\"\n\n        Args:\n            data_root (str): Path to the root directory of the dataset.\n            split (str): Dataset split to load. Options are 'train', 'val', or 'test'. Defaults to 'train'.\n            bands (list of str, optional): List of band names to load. Defaults to ['gsd_10', 'gsd_20', 'gsd_60'].\n            transform (albumentations.Compose, optional): Albumentations transformations to apply to the data.\n            spatial_interpolate_and_stack_temporally (bool): If True, the bands are interpolated and concatenated over time.\n                Default is True.\n            pad_image (int, optional): Number of timesteps to pad the time dimension of the image.\n                If None, no padding is applied.\n            truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image.\n                If None, no truncation is performed.\n            target (int): Specifies which target class to use from the mask. Default is 0.\n            pick_random_pair (bool): If True, selects two random images from the temporal sequence. Default is True.\n        \"\"\"\n        split = \"test\"\n        if bands is None:\n            bands = [\"gsd_10\", \"gsd_20\", \"gsd_60\"]\n\n        allowed_bands = {\"gsd_10\", \"gsd_20\", \"gsd_60\"}\n        for band in bands:\n            if band not in allowed_bands:\n                msg = f\"Band '{band}' is not recognized. Available values are: {', '.join(allowed_bands)}\"\n                raise ValueError(msg)\n\n        if split not in [\"train\", \"val\", \"test\"]:\n            msg = f\"Split '{split}' not recognized. Use 'train', 'val', or 'test'.\"\n            raise ValueError(msg)\n\n        self.data_root = Path(data_root)\n        split_mapping = {\"train\": \"training\", \"val\": \"validation\", \"test\": \"testing\"}\n        split = split_mapping[split]\n        self.imagery_root = self.data_root / \"osm_sentinel_imagery\"\n        self.label_root = self.data_root / \"osm_label_images_v10\"\n        self.auxiliary_data = pd.read_csv(self.data_root / \"spatial_cell_info.csv\")\n        self.auxiliary_data = self.auxiliary_data[self.auxiliary_data[\"split\"] == split]\n        self.bands = bands\n        self.transform = transform if transform else lambda **batch: to_tensor(batch)\n        self.label_mappings = self._load_label_mappings()\n        self.split_data = self.auxiliary_data[self.auxiliary_data[\"split\"] == split]\n        self.spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally\n        self.pad_image = pad_image\n        self.truncate_image = truncate_image\n        self.target = target\n        self.pick_random_pair = pick_random_pair\n\n        self.image_files = []\n        self.label_files = []\n\n        for _, row in self.split_data.iterrows():\n            mgrs_tile = row[\"MGRS_tile\"]\n            spatial_cell = str(row[\"cell_id\"])\n\n            label_file = self.label_root / mgrs_tile / f\"{spatial_cell}.png\"\n\n            if label_file.exists():\n                self.image_files.append((mgrs_tile, spatial_cell))\n                self.label_files.append(label_file)\n\n    def _load_label_mappings(self):\n        with open(self.data_root / \"osm_categories.json\") as f:\n            return json.load(f)\n\n    def _extract_date_from_filename(self, filename: str) -&gt; str:\n        match = re.search(r\"(\\d{8})\", filename)\n        if match:\n            return match.group(1)\n        else:\n            msg = f\"Date not found in filename {filename}\"\n            raise ValueError(msg)\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n        if \"gsd_10\" not in self.bands:\n            return None\n\n        num_images = len([key for key in sample if key.startswith(\"image\")])\n        images = []\n\n        for i in range(1, num_images + 1):\n            image_dict = sample[f\"image{i}\"]\n            image = image_dict[\"gsd_10\"]\n            if isinstance(image, Tensor):\n                image = image.numpy()\n\n            image = image.take(range(3), axis=2)\n            image = image.squeeze()\n            image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n            image = np.clip(image, 0, 1)\n            images.append(image)\n\n        label_mask = sample[\"mask\"]\n        if isinstance(label_mask, Tensor):\n            label_mask = label_mask.numpy()\n\n        return self._plot_sample(images, label_mask, suptitle=suptitle)\n\n\n    def _plot_sample(\n        self, images: list[np.ndarray],\n        label: np.ndarray,\n        suptitle: str | None = None,\n    ) -&gt; Figure:\n        num_images = len(images)\n        fig, ax = plt.subplots(1, num_images + 1, figsize=(15, 5))\n\n        for i, image in enumerate(images):\n            ax[i].imshow(image)\n            ax[i].set_title(f\"Image {i + 1}\")\n            ax[i].axis(\"off\")\n\n        ax[-1].imshow(label, cmap=\"gray\")\n        ax[-1].set_title(\"Ground Truth Mask\")\n        ax[-1].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        return fig\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        mgrs_tile, spatial_cell = self.image_files[index]\n        spatial_cell_path = self.imagery_root / mgrs_tile / spatial_cell\n\n        npz_files = list(spatial_cell_path.glob(\"*.npz\"))\n        npz_files.sort(key=lambda x: self._extract_date_from_filename(x.stem))\n\n        if self.pick_random_pair:\n            npz_files = random.sample(npz_files, 2)\n            npz_files.sort(key=lambda x: self._extract_date_from_filename(x.stem))\n\n        output = {}\n\n        if self.spatial_interpolate_and_stack_temporally:\n            images_over_time = []\n            for _, npz_file in enumerate(npz_files):\n                data = np.load(npz_file)\n                interpolated_bands = []\n                for band in self.bands:\n                    band_frame = data[band]\n                    band_frame = torch.from_numpy(band_frame).float()\n                    band_frame = band_frame.permute(2, 0, 1)\n                    interpolated = F.interpolate(\n                        band_frame.unsqueeze(0), size=MAX_TEMPORAL_IMAGE_SIZE, mode=\"bilinear\", align_corners=False\n                    ).squeeze(0)\n                    interpolated_bands.append(interpolated)\n                concatenated_bands = torch.cat(interpolated_bands, dim=0)\n                images_over_time.append(concatenated_bands)\n\n            images = torch.stack(images_over_time, dim=0).numpy()\n            if self.truncate_image:\n                images = images[-self.truncate_image:]\n            if self.pad_image:\n                images = pad_numpy(images, self.pad_image)\n\n            output[\"image\"] = images.transpose(0, 2, 3, 1)\n        else:\n            image_dict = {band: [] for band in self.bands}\n            for _, npz_file in enumerate(npz_files):\n                data = np.load(npz_file)\n                for band in self.bands:\n                    band_frames = data[band]\n                    band_frames = band_frames.astype(np.float32)\n                    band_frames = np.transpose(band_frames, (2, 0, 1))\n                    image_dict[band].append(band_frames)\n\n            final_image_dict = {}\n            for band in self.bands:\n                band_images = image_dict[band]\n                if self.truncate_image:\n                    band_images = band_images[-self.truncate_image:]\n                if self.pad_image:\n                    band_images = [pad_numpy(img, self.pad_image) for img in band_images]\n                band_images = np.stack(band_images, axis=0)\n                final_image_dict[band] = band_images\n\n            output[\"image\"] = final_image_dict\n\n        label_file = self.label_files[index]\n        mask = np.array(Image.open(label_file)).astype(int)\n\n        # Map 'unlabel' (254) and 'none' (255) to unused classes 15 and 16 for processing\n        mask[mask == 254] = 15  # noqa: PLR2004\n        mask[mask == 255] = 16  # noqa: PLR2004\n        output[\"mask\"] = mask[:, :, self.target]\n\n        if self.transform:\n            output = self.transform(**output)\n\n        return output\n</code></pre>"},{"location":"datasets/#terratorch.datasets.open_sentinel_map.OpenSentinelMap.__init__","title":"<code>__init__(data_root, split='train', bands=None, transform=None, spatial_interpolate_and_stack_temporally=True, pad_image=None, truncate_image=None, target=0, pick_random_pair=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the root directory of the dataset.</p> required <code>split</code> <code>str</code> <p>Dataset split to load. Options are 'train', 'val', or 'test'. Defaults to 'train'.</p> <code>'train'</code> <code>bands</code> <code>list of str</code> <p>List of band names to load. Defaults to ['gsd_10', 'gsd_20', 'gsd_60'].</p> <code>None</code> <code>transform</code> <code>Compose</code> <p>Albumentations transformations to apply to the data.</p> <code>None</code> <code>spatial_interpolate_and_stack_temporally</code> <code>bool</code> <p>If True, the bands are interpolated and concatenated over time. Default is True.</p> <code>True</code> <code>pad_image</code> <code>int</code> <p>Number of timesteps to pad the time dimension of the image. If None, no padding is applied.</p> <code>None</code> <code>truncate_image</code> <code>int</code> <p>Number of timesteps to truncate the time dimension of the image. If None, no truncation is performed.</p> <code>None</code> <code>target</code> <code>int</code> <p>Specifies which target class to use from the mask. Default is 0.</p> <code>0</code> <code>pick_random_pair</code> <code>bool</code> <p>If True, selects two random images from the temporal sequence. Default is True.</p> <code>True</code> Source code in <code>terratorch/datasets/open_sentinel_map.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: list[str] | None = None,\n    transform: A.Compose | None = None,\n    spatial_interpolate_and_stack_temporally: bool = True,  # noqa: FBT001, FBT002\n    pad_image: int | None = None,\n    truncate_image: int | None = None,\n    target: int = 0,\n    pick_random_pair: bool = True,  # noqa: FBT002, FBT001\n) -&gt; None:\n    \"\"\"\n\n    Args:\n        data_root (str): Path to the root directory of the dataset.\n        split (str): Dataset split to load. Options are 'train', 'val', or 'test'. Defaults to 'train'.\n        bands (list of str, optional): List of band names to load. Defaults to ['gsd_10', 'gsd_20', 'gsd_60'].\n        transform (albumentations.Compose, optional): Albumentations transformations to apply to the data.\n        spatial_interpolate_and_stack_temporally (bool): If True, the bands are interpolated and concatenated over time.\n            Default is True.\n        pad_image (int, optional): Number of timesteps to pad the time dimension of the image.\n            If None, no padding is applied.\n        truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image.\n            If None, no truncation is performed.\n        target (int): Specifies which target class to use from the mask. Default is 0.\n        pick_random_pair (bool): If True, selects two random images from the temporal sequence. Default is True.\n    \"\"\"\n    split = \"test\"\n    if bands is None:\n        bands = [\"gsd_10\", \"gsd_20\", \"gsd_60\"]\n\n    allowed_bands = {\"gsd_10\", \"gsd_20\", \"gsd_60\"}\n    for band in bands:\n        if band not in allowed_bands:\n            msg = f\"Band '{band}' is not recognized. Available values are: {', '.join(allowed_bands)}\"\n            raise ValueError(msg)\n\n    if split not in [\"train\", \"val\", \"test\"]:\n        msg = f\"Split '{split}' not recognized. Use 'train', 'val', or 'test'.\"\n        raise ValueError(msg)\n\n    self.data_root = Path(data_root)\n    split_mapping = {\"train\": \"training\", \"val\": \"validation\", \"test\": \"testing\"}\n    split = split_mapping[split]\n    self.imagery_root = self.data_root / \"osm_sentinel_imagery\"\n    self.label_root = self.data_root / \"osm_label_images_v10\"\n    self.auxiliary_data = pd.read_csv(self.data_root / \"spatial_cell_info.csv\")\n    self.auxiliary_data = self.auxiliary_data[self.auxiliary_data[\"split\"] == split]\n    self.bands = bands\n    self.transform = transform if transform else lambda **batch: to_tensor(batch)\n    self.label_mappings = self._load_label_mappings()\n    self.split_data = self.auxiliary_data[self.auxiliary_data[\"split\"] == split]\n    self.spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally\n    self.pad_image = pad_image\n    self.truncate_image = truncate_image\n    self.target = target\n    self.pick_random_pair = pick_random_pair\n\n    self.image_files = []\n    self.label_files = []\n\n    for _, row in self.split_data.iterrows():\n        mgrs_tile = row[\"MGRS_tile\"]\n        spatial_cell = str(row[\"cell_id\"])\n\n        label_file = self.label_root / mgrs_tile / f\"{spatial_cell}.png\"\n\n        if label_file.exists():\n            self.image_files.append((mgrs_tile, spatial_cell))\n            self.label_files.append(label_file)\n</code></pre>"},{"location":"datasets/#terratorch.datasets.openearthmap","title":"<code>terratorch.datasets.openearthmap</code>","text":""},{"location":"datasets/#terratorch.datasets.openearthmap.OpenEarthMapNonGeo","title":"<code>OpenEarthMapNonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>OpenEarthMapNonGeo Dataset for non-georeferenced imagery.</p> <p>This dataset class handles non-georeferenced image data from the OpenEarthMap dataset. It supports configurable band sets and transformations, and performs cropping operations to ensure that the images conform to the required input dimensions. The dataset is split into \"train\", \"test\", and \"val\" subsets based on the provided split parameter.</p> Source code in <code>terratorch/datasets/openearthmap.py</code> <pre><code>class OpenEarthMapNonGeo(NonGeoDataset):\n    \"\"\"\n    [OpenEarthMapNonGeo](https://open-earth-map.org/) Dataset for non-georeferenced imagery.\n\n    This dataset class handles non-georeferenced image data from the OpenEarthMap dataset.\n    It supports configurable band sets and transformations, and performs cropping operations\n    to ensure that the images conform to the required input dimensions. The dataset is split\n    into \"train\", \"test\", and \"val\" subsets based on the provided split parameter.\n    \"\"\"\n\n\n    all_band_names = (\"BLUE\",\"GREEN\",\"RED\")\n\n    rgb_bands = (\"RED\",\"GREEN\",\"BLUE\")\n\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n\n    def __init__(self, data_root: str,\n                 bands: Sequence[str] = BAND_SETS[\"all\"],\n                 transform: A.Compose | None = None,\n                 split=\"train\",\n                 crop_size: int = 256,\n                 random_crop: bool = True) -&gt; None:\n        \"\"\"\n        Initialize a new instance of the OpenEarthMapNonGeo dataset.\n\n        Args:\n            data_root (str): The root directory containing the dataset files.\n            bands (Sequence[str], optional): A list of band names to be used. Default is BAND_SETS[\"all\"].\n            transform (A.Compose or None, optional): A transformation pipeline to be applied to the data.\n                If None, a default transform converting the data to a tensor is applied.\n            split (str, optional): The dataset split to use (\"train\", \"test\", or \"val\"). Default is \"train\".\n            crop_size (int, optional): The size (in pixels) of the crop to apply to images. Must be greater than 0.\n                Default is 256.\n            random_crop (bool, optional): If True, performs a random crop; otherwise, performs a center crop.\n                Default is True.\n\n        Raises:\n            Exception: If the provided split is not one of \"train\", \"test\", or \"val\".\n            AssertionError: If crop_size is not greater than 0.\n        \"\"\"\n        super().__init__()\n        if split not in [\"train\", \"test\", \"val\"]:\n            msg = \"Split must be one of train, test, val.\"\n            raise Exception(msg)\n\n        self.transform = transform if transform else lambda **batch: to_tensor(batch, transpose=False)\n        self.split = split\n        self.data_root = data_root\n\n        # images in openearthmap are not all 1024x1024 and must be cropped\n        self.crop_size = crop_size\n        self.random_crop = random_crop\n\n        assert self.crop_size &gt; 0, \"Crop size must be greater than 0\"\n\n        self.image_files = self._get_file_paths(Path(self.data_root, f\"{split}.txt\"))\n\n    def __getitem__(self, index: int) -&gt; dict[str, torch.Tensor]:\n        image_path, label_path = self.image_files[index]\n\n        with rasterio.open(image_path) as src:\n            image = src.read()\n        with rasterio.open(label_path) as src:\n            mask = src.read()\n\n        # some images in the dataset are not perfect squares\n        # cropping to fit to the prepare_features_for_image_model call\n        if self.random_crop:\n            image, mask = self._random_crop(image, mask)\n        else:\n            image, mask = self._center_crop(image, mask)\n\n        output =  {\n            \"image\": image.astype(np.float32),\n            \"mask\": mask\n        }\n\n        output = self.transform(**output)\n        output['mask'] = output['mask'].long()\n\n        return output\n\n    def _parse_file_name(self, file_name: str):\n        underscore_pos = file_name.rfind('_')\n        folder_name = file_name[:underscore_pos]\n        region_path = Path(self.data_root, folder_name)\n        image_path = Path(region_path, \"images\", file_name)\n        label_path = Path(region_path, \"labels\", file_name)\n        return image_path, label_path\n\n    def _get_file_paths(self, text_file_path: str):\n        with open(text_file_path, 'r') as file:\n            lines = file.readlines()\n            file_paths = [self._parse_file_name(line.strip()) for line in lines]\n        return file_paths\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def _random_crop(self, image, mask):\n        h, w = image.shape[1:]\n        top = np.random.randint(0, h - self.crop_size)\n        left = np.random.randint(0, w - self.crop_size)\n\n        image = image[:, top: top + self.crop_size, left: left + self.crop_size]\n        mask = mask[:, top: top + self.crop_size, left: left + self.crop_size]\n\n        return image, mask\n\n    def _center_crop(self, image, mask):\n        h, w = image.shape[1:]\n        top = (h - self.crop_size) // 2\n        left = (w - self.crop_size) // 2\n\n        image = image[:, top: top + self.crop_size, left: left + self.crop_size]\n        mask = mask[:, top: top + self.crop_size, left: left + self.crop_size]\n\n        return image, mask\n\n    def plot(self, arg, suptitle: str | None = None) -&gt; None:\n        pass\n\n    def plot_sample(self, sample, prediction=None, suptitle: str | None = None, class_names=None):\n        pass\n</code></pre>"},{"location":"datasets/#terratorch.datasets.openearthmap.OpenEarthMapNonGeo.__init__","title":"<code>__init__(data_root, bands=BAND_SETS['all'], transform=None, split='train', crop_size=256, random_crop=True)</code>","text":"<p>Initialize a new instance of the OpenEarthMapNonGeo dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>The root directory containing the dataset files.</p> required <code>bands</code> <code>Sequence[str]</code> <p>A list of band names to be used. Default is BAND_SETS[\"all\"].</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose or None</code> <p>A transformation pipeline to be applied to the data. If None, a default transform converting the data to a tensor is applied.</p> <code>None</code> <code>split</code> <code>str</code> <p>The dataset split to use (\"train\", \"test\", or \"val\"). Default is \"train\".</p> <code>'train'</code> <code>crop_size</code> <code>int</code> <p>The size (in pixels) of the crop to apply to images. Must be greater than 0. Default is 256.</p> <code>256</code> <code>random_crop</code> <code>bool</code> <p>If True, performs a random crop; otherwise, performs a center crop. Default is True.</p> <code>True</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the provided split is not one of \"train\", \"test\", or \"val\".</p> <code>AssertionError</code> <p>If crop_size is not greater than 0.</p> Source code in <code>terratorch/datasets/openearthmap.py</code> <pre><code>def __init__(self, data_root: str,\n             bands: Sequence[str] = BAND_SETS[\"all\"],\n             transform: A.Compose | None = None,\n             split=\"train\",\n             crop_size: int = 256,\n             random_crop: bool = True) -&gt; None:\n    \"\"\"\n    Initialize a new instance of the OpenEarthMapNonGeo dataset.\n\n    Args:\n        data_root (str): The root directory containing the dataset files.\n        bands (Sequence[str], optional): A list of band names to be used. Default is BAND_SETS[\"all\"].\n        transform (A.Compose or None, optional): A transformation pipeline to be applied to the data.\n            If None, a default transform converting the data to a tensor is applied.\n        split (str, optional): The dataset split to use (\"train\", \"test\", or \"val\"). Default is \"train\".\n        crop_size (int, optional): The size (in pixels) of the crop to apply to images. Must be greater than 0.\n            Default is 256.\n        random_crop (bool, optional): If True, performs a random crop; otherwise, performs a center crop.\n            Default is True.\n\n    Raises:\n        Exception: If the provided split is not one of \"train\", \"test\", or \"val\".\n        AssertionError: If crop_size is not greater than 0.\n    \"\"\"\n    super().__init__()\n    if split not in [\"train\", \"test\", \"val\"]:\n        msg = \"Split must be one of train, test, val.\"\n        raise Exception(msg)\n\n    self.transform = transform if transform else lambda **batch: to_tensor(batch, transpose=False)\n    self.split = split\n    self.data_root = data_root\n\n    # images in openearthmap are not all 1024x1024 and must be cropped\n    self.crop_size = crop_size\n    self.random_crop = random_crop\n\n    assert self.crop_size &gt; 0, \"Crop size must be greater than 0\"\n\n    self.image_files = self._get_file_paths(Path(self.data_root, f\"{split}.txt\"))\n</code></pre>"},{"location":"datasets/#terratorch.datasets.pastis","title":"<code>terratorch.datasets.pastis</code>","text":""},{"location":"datasets/#terratorch.datasets.pastis.PASTIS","title":"<code>PASTIS</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>\" Pytorch Dataset class to load samples from the PASTIS dataset, for semantic and panoptic segmentation.</p> Source code in <code>terratorch/datasets/pastis.py</code> <pre><code>class PASTIS(NonGeoDataset):\n    \"\"\"\"\n        Pytorch Dataset class to load samples from the [PASTIS](https://github.com/VSainteuf/pastis-benchmark) dataset,\n        for semantic and panoptic segmentation.\n    \"\"\"\n    def __init__(\n        self,\n        data_root,\n        norm=True,  # noqa: FBT002\n        target=\"semantic\",\n        folds=None,\n        reference_date=\"2018-09-01\",\n        date_interval = (-200,600),\n        class_mapping=None,\n        transform = None,\n        truncate_image = None,\n        pad_image = None,\n        satellites=[\"S2\"],  # noqa: B006\n    ):\n        \"\"\"\n\n        Args:\n            data_root (str): Path to the dataset.\n            norm (bool): If true, images are standardised using pre-computed\n                channel-wise means and standard deviations.\n            reference_date (str, Format : 'YYYY-MM-DD'): Defines the reference date\n                based on which all observation dates are expressed. Along with the image\n                time series and the target tensor, this dataloader yields the sequence\n                of observation dates (in terms of number of days since the reference\n                date). This sequence of dates is used for instance for the positional\n                encoding in attention based approaches.\n            target (str): 'semantic' or 'instance'. Defines which type of target is\n                returned by the dataloader.\n                * If 'semantic' the target tensor is a tensor containing the class of\n                each pixel.\n                * If 'instance' the target tensor is the concatenation of several\n                signals, necessary to train the Parcel-as-Points module:\n                    - the centerness heatmap,\n                    - the instance ids,\n                    - the voronoi partitioning of the patch with regards to the parcels'\n                    centers,\n                    - the (height, width) size of each parcel,\n                    - the semantic label of each parcel,\n                    - the semantic label of each pixel.\n            folds (list, optional): List of ints specifying which of the 5 official\n                folds to load. By default (when None is specified), all folds are loaded.\n            class_mapping (dict, optional): A dictionary to define a mapping between the\n                default 18 class nomenclature and another class grouping. If not provided, \n                the default class mapping is used.\n            transform (callable, optional): A transform to apply to the loaded data \n                (images, dates, and masks). By default, no transformation is applied.\n            truncate_image (int, optional): Truncate the time dimension of the image to \n                a specified number of timesteps. If None, no truncation is performed.\n            pad_image (int, optional): Pad the time dimension of the image to a specified \n                number of timesteps. If None, no padding is applied.\n            satellites (list): Defines the satellites to use. If you are using PASTIS-R, you\n                have access to Sentinel-2 imagery and Sentinel-1 observations in Ascending\n                and Descending orbits, respectively S2, S1A, and S1D. For example, use\n                satellites=['S2', 'S1A'] for Sentinel-2 + Sentinel-1 ascending time series,\n                or satellites=['S2', 'S1A', 'S1D'] to retrieve all time series. If you are using\n                PASTIS, only S2 observations are available.\n        \"\"\"\n        if target not in [\"semantic\", \"instance\"]:\n            msg = f\"Target '{target}' not recognized. Use 'semantic', or 'instance'.\"\n            raise ValueError(msg)\n        valid_satellites = {\"S2\", \"S1A\", \"S1D\"}\n        for sat in satellites:\n            if sat not in valid_satellites:\n                msg = f\"Satellite '{sat}' not recognized. Valid options are {valid_satellites}.\"\n                raise ValueError(msg)\n\n        super().__init__()\n        self.data_root = data_root\n        self.norm = norm\n        self.reference_date = datetime(*map(int, reference_date.split(\"-\")), tzinfo=timezone.utc)\n        self.class_mapping = (\n            np.vectorize(lambda x: class_mapping[x])\n            if class_mapping is not None\n            else class_mapping\n        )\n        self.target = target\n        self.satellites = satellites\n        self.transform = transform\n        self.truncate_image = truncate_image\n        self.pad_image = pad_image\n        # loads patches metadata\n        self.meta_patch = gpd.read_file(os.path.join(data_root, \"metadata.geojson\"))\n        self.meta_patch.index = self.meta_patch[\"ID_PATCH\"].astype(int)\n        self.meta_patch.sort_index(inplace=True)\n        # stores table for each satalite date\n        self.date_tables = {s: None for s in satellites}\n        # date interval used in the PASTIS benchmark paper.\n        date_interval_begin, date_interval_end = date_interval\n        self.date_range = np.array(range(date_interval_begin, date_interval_end))\n        for s in satellites:\n            # maps patches to its observation dates\n            dates = self.meta_patch[f\"dates-{s}\"]\n            date_table = pd.DataFrame(\n                index=self.meta_patch.index, columns=self.date_range, dtype=int\n            )\n            for pid, date_seq in dates.items():\n                if type(date_seq) is str:\n                    date_seq = json.loads(date_seq)  # noqa: PLW2901\n                # convert date to days since obersavation format\n                d = pd.DataFrame().from_dict(date_seq, orient=\"index\")\n                d = d[0].apply(\n                    lambda x: (\n                        datetime(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:]), tzinfo=timezone.utc)\n                        - self.reference_date\n                    ).days\n                )\n                date_table.loc[pid, d.values] = 1\n            date_table = date_table.fillna(0)\n            self.date_tables[s] = {\n                index: np.array(list(d.values()))\n                for index, d in date_table.to_dict(orient=\"index\").items()\n            }\n\n        # selects patches correspondig to selected folds\n        if folds is not None:\n            self.meta_patch = pd.concat(\n                [self.meta_patch[self.meta_patch[\"Fold\"] == f] for f in folds]\n            )\n\n        self.len = self.meta_patch.shape[0]\n        self.id_patches = self.meta_patch.index\n\n        # loads normalization values\n        if norm:\n            self.norm = {}\n            for s in self.satellites:\n                with open(\n                    os.path.join(data_root, f\"NORM_{s}_patch.json\")\n                ) as file:\n                    normvals = json.loads(file.read())\n                selected_folds = folds if folds is not None else range(1, 6)\n                means = [normvals[f\"Fold_{f}\"][\"mean\"] for f in selected_folds]\n                stds = [normvals[f\"Fold_{f}\"][\"std\"] for f in selected_folds]\n                self.norm[s] = np.stack(means).mean(axis=0), np.stack(stds).mean(axis=0)\n                self.norm[s] = (\n                    self.norm[s][0],\n                    self.norm[s][1],\n                )\n        else:\n            self.norm = None\n\n    def __len__(self):\n        return self.len\n\n    def get_dates(self, id_patch, sat):\n        return self.date_range[np.where(self.date_tables[sat][id_patch] == 1)[0]]\n\n    def __getitem__(self, item):\n        id_patch = self.id_patches[item]\n        output = {}\n        satellites = {}\n        for satellite in self.satellites:\n            data = np.load(\n                os.path.join(\n                    self.data_root,\n                    f\"DATA_{satellite}\",\n                    f\"{satellite}_{id_patch}.npy\",\n                )\n            ).astype(np.float32)\n\n            if self.norm is not None:\n                    data = data - self.norm[satellite][0][None, :, None, None]\n                    data = data / self.norm[satellite][1][None, :, None, None]\n\n            if self.truncate_image and data.shape[0] &gt; self.truncate_image:\n                data = data[-self.truncate_image:]\n\n            if self.pad_image and data.shape[0] &lt; self.pad_image:\n                data = pad_numpy(data, self.pad_image)\n\n            satellites[satellite] = data.astype(np.float32)\n\n\n        if self.target == \"semantic\":\n            target = np.load(\n                os.path.join(self.data_root, \"ANNOTATIONS\", f\"TARGET_{id_patch}.npy\")\n            )\n            target = target[0].astype(int)\n            if self.class_mapping is not None:\n                target = self.class_mapping(target)\n        elif self.target == \"instance\":\n            heatmap = np.load(os.path.join(self.data_root, \"INSTANCE_ANNOTATIONS\", f\"HEATMAP_{id_patch}.npy\"))\n            instance_ids = np.load(os.path.join(self.data_root, \"INSTANCE_ANNOTATIONS\", f\"INSTANCES_{id_patch}.npy\"))\n            zones_path = os.path.join(self.data_root, \"INSTANCE_ANNOTATIONS\", f\"ZONES_{id_patch}.npy\")\n            pixel_to_object_mapping = np.load(zones_path)\n            pixel_semantic_annotation = np.load(os.path.join(self.data_root, \"ANNOTATIONS\", f\"TARGET_{id_patch}.npy\"))\n\n            if self.class_mapping is not None:\n                pixel_semantic_annotation = self.class_mapping(pixel_semantic_annotation[0])\n            else:\n                pixel_semantic_annotation = pixel_semantic_annotation[0]\n\n            size = np.zeros((*instance_ids.shape, 2))\n            object_semantic_annotation = np.zeros(instance_ids.shape)\n            for instance_id in np.unique(instance_ids):\n                if instance_id != 0:\n                    h = (instance_ids == instance_id).any(axis=-1).sum()\n                    w = (instance_ids == instance_id).any(axis=-2).sum()\n                    size[pixel_to_object_mapping == instance_id] = (h, w)\n                    semantic_value = pixel_semantic_annotation[instance_ids == instance_id][0]\n                    object_semantic_annotation[pixel_to_object_mapping == instance_id] = semantic_value\n\n            target = np.concatenate(\n                [\n                    heatmap[:, :, None],\n                    instance_ids[:, :, None],\n                    pixel_to_object_mapping[:, :, None],\n                    size,\n                    object_semantic_annotation[:, :, None],\n                    pixel_semantic_annotation[:, :, None],\n                ], axis=-1).astype(np.float32)\n\n        dates = {}\n        for satellite in self.satellites:\n            date = np.array(self.get_dates(id_patch, satellite))\n\n            if self.truncate_image and len(date) &gt; self.truncate_image:\n                date = date[-self.truncate_image:]\n\n            if self.pad_image and len(date) &lt; self.pad_image:\n                date = pad_dates_numpy(date, self.pad_image)\n\n            dates[satellite] = torch.from_numpy(date)\n\n        output[\"image\"] = satellites[\"S2\"].transpose(0, 2, 3, 1)\n        output[\"mask\"] = target\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output.update(satellites)\n        output[\"dates\"] = dates\n\n        return output\n\n\n    def plot(self, sample, suptitle=None):\n        dates = sample[\"dates\"]\n        target = sample[\"target\"]\n\n        if \"S2\" not in sample:\n            warnings.warn(\"No RGB image.\", stacklevel=2)\n            return None\n\n        image_data = sample[\"S2\"]\n        date_data = dates[\"S2\"]\n\n        rgb_images = []\n        for i in range(image_data.shape[0]):\n            rgb_image = image_data[i, :3, :, :].numpy().transpose(1, 2, 0)\n\n            rgb_min = rgb_image.min(axis=(0, 1), keepdims=True)\n            rgb_max = rgb_image.max(axis=(0, 1), keepdims=True)\n            denom = rgb_max - rgb_min\n            denom[denom == 0] = 1\n            rgb_image = (rgb_image - rgb_min) / denom\n\n            rgb_images.append(np.clip(rgb_image, 0, 1))\n\n        return self._plot_sample(rgb_images, date_data, target, suptitle=suptitle)\n\n    def _plot_sample(\n        self,\n        images: list[np.ndarray],\n        dates: torch.Tensor,\n        target: torch.Tensor | None,\n        suptitle: str | None = None\n    ):\n        num_images = len(images)\n        cols = 5\n        rows = (num_images + cols) // cols\n\n        fig, ax = plt.subplots(rows, cols, figsize=(20, 4 * rows))\n\n        for i, image in enumerate(images):\n            ax[i // cols, i % cols].imshow(image)\n            ax[i // cols, i % cols].set_title(f\"Image {i + 1} - Day {dates[i].item()}\")\n            ax[i // cols, i % cols].axis(\"off\")\n\n        if target is not None:\n            if rows * cols &gt; num_images:\n                target_ax = ax[(num_images) // cols, (num_images) % cols]\n            else:\n                fig.add_subplot(rows + 1, 1, 1)\n                target_ax = fig.gca()\n\n            target_ax.imshow(target.numpy(), cmap=\"tab20\")\n            target_ax.set_title(\"Target\")\n            target_ax.axis(\"off\")\n\n        for k in range(num_images + 1, rows * cols):\n            ax[k // cols, k % cols].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        plt.tight_layout()\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.pastis.PASTIS.__init__","title":"<code>__init__(data_root, norm=True, target='semantic', folds=None, reference_date='2018-09-01', date_interval=(-200, 600), class_mapping=None, transform=None, truncate_image=None, pad_image=None, satellites=['S2'])</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the dataset.</p> required <code>norm</code> <code>bool</code> <p>If true, images are standardised using pre-computed channel-wise means and standard deviations.</p> <code>True</code> <code>reference_date</code> <code>(str, Format)</code> <p>'YYYY-MM-DD'): Defines the reference date based on which all observation dates are expressed. Along with the image time series and the target tensor, this dataloader yields the sequence of observation dates (in terms of number of days since the reference date). This sequence of dates is used for instance for the positional encoding in attention based approaches.</p> <code>'2018-09-01'</code> <code>target</code> <code>str</code> <p>'semantic' or 'instance'. Defines which type of target is returned by the dataloader. * If 'semantic' the target tensor is a tensor containing the class of each pixel. * If 'instance' the target tensor is the concatenation of several signals, necessary to train the Parcel-as-Points module:     - the centerness heatmap,     - the instance ids,     - the voronoi partitioning of the patch with regards to the parcels'     centers,     - the (height, width) size of each parcel,     - the semantic label of each parcel,     - the semantic label of each pixel.</p> <code>'semantic'</code> <code>folds</code> <code>list</code> <p>List of ints specifying which of the 5 official folds to load. By default (when None is specified), all folds are loaded.</p> <code>None</code> <code>class_mapping</code> <code>dict</code> <p>A dictionary to define a mapping between the default 18 class nomenclature and another class grouping. If not provided,  the default class mapping is used.</p> <code>None</code> <code>transform</code> <code>callable</code> <p>A transform to apply to the loaded data  (images, dates, and masks). By default, no transformation is applied.</p> <code>None</code> <code>truncate_image</code> <code>int</code> <p>Truncate the time dimension of the image to  a specified number of timesteps. If None, no truncation is performed.</p> <code>None</code> <code>pad_image</code> <code>int</code> <p>Pad the time dimension of the image to a specified  number of timesteps. If None, no padding is applied.</p> <code>None</code> <code>satellites</code> <code>list</code> <p>Defines the satellites to use. If you are using PASTIS-R, you have access to Sentinel-2 imagery and Sentinel-1 observations in Ascending and Descending orbits, respectively S2, S1A, and S1D. For example, use satellites=['S2', 'S1A'] for Sentinel-2 + Sentinel-1 ascending time series, or satellites=['S2', 'S1A', 'S1D'] to retrieve all time series. If you are using PASTIS, only S2 observations are available.</p> <code>['S2']</code> Source code in <code>terratorch/datasets/pastis.py</code> <pre><code>def __init__(\n    self,\n    data_root,\n    norm=True,  # noqa: FBT002\n    target=\"semantic\",\n    folds=None,\n    reference_date=\"2018-09-01\",\n    date_interval = (-200,600),\n    class_mapping=None,\n    transform = None,\n    truncate_image = None,\n    pad_image = None,\n    satellites=[\"S2\"],  # noqa: B006\n):\n    \"\"\"\n\n    Args:\n        data_root (str): Path to the dataset.\n        norm (bool): If true, images are standardised using pre-computed\n            channel-wise means and standard deviations.\n        reference_date (str, Format : 'YYYY-MM-DD'): Defines the reference date\n            based on which all observation dates are expressed. Along with the image\n            time series and the target tensor, this dataloader yields the sequence\n            of observation dates (in terms of number of days since the reference\n            date). This sequence of dates is used for instance for the positional\n            encoding in attention based approaches.\n        target (str): 'semantic' or 'instance'. Defines which type of target is\n            returned by the dataloader.\n            * If 'semantic' the target tensor is a tensor containing the class of\n            each pixel.\n            * If 'instance' the target tensor is the concatenation of several\n            signals, necessary to train the Parcel-as-Points module:\n                - the centerness heatmap,\n                - the instance ids,\n                - the voronoi partitioning of the patch with regards to the parcels'\n                centers,\n                - the (height, width) size of each parcel,\n                - the semantic label of each parcel,\n                - the semantic label of each pixel.\n        folds (list, optional): List of ints specifying which of the 5 official\n            folds to load. By default (when None is specified), all folds are loaded.\n        class_mapping (dict, optional): A dictionary to define a mapping between the\n            default 18 class nomenclature and another class grouping. If not provided, \n            the default class mapping is used.\n        transform (callable, optional): A transform to apply to the loaded data \n            (images, dates, and masks). By default, no transformation is applied.\n        truncate_image (int, optional): Truncate the time dimension of the image to \n            a specified number of timesteps. If None, no truncation is performed.\n        pad_image (int, optional): Pad the time dimension of the image to a specified \n            number of timesteps. If None, no padding is applied.\n        satellites (list): Defines the satellites to use. If you are using PASTIS-R, you\n            have access to Sentinel-2 imagery and Sentinel-1 observations in Ascending\n            and Descending orbits, respectively S2, S1A, and S1D. For example, use\n            satellites=['S2', 'S1A'] for Sentinel-2 + Sentinel-1 ascending time series,\n            or satellites=['S2', 'S1A', 'S1D'] to retrieve all time series. If you are using\n            PASTIS, only S2 observations are available.\n    \"\"\"\n    if target not in [\"semantic\", \"instance\"]:\n        msg = f\"Target '{target}' not recognized. Use 'semantic', or 'instance'.\"\n        raise ValueError(msg)\n    valid_satellites = {\"S2\", \"S1A\", \"S1D\"}\n    for sat in satellites:\n        if sat not in valid_satellites:\n            msg = f\"Satellite '{sat}' not recognized. Valid options are {valid_satellites}.\"\n            raise ValueError(msg)\n\n    super().__init__()\n    self.data_root = data_root\n    self.norm = norm\n    self.reference_date = datetime(*map(int, reference_date.split(\"-\")), tzinfo=timezone.utc)\n    self.class_mapping = (\n        np.vectorize(lambda x: class_mapping[x])\n        if class_mapping is not None\n        else class_mapping\n    )\n    self.target = target\n    self.satellites = satellites\n    self.transform = transform\n    self.truncate_image = truncate_image\n    self.pad_image = pad_image\n    # loads patches metadata\n    self.meta_patch = gpd.read_file(os.path.join(data_root, \"metadata.geojson\"))\n    self.meta_patch.index = self.meta_patch[\"ID_PATCH\"].astype(int)\n    self.meta_patch.sort_index(inplace=True)\n    # stores table for each satalite date\n    self.date_tables = {s: None for s in satellites}\n    # date interval used in the PASTIS benchmark paper.\n    date_interval_begin, date_interval_end = date_interval\n    self.date_range = np.array(range(date_interval_begin, date_interval_end))\n    for s in satellites:\n        # maps patches to its observation dates\n        dates = self.meta_patch[f\"dates-{s}\"]\n        date_table = pd.DataFrame(\n            index=self.meta_patch.index, columns=self.date_range, dtype=int\n        )\n        for pid, date_seq in dates.items():\n            if type(date_seq) is str:\n                date_seq = json.loads(date_seq)  # noqa: PLW2901\n            # convert date to days since obersavation format\n            d = pd.DataFrame().from_dict(date_seq, orient=\"index\")\n            d = d[0].apply(\n                lambda x: (\n                    datetime(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:]), tzinfo=timezone.utc)\n                    - self.reference_date\n                ).days\n            )\n            date_table.loc[pid, d.values] = 1\n        date_table = date_table.fillna(0)\n        self.date_tables[s] = {\n            index: np.array(list(d.values()))\n            for index, d in date_table.to_dict(orient=\"index\").items()\n        }\n\n    # selects patches correspondig to selected folds\n    if folds is not None:\n        self.meta_patch = pd.concat(\n            [self.meta_patch[self.meta_patch[\"Fold\"] == f] for f in folds]\n        )\n\n    self.len = self.meta_patch.shape[0]\n    self.id_patches = self.meta_patch.index\n\n    # loads normalization values\n    if norm:\n        self.norm = {}\n        for s in self.satellites:\n            with open(\n                os.path.join(data_root, f\"NORM_{s}_patch.json\")\n            ) as file:\n                normvals = json.loads(file.read())\n            selected_folds = folds if folds is not None else range(1, 6)\n            means = [normvals[f\"Fold_{f}\"][\"mean\"] for f in selected_folds]\n            stds = [normvals[f\"Fold_{f}\"][\"std\"] for f in selected_folds]\n            self.norm[s] = np.stack(means).mean(axis=0), np.stack(stds).mean(axis=0)\n            self.norm[s] = (\n                self.norm[s][0],\n                self.norm[s][1],\n            )\n    else:\n        self.norm = None\n</code></pre>"},{"location":"datasets/#terratorch.datasets.sen1floods11","title":"<code>terratorch.datasets.sen1floods11</code>","text":""},{"location":"datasets/#terratorch.datasets.sen1floods11.Sen1Floods11NonGeo","title":"<code>Sen1Floods11NonGeo</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>NonGeo dataset implementation for sen1floods11.</p> Source code in <code>terratorch/datasets/sen1floods11.py</code> <pre><code>class Sen1Floods11NonGeo(NonGeoDataset):\n    \"\"\"NonGeo dataset implementation for [sen1floods11](https://github.com/cloudtostreet/Sen1Floods11).\"\"\"\n\n    all_band_names = (\n            \"COASTAL_AEROSOL\",\n            \"BLUE\",\n            \"GREEN\",\n            \"RED\",\n            \"RED_EDGE_1\",\n            \"RED_EDGE_2\",\n            \"RED_EDGE_3\",\n            \"NIR_BROAD\",\n            \"NIR_NARROW\",\n            \"WATER_VAPOR\",\n            \"CIRRUS\",\n            \"SWIR_1\",\n            \"SWIR_2\",\n    )\n    rgb_bands = (\"RED\", \"GREEN\", \"BLUE\")\n    BAND_SETS = {\"all\": all_band_names, \"rgb\": rgb_bands}\n    num_classes = 2\n    splits = {\"train\": \"train\", \"val\": \"valid\", \"test\": \"test\"}\n    data_dir = \"v1.1/data/flood_events/HandLabeled/S2Hand\"\n    label_dir = \"v1.1/data/flood_events/HandLabeled/LabelHand\"\n    split_dir = \"v1.1/splits/flood_handlabeled\"\n    metadata_file = \"v1.1/Sen1Floods11_Metadata.geojson\"\n\n    def __init__(\n        self,\n        data_root: str,\n        split: str = \"train\",\n        bands: Sequence[str] = BAND_SETS[\"all\"],\n        transform: A.Compose | None = None,\n        constant_scale: float = 0.0001,\n        no_data_replace: float | None = 0,\n        no_label_replace: int | None = -1,\n        use_metadata: bool = False,  # noqa: FBT001, FBT002\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (str): Path to the data root directory.\n            split (str): one of 'train', 'val' or 'test'.\n            bands (list[str]): Bands that should be output by the dataset. Defaults to all bands.\n            transform (A.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). Defaults to None, which applies ToTensorV2().\n            constant_scale (float): Factor to multiply image values by. Defaults to 0.0001.\n            no_data_replace (float | None): Replace nan values in input images with this value.\n                If None, does no replacement. Defaults to 0.\n            no_label_replace (int | None): Replace nan values in label with this value.\n                If none, does no replacement. Defaults to -1.\n            use_metadata (bool): whether to return metadata info (time and location).\n        \"\"\"\n        super().__init__()\n        if split not in self.splits:\n            msg = f\"Incorrect split '{split}', please choose one of {self.splits}.\"\n            raise ValueError(msg)\n        split_name = self.splits[split]\n        self.split = split\n\n        validate_bands(bands, self.all_band_names)\n        self.bands = bands\n        self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n        self.constant_scale = constant_scale\n        self.data_root = Path(data_root)\n\n        data_dir = self.data_root / self.data_dir\n        label_dir = self.data_root / self.label_dir\n\n        self.image_files = sorted(glob.glob(os.path.join(data_dir, \"*_S2Hand.tif\")))\n        self.segmentation_mask_files = sorted(glob.glob(os.path.join(label_dir, \"*_LabelHand.tif\")))\n\n        split_file = self.data_root / self.split_dir / f\"flood_{split_name}_data.txt\"\n        with open(split_file) as f:\n            split = f.readlines()\n        valid_files = {rf\"{substring.strip()}\" for substring in split}\n        self.image_files = filter_valid_files(\n            self.image_files,\n            valid_files=valid_files,\n            ignore_extensions=True,\n            allow_substring=True,\n        )\n        self.segmentation_mask_files = filter_valid_files(\n            self.segmentation_mask_files,\n            valid_files=valid_files,\n            ignore_extensions=True,\n            allow_substring=True,\n        )\n\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.use_metadata = use_metadata\n        self.metadata = None\n        if self.use_metadata:\n            self.metadata = geopandas.read_file(self.data_root / self.metadata_file)\n\n        # If no transform is given, apply only to transform to torch tensor\n        self.transform = transform if transform else default_transform\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def _get_date(self, index: int) -&gt; torch.Tensor:\n        file_name = self.image_files[index]\n        location = os.path.basename(file_name).split(\"_\")[0]\n        if self.metadata[self.metadata[\"location\"] == location].shape[0] != 1:\n            date = pd.to_datetime(\"13-10-1998\", dayfirst=True)\n        else:\n            date = pd.to_datetime(self.metadata[self.metadata[\"location\"] == location][\"s2_date\"].item())\n\n        return torch.tensor([[date.year, date.dayofyear - 1]], dtype=torch.float32)  # (n_timesteps, coords)\n\n    def _get_coords(self, image: DataArray) -&gt; torch.Tensor:\n\n        center_lat = image.y[image.y.shape[0] // 2]\n        center_lon = image.x[image.x.shape[0] // 2]\n        lat_lon = np.asarray([center_lat, center_lon])\n\n        return torch.tensor(lat_lon, dtype=torch.float32)\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        image = self._load_file(self.image_files[index], nan_replace=self.no_data_replace)\n\n        location_coords, temporal_coords = None, None\n        if self.use_metadata:\n            location_coords = self._get_coords(image)\n            temporal_coords = self._get_date(index)\n\n        # to channels last\n        image = image.to_numpy()\n        image = np.moveaxis(image, 0, -1)\n\n        # filter bands\n        image = image[..., self.band_indices]\n\n        output = {\n            \"image\": image.astype(np.float32) * self.constant_scale,\n            \"mask\": self._load_file(\n                self.segmentation_mask_files[index], nan_replace=self.no_label_replace).to_numpy()[0],\n        }\n        if self.transform:\n            output = self.transform(**output)\n        output[\"mask\"] = output[\"mask\"].long()\n\n        if self.use_metadata:\n            output[\"location_coords\"] = location_coords\n            output[\"temporal_coords\"] = temporal_coords\n\n        return output\n\n    def _load_file(self, path: Path, nan_replace: int | float | None = None) -&gt; DataArray:\n        data = rioxarray.open_rasterio(path, masked=True)\n        if nan_replace is not None:\n            data = data.fillna(nan_replace)\n        return data\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            suptitle: optional string to use as a suptitle\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n        \"\"\"\n        num_images = 4\n\n        rgb_indices = [self.bands.index(band) for band in self.rgb_bands]\n        if len(rgb_indices) != 3:\n            msg = \"Dataset doesn't contain some of the RGB bands\"\n            raise ValueError(msg)\n\n        # RGB -&gt; channels-last\n        image = sample[\"image\"][rgb_indices, ...].permute(1, 2, 0).numpy()\n        mask = sample[\"mask\"].numpy()\n\n        image = clip_image(image)\n\n        if \"prediction\" in sample:\n            prediction = sample[\"prediction\"]\n            num_images += 1\n        else:\n            prediction = None\n\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 5), layout=\"compressed\")\n\n        ax[0].axis(\"off\")\n\n        norm = mpl.colors.Normalize(vmin=0, vmax=self.num_classes - 1)\n        ax[1].axis(\"off\")\n        ax[1].title.set_text(\"Image\")\n        ax[1].imshow(image)\n\n        ax[2].axis(\"off\")\n        ax[2].title.set_text(\"Ground Truth Mask\")\n        ax[2].imshow(mask, cmap=\"jet\", norm=norm)\n\n        ax[3].axis(\"off\")\n        ax[3].title.set_text(\"GT Mask on Image\")\n        ax[3].imshow(image)\n        ax[3].imshow(mask, cmap=\"jet\", alpha=0.3, norm=norm)\n\n        if \"prediction\" in sample:\n            ax[4].title.set_text(\"Predicted Mask\")\n            ax[4].imshow(prediction, cmap=\"jet\", norm=norm)\n\n        cmap = plt.get_cmap(\"jet\")\n        legend_data = [[i, cmap(norm(i)), str(i)] for i in range(self.num_classes)]\n        handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n        labels = [n for k, c, n in legend_data]\n        ax[0].legend(handles, labels, loc=\"center\")\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n\n        return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.sen1floods11.Sen1Floods11NonGeo.__init__","title":"<code>__init__(data_root, split='train', bands=BAND_SETS['all'], transform=None, constant_scale=0.0001, no_data_replace=0, no_label_replace=-1, use_metadata=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Path to the data root directory.</p> required <code>split</code> <code>str</code> <p>one of 'train', 'val' or 'test'.</p> <code>'train'</code> <code>bands</code> <code>list[str]</code> <p>Bands that should be output by the dataset. Defaults to all bands.</p> <code>BAND_SETS['all']</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). Defaults to None, which applies ToTensorV2().</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 0.0001.</p> <code>0.0001</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If None, does no replacement. Defaults to 0.</p> <code>0</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>-1</code> <code>use_metadata</code> <code>bool</code> <p>whether to return metadata info (time and location).</p> <code>False</code> Source code in <code>terratorch/datasets/sen1floods11.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    split: str = \"train\",\n    bands: Sequence[str] = BAND_SETS[\"all\"],\n    transform: A.Compose | None = None,\n    constant_scale: float = 0.0001,\n    no_data_replace: float | None = 0,\n    no_label_replace: int | None = -1,\n    use_metadata: bool = False,  # noqa: FBT001, FBT002\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (str): Path to the data root directory.\n        split (str): one of 'train', 'val' or 'test'.\n        bands (list[str]): Bands that should be output by the dataset. Defaults to all bands.\n        transform (A.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). Defaults to None, which applies ToTensorV2().\n        constant_scale (float): Factor to multiply image values by. Defaults to 0.0001.\n        no_data_replace (float | None): Replace nan values in input images with this value.\n            If None, does no replacement. Defaults to 0.\n        no_label_replace (int | None): Replace nan values in label with this value.\n            If none, does no replacement. Defaults to -1.\n        use_metadata (bool): whether to return metadata info (time and location).\n    \"\"\"\n    super().__init__()\n    if split not in self.splits:\n        msg = f\"Incorrect split '{split}', please choose one of {self.splits}.\"\n        raise ValueError(msg)\n    split_name = self.splits[split]\n    self.split = split\n\n    validate_bands(bands, self.all_band_names)\n    self.bands = bands\n    self.band_indices = np.asarray([self.all_band_names.index(b) for b in bands])\n    self.constant_scale = constant_scale\n    self.data_root = Path(data_root)\n\n    data_dir = self.data_root / self.data_dir\n    label_dir = self.data_root / self.label_dir\n\n    self.image_files = sorted(glob.glob(os.path.join(data_dir, \"*_S2Hand.tif\")))\n    self.segmentation_mask_files = sorted(glob.glob(os.path.join(label_dir, \"*_LabelHand.tif\")))\n\n    split_file = self.data_root / self.split_dir / f\"flood_{split_name}_data.txt\"\n    with open(split_file) as f:\n        split = f.readlines()\n    valid_files = {rf\"{substring.strip()}\" for substring in split}\n    self.image_files = filter_valid_files(\n        self.image_files,\n        valid_files=valid_files,\n        ignore_extensions=True,\n        allow_substring=True,\n    )\n    self.segmentation_mask_files = filter_valid_files(\n        self.segmentation_mask_files,\n        valid_files=valid_files,\n        ignore_extensions=True,\n        allow_substring=True,\n    )\n\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.use_metadata = use_metadata\n    self.metadata = None\n    if self.use_metadata:\n        self.metadata = geopandas.read_file(self.data_root / self.metadata_file)\n\n    # If no transform is given, apply only to transform to torch tensor\n    self.transform = transform if transform else default_transform\n</code></pre>"},{"location":"datasets/#terratorch.datasets.sen1floods11.Sen1Floods11NonGeo.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> Source code in <code>terratorch/datasets/sen1floods11.py</code> <pre><code>def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        suptitle: optional string to use as a suptitle\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n    \"\"\"\n    num_images = 4\n\n    rgb_indices = [self.bands.index(band) for band in self.rgb_bands]\n    if len(rgb_indices) != 3:\n        msg = \"Dataset doesn't contain some of the RGB bands\"\n        raise ValueError(msg)\n\n    # RGB -&gt; channels-last\n    image = sample[\"image\"][rgb_indices, ...].permute(1, 2, 0).numpy()\n    mask = sample[\"mask\"].numpy()\n\n    image = clip_image(image)\n\n    if \"prediction\" in sample:\n        prediction = sample[\"prediction\"]\n        num_images += 1\n    else:\n        prediction = None\n\n    fig, ax = plt.subplots(1, num_images, figsize=(12, 5), layout=\"compressed\")\n\n    ax[0].axis(\"off\")\n\n    norm = mpl.colors.Normalize(vmin=0, vmax=self.num_classes - 1)\n    ax[1].axis(\"off\")\n    ax[1].title.set_text(\"Image\")\n    ax[1].imshow(image)\n\n    ax[2].axis(\"off\")\n    ax[2].title.set_text(\"Ground Truth Mask\")\n    ax[2].imshow(mask, cmap=\"jet\", norm=norm)\n\n    ax[3].axis(\"off\")\n    ax[3].title.set_text(\"GT Mask on Image\")\n    ax[3].imshow(image)\n    ax[3].imshow(mask, cmap=\"jet\", alpha=0.3, norm=norm)\n\n    if \"prediction\" in sample:\n        ax[4].title.set_text(\"Predicted Mask\")\n        ax[4].imshow(prediction, cmap=\"jet\", norm=norm)\n\n    cmap = plt.get_cmap(\"jet\")\n    legend_data = [[i, cmap(norm(i)), str(i)] for i in range(self.num_classes)]\n    handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n    labels = [n for k, c, n in legend_data]\n    ax[0].legend(handles, labels, loc=\"center\")\n    if suptitle is not None:\n        plt.suptitle(suptitle)\n\n    return fig\n</code></pre>"},{"location":"datasets/#terratorch.datasets.sen4agrinet","title":"<code>terratorch.datasets.sen4agrinet</code>","text":""},{"location":"datasets/#terratorch.datasets.sen4agrinet.Sen4AgriNet","title":"<code>Sen4AgriNet</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> Source code in <code>terratorch/datasets/sen4agrinet.py</code> <pre><code>class Sen4AgriNet(NonGeoDataset):\n    def __init__(\n        self,\n        data_root: str,\n        bands: list[str] | None = None,\n        scenario: str = \"random\",\n        split: str = \"train\",\n        transform: A.Compose = None,\n        truncate_image: int | None = 4,\n        pad_image: int | None = 4,\n        spatial_interpolate_and_stack_temporally: bool = True,  # noqa: FBT001, FBT002\n        seed: int = 42,\n    ):\n        \"\"\"\n        Pytorch Dataset class to load samples from the [Sen4AgriNet](https://github.com/Orion-AI-Lab/S4A) dataset, supporting\n        multiple scenarios for splitting the data.\n\n        Args:\n            data_root (str): Root directory of the dataset.\n            bands (list of str, optional): List of band names to load. Defaults to all available bands.\n            scenario (str): Defines the splitting scenario to use. Options are:\n                - 'random': Random split of the data.\n                - 'spatial': Split by geographical regions (Catalonia and France).\n                - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020).\n            split (str): Specifies the dataset split. Options are 'train', 'val', or 'test'.\n            transform (albumentations.Compose, optional): Albumentations transformations to apply to the data.\n            truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image.\n                If None, no truncation is applied. Default is 4.\n            pad_image (int, optional): Number of timesteps to pad the time dimension of the image.\n                If None, no padding is applied. Default is 4.\n            spatial_interpolate_and_stack_temporally (bool): Whether to interpolate bands and concatenate them over time\n            seed (int): Random seed used for data splitting.\n        \"\"\"\n        self.data_root = Path(data_root) / \"data\"\n        self.transform = transform if transform else lambda **batch: to_tensor(batch)\n        self.scenario = scenario\n        self.seed = seed\n        self.truncate_image = truncate_image\n        self.pad_image = pad_image\n        self.spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally\n\n        if bands is None:\n            bands = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B09\", \"B10\", \"B11\", \"B12\", \"B8A\"]\n        self.bands = bands\n\n        self.image_files = list(self.data_root.glob(\"**/*.nc\"))\n\n        self.train_files, self.val_files, self.test_files = self.split_data()\n\n        if split == \"train\":\n            self.image_files = self.train_files\n        elif split == \"val\":\n            self.image_files = self.val_files\n        elif split == \"test\":\n            self.image_files = self.test_files\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def split_data(self):\n        random.seed(self.seed)\n\n        if self.scenario == \"random\":\n            random.shuffle(self.image_files)\n            total_files = len(self.image_files)\n            train_split = int(0.6 * total_files)\n            val_split = int(0.8 * total_files)\n\n            train_files = self.image_files[:train_split]\n            val_files = self.image_files[train_split:val_split]\n            test_files = self.image_files[val_split:]\n\n        elif self.scenario == \"spatial\":\n            catalonia_files = [f for f in self.image_files if any(tile in f.stem for tile in CAT_TILES)]\n            france_files = [f for f in self.image_files if any(tile in f.stem for tile in FR_TILES)]\n\n            val_split_cat = int(0.2 * len(catalonia_files))\n            train_files = catalonia_files[val_split_cat:]\n            val_files = catalonia_files[:val_split_cat]\n            test_files = france_files\n\n        elif self.scenario == \"spatio-temporal\":\n            france_files = [f for f in self.image_files if any(tile in f.stem for tile in FR_TILES)]\n            catalonia_files = [f for f in self.image_files if any(tile in f.stem for tile in CAT_TILES)]\n\n            france_2019_files = [f for f in france_files if \"2019\" in f.stem]\n            catalonia_2020_files = [f for f in catalonia_files if \"2020\" in f.stem]\n\n            val_split_france_2019 = int(0.2 * len(france_2019_files))\n            train_files = france_2019_files[val_split_france_2019:]\n            val_files = france_2019_files[:val_split_france_2019]\n            test_files = catalonia_2020_files\n\n        return train_files, val_files, test_files\n\n\n    def __getitem__(self, index: int):\n        patch_file = self.image_files[index]\n\n        with h5py.File(patch_file, \"r\") as patch_data:\n            output = {}\n            images_over_time = []\n            for band in self.bands:\n                band_group = patch_data[band]\n                band_data = band_group[f\"{band}\"][:]\n                time_vector = band_group[\"time\"][:]\n\n                sorted_indices = np.argsort(time_vector)\n                band_data = band_data[sorted_indices].astype(np.float32)\n\n                if self.truncate_image:\n                    band_data = band_data[-self.truncate_image:]\n                if self.pad_image:\n                    band_data = pad_numpy(band_data, self.pad_image)\n\n                if self.spatial_interpolate_and_stack_temporally:\n                    band_data = torch.from_numpy(band_data)\n                    band_data = band_data.clone().detach()\n\n                    interpolated = F.interpolate(\n                        band_data.unsqueeze(0), size=MAX_TEMPORAL_IMAGE_SIZE, mode=\"bilinear\", align_corners=False\n                    ).squeeze(0)\n                    images_over_time.append(interpolated)\n                else:\n                    output[band] = band_data\n\n            if self.spatial_interpolate_and_stack_temporally:\n                images = torch.stack(images_over_time, dim=0).numpy()\n                output[\"image\"] = images\n\n            labels = patch_data[\"labels\"][\"labels\"][:].astype(int)\n            parcels = patch_data[\"parcels\"][\"parcels\"][:].astype(int)\n\n        output[\"mask\"] = labels\n\n        image_shape = output[\"image\"].shape[-2:]\n        mask_shape = output[\"mask\"].shape\n\n        if image_shape != mask_shape:\n            diff_h = mask_shape[0] - image_shape[0]\n            diff_w = mask_shape[1] - image_shape[1]\n\n            output[\"image\"] = np.pad(output[\"image\"],\n                                [(0, 0), (0, 0),\n                                    (diff_h // 2, diff_h - diff_h // 2),\n                                    (diff_w // 2, diff_w - diff_w // 2)],\n                                mode=\"constant\", constant_values=0)\n\n        linear_encoder = {val: i + 1 for i, val in enumerate(sorted(SELECTED_CLASSES))}\n        linear_encoder[0] = 0\n\n        output[\"image\"] = output[\"image\"].transpose(0, 2, 3, 1)\n        output[\"mask\"] = self.map_mask_to_discrete_classes(output[\"mask\"], linear_encoder)\n\n        if self.transform:\n            output = self.transform(**output)\n\n        output[\"parcels\"] = parcels\n\n        return output\n\n    def plot(self, sample, suptitle=None):\n        rgb_bands = [\"B04\", \"B03\", \"B02\"]\n\n        if not all(band in sample for band in rgb_bands):\n            warnings.warn(\"No RGB image.\")  # noqa: B028\n            return None\n\n        rgb_images = []\n        for t in range(sample[\"B04\"].shape[0]):\n            rgb_image = torch.stack([sample[band][t] for band in rgb_bands])\n\n            # Normalization\n            rgb_min = rgb_image.min(dim=1, keepdim=True).values.min(dim=2, keepdim=True).values\n            rgb_max = rgb_image.max(dim=1, keepdim=True).values.max(dim=2, keepdim=True).values\n            denom = rgb_max - rgb_min\n            denom[denom == 0] = 1\n            rgb_image = (rgb_image - rgb_min) / denom\n\n            rgb_image = rgb_image.permute(1, 2, 0).numpy()\n            rgb_images.append(np.clip(rgb_image, 0, 1))\n\n        dates = torch.arange(sample[\"B04\"].shape[0])\n\n        return self._plot_sample(rgb_images, dates, sample.get(\"labels\"), suptitle=suptitle)\n\n    def _plot_sample(self, images, dates, labels=None, suptitle=None):\n        num_images = len(images)\n        cols = 5\n        rows = (num_images + cols - 1) // cols\n\n        fig, ax = plt.subplots(rows, cols, figsize=(20, 4 * rows))\n\n        for i, image in enumerate(images):\n            ax[i // cols, i % cols].imshow(image)\n            ax[i // cols, i % cols].set_title(f\"T{i+1} - Day {dates[i].item()}\")\n            ax[i // cols, i % cols].axis(\"off\")\n\n        if labels is not None:\n            if rows * cols &gt; num_images:\n                target_ax = ax[(num_images) // cols, (num_images) % cols]\n            else:\n                fig.add_subplot(rows + 1, 1, 1)\n                target_ax = fig.gca()\n\n            target_ax.imshow(labels.numpy(), cmap=\"tab20\")\n            target_ax.set_title(\"Labels\")\n            target_ax.axis(\"off\")\n\n        for k in range(num_images, rows * cols):\n            ax[k // cols, k % cols].axis(\"off\")\n\n        if suptitle:\n            plt.suptitle(suptitle)\n\n        plt.tight_layout()\n        plt.show()\n\n    def map_mask_to_discrete_classes(self, mask, encoder):\n        map_func = np.vectorize(lambda x: encoder.get(x, 0))\n        return map_func(mask)\n</code></pre>"},{"location":"datasets/#terratorch.datasets.sen4agrinet.Sen4AgriNet.__init__","title":"<code>__init__(data_root, bands=None, scenario='random', split='train', transform=None, truncate_image=4, pad_image=4, spatial_interpolate_and_stack_temporally=True, seed=42)</code>","text":"<p>Pytorch Dataset class to load samples from the Sen4AgriNet dataset, supporting multiple scenarios for splitting the data.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>str</code> <p>Root directory of the dataset.</p> required <code>bands</code> <code>list of str</code> <p>List of band names to load. Defaults to all available bands.</p> <code>None</code> <code>scenario</code> <code>str</code> <p>Defines the splitting scenario to use. Options are: - 'random': Random split of the data. - 'spatial': Split by geographical regions (Catalonia and France). - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020).</p> <code>'random'</code> <code>split</code> <code>str</code> <p>Specifies the dataset split. Options are 'train', 'val', or 'test'.</p> <code>'train'</code> <code>transform</code> <code>Compose</code> <p>Albumentations transformations to apply to the data.</p> <code>None</code> <code>truncate_image</code> <code>int</code> <p>Number of timesteps to truncate the time dimension of the image. If None, no truncation is applied. Default is 4.</p> <code>4</code> <code>pad_image</code> <code>int</code> <p>Number of timesteps to pad the time dimension of the image. If None, no padding is applied. Default is 4.</p> <code>4</code> <code>spatial_interpolate_and_stack_temporally</code> <code>bool</code> <p>Whether to interpolate bands and concatenate them over time</p> <code>True</code> <code>seed</code> <code>int</code> <p>Random seed used for data splitting.</p> <code>42</code> Source code in <code>terratorch/datasets/sen4agrinet.py</code> <pre><code>def __init__(\n    self,\n    data_root: str,\n    bands: list[str] | None = None,\n    scenario: str = \"random\",\n    split: str = \"train\",\n    transform: A.Compose = None,\n    truncate_image: int | None = 4,\n    pad_image: int | None = 4,\n    spatial_interpolate_and_stack_temporally: bool = True,  # noqa: FBT001, FBT002\n    seed: int = 42,\n):\n    \"\"\"\n    Pytorch Dataset class to load samples from the [Sen4AgriNet](https://github.com/Orion-AI-Lab/S4A) dataset, supporting\n    multiple scenarios for splitting the data.\n\n    Args:\n        data_root (str): Root directory of the dataset.\n        bands (list of str, optional): List of band names to load. Defaults to all available bands.\n        scenario (str): Defines the splitting scenario to use. Options are:\n            - 'random': Random split of the data.\n            - 'spatial': Split by geographical regions (Catalonia and France).\n            - 'spatio-temporal': Split by region and year (France 2019 and Catalonia 2020).\n        split (str): Specifies the dataset split. Options are 'train', 'val', or 'test'.\n        transform (albumentations.Compose, optional): Albumentations transformations to apply to the data.\n        truncate_image (int, optional): Number of timesteps to truncate the time dimension of the image.\n            If None, no truncation is applied. Default is 4.\n        pad_image (int, optional): Number of timesteps to pad the time dimension of the image.\n            If None, no padding is applied. Default is 4.\n        spatial_interpolate_and_stack_temporally (bool): Whether to interpolate bands and concatenate them over time\n        seed (int): Random seed used for data splitting.\n    \"\"\"\n    self.data_root = Path(data_root) / \"data\"\n    self.transform = transform if transform else lambda **batch: to_tensor(batch)\n    self.scenario = scenario\n    self.seed = seed\n    self.truncate_image = truncate_image\n    self.pad_image = pad_image\n    self.spatial_interpolate_and_stack_temporally = spatial_interpolate_and_stack_temporally\n\n    if bands is None:\n        bands = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B09\", \"B10\", \"B11\", \"B12\", \"B8A\"]\n    self.bands = bands\n\n    self.image_files = list(self.data_root.glob(\"**/*.nc\"))\n\n    self.train_files, self.val_files, self.test_files = self.split_data()\n\n    if split == \"train\":\n        self.image_files = self.train_files\n    elif split == \"val\":\n        self.image_files = self.val_files\n    elif split == \"test\":\n        self.image_files = self.test_files\n</code></pre>"},{"location":"datasets/#terratorch.datasets.sen4map","title":"<code>terratorch.datasets.sen4map</code>","text":""},{"location":"datasets/#terratorch.datasets.sen4map.Sen4MapDatasetMonthlyComposites","title":"<code>Sen4MapDatasetMonthlyComposites</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Sen4Map Dataset for Monthly Composites.</p> <p>Dataset intended for land-cover and crop classification tasks based on monthly composites derived from multi-temporal satellite data stored in HDF5 files.</p> <p>Dataset Format:</p> <ul> <li>HDF5 files containing multi-temporal acquisitions with spectral bands (e.g., B2, B3, \u2026, B12)</li> <li>Composite images computed as the median across available acquisitions for each month.</li> <li>Classification labels provided via HDF5 attributes (e.g., 'lc1') with mappings defined for:<ul> <li>Land-cover: using <code>land_cover_classification_map</code></li> <li>Crops: using <code>crop_classification_map</code></li> </ul> </li> </ul> <p>Dataset Features:</p> <ul> <li>Supports two classification tasks: \"land-cover\" (default) and \"crops\".</li> <li>Pre-processing options include center cropping, reverse tiling, and resizing.</li> <li>Option to save the keys HDF5 for later filtering.</li> <li>Input channel selection via a mapping between available bands and input bands.</li> </ul> Source code in <code>terratorch/datasets/sen4map.py</code> <pre><code>class Sen4MapDatasetMonthlyComposites(Dataset):\n    \"\"\"[Sen4Map](https://gitlab.jsc.fz-juelich.de/sdlrs/sen4map-benchmark-dataset) Dataset for Monthly Composites.\n\n    Dataset intended for land-cover and crop classification tasks based on monthly composites\n    derived from multi-temporal satellite data stored in HDF5 files.\n\n    Dataset Format:\n\n    * HDF5 files containing multi-temporal acquisitions with spectral bands (e.g., B2, B3, \u2026, B12)\n    * Composite images computed as the median across available acquisitions for each month.\n    * Classification labels provided via HDF5 attributes (e.g., 'lc1') with mappings defined for:\n        - Land-cover: using `land_cover_classification_map`\n        - Crops: using `crop_classification_map`\n\n    Dataset Features:\n\n    * Supports two classification tasks: \"land-cover\" (default) and \"crops\".\n    * Pre-processing options include center cropping, reverse tiling, and resizing.\n    * Option to save the keys HDF5 for later filtering.\n    * Input channel selection via a mapping between available bands and input bands.\n\n\n    \"\"\"\n    land_cover_classification_map={'A10':0, 'A11':0, 'A12':0, 'A13':0, \n    'A20':0, 'A21':0, 'A30':0, \n    'A22':1, 'F10':1, 'F20':1, \n    'F30':1, 'F40':1,\n    'E10':2, 'E20':2, 'E30':2, 'B50':2, 'B51':2, 'B52':2,\n    'B53':2, 'B54':2, 'B55':2,\n    'B10':3, 'B11':3, 'B12':3, 'B13':3, 'B14':3, 'B15':3,\n    'B16':3, 'B17':3, 'B18':3, 'B19':3, 'B10':3, 'B20':3, \n    'B21':3, 'B22':3, 'B23':3, 'B30':3, 'B31':3, 'B32':3,\n    'B33':3, 'B34':3, 'B35':3, 'B30':3, 'B36':3, 'B37':3,\n    'B40':3, 'B41':3, 'B42':3, 'B43':3, 'B44':3, 'B45':3,\n    'B70':3, 'B71':3, 'B72':3, 'B73':3, 'B74':3, 'B75':3,\n    'B76':3, 'B77':3, 'B80':3, 'B81':3, 'B82':3, 'B83':3,\n    'B84':3, \n    'BX1':3, 'BX2':3,\n    'C10':4, 'C20':5, 'C21':5, 'C22':5,\n    'C23':5, 'C30':5, 'C31':5, 'C32':5,\n    'C33':5, \n    'CXX1':5, 'CXX2':5, 'CXX3':5, 'CXX4':5, 'CXX5':5,\n    'CXX5':5, 'CXX6':5, 'CXX7':5, 'CXX8':5, 'CXX9':5,\n    'CXXA':5, 'CXXB':5, 'CXXC':5, 'CXXD':5, 'CXXE':5,\n    'D10':6, 'D20':6, 'D10':6,\n    'G10':7, 'G11':7, 'G12':7, 'G20':7, 'G21':7, 'G22':7, 'G30':7, \n    'G40':7,\n    'G50':7,\n    'H10':8, 'H11':8, 'H12':8, 'H11':8,'H20':8, 'H21':8,\n    'H22':8, 'H23':8, '': 9}\n    #  This dictionary maps the LUCAS classes to crop classes.\n    crop_classification_map = {\n        \"B11\":0, \"B12\":0, \"B13\":0, \"B14\":0, \"B15\":0, \"B16\":0, \"B17\":0, \"B18\":0, \"B19\":0,  # Cereals\n        \"B21\":1, \"B22\":1, \"B23\":1,  # Root Crops\n        \"B31\":2, \"B32\":2, \"B33\":2, \"B34\":2, \"B35\":2, \"B36\":2, \"B37\":2,  # Nonpermanent Industrial Crops\n        \"B41\":3, \"B42\":3, \"B43\":3, \"B44\":3, \"B45\":3,  # Dry Pulses, Vegetables and Flowers\n        \"B51\":4, \"B52\":4, \"B53\":4, \"B54\":4,  # Fodder Crops\n        \"F10\":5, \"F20\":5, \"F30\":5, \"F40\":5,  # Bareland\n        \"B71\":6, \"B72\":6, \"B73\":6, \"B74\":6, \"B75\":6, \"B76\":6, \"B77\":6, \n        \"B81\":6, \"B82\":6, \"B83\":6, \"B84\":6, \"C10\":6, \"C21\":6, \"C22\":6, \"C23\":6, \"C31\":6, \"C32\":6, \"C33\":6, \"D10\":6, \"D20\":6,  # Woodland and Shrubland\n        \"B55\":7, \"E10\":7, \"E20\":7, \"E30\":7,  # Grassland\n    }\n\n    def __init__(\n            self,\n            h5py_file_object:h5py.File,\n            h5data_keys = None,\n            crop_size:None|int = None,\n            dataset_bands:list[HLSBands|int]|None = None,\n            input_bands:list[HLSBands|int]|None = None,\n            resize = False,\n            resize_to = [224, 224],\n            resize_interpolation = InterpolationMode.BILINEAR,\n            resize_antialiasing = True,\n            reverse_tile = False,\n            reverse_tile_size = 3,\n            save_keys_path = None,\n            classification_map = \"land-cover\"\n            ):\n        \"\"\"Initialize a new instance of Sen4MapDatasetMonthlyComposites.\n\n        This dataset loads data from an HDF5 file object containing multi-temporal satellite data and computes\n        monthly composite images by aggregating acquisitions (via median).\n\n        Args:\n            h5py_file_object: An open h5py.File object containing the dataset.\n            h5data_keys: Optional list of keys to select a subset of data samples from the HDF5 file.\n                If None, all keys are used.\n            crop_size: Optional integer specifying the square crop size for the output image.\n            dataset_bands: Optional list of bands available in the dataset.\n            input_bands: Optional list of bands to be used as input channels.\n                Must be provided along with `dataset_bands`.\n            resize: Boolean flag indicating whether the image should be resized. Default is False.\n            resize_to: Target dimensions [height, width] for resizing. Default is [224, 224].\n            resize_interpolation: Interpolation mode used for resizing. Default is InterpolationMode.BILINEAR.\n            resize_antialiasing: Boolean flag to apply antialiasing during resizing. Default is True.\n            reverse_tile: Boolean flag indicating whether to apply reverse tiling to the image. Default is False.\n            reverse_tile_size: Kernel size for the reverse tiling operation. Must be an odd number &gt;= 3. Default is 3.\n            save_keys_path: Optional file path to save the list of dataset keys.\n            classification_map: String specifying the classification mapping to use (\"land-cover\" or \"crops\").\n                Default is \"land-cover\".\n\n        Raises:\n            ValueError: If `input_bands` is provided without specifying `dataset_bands`.\n            ValueError: If an invalid `classification_map` is provided.\n        \"\"\"\n        self.h5data = h5py_file_object\n        if h5data_keys is None:\n            if classification_map == \"crops\": print(f\"Crop classification task chosen but no keys supplied. Will fail unless dataset hdf5 files have been filtered. Either filter dataset files or create a filtered set of keys.\")\n            self.h5data_keys = list(self.h5data.keys())\n            if save_keys_path is not None:\n                with open(save_keys_path, \"wb\") as file:\n                    pickle.dump(self.h5data_keys, file)\n        else:\n            self.h5data_keys = h5data_keys\n        self.crop_size = crop_size\n        if input_bands and not dataset_bands:\n            raise ValueError(f\"input_bands was provided without specifying the dataset_bands\")\n        # self.dataset_bands = dataset_bands\n        # self.input_bands = input_bands\n        if input_bands and dataset_bands:\n            self.input_channels = [dataset_bands.index(band_ind) for band_ind in input_bands if band_ind in dataset_bands]\n        else: self.input_channels = None\n\n        classification_maps = {\"land-cover\": Sen4MapDatasetMonthlyComposites.land_cover_classification_map,\n                               \"crops\": Sen4MapDatasetMonthlyComposites.crop_classification_map}\n        if classification_map not in classification_maps.keys():\n            raise ValueError(f\"Provided classification_map of: {classification_map}, is not from the list of valid ones: {classification_maps}\")\n        self.classification_map = classification_maps[classification_map]\n\n        self.resize = resize\n        self.resize_to = resize_to\n        self.resize_interpolation = resize_interpolation\n        self.resize_antialiasing = resize_antialiasing\n\n        self.reverse_tile = reverse_tile\n        self.reverse_tile_size = reverse_tile_size\n\n    def __getitem__(self, index):\n        # we can call dataset with an index, eg. dataset[0]\n        im = self.h5data[self.h5data_keys[index]]\n        Image, Label = self.get_data(im)\n        Image = self.min_max_normalize(Image, [67.0, 122.0, 93.27, 158.5, 160.77, 174.27, 162.27, 149.0, 84.5, 66.27 ],\n                                    [2089.0, 2598.45, 3214.5, 3620.45, 4033.61, 4613.0, 4825.45, 4945.72, 5140.84, 4414.45])\n\n        Image = Image.clip(0,1)\n        Label = torch.LongTensor(Label)\n        if self.input_channels:\n            Image = Image[self.input_channels, ...]\n\n        return {\"image\":Image, \"label\":Label}\n\n    def __len__(self):\n        return len(self.h5data_keys)\n\n    def get_data(self, im):\n        mask = im['SCL'] &lt; 9\n\n        B2= np.where(mask==1, im['B2'], 0)\n        B3= np.where(mask==1, im['B3'], 0)\n        B4= np.where(mask==1, im['B4'], 0)\n        B5= np.where(mask==1, im['B5'], 0)\n        B6= np.where(mask==1, im['B6'], 0)\n        B7= np.where(mask==1, im['B7'], 0)\n        B8= np.where(mask==1, im['B8'], 0)\n        B8A= np.where(mask==1, im['B8A'], 0)\n        B11= np.where(mask==1, im['B11'], 0)\n        B12= np.where(mask==1, im['B12'], 0)\n        Image = np.stack((B2,B3,B4,B5,B6,B7,B8,B8A,B11,B12), axis=0, dtype=\"float32\")\n        Image = np.moveaxis(Image, [0],[1])\n        Image = torch.from_numpy(Image)\n\n        # Composites:\n        n1= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201801' in s]\n        n2= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201802' in s]\n        n3= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201803' in s]\n        n4= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201804' in s]\n        n5= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201805' in s]\n        n6= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201806' in s]\n        n7= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201807' in s]\n        n8= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201808' in s]\n        n9= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201809' in s]\n        n10= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201810' in s]\n        n11= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201811' in s]\n        n12= [i for i, s in enumerate(im.attrs['Image_ID'].tolist()) if '201812' in s]\n\n\n        Jan= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n1 else n1\n        Feb= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n2 else n2\n        Mar= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n3 else n3\n        Apr= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n4 else n4\n        May= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n5 else n5\n        Jun= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n6 else n6\n        Jul= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n7 else n7\n        Aug= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n8 else n8\n        Sep= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n9 else n9\n        Oct= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n10 else n10\n        Nov= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n11 else n11\n        Dec= n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9 + n10 + n11 + n12 if not n12 else n12\n\n        month_indices = [Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec]\n\n        month_medians = [torch.stack([Image[month_indices[i][j]] for j in range(len(month_indices[i]))]).median(dim=0).values for i in range(12)]\n\n\n        Image = torch.stack(month_medians, dim=0)\n        Image = torch.moveaxis(Image, 0, 1)\n\n        if self.crop_size: Image = self.crop_center(Image, self.crop_size, self.crop_size)\n        if self.reverse_tile:\n            Image = self.reverse_tiling_pytorch(Image, kernel_size=self.reverse_tile_size)\n        if self.resize:\n            Image = resize(Image, size=self.resize_to, interpolation=self.resize_interpolation, antialias=self.resize_antialiasing)\n\n        Label = im.attrs['lc1']\n        Label = self.classification_map[Label]\n        Label = np.array(Label)\n        Label = Label.astype('float32')\n\n        return Image, Label\n\n    def crop_center(self, img_b:torch.Tensor, cropx, cropy) -&gt; torch.Tensor:\n        c, t, y, x = img_b.shape\n        startx = x//2-(cropx//2)\n        starty = y//2-(cropy//2)    \n        return img_b[0:c, 0:t, starty:starty+cropy, startx:startx+cropx]\n\n\n    def reverse_tiling_pytorch(self, img_tensor: torch.Tensor, kernel_size: int=3):\n        \"\"\"\n        Upscales an image where every pixel is expanded into `kernel_size`*`kernel_size` pixels.\n        Used to test whether the benefit of resizing images to the pre-trained size comes from the bilnearly interpolated pixels,\n        or if the same would be realized with no interpolated pixels.\n        \"\"\"\n        assert kernel_size % 2 == 1\n        assert kernel_size &gt;= 3\n        padding = (kernel_size - 1) // 2\n        # img_tensor shape: (batch_size, channels, H, W)\n        batch_size, channels, H, W = img_tensor.shape\n        # Unfold: Extract 3x3 patches with padding of 1 to cover borders\n        img_tensor = F.pad(img_tensor, pad=(padding,padding,padding,padding), mode=\"replicate\")\n        patches = F.unfold(img_tensor, kernel_size=kernel_size, padding=0)  # Shape: (batch_size, channels*9, H*W)\n        # Reshape to organize the 9 values from each 3x3 neighborhood\n        patches = patches.view(batch_size, channels, kernel_size*kernel_size, H, W)  # Shape: (batch_size, channels, 9, H, W)\n        # Rearrange the patches into (batch_size, channels, 3, 3, H, W)\n        patches = patches.view(batch_size, channels, kernel_size, kernel_size, H, W)\n        # Permute to have the spatial dimensions first and unfold them\n        patches = patches.permute(0, 1, 4, 2, 5, 3)  # Shape: (batch_size, channels, H, 3, W, 3)\n        # Reshape to get the final expanded image of shape (batch_size, channels, H*3, W*3)\n        expanded_img = patches.reshape(batch_size, channels, H * kernel_size, W * kernel_size)\n        return expanded_img\n\n    def min_max_normalize(self, tensor:torch.Tensor, q_low:list[float], q_hi:list[float]) -&gt; torch.Tensor:\n        dtype = tensor.dtype\n        q_low = torch.as_tensor(q_low, dtype=dtype, device=tensor.device)\n        q_hi = torch.as_tensor(q_hi, dtype=dtype, device=tensor.device)\n        x = torch.tensor(-12.0)\n        y = torch.exp(x)\n        tensor.sub_(q_low[:, None, None, None]).div_((q_hi[:, None, None, None].sub_(q_low[:, None, None, None])).add(y))\n        return tensor\n</code></pre>"},{"location":"datasets/#terratorch.datasets.sen4map.Sen4MapDatasetMonthlyComposites.__init__","title":"<code>__init__(h5py_file_object, h5data_keys=None, crop_size=None, dataset_bands=None, input_bands=None, resize=False, resize_to=[224, 224], resize_interpolation=InterpolationMode.BILINEAR, resize_antialiasing=True, reverse_tile=False, reverse_tile_size=3, save_keys_path=None, classification_map='land-cover')</code>","text":"<p>Initialize a new instance of Sen4MapDatasetMonthlyComposites.</p> <p>This dataset loads data from an HDF5 file object containing multi-temporal satellite data and computes monthly composite images by aggregating acquisitions (via median).</p> <p>Parameters:</p> Name Type Description Default <code>h5py_file_object</code> <code>File</code> <p>An open h5py.File object containing the dataset.</p> required <code>h5data_keys</code> <p>Optional list of keys to select a subset of data samples from the HDF5 file. If None, all keys are used.</p> <code>None</code> <code>crop_size</code> <code>None | int</code> <p>Optional integer specifying the square crop size for the output image.</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Optional list of bands available in the dataset.</p> <code>None</code> <code>input_bands</code> <code>list[HLSBands | int] | None</code> <p>Optional list of bands to be used as input channels. Must be provided along with <code>dataset_bands</code>.</p> <code>None</code> <code>resize</code> <p>Boolean flag indicating whether the image should be resized. Default is False.</p> <code>False</code> <code>resize_to</code> <p>Target dimensions [height, width] for resizing. Default is [224, 224].</p> <code>[224, 224]</code> <code>resize_interpolation</code> <p>Interpolation mode used for resizing. Default is InterpolationMode.BILINEAR.</p> <code>BILINEAR</code> <code>resize_antialiasing</code> <p>Boolean flag to apply antialiasing during resizing. Default is True.</p> <code>True</code> <code>reverse_tile</code> <p>Boolean flag indicating whether to apply reverse tiling to the image. Default is False.</p> <code>False</code> <code>reverse_tile_size</code> <p>Kernel size for the reverse tiling operation. Must be an odd number &gt;= 3. Default is 3.</p> <code>3</code> <code>save_keys_path</code> <p>Optional file path to save the list of dataset keys.</p> <code>None</code> <code>classification_map</code> <p>String specifying the classification mapping to use (\"land-cover\" or \"crops\"). Default is \"land-cover\".</p> <code>'land-cover'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>input_bands</code> is provided without specifying <code>dataset_bands</code>.</p> <code>ValueError</code> <p>If an invalid <code>classification_map</code> is provided.</p> Source code in <code>terratorch/datasets/sen4map.py</code> <pre><code>def __init__(\n        self,\n        h5py_file_object:h5py.File,\n        h5data_keys = None,\n        crop_size:None|int = None,\n        dataset_bands:list[HLSBands|int]|None = None,\n        input_bands:list[HLSBands|int]|None = None,\n        resize = False,\n        resize_to = [224, 224],\n        resize_interpolation = InterpolationMode.BILINEAR,\n        resize_antialiasing = True,\n        reverse_tile = False,\n        reverse_tile_size = 3,\n        save_keys_path = None,\n        classification_map = \"land-cover\"\n        ):\n    \"\"\"Initialize a new instance of Sen4MapDatasetMonthlyComposites.\n\n    This dataset loads data from an HDF5 file object containing multi-temporal satellite data and computes\n    monthly composite images by aggregating acquisitions (via median).\n\n    Args:\n        h5py_file_object: An open h5py.File object containing the dataset.\n        h5data_keys: Optional list of keys to select a subset of data samples from the HDF5 file.\n            If None, all keys are used.\n        crop_size: Optional integer specifying the square crop size for the output image.\n        dataset_bands: Optional list of bands available in the dataset.\n        input_bands: Optional list of bands to be used as input channels.\n            Must be provided along with `dataset_bands`.\n        resize: Boolean flag indicating whether the image should be resized. Default is False.\n        resize_to: Target dimensions [height, width] for resizing. Default is [224, 224].\n        resize_interpolation: Interpolation mode used for resizing. Default is InterpolationMode.BILINEAR.\n        resize_antialiasing: Boolean flag to apply antialiasing during resizing. Default is True.\n        reverse_tile: Boolean flag indicating whether to apply reverse tiling to the image. Default is False.\n        reverse_tile_size: Kernel size for the reverse tiling operation. Must be an odd number &gt;= 3. Default is 3.\n        save_keys_path: Optional file path to save the list of dataset keys.\n        classification_map: String specifying the classification mapping to use (\"land-cover\" or \"crops\").\n            Default is \"land-cover\".\n\n    Raises:\n        ValueError: If `input_bands` is provided without specifying `dataset_bands`.\n        ValueError: If an invalid `classification_map` is provided.\n    \"\"\"\n    self.h5data = h5py_file_object\n    if h5data_keys is None:\n        if classification_map == \"crops\": print(f\"Crop classification task chosen but no keys supplied. Will fail unless dataset hdf5 files have been filtered. Either filter dataset files or create a filtered set of keys.\")\n        self.h5data_keys = list(self.h5data.keys())\n        if save_keys_path is not None:\n            with open(save_keys_path, \"wb\") as file:\n                pickle.dump(self.h5data_keys, file)\n    else:\n        self.h5data_keys = h5data_keys\n    self.crop_size = crop_size\n    if input_bands and not dataset_bands:\n        raise ValueError(f\"input_bands was provided without specifying the dataset_bands\")\n    # self.dataset_bands = dataset_bands\n    # self.input_bands = input_bands\n    if input_bands and dataset_bands:\n        self.input_channels = [dataset_bands.index(band_ind) for band_ind in input_bands if band_ind in dataset_bands]\n    else: self.input_channels = None\n\n    classification_maps = {\"land-cover\": Sen4MapDatasetMonthlyComposites.land_cover_classification_map,\n                           \"crops\": Sen4MapDatasetMonthlyComposites.crop_classification_map}\n    if classification_map not in classification_maps.keys():\n        raise ValueError(f\"Provided classification_map of: {classification_map}, is not from the list of valid ones: {classification_maps}\")\n    self.classification_map = classification_maps[classification_map]\n\n    self.resize = resize\n    self.resize_to = resize_to\n    self.resize_interpolation = resize_interpolation\n    self.resize_antialiasing = resize_antialiasing\n\n    self.reverse_tile = reverse_tile\n    self.reverse_tile_size = reverse_tile_size\n</code></pre>"},{"location":"datasets/#terratorch.datasets.sen4map.Sen4MapDatasetMonthlyComposites.reverse_tiling_pytorch","title":"<code>reverse_tiling_pytorch(img_tensor, kernel_size=3)</code>","text":"<p>Upscales an image where every pixel is expanded into <code>kernel_size</code>*<code>kernel_size</code> pixels. Used to test whether the benefit of resizing images to the pre-trained size comes from the bilnearly interpolated pixels, or if the same would be realized with no interpolated pixels.</p> Source code in <code>terratorch/datasets/sen4map.py</code> <pre><code>def reverse_tiling_pytorch(self, img_tensor: torch.Tensor, kernel_size: int=3):\n    \"\"\"\n    Upscales an image where every pixel is expanded into `kernel_size`*`kernel_size` pixels.\n    Used to test whether the benefit of resizing images to the pre-trained size comes from the bilnearly interpolated pixels,\n    or if the same would be realized with no interpolated pixels.\n    \"\"\"\n    assert kernel_size % 2 == 1\n    assert kernel_size &gt;= 3\n    padding = (kernel_size - 1) // 2\n    # img_tensor shape: (batch_size, channels, H, W)\n    batch_size, channels, H, W = img_tensor.shape\n    # Unfold: Extract 3x3 patches with padding of 1 to cover borders\n    img_tensor = F.pad(img_tensor, pad=(padding,padding,padding,padding), mode=\"replicate\")\n    patches = F.unfold(img_tensor, kernel_size=kernel_size, padding=0)  # Shape: (batch_size, channels*9, H*W)\n    # Reshape to organize the 9 values from each 3x3 neighborhood\n    patches = patches.view(batch_size, channels, kernel_size*kernel_size, H, W)  # Shape: (batch_size, channels, 9, H, W)\n    # Rearrange the patches into (batch_size, channels, 3, 3, H, W)\n    patches = patches.view(batch_size, channels, kernel_size, kernel_size, H, W)\n    # Permute to have the spatial dimensions first and unfold them\n    patches = patches.permute(0, 1, 4, 2, 5, 3)  # Shape: (batch_size, channels, H, 3, W, 3)\n    # Reshape to get the final expanded image of shape (batch_size, channels, H*3, W*3)\n    expanded_img = patches.reshape(batch_size, channels, H * kernel_size, W * kernel_size)\n    return expanded_img\n</code></pre>"},{"location":"decoders/","title":"Decoders","text":""},{"location":"decoders/#terratorch.models.decoders.fcn_decoder","title":"<code>terratorch.models.decoders.fcn_decoder</code>","text":""},{"location":"decoders/#terratorch.models.decoders.fcn_decoder.FCNDecoder","title":"<code>FCNDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Fully Convolutional Decoder</p> Source code in <code>terratorch/models/decoders/fcn_decoder.py</code> <pre><code>@TERRATORCH_DECODER_REGISTRY.register\nclass FCNDecoder(nn.Module):\n    \"\"\"Fully Convolutional Decoder\"\"\"\n\n    def __init__(self, embed_dim: int, channels: int = 256, num_convs: int = 4, in_index: int = -1) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            embed_dim (_type_): Input embedding dimension\n            channels (int, optional): Number of channels for each conv. Defaults to 256.\n            num_convs (int, optional): Number of convs. Defaults to 4.\n            in_index (int, optional): Index of the input list to take. Defaults to -1.\n        \"\"\"\n        super().__init__()\n        kernel_size = 2\n        stride = 2\n        dilation = 1\n        padding = 0\n        output_padding = 0\n        self.channels = channels\n        self.num_convs = num_convs\n        self.in_index = in_index\n        self.embed_dim = embed_dim[in_index]\n        if num_convs &lt; 1:\n            msg = \"num_convs must be &gt;= 1\"\n            raise Exception(msg)\n\n        convs = []\n\n        for i in range(num_convs):\n            in_channels = self.embed_dim if i == 0 else self.channels\n            convs.append(\n                _conv_upscale_block(in_channels, self.channels, kernel_size, stride, dilation, padding, output_padding)\n            )\n\n        self.convs = nn.Sequential(*convs)\n\n    @property\n    def out_channels(self):\n        return self.channels\n\n    def forward(self, x: list[Tensor]):\n        x = x[self.in_index]\n        decoded = self.convs(x)\n        return decoded\n</code></pre>"},{"location":"decoders/#terratorch.models.decoders.fcn_decoder.FCNDecoder.__init__","title":"<code>__init__(embed_dim, channels=256, num_convs=4, in_index=-1)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>_type_</code> <p>Input embedding dimension</p> required <code>channels</code> <code>int</code> <p>Number of channels for each conv. Defaults to 256.</p> <code>256</code> <code>num_convs</code> <code>int</code> <p>Number of convs. Defaults to 4.</p> <code>4</code> <code>in_index</code> <code>int</code> <p>Index of the input list to take. Defaults to -1.</p> <code>-1</code> Source code in <code>terratorch/models/decoders/fcn_decoder.py</code> <pre><code>def __init__(self, embed_dim: int, channels: int = 256, num_convs: int = 4, in_index: int = -1) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        embed_dim (_type_): Input embedding dimension\n        channels (int, optional): Number of channels for each conv. Defaults to 256.\n        num_convs (int, optional): Number of convs. Defaults to 4.\n        in_index (int, optional): Index of the input list to take. Defaults to -1.\n    \"\"\"\n    super().__init__()\n    kernel_size = 2\n    stride = 2\n    dilation = 1\n    padding = 0\n    output_padding = 0\n    self.channels = channels\n    self.num_convs = num_convs\n    self.in_index = in_index\n    self.embed_dim = embed_dim[in_index]\n    if num_convs &lt; 1:\n        msg = \"num_convs must be &gt;= 1\"\n        raise Exception(msg)\n\n    convs = []\n\n    for i in range(num_convs):\n        in_channels = self.embed_dim if i == 0 else self.channels\n        convs.append(\n            _conv_upscale_block(in_channels, self.channels, kernel_size, stride, dilation, padding, output_padding)\n        )\n\n    self.convs = nn.Sequential(*convs)\n</code></pre>"},{"location":"decoders/#terratorch.models.decoders.identity_decoder","title":"<code>terratorch.models.decoders.identity_decoder</code>","text":"<p>Pass the features straight through</p>"},{"location":"decoders/#terratorch.models.decoders.identity_decoder.IdentityDecoder","title":"<code>IdentityDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>Identity decoder. Useful to pass the feature straight to the head.</p> Source code in <code>terratorch/models/decoders/identity_decoder.py</code> <pre><code>@TERRATORCH_DECODER_REGISTRY.register\nclass IdentityDecoder(nn.Module):\n    \"\"\"Identity decoder. Useful to pass the feature straight to the head.\"\"\"\n\n    def __init__(self, embed_dim: int, out_index=-1) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            embed_dim (int): Input embedding dimension\n            out_index (int, optional): Index of the input list to take.. Defaults to -1.\n        \"\"\"\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.dim = out_index\n\n    @property\n    def out_channels(self):\n        return self.embed_dim[self.dim]\n\n    def forward(self, x: list[Tensor]):\n        return x[self.dim]\n</code></pre>"},{"location":"decoders/#terratorch.models.decoders.identity_decoder.IdentityDecoder.__init__","title":"<code>__init__(embed_dim, out_index=-1)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Input embedding dimension</p> required <code>out_index</code> <code>int</code> <p>Index of the input list to take.. Defaults to -1.</p> <code>-1</code> Source code in <code>terratorch/models/decoders/identity_decoder.py</code> <pre><code>def __init__(self, embed_dim: int, out_index=-1) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        embed_dim (int): Input embedding dimension\n        out_index (int, optional): Index of the input list to take.. Defaults to -1.\n    \"\"\"\n    super().__init__()\n    self.embed_dim = embed_dim\n    self.dim = out_index\n</code></pre>"},{"location":"decoders/#terratorch.models.decoders.upernet_decoder","title":"<code>terratorch.models.decoders.upernet_decoder</code>","text":""},{"location":"decoders/#terratorch.models.decoders.upernet_decoder.PPM","title":"<code>PPM</code>","text":"<p>               Bases: <code>ModuleList</code></p> <p>Pooling Pyramid Module used in PSPNet.</p> Source code in <code>terratorch/models/decoders/upernet_decoder.py</code> <pre><code>class PPM(nn.ModuleList):\n    \"\"\"Pooling Pyramid Module used in PSPNet.\"\"\"\n\n    def __init__(self, pool_scales, in_channels, channels, align_corners):\n        \"\"\"Constructor\n\n        Args:\n            pool_scales (tuple[int]): Pooling scales used in Pooling Pyramid\n                Module.\n            in_channels (int): Input channels.\n            channels (int): Channels after modules, before conv_seg.\n            align_corners (bool): align_corners argument of F.interpolate.\n        \"\"\"\n        super().__init__()\n        self.pool_scales = pool_scales\n        self.align_corners = align_corners\n        self.in_channels = in_channels\n        self.channels = channels\n\n        for pool_scale in pool_scales:\n            self.append(\n                nn.Sequential(\n                    nn.AdaptiveAvgPool2d(pool_scale),\n                    ConvModule(self.in_channels, self.channels, 1, inplace=True),\n                )\n            )\n\n    def forward(self, x):\n        \"\"\"Forward function.\"\"\"\n        ppm_outs = []\n        for ppm in self:\n            ppm_out = ppm(x)\n            upsampled_ppm_out = torch.nn.functional.interpolate(\n                ppm_out, size=x.size()[2:], mode=\"bilinear\", align_corners=self.align_corners\n            )\n            ppm_outs.append(upsampled_ppm_out)\n        return ppm_outs\n</code></pre>"},{"location":"decoders/#terratorch.models.decoders.upernet_decoder.PPM.__init__","title":"<code>__init__(pool_scales, in_channels, channels, align_corners)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>pool_scales</code> <code>tuple[int]</code> <p>Pooling scales used in Pooling Pyramid Module.</p> required <code>in_channels</code> <code>int</code> <p>Input channels.</p> required <code>channels</code> <code>int</code> <p>Channels after modules, before conv_seg.</p> required <code>align_corners</code> <code>bool</code> <p>align_corners argument of F.interpolate.</p> required Source code in <code>terratorch/models/decoders/upernet_decoder.py</code> <pre><code>def __init__(self, pool_scales, in_channels, channels, align_corners):\n    \"\"\"Constructor\n\n    Args:\n        pool_scales (tuple[int]): Pooling scales used in Pooling Pyramid\n            Module.\n        in_channels (int): Input channels.\n        channels (int): Channels after modules, before conv_seg.\n        align_corners (bool): align_corners argument of F.interpolate.\n    \"\"\"\n    super().__init__()\n    self.pool_scales = pool_scales\n    self.align_corners = align_corners\n    self.in_channels = in_channels\n    self.channels = channels\n\n    for pool_scale in pool_scales:\n        self.append(\n            nn.Sequential(\n                nn.AdaptiveAvgPool2d(pool_scale),\n                ConvModule(self.in_channels, self.channels, 1, inplace=True),\n            )\n        )\n</code></pre>"},{"location":"decoders/#terratorch.models.decoders.upernet_decoder.PPM.forward","title":"<code>forward(x)</code>","text":"<p>Forward function.</p> Source code in <code>terratorch/models/decoders/upernet_decoder.py</code> <pre><code>def forward(self, x):\n    \"\"\"Forward function.\"\"\"\n    ppm_outs = []\n    for ppm in self:\n        ppm_out = ppm(x)\n        upsampled_ppm_out = torch.nn.functional.interpolate(\n            ppm_out, size=x.size()[2:], mode=\"bilinear\", align_corners=self.align_corners\n        )\n        ppm_outs.append(upsampled_ppm_out)\n    return ppm_outs\n</code></pre>"},{"location":"decoders/#terratorch.models.decoders.upernet_decoder.UperNetDecoder","title":"<code>UperNetDecoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>UperNetDecoder. Adapted from MMSegmentation.</p> Source code in <code>terratorch/models/decoders/upernet_decoder.py</code> <pre><code>@TERRATORCH_DECODER_REGISTRY.register\nclass UperNetDecoder(nn.Module):\n    \"\"\"UperNetDecoder. Adapted from MMSegmentation.\"\"\"\n\n    def __init__(\n        self,\n        embed_dim: list[int],\n        pool_scales: tuple[int] = (1, 2, 3, 6),\n        channels: int = 256,\n        align_corners: bool = True,  # noqa: FBT001, FBT002\n        scale_modules: bool = False,\n    ):\n        \"\"\"Constructor\n\n        Args:\n            embed_dim (list[int]): Input embedding dimension for each input.\n            pool_scales (tuple[int], optional): Pooling scales used in Pooling Pyramid\n                Module applied on the last feature. Default: (1, 2, 3, 6).\n            channels (int, optional): Channels used in the decoder. Defaults to 256.\n            align_corners (bool, optional): Wheter to align corners in rescaling. Defaults to True.\n            scale_modules (bool, optional): Whether to apply scale modules to the inputs. Needed for plain ViT.\n                Defaults to False.\n        \"\"\"\n        super().__init__()\n        if scale_modules:\n            # TODO: remove scale_modules before v1?\n            warnings.warn(\n                \"DeprecationWarning: scale_modules is deprecated and will be removed in future versions. \"\n                \"Use LearnedInterpolateToPyramidal neck instead.\",\n                stacklevel=1,\n            )\n\n        self.scale_modules = scale_modules\n        if scale_modules:\n            self.fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim[0],\n                                embed_dim[0] // 2, 2, 2),\n                nn.BatchNorm2d(embed_dim[0] // 2),\n                nn.GELU(),\n                nn.ConvTranspose2d(embed_dim[0] // 2,\n                                embed_dim[0] // 4, 2, 2))\n            self.fpn2 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim[1],\n                                embed_dim[1] // 2, 2, 2))\n            self.fpn3 = nn.Sequential(nn.Identity())\n            self.fpn4 = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2))\n            self.embed_dim = [embed_dim[0] // 4, embed_dim[1] // 2, embed_dim[2], embed_dim[3]]\n        else:\n            self.embed_dim = embed_dim\n\n        self.out_channels = channels\n        self.channels = channels\n        self.align_corners = align_corners\n        # PSP Module\n        self.psp_modules = PPM(\n            pool_scales,\n            self.embed_dim[-1],\n            self.channels,\n            align_corners=self.align_corners,\n        )\n        self.bottleneck = ConvModule(\n            self.embed_dim[-1] + len(pool_scales) * self.channels, self.channels, 3, padding=1, inplace=True\n        )\n        # FPN Module\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n        for embed_dim in self.embed_dim[:-1]:  # skip the top layer\n            l_conv = ConvModule(\n                embed_dim,\n                self.channels,\n                1,\n                inplace=False,\n            )\n            fpn_conv = ConvModule(\n                self.channels,\n                self.channels,\n                3,\n                padding=1,\n                inplace=False,\n            )\n            self.lateral_convs.append(l_conv)\n            self.fpn_convs.append(fpn_conv)\n\n        self.fpn_bottleneck = ConvModule(len(self.embed_dim) * self.channels, self.channels, 3, padding=1, inplace=True)\n\n    def psp_forward(self, inputs):\n        \"\"\"Forward function of PSP module.\"\"\"\n        x = inputs[-1]\n        psp_outs = [x]\n        psp_outs.extend(self.psp_modules(x))\n        psp_outs = torch.cat(psp_outs, dim=1)\n        output = self.bottleneck(psp_outs)\n\n        return output\n\n    def forward(self, inputs):\n        \"\"\"Forward function for feature maps before classifying each pixel with\n        Args:\n            inputs (list[Tensor]): List of multi-level img features.\n\n        Returns:\n            feats (Tensor): A tensor of shape (batch_size, self.channels,\n                H, W) which is feature map for last layer of decoder head.\n        \"\"\"\n\n        if self.scale_modules:\n            scaled_inputs = []\n            scaled_inputs.append(self.fpn1(inputs[0]))\n            scaled_inputs.append(self.fpn2(inputs[1]))\n            scaled_inputs.append(self.fpn3(inputs[2]))\n            scaled_inputs.append(self.fpn4(inputs[3]))\n            inputs = scaled_inputs\n        # build laterals\n        laterals = [lateral_conv(inputs[i]) for i, lateral_conv in enumerate(self.lateral_convs)]\n        laterals.append(self.psp_forward(inputs))\n\n        # build top-down path\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            prev_shape = laterals[i - 1].shape[2:]\n            laterals[i - 1] = laterals[i - 1] + torch.nn.functional.interpolate(\n                laterals[i], size=prev_shape, mode=\"bilinear\", align_corners=self.align_corners\n            )\n\n        # build outputs\n        fpn_outs = [self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels - 1)]\n        # append psp feature\n        fpn_outs.append(laterals[-1])\n\n        for i in range(used_backbone_levels - 1, 0, -1):\n            fpn_outs[i] = torch.nn.functional.interpolate(\n                fpn_outs[i], size=fpn_outs[0].shape[2:], mode=\"bilinear\", align_corners=self.align_corners\n            )\n        fpn_outs = torch.cat(fpn_outs, dim=1)\n        feats = self.fpn_bottleneck(fpn_outs)\n        return feats\n</code></pre>"},{"location":"decoders/#terratorch.models.decoders.upernet_decoder.UperNetDecoder.__init__","title":"<code>__init__(embed_dim, pool_scales=(1, 2, 3, 6), channels=256, align_corners=True, scale_modules=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>list[int]</code> <p>Input embedding dimension for each input.</p> required <code>pool_scales</code> <code>tuple[int]</code> <p>Pooling scales used in Pooling Pyramid Module applied on the last feature. Default: (1, 2, 3, 6).</p> <code>(1, 2, 3, 6)</code> <code>channels</code> <code>int</code> <p>Channels used in the decoder. Defaults to 256.</p> <code>256</code> <code>align_corners</code> <code>bool</code> <p>Wheter to align corners in rescaling. Defaults to True.</p> <code>True</code> <code>scale_modules</code> <code>bool</code> <p>Whether to apply scale modules to the inputs. Needed for plain ViT. Defaults to False.</p> <code>False</code> Source code in <code>terratorch/models/decoders/upernet_decoder.py</code> <pre><code>def __init__(\n    self,\n    embed_dim: list[int],\n    pool_scales: tuple[int] = (1, 2, 3, 6),\n    channels: int = 256,\n    align_corners: bool = True,  # noqa: FBT001, FBT002\n    scale_modules: bool = False,\n):\n    \"\"\"Constructor\n\n    Args:\n        embed_dim (list[int]): Input embedding dimension for each input.\n        pool_scales (tuple[int], optional): Pooling scales used in Pooling Pyramid\n            Module applied on the last feature. Default: (1, 2, 3, 6).\n        channels (int, optional): Channels used in the decoder. Defaults to 256.\n        align_corners (bool, optional): Wheter to align corners in rescaling. Defaults to True.\n        scale_modules (bool, optional): Whether to apply scale modules to the inputs. Needed for plain ViT.\n            Defaults to False.\n    \"\"\"\n    super().__init__()\n    if scale_modules:\n        # TODO: remove scale_modules before v1?\n        warnings.warn(\n            \"DeprecationWarning: scale_modules is deprecated and will be removed in future versions. \"\n            \"Use LearnedInterpolateToPyramidal neck instead.\",\n            stacklevel=1,\n        )\n\n    self.scale_modules = scale_modules\n    if scale_modules:\n        self.fpn1 = nn.Sequential(\n            nn.ConvTranspose2d(embed_dim[0],\n                            embed_dim[0] // 2, 2, 2),\n            nn.BatchNorm2d(embed_dim[0] // 2),\n            nn.GELU(),\n            nn.ConvTranspose2d(embed_dim[0] // 2,\n                            embed_dim[0] // 4, 2, 2))\n        self.fpn2 = nn.Sequential(\n            nn.ConvTranspose2d(embed_dim[1],\n                            embed_dim[1] // 2, 2, 2))\n        self.fpn3 = nn.Sequential(nn.Identity())\n        self.fpn4 = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2))\n        self.embed_dim = [embed_dim[0] // 4, embed_dim[1] // 2, embed_dim[2], embed_dim[3]]\n    else:\n        self.embed_dim = embed_dim\n\n    self.out_channels = channels\n    self.channels = channels\n    self.align_corners = align_corners\n    # PSP Module\n    self.psp_modules = PPM(\n        pool_scales,\n        self.embed_dim[-1],\n        self.channels,\n        align_corners=self.align_corners,\n    )\n    self.bottleneck = ConvModule(\n        self.embed_dim[-1] + len(pool_scales) * self.channels, self.channels, 3, padding=1, inplace=True\n    )\n    # FPN Module\n    self.lateral_convs = nn.ModuleList()\n    self.fpn_convs = nn.ModuleList()\n    for embed_dim in self.embed_dim[:-1]:  # skip the top layer\n        l_conv = ConvModule(\n            embed_dim,\n            self.channels,\n            1,\n            inplace=False,\n        )\n        fpn_conv = ConvModule(\n            self.channels,\n            self.channels,\n            3,\n            padding=1,\n            inplace=False,\n        )\n        self.lateral_convs.append(l_conv)\n        self.fpn_convs.append(fpn_conv)\n\n    self.fpn_bottleneck = ConvModule(len(self.embed_dim) * self.channels, self.channels, 3, padding=1, inplace=True)\n</code></pre>"},{"location":"decoders/#terratorch.models.decoders.upernet_decoder.UperNetDecoder.forward","title":"<code>forward(inputs)</code>","text":"<p>Forward function for feature maps before classifying each pixel with Args:     inputs (list[Tensor]): List of multi-level img features.</p> <p>Returns:</p> Name Type Description <code>feats</code> <code>Tensor</code> <p>A tensor of shape (batch_size, self.channels, H, W) which is feature map for last layer of decoder head.</p> Source code in <code>terratorch/models/decoders/upernet_decoder.py</code> <pre><code>def forward(self, inputs):\n    \"\"\"Forward function for feature maps before classifying each pixel with\n    Args:\n        inputs (list[Tensor]): List of multi-level img features.\n\n    Returns:\n        feats (Tensor): A tensor of shape (batch_size, self.channels,\n            H, W) which is feature map for last layer of decoder head.\n    \"\"\"\n\n    if self.scale_modules:\n        scaled_inputs = []\n        scaled_inputs.append(self.fpn1(inputs[0]))\n        scaled_inputs.append(self.fpn2(inputs[1]))\n        scaled_inputs.append(self.fpn3(inputs[2]))\n        scaled_inputs.append(self.fpn4(inputs[3]))\n        inputs = scaled_inputs\n    # build laterals\n    laterals = [lateral_conv(inputs[i]) for i, lateral_conv in enumerate(self.lateral_convs)]\n    laterals.append(self.psp_forward(inputs))\n\n    # build top-down path\n    used_backbone_levels = len(laterals)\n    for i in range(used_backbone_levels - 1, 0, -1):\n        prev_shape = laterals[i - 1].shape[2:]\n        laterals[i - 1] = laterals[i - 1] + torch.nn.functional.interpolate(\n            laterals[i], size=prev_shape, mode=\"bilinear\", align_corners=self.align_corners\n        )\n\n    # build outputs\n    fpn_outs = [self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels - 1)]\n    # append psp feature\n    fpn_outs.append(laterals[-1])\n\n    for i in range(used_backbone_levels - 1, 0, -1):\n        fpn_outs[i] = torch.nn.functional.interpolate(\n            fpn_outs[i], size=fpn_outs[0].shape[2:], mode=\"bilinear\", align_corners=self.align_corners\n        )\n    fpn_outs = torch.cat(fpn_outs, dim=1)\n    feats = self.fpn_bottleneck(fpn_outs)\n    return feats\n</code></pre>"},{"location":"decoders/#terratorch.models.decoders.upernet_decoder.UperNetDecoder.psp_forward","title":"<code>psp_forward(inputs)</code>","text":"<p>Forward function of PSP module.</p> Source code in <code>terratorch/models/decoders/upernet_decoder.py</code> <pre><code>def psp_forward(self, inputs):\n    \"\"\"Forward function of PSP module.\"\"\"\n    x = inputs[-1]\n    psp_outs = [x]\n    psp_outs.extend(self.psp_modules(x))\n    psp_outs = torch.cat(psp_outs, dim=1)\n    output = self.bottleneck(psp_outs)\n\n    return output\n</code></pre>"},{"location":"encoder_decoder_factory/","title":"EncoderDecoderFactory","text":"<p>Check the Glossary for more information about the terms used in this page.</p> <p>The EncoderDecoderFactory is the main class used to instantiate and compose models for general tasks. </p> <p>This factory leverages the <code>BACKBONE_REGISTRY</code>, <code>DECODER_REGISTRY</code> and <code>NECK_REGISTRY</code> to compose models formed as encoder + decoder, with some optional glue in between provided by the necks. As most current models work this way, this is a particularly important factory, allowing for great flexibility in combining encoders and decoders from different sources.</p> <p>The factory allows arguments to be passed to the encoder, decoder and head. Arguments with the prefix <code>backbone_</code> will be routed to the backbone constructor, with <code>decoder_</code> and <code>head_</code> working the same way. These are accepted dynamically and not checked. Any unused arguments will raise a <code>ValueError</code>.</p> <p>Both encoder and decoder may be passed as strings, in which case they will be looked in the respective registry, or as <code>nn.Modules</code>, in which case they will be used as is. In the second case, the factory assumes in good faith that the encoder or decoder which is passed conforms to the expected contract.</p> <p>Not all decoders will readily accept the raw output of the given encoder. This is where necks come in.  Necks are a sequence of operations which are applied to the output of the encoder before it is passed to the decoder. They must be instances of Neck, which is a subclass of <code>nn.Module</code>, meaning they can even define new trainable parameters.</p> <p>The EncoderDecoderFactory returns a PixelWiseModel or a ScalarOutputModel depending on the task.</p>"},{"location":"encoder_decoder_factory/#terratorch.models.encoder_decoder_factory.EncoderDecoderFactory","title":"<code>terratorch.models.encoder_decoder_factory.EncoderDecoderFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/encoder_decoder_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass EncoderDecoderFactory(ModelFactory):\n    def build_model(\n        self,\n        task: str,\n        backbone: str | nn.Module,\n        decoder: str | nn.Module,\n        num_classes: int | None = None,\n        necks: list[dict] | None = None,\n        aux_decoders: list[AuxiliaryHead] | None = None,\n        rescale: bool = True,  # noqa: FBT002, FBT001\n        peft_config: dict | None = None,\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"Generic model factory that combines an encoder and decoder, together with a head, for a specific task.\n\n        Further arguments to be passed to the backbone, decoder or head. They should be prefixed with\n        `backbone_`, `decoder_` and `head_` respectively.\n\n        Args:\n            task (str): Task to be performed. Currently supports \"segmentation\", \"regression\" and \"classification\".\n            backbone (str, nn.Module): Backbone to be used. If a string, will look for such models in the different\n                registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it\n                directly. The backbone should have and `out_channels` attribute and its `forward` should return a list[Tensor].\n            decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model.\n                    If a string, will look for such decoders in the different\n                    registries supported (internal terratorch registry, smp, ...).\n                    If an nn.Module, we expect it to expose a property `decoder.out_channels`.\n                    Pixel wise tasks will be concatenated with a Conv2d for the final convolution.\n                    Defaults to \"FCNDecoder\".\n            num_classes (int, optional): Number of classes. None for regression tasks.\n            necks (list[dict]): nn.Modules to be called in succession on encoder features\n                before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry.\n                Expects each one to have a key \"name\" and subsequent keys for arguments, if any.\n                Defaults to None, which applies the identity function.\n            aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead decoders to be added to the model.\n                These decoders take the input from the encoder as well.\n            rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n                is different from the ground truth. Only applicable to pixel wise models\n                (e.g. segmentation, pixel wise regression). Defaults to True.\n            peft_config (dict): Configuration options for using [PEFT](https://huggingface.co/docs/peft/index).\n                The dictionary should have the following keys:\n\n                - \"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available [here](https://huggingface.co/docs/peft/package_reference/peft_types#peft.PeftType).\n                - \"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep.\n                  This should be used when the qkv matrices are merged together in a single linear layer and the PEFT\n                  method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in\n                  Q and V matrices). e.g. If using Prithvi this should be \"qkv\"\n                - \"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to [PeftConfig](https://huggingface.co/docs/peft/package_reference/config#peft.PeftConfig)\n\n\n        Returns:\n            nn.Module: Full model with encoder, decoder and head.\n        \"\"\"\n        task = task.lower()\n        if task not in SUPPORTED_TASKS:\n            msg = f\"Task {task} not supported. Please choose one of {SUPPORTED_TASKS}\"\n            raise NotImplementedError(msg)\n\n        backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n        backbone = _get_backbone(backbone, **backbone_kwargs)\n\n        # If patch size is not provided in the config or by the model, it might lead to errors due to irregular images.\n        patch_size = backbone_kwargs.get(\"patch_size\", None)\n\n        if patch_size is None:\n            # Infer patch size from model by checking all backbone modules\n            for module in backbone.modules():\n                if hasattr(module, \"patch_size\"):\n                    patch_size = module.patch_size\n                    break\n        padding = backbone_kwargs.get(\"padding\", \"reflect\")\n\n        if peft_config is not None:\n            if not backbone_kwargs.get(\"pretrained\", False):\n                msg = (\n                    \"You are using PEFT without a pretrained backbone. If you are loading a checkpoint afterwards \"\n                    \"this is probably fine, but if you are training a model check the backbone_pretrained parameter.\"\n                )\n                warnings.warn(msg, stacklevel=1)\n\n            backbone = get_peft_backbone(peft_config, backbone)\n\n        try:\n            out_channels = backbone.out_channels\n        except AttributeError as e:\n            msg = \"backbone must have out_channels attribute\"\n            raise AttributeError(msg) from e\n\n        if necks is None:\n            necks = []\n        neck_list, channel_list = build_neck_list(necks, out_channels)\n\n        # some decoders already include a head\n        # for these, we pass the num_classes to them\n        # others dont include a head\n        # for those, we dont pass num_classes\n        decoder_kwargs, kwargs = extract_prefix_keys(kwargs, \"decoder_\")\n        head_kwargs, kwargs = extract_prefix_keys(kwargs, \"head_\")\n\n        decoder, head_kwargs, decoder_includes_head = _get_decoder_and_head_kwargs(\n            decoder, channel_list, decoder_kwargs, head_kwargs, num_classes=num_classes\n        )\n\n        if aux_decoders is None:\n            _check_all_args_used(kwargs)\n            return _build_appropriate_model(\n                task,\n                backbone,\n                decoder,\n                head_kwargs,\n                patch_size=patch_size,\n                padding=padding,\n                necks=neck_list,\n                decoder_includes_head=decoder_includes_head,\n                rescale=rescale,\n            )\n\n        to_be_aux_decoders: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] = []\n        for aux_decoder in aux_decoders:\n            args = aux_decoder.decoder_args if aux_decoder.decoder_args else {}\n            aux_decoder_kwargs, args = extract_prefix_keys(args, \"decoder_\")\n            aux_head_kwargs, args = extract_prefix_keys(args, \"head_\")\n            aux_decoder_instance, aux_head_kwargs, aux_decoder_includes_head = _get_decoder_and_head_kwargs(\n                aux_decoder.decoder, channel_list, aux_decoder_kwargs, aux_head_kwargs, num_classes=num_classes\n            )\n            to_be_aux_decoders.append(\n                AuxiliaryHeadWithDecoderWithoutInstantiatedHead(aux_decoder.name, aux_decoder_instance, aux_head_kwargs)\n            )\n            _check_all_args_used(args)\n\n        _check_all_args_used(kwargs)\n\n        return _build_appropriate_model(\n            task,\n            backbone,\n            decoder,\n            head_kwargs,\n            patch_size=patch_size,\n            padding=padding,\n            necks=neck_list,\n            decoder_includes_head=decoder_includes_head,\n            rescale=rescale,\n            auxiliary_heads=to_be_aux_decoders,\n        )\n</code></pre>"},{"location":"encoder_decoder_factory/#terratorch.models.encoder_decoder_factory.EncoderDecoderFactory.build_model","title":"<code>build_model(task, backbone, decoder, num_classes=None, necks=None, aux_decoders=None, rescale=True, peft_config=None, **kwargs)</code>","text":"<p>Generic model factory that combines an encoder and decoder, together with a head, for a specific task.</p> <p>Further arguments to be passed to the backbone, decoder or head. They should be prefixed with <code>backbone_</code>, <code>decoder_</code> and <code>head_</code> respectively.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Currently supports \"segmentation\", \"regression\" and \"classification\".</p> required <code>backbone</code> <code>(str, Module)</code> <p>Backbone to be used. If a string, will look for such models in the different registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it directly. The backbone should have and <code>out_channels</code> attribute and its <code>forward</code> should return a list[Tensor].</p> required <code>decoder</code> <code>Union[str, Module]</code> <p>Decoder to be used for the segmentation model.     If a string, will look for such decoders in the different     registries supported (internal terratorch registry, smp, ...).     If an nn.Module, we expect it to expose a property <code>decoder.out_channels</code>.     Pixel wise tasks will be concatenated with a Conv2d for the final convolution.     Defaults to \"FCNDecoder\".</p> required <code>num_classes</code> <code>int</code> <p>Number of classes. None for regression tasks.</p> <code>None</code> <code>necks</code> <code>list[dict]</code> <p>nn.Modules to be called in succession on encoder features before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry. Expects each one to have a key \"name\" and subsequent keys for arguments, if any. Defaults to None, which applies the identity function.</p> <code>None</code> <code>aux_decoders</code> <code>list[AuxiliaryHead] | None</code> <p>List of AuxiliaryHead decoders to be added to the model. These decoders take the input from the encoder as well.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.</p> <code>True</code> <code>peft_config</code> <code>dict</code> <p>Configuration options for using PEFT. The dictionary should have the following keys:</p> <ul> <li>\"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available here.</li> <li>\"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep.   This should be used when the qkv matrices are merged together in a single linear layer and the PEFT   method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in   Q and V matrices). e.g. If using Prithvi this should be \"qkv\"</li> <li>\"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to PeftConfig</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: Full model with encoder, decoder and head.</p> Source code in <code>terratorch/models/encoder_decoder_factory.py</code> <pre><code>def build_model(\n    self,\n    task: str,\n    backbone: str | nn.Module,\n    decoder: str | nn.Module,\n    num_classes: int | None = None,\n    necks: list[dict] | None = None,\n    aux_decoders: list[AuxiliaryHead] | None = None,\n    rescale: bool = True,  # noqa: FBT002, FBT001\n    peft_config: dict | None = None,\n    **kwargs,\n) -&gt; Model:\n    \"\"\"Generic model factory that combines an encoder and decoder, together with a head, for a specific task.\n\n    Further arguments to be passed to the backbone, decoder or head. They should be prefixed with\n    `backbone_`, `decoder_` and `head_` respectively.\n\n    Args:\n        task (str): Task to be performed. Currently supports \"segmentation\", \"regression\" and \"classification\".\n        backbone (str, nn.Module): Backbone to be used. If a string, will look for such models in the different\n            registries supported (internal terratorch registry, timm, ...). If a torch nn.Module, will use it\n            directly. The backbone should have and `out_channels` attribute and its `forward` should return a list[Tensor].\n        decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model.\n                If a string, will look for such decoders in the different\n                registries supported (internal terratorch registry, smp, ...).\n                If an nn.Module, we expect it to expose a property `decoder.out_channels`.\n                Pixel wise tasks will be concatenated with a Conv2d for the final convolution.\n                Defaults to \"FCNDecoder\".\n        num_classes (int, optional): Number of classes. None for regression tasks.\n        necks (list[dict]): nn.Modules to be called in succession on encoder features\n            before passing them to the decoder. Should be registered in the NECKS_REGISTRY registry.\n            Expects each one to have a key \"name\" and subsequent keys for arguments, if any.\n            Defaults to None, which applies the identity function.\n        aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead decoders to be added to the model.\n            These decoders take the input from the encoder as well.\n        rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n            is different from the ground truth. Only applicable to pixel wise models\n            (e.g. segmentation, pixel wise regression). Defaults to True.\n        peft_config (dict): Configuration options for using [PEFT](https://huggingface.co/docs/peft/index).\n            The dictionary should have the following keys:\n\n            - \"method\": Which PEFT method to use. Should be one implemented in PEFT, a list is available [here](https://huggingface.co/docs/peft/package_reference/peft_types#peft.PeftType).\n            - \"replace_qkv\": String containing a substring of the name of the submodules to replace with QKVSep.\n              This should be used when the qkv matrices are merged together in a single linear layer and the PEFT\n              method should be applied separately to query, key and value matrices (e.g. if LoRA is only desired in\n              Q and V matrices). e.g. If using Prithvi this should be \"qkv\"\n            - \"peft_config_kwargs\": Dictionary containing keyword arguments which will be passed to [PeftConfig](https://huggingface.co/docs/peft/package_reference/config#peft.PeftConfig)\n\n\n    Returns:\n        nn.Module: Full model with encoder, decoder and head.\n    \"\"\"\n    task = task.lower()\n    if task not in SUPPORTED_TASKS:\n        msg = f\"Task {task} not supported. Please choose one of {SUPPORTED_TASKS}\"\n        raise NotImplementedError(msg)\n\n    backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n    backbone = _get_backbone(backbone, **backbone_kwargs)\n\n    # If patch size is not provided in the config or by the model, it might lead to errors due to irregular images.\n    patch_size = backbone_kwargs.get(\"patch_size\", None)\n\n    if patch_size is None:\n        # Infer patch size from model by checking all backbone modules\n        for module in backbone.modules():\n            if hasattr(module, \"patch_size\"):\n                patch_size = module.patch_size\n                break\n    padding = backbone_kwargs.get(\"padding\", \"reflect\")\n\n    if peft_config is not None:\n        if not backbone_kwargs.get(\"pretrained\", False):\n            msg = (\n                \"You are using PEFT without a pretrained backbone. If you are loading a checkpoint afterwards \"\n                \"this is probably fine, but if you are training a model check the backbone_pretrained parameter.\"\n            )\n            warnings.warn(msg, stacklevel=1)\n\n        backbone = get_peft_backbone(peft_config, backbone)\n\n    try:\n        out_channels = backbone.out_channels\n    except AttributeError as e:\n        msg = \"backbone must have out_channels attribute\"\n        raise AttributeError(msg) from e\n\n    if necks is None:\n        necks = []\n    neck_list, channel_list = build_neck_list(necks, out_channels)\n\n    # some decoders already include a head\n    # for these, we pass the num_classes to them\n    # others dont include a head\n    # for those, we dont pass num_classes\n    decoder_kwargs, kwargs = extract_prefix_keys(kwargs, \"decoder_\")\n    head_kwargs, kwargs = extract_prefix_keys(kwargs, \"head_\")\n\n    decoder, head_kwargs, decoder_includes_head = _get_decoder_and_head_kwargs(\n        decoder, channel_list, decoder_kwargs, head_kwargs, num_classes=num_classes\n    )\n\n    if aux_decoders is None:\n        _check_all_args_used(kwargs)\n        return _build_appropriate_model(\n            task,\n            backbone,\n            decoder,\n            head_kwargs,\n            patch_size=patch_size,\n            padding=padding,\n            necks=neck_list,\n            decoder_includes_head=decoder_includes_head,\n            rescale=rescale,\n        )\n\n    to_be_aux_decoders: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] = []\n    for aux_decoder in aux_decoders:\n        args = aux_decoder.decoder_args if aux_decoder.decoder_args else {}\n        aux_decoder_kwargs, args = extract_prefix_keys(args, \"decoder_\")\n        aux_head_kwargs, args = extract_prefix_keys(args, \"head_\")\n        aux_decoder_instance, aux_head_kwargs, aux_decoder_includes_head = _get_decoder_and_head_kwargs(\n            aux_decoder.decoder, channel_list, aux_decoder_kwargs, aux_head_kwargs, num_classes=num_classes\n        )\n        to_be_aux_decoders.append(\n            AuxiliaryHeadWithDecoderWithoutInstantiatedHead(aux_decoder.name, aux_decoder_instance, aux_head_kwargs)\n        )\n        _check_all_args_used(args)\n\n    _check_all_args_used(kwargs)\n\n    return _build_appropriate_model(\n        task,\n        backbone,\n        decoder,\n        head_kwargs,\n        patch_size=patch_size,\n        padding=padding,\n        necks=neck_list,\n        decoder_includes_head=decoder_includes_head,\n        rescale=rescale,\n        auxiliary_heads=to_be_aux_decoders,\n    )\n</code></pre>"},{"location":"encoder_decoder_factory/#encoders","title":"Encoders","text":"<p>To be a valid encoder, an object must be an <code>nn.Module</code> and contain an attribute <code>out_channels</code>, basically a list of the channel dimensions corresponding to the features it returns. The forward method of any encoder should return a list of <code>torch.Tensor</code>.</p> <pre><code>In [19]: backbone = BACKBONE_REGISTRY.build(\"prithvi_eo_v2_300\", pretrained=True)\n\nIn [20]: import numpy as np\n\nIn [21]: import torch\n\nIn [22]: input_image = torch.tensor(np.random.rand(1,6,224,224).astype(\"float32\"))\n\nIn [23]: output = backbone.forward(input_image)\n\nIn [24]: [item.shape for item in output]\n\nOut[24]: \n\n[torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024]),\n torch.Size([1, 197, 1024])]\n</code></pre>"},{"location":"encoder_decoder_factory/#necks","title":"Necks","text":"<p>Necks are the connectors between encoder and decoder. They can perform operations such as selecting elements from the output of the encoder (SelectIndices), reshaping the outputs of ViTs so they are compatible with CNNs (ReshapeTokensToImage), amongst others. Necks are <code>nn.Modules</code>, with an additional method <code>process_channel_list</code> which informs the EncoderDecoderFactory about how it will alter the channel list provided by <code>encoder.out_channels</code>. See a better description about necks here.</p>"},{"location":"encoder_decoder_factory/#decoders","title":"Decoders","text":"<p>To be a valid decoder, an object must be an <code>nn.Module</code> with an attribute <code>out_channels</code>, an <code>int</code> representing the channel dimension of the output. The first argument to its constructor will be a list of channel dimensions it should expect as input. It's forward method should accept a list of embeddings. To see a list of builtin decoders check the related documentation. </p>"},{"location":"encoder_decoder_factory/#heads","title":"Heads","text":"<p>Most decoders require a final head to be added for a specific task (e.g. semantic segmentation vs pixel wise regression). Those registries producing decoders that dont require a head must expose the attribute <code>includes_head=True</code> so that a head is not added. Decoders passed as <code>nn.Modules</code> which do not require a head must expose the same attribute themselves. More about heads can be seen in its documentation. </p>"},{"location":"encoder_decoder_factory/#decoder-compatibilities","title":"Decoder compatibilities","text":"<p>Not all encoders and decoders are compatible. Below we include some caveats. Some decoders expect pyramidal outputs, but some encoders do not produce such outputs (e.g. vanilla ViT models). In this case, the InterpolateToPyramidal, MaxpoolToPyramidal and LearnedInterpolateToPyramidal necks may be particularly useful.</p>"},{"location":"encoder_decoder_factory/#smp-decoders","title":"SMP decoders","text":"<p>Not all decoders are guaranteed to work with all encoders without additional necks. Please check smp documentation to understand the embedding spatial dimensions expected by each decoder.</p> <p>In particular, smp seems to assume the first feature in the passed feature list has the same spatial resolution as the input, which may not always be true, and may break some decoders.</p> <p>In addition, for some decoders, the final 2 features have the same spatial resolution. Adding the AddBottleneckLayer neck will make this compatible.</p> <p>Some smp decoders require additional parameters, such as <code>decoder_channels</code>. These must be passed through the factory. In the case of <code>decoder_channels</code>, it would be passed as <code>decoder_decoder_channels</code> (the first <code>decoder_</code> routes the parameter to the decoder, where it is passed as <code>decoder_channels</code>).</p>"},{"location":"encoder_decoder_factory/#mmsegmentation-decoders","title":"MMSegmentation decoders","text":"<p>MMSegmentation decoders are available through the BACKBONE_REGISTRY. </p> <p>Warning</p> <p>MMSegmentation currently requires <code>mmcv==2.1.0</code>. Pre-built wheels for this only exist for <code>torch==2.1.0</code>. In order to use mmseg without building from source, you must downgrade your <code>torch</code> to this version. Install mmseg with: <pre><code>pip install -U openmim\nmim install mmengine\nmim install mmcv==2.1.0\npip install regex ftfy mmsegmentation\n</code></pre></p> <p>We provide access to mmseg decoders as an external source of decoders, but are not directly responsible for the maintainence of that library.</p> <p>Some mmseg decoders require the parameter <code>in_index</code>, which performs the same function as the <code>SelectIndices</code> neck. For use for pixel wise regression, mmseg decoders should take <code>num_classes=1</code>.</p>"},{"location":"examples/","title":"Performing an inference task with TerraTorch","text":""},{"location":"examples/#step-1-download-the-test-case-from-huggingface","title":"Step 1: Download the test case from HuggingFace","text":"<p>We will use the burn scars identification test case, in which we are interested in estimating the area affected by wildfires using a finetuned model (Prithvi-EO backbone + CNN decoder). To download the complete example, do: <pre><code>git clone https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-300M-BurnScars/\n</code></pre></p>"},{"location":"examples/#step-2-run-the-default-inference-case","title":"Step 2: Run the default inference case","text":"<p>The example you download already contains some sample images to be used as input, so you just need to go to the local repository and create a directory to save the outputs: <pre><code>cd Prithvi-EO-2.0-300M-BurnScars\nmkdir outputs\n</code></pre> and to execute a command line like: <pre><code>terratorch predict -c burn_scars_config.yaml --predict_output_dir outputs/ --data.init_args.predict_data_root examples/ --ckpt_path Prithvi_EO_V2_300M_BurnScars.pt\n</code></pre> You will see the outputs being saved in the <code>outputs</code> directory. </p>"},{"location":"examples/#input-image-rgb-components","title":"Input image (RGB components)","text":""},{"location":"examples/#predicted-mask","title":"Predicted mask","text":"<p>}</p>"},{"location":"examples/#more-examples","title":"More examples","text":"<p>For some examples of training using the existing tasks, check out the following pages on our github repo:</p>"},{"location":"examples/#from-config-files","title":"From config files","text":"<p>Under <code>examples/confs</code></p> <ul> <li> <p>Flood Segmentation with ViT: <code>sen1floods11_vit.yaml</code></p> </li> <li> <p>Flood Segmentation with ViT and an SMP head: <code>sen1floods11_vit_smp.yaml</code></p> </li> <li> <p>Flood Segmentation with ViT and an MMSeg head: <code>sen1floods11_vit_mmseg.yaml</code></p> </li> <li> <p>Multitemporal Crop Segmentation: <code>multitemporal_crop.yaml</code></p> </li> <li> <p>Burn Scar Segmentation: <code>burn_scars.yaml</code></p> </li> <li> <p>Scene Classification: <code>eurosat.yaml</code></p> </li> </ul> <p>External examples available in Prithvi-EO-2.0</p> <ul> <li>Carbon Flux</li> <li>Landslide</li> <li>Multitemporal Crop</li> </ul>"},{"location":"extra_model_structures/","title":"Extra model structures","text":""},{"location":"extra_model_structures/#terratorch.models.model.Model","title":"<code>terratorch.models.model.Model</code>","text":"<p>               Bases: <code>ABC</code>, <code>Module</code></p> Source code in <code>terratorch/models/model.py</code> <pre><code>class Model(ABC, nn.Module):\n    def __init__(self, *args, **kwargs) -&gt; None:\n        super().__init__(*args, **kwargs)\n    @abstractmethod\n    def freeze_encoder(self):\n        pass\n\n    @abstractmethod\n    def freeze_decoder(self):\n        pass\n\n    @abstractmethod\n    def forward(self, *args, **kwargs) -&gt; ModelOutput:\n        pass\n</code></pre>"},{"location":"extra_model_structures/#terratorch.models.model.AuxiliaryHead","title":"<code>terratorch.models.model.AuxiliaryHead</code>  <code>dataclass</code>","text":"<p>Class containing all information to create auxiliary heads.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the head. Should match the name given to the auxiliary loss.</p> required <code>decoder</code> <code>str</code> <p>Name of the decoder class to be used.</p> required <code>decoder_args</code> <code>dict | None</code> <p>parameters to be passed to the decoder constructor. Parameters for the decoder should be prefixed with <code>decoder_</code>. Parameters for the head should be prefixed with <code>head_</code>.</p> required Source code in <code>terratorch/models/model.py</code> <pre><code>@dataclass\nclass AuxiliaryHead:\n    \"\"\"Class containing all information to create auxiliary heads.\n\n    Args:\n        name (str): Name of the head. Should match the name given to the auxiliary loss.\n        decoder (str): Name of the decoder class to be used.\n        decoder_args (dict | None): parameters to be passed to the decoder constructor.\n            Parameters for the decoder should be prefixed with `decoder_`.\n            Parameters for the head should be prefixed with `head_`.\n    \"\"\"\n\n    name: str\n    decoder: str\n    decoder_args: dict | None\n</code></pre>"},{"location":"extra_model_structures/#terratorch.models.model.ModelOutput","title":"<code>terratorch.models.model.ModelOutput</code>  <code>dataclass</code>","text":"Source code in <code>terratorch/models/model.py</code> <pre><code>@dataclass\nclass ModelOutput:\n    output: Tensor\n    auxiliary_heads: dict[str, Tensor] = None\n</code></pre>"},{"location":"generic_datamodules/","title":"Generic datamodules","text":""},{"location":"generic_datamodules/#terratorch.datamodules.generic_pixel_wise_data_module","title":"<code>terratorch.datamodules.generic_pixel_wise_data_module</code>","text":"<p>This module contains generic data modules for instantiation at runtime.</p>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoPixelwiseRegressionDataModule","title":"<code>GenericNonGeoPixelwiseRegressionDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoPixelwiseRegressionDataset</p> Source code in <code>terratorch/datamodules/generic_pixel_wise_data_module.py</code> <pre><code>class GenericNonGeoPixelwiseRegressionDataModule(NonGeoDataModule):\n    \"\"\"This is a generic datamodule class for instantiating data modules at runtime.\n    Composes several\n    [GenericNonGeoPixelwiseRegressionDataset][terratorch.datasets.GenericNonGeoPixelwiseRegressionDataset]\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        num_workers: int,\n        train_data_root: Path,\n        val_data_root: Path,\n        test_data_root: Path,\n        means: list[float] | str,\n        stds: list[float] | str,\n        predict_data_root: Path | None = None,\n        img_grep: str | None = \"*\",\n        label_grep: str | None = \"*\",\n        train_label_data_root: Path | None = None,\n        val_label_data_root: Path | None = None,\n        test_label_data_root: Path | None = None,\n        train_split: Path | None = None,\n        val_split: Path | None = None,\n        test_split: Path | None = None,\n        ignore_split_file_extensions: bool = True,\n        allow_substring_split_file: bool = True,\n        dataset_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n        output_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n        predict_dataset_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n        predict_output_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n        constant_scale: float = 1,\n        rgb_indices: list[int] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = None,\n        drop_last: bool = True,\n        pin_memory: bool = False,\n        check_stackability: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            batch_size (int): _description_\n            num_workers (int): _description_\n            train_data_root (Path): _description_\n            val_data_root (Path): _description_\n            test_data_root (Path): _description_\n            predict_data_root (Path): _description_\n            img_grep (str): _description_\n            label_grep (str): _description_\n            means (list[float]): _description_\n            stds (list[float]): _description_\n            train_label_data_root (Path | None, optional): _description_. Defaults to None.\n            val_label_data_root (Path | None, optional): _description_. Defaults to None.\n            test_label_data_root (Path | None, optional): _description_. Defaults to None.\n            train_split (Path | None, optional): _description_. Defaults to None.\n            val_split (Path | None, optional): _description_. Defaults to None.\n            test_split (Path | None, optional): _description_. Defaults to None.\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\". Defaults to True.\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None.\n            output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n                Naming must match that of dataset_bands. Defaults to None.\n            predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands\n                with this value at predict time.\n                Defaults to None, which does not overwrite.\n            predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands\n                with this value at predict time. Defaults to None, which does not overwrite.\n            constant_scale (float, optional): _description_. Defaults to 1.\n            rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n            train_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            val_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            test_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n            no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n            drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n            pin_memory (bool): If ``True``, the data loader will copy Tensors\n            into device/CUDA pinned memory before returning them. Defaults to False.\n            check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked.\n        \"\"\"\n        super().__init__(GenericNonGeoPixelwiseRegressionDataset, batch_size, num_workers, **kwargs)\n        self.img_grep = img_grep\n        self.label_grep = label_grep\n        self.train_root = train_data_root\n        self.val_root = val_data_root\n        self.test_root = test_data_root\n        self.predict_root = predict_data_root\n        self.train_split = train_split\n        self.val_split = val_split\n        self.test_split = test_split\n        self.ignore_split_file_extensions = ignore_split_file_extensions\n        self.allow_substring_split_file = allow_substring_split_file\n        self.drop_last = drop_last\n        self.pin_memory = pin_memory\n        self.expand_temporal_dimension = expand_temporal_dimension\n        self.reduce_zero_label = reduce_zero_label\n\n        self.train_label_data_root = train_label_data_root\n        self.val_label_data_root = val_label_data_root\n        self.test_label_data_root = test_label_data_root\n\n        self.constant_scale = constant_scale\n\n        self.dataset_bands = dataset_bands\n        self.predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands\n        self.predict_output_bands = predict_output_bands if predict_output_bands else output_bands\n        self.output_bands = output_bands\n        self.rgb_indices = rgb_indices\n\n        # self.aug = AugmentationSequential(\n        #     K.Normalize(means, stds),\n        #     data_keys=[\"image\"],\n        # )\n        means = load_from_file_or_attribute(means)\n        stds = load_from_file_or_attribute(stds)\n\n        self.aug = Normalize(means, stds)\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n\n        self.check_stackability = check_stackability\n\n    def setup(self, stage: str) -&gt; None:\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                self.train_root,\n                image_grep=self.img_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.train_label_data_root,\n                split=self.train_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.train_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                self.val_root,\n                image_grep=self.img_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.val_label_data_root,\n                split=self.val_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.val_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                self.test_root,\n                image_grep=self.img_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.test_label_data_root,\n                split=self.test_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n\n        if stage in [\"predict\"] and self.predict_root:\n            self.predict_dataset = self.dataset_class(\n                self.predict_root,\n                image_grep=self.img_grep,\n                dataset_bands=self.predict_dataset_bands,\n                output_bands=self.predict_output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n\n    def _dataloader_factory(self, split: str) -&gt; DataLoader[dict[str, Tensor]]:\n        \"\"\"Implement one or more PyTorch DataLoaders.\n\n        Args:\n            split: Either 'train', 'val', 'test', or 'predict'.\n\n        Returns:\n            A collection of data loaders specifying samples.\n\n        Raises:\n            MisconfigurationException: If :meth:`setup` does not define a\n                dataset or sampler, or if the dataset or sampler has length 0.\n        \"\"\"\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n\n        if self.check_stackability:\n            print(\"Checking stackability.\")\n            batch_size = check_dataset_stackability(dataset, batch_size)\n\n        return DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=split == \"train\",\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            drop_last=split == \"train\" and self.drop_last,\n            pin_memory=self.pin_memory,\n        )\n</code></pre>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoPixelwiseRegressionDataModule.__init__","title":"<code>__init__(batch_size, num_workers, train_data_root, val_data_root, test_data_root, means, stds, predict_data_root=None, img_grep='*', label_grep='*', train_label_data_root=None, val_label_data_root=None, test_label_data_root=None, train_split=None, val_split=None, test_split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, dataset_bands=None, output_bands=None, predict_dataset_bands=None, predict_output_bands=None, constant_scale=1, rgb_indices=None, train_transform=None, val_transform=None, test_transform=None, expand_temporal_dimension=False, reduce_zero_label=False, no_data_replace=None, no_label_replace=None, drop_last=True, pin_memory=False, check_stackability=True, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>description</p> required <code>num_workers</code> <code>int</code> <p>description</p> required <code>train_data_root</code> <code>Path</code> <p>description</p> required <code>val_data_root</code> <code>Path</code> <p>description</p> required <code>test_data_root</code> <code>Path</code> <p>description</p> required <code>predict_data_root</code> <code>Path</code> <p>description</p> <code>None</code> <code>img_grep</code> <code>str</code> <p>description</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>description</p> <code>'*'</code> <code>means</code> <code>list[float]</code> <p>description</p> required <code>stds</code> <code>list[float]</code> <p>description</p> required <code>train_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None.</p> <code>None</code> <code>predict_dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>predict_output_bands</code> <code>list[HLSBands | int] | None</code> <p>Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>description. Defaults to 1.</p> <code>1</code> <code>rgb_indices</code> <code>list[int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>val_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>test_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Drop the last batch if it is not complete. Defaults to True.</p> <code>True</code> <code>pin_memory</code> <code>bool</code> <p>If <code>True</code>, the data loader will copy Tensors</p> <code>False</code> <code>check_stackability</code> <code>bool</code> <p>Check if all the files in the dataset has the same size and can be stacked.</p> <code>True</code> Source code in <code>terratorch/datamodules/generic_pixel_wise_data_module.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    num_workers: int,\n    train_data_root: Path,\n    val_data_root: Path,\n    test_data_root: Path,\n    means: list[float] | str,\n    stds: list[float] | str,\n    predict_data_root: Path | None = None,\n    img_grep: str | None = \"*\",\n    label_grep: str | None = \"*\",\n    train_label_data_root: Path | None = None,\n    val_label_data_root: Path | None = None,\n    test_label_data_root: Path | None = None,\n    train_split: Path | None = None,\n    val_split: Path | None = None,\n    test_split: Path | None = None,\n    ignore_split_file_extensions: bool = True,\n    allow_substring_split_file: bool = True,\n    dataset_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n    output_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n    predict_dataset_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n    predict_output_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n    constant_scale: float = 1,\n    rgb_indices: list[int] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = None,\n    drop_last: bool = True,\n    pin_memory: bool = False,\n    check_stackability: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        batch_size (int): _description_\n        num_workers (int): _description_\n        train_data_root (Path): _description_\n        val_data_root (Path): _description_\n        test_data_root (Path): _description_\n        predict_data_root (Path): _description_\n        img_grep (str): _description_\n        label_grep (str): _description_\n        means (list[float]): _description_\n        stds (list[float]): _description_\n        train_label_data_root (Path | None, optional): _description_. Defaults to None.\n        val_label_data_root (Path | None, optional): _description_. Defaults to None.\n        test_label_data_root (Path | None, optional): _description_. Defaults to None.\n        train_split (Path | None, optional): _description_. Defaults to None.\n        val_split (Path | None, optional): _description_. Defaults to None.\n        test_split (Path | None, optional): _description_. Defaults to None.\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\". Defaults to True.\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None.\n        output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n            Naming must match that of dataset_bands. Defaults to None.\n        predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands\n            with this value at predict time.\n            Defaults to None, which does not overwrite.\n        predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands\n            with this value at predict time. Defaults to None, which does not overwrite.\n        constant_scale (float, optional): _description_. Defaults to 1.\n        rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n        train_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        val_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        test_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n        no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n        drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n        pin_memory (bool): If ``True``, the data loader will copy Tensors\n        into device/CUDA pinned memory before returning them. Defaults to False.\n        check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked.\n    \"\"\"\n    super().__init__(GenericNonGeoPixelwiseRegressionDataset, batch_size, num_workers, **kwargs)\n    self.img_grep = img_grep\n    self.label_grep = label_grep\n    self.train_root = train_data_root\n    self.val_root = val_data_root\n    self.test_root = test_data_root\n    self.predict_root = predict_data_root\n    self.train_split = train_split\n    self.val_split = val_split\n    self.test_split = test_split\n    self.ignore_split_file_extensions = ignore_split_file_extensions\n    self.allow_substring_split_file = allow_substring_split_file\n    self.drop_last = drop_last\n    self.pin_memory = pin_memory\n    self.expand_temporal_dimension = expand_temporal_dimension\n    self.reduce_zero_label = reduce_zero_label\n\n    self.train_label_data_root = train_label_data_root\n    self.val_label_data_root = val_label_data_root\n    self.test_label_data_root = test_label_data_root\n\n    self.constant_scale = constant_scale\n\n    self.dataset_bands = dataset_bands\n    self.predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands\n    self.predict_output_bands = predict_output_bands if predict_output_bands else output_bands\n    self.output_bands = output_bands\n    self.rgb_indices = rgb_indices\n\n    # self.aug = AugmentationSequential(\n    #     K.Normalize(means, stds),\n    #     data_keys=[\"image\"],\n    # )\n    means = load_from_file_or_attribute(means)\n    stds = load_from_file_or_attribute(stds)\n\n    self.aug = Normalize(means, stds)\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n\n    self.check_stackability = check_stackability\n</code></pre>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoSegmentationDataModule","title":"<code>GenericNonGeoSegmentationDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoSegmentationDatasets</p> Source code in <code>terratorch/datamodules/generic_pixel_wise_data_module.py</code> <pre><code>class GenericNonGeoSegmentationDataModule(NonGeoDataModule):\n    \"\"\"\n    This is a generic datamodule class for instantiating data modules at runtime.\n    Composes several [GenericNonGeoSegmentationDatasets][terratorch.datasets.GenericNonGeoSegmentationDataset]\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        num_workers: int,\n        train_data_root: Path,\n        val_data_root: Path,\n        test_data_root: Path,\n        img_grep: str,\n        label_grep: str,\n        means: list[float] | str,\n        stds: list[float] | str,\n        num_classes: int,\n        predict_data_root: Path | None = None,\n        train_label_data_root: Path | None = None,\n        val_label_data_root: Path | None = None,\n        test_label_data_root: Path | None = None,\n        train_split: Path | None = None,\n        val_split: Path | None = None,\n        test_split: Path | None = None,\n        ignore_split_file_extensions: bool = True,\n        allow_substring_split_file: bool = True,\n        dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        predict_dataset_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n        predict_output_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n        constant_scale: float = 1,\n        rgb_indices: list[int] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = None,\n        drop_last: bool = True,\n        pin_memory: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            batch_size (int): _description_\n            num_workers (int): _description_\n            train_data_root (Path): _description_\n            val_data_root (Path): _description_\n            test_data_root (Path): _description_\n            predict_data_root (Path): _description_\n            img_grep (str): _description_\n            label_grep (str): _description_\n            means (list[float]): _description_\n            stds (list[float]): _description_\n            num_classes (int): _description_\n            train_label_data_root (Path | None, optional): _description_. Defaults to None.\n            val_label_data_root (Path | None, optional): _description_. Defaults to None.\n            test_label_data_root (Path | None, optional): _description_. Defaults to None.\n            train_split (Path | None, optional): _description_. Defaults to None.\n            val_split (Path | None, optional): _description_. Defaults to None.\n            test_split (Path | None, optional): _description_. Defaults to None.\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\". Defaults to True.\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None.\n            output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n                Naming must match that of dataset_bands. Defaults to None.\n            predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands\n                with this value at predict time.\n                Defaults to None, which does not overwrite.\n            predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands\n                with this value at predict time. Defaults to None, which does not overwrite.\n            constant_scale (float, optional): _description_. Defaults to 1.\n            rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n            train_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            val_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            test_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n            no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n            drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n            pin_memory (bool): If ``True``, the data loader will copy Tensors\n            into device/CUDA pinned memory before returning them. Defaults to False.\n        \"\"\"\n        super().__init__(GenericNonGeoSegmentationDataset, batch_size, num_workers, **kwargs)\n        self.num_classes = num_classes\n        self.img_grep = img_grep\n        self.label_grep = label_grep\n        self.train_root = train_data_root\n        self.val_root = val_data_root\n        self.test_root = test_data_root\n        self.predict_root = predict_data_root\n        self.train_split = train_split\n        self.val_split = val_split\n        self.test_split = test_split\n        self.ignore_split_file_extensions = ignore_split_file_extensions\n        self.allow_substring_split_file = allow_substring_split_file\n        self.constant_scale = constant_scale\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.drop_last = drop_last\n        self.pin_memory = pin_memory\n\n        self.train_label_data_root = train_label_data_root\n        self.val_label_data_root = val_label_data_root\n        self.test_label_data_root = test_label_data_root\n\n        self.dataset_bands = dataset_bands\n        self.predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands\n        self.predict_output_bands = predict_output_bands if predict_output_bands else output_bands\n        self.output_bands = output_bands\n        self.rgb_indices = rgb_indices\n        self.expand_temporal_dimension = expand_temporal_dimension\n        self.reduce_zero_label = reduce_zero_label\n\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n\n        # self.aug = AugmentationSequential(\n        #     K.Normalize(means, stds),\n        #     data_keys=[\"image\"],\n        # )\n        means = load_from_file_or_attribute(means)\n        stds = load_from_file_or_attribute(stds)\n\n        self.aug = Normalize(means, stds)\n\n        # self.aug = Normalize(means, stds)\n        # self.collate_fn = collate_fn_list_dicts\n\n    def setup(self, stage: str) -&gt; None:\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                self.train_root,\n                self.num_classes,\n                image_grep=self.img_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.train_label_data_root,\n                split=self.train_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.train_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                self.val_root,\n                self.num_classes,\n                image_grep=self.img_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.val_label_data_root,\n                split=self.val_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.val_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                self.test_root,\n                self.num_classes,\n                image_grep=self.img_grep,\n                label_grep=self.label_grep,\n                label_data_root=self.test_label_data_root,\n                split=self.test_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n        if stage in [\"predict\"] and self.predict_root:\n            self.predict_dataset = self.dataset_class(\n                self.predict_root,\n                self.num_classes,\n                dataset_bands=self.predict_dataset_bands,\n                output_bands=self.predict_output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                no_label_replace=self.no_label_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n                reduce_zero_label=self.reduce_zero_label,\n            )\n\n    def _dataloader_factory(self, split: str) -&gt; DataLoader[dict[str, Tensor]]:\n        \"\"\"Implement one or more PyTorch DataLoaders.\n\n        Args:\n            split: Either 'train', 'val', 'test', or 'predict'.\n\n        Returns:\n            A collection of data loaders specifying samples.\n\n        Raises:\n            MisconfigurationException: If :meth:`setup` does not define a\n                dataset or sampler, or if the dataset or sampler has length 0.\n        \"\"\"\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n\n        batch_size = check_dataset_stackability(dataset, batch_size)\n\n        return DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=split == \"train\",\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            drop_last=split == \"train\" and self.drop_last,\n            pin_memory=self.pin_memory,\n        )\n</code></pre>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoSegmentationDataModule.__init__","title":"<code>__init__(batch_size, num_workers, train_data_root, val_data_root, test_data_root, img_grep, label_grep, means, stds, num_classes, predict_data_root=None, train_label_data_root=None, val_label_data_root=None, test_label_data_root=None, train_split=None, val_split=None, test_split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, dataset_bands=None, output_bands=None, predict_dataset_bands=None, predict_output_bands=None, constant_scale=1, rgb_indices=None, train_transform=None, val_transform=None, test_transform=None, expand_temporal_dimension=False, reduce_zero_label=False, no_data_replace=None, no_label_replace=None, drop_last=True, pin_memory=False, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>description</p> required <code>num_workers</code> <code>int</code> <p>description</p> required <code>train_data_root</code> <code>Path</code> <p>description</p> required <code>val_data_root</code> <code>Path</code> <p>description</p> required <code>test_data_root</code> <code>Path</code> <p>description</p> required <code>predict_data_root</code> <code>Path</code> <p>description</p> <code>None</code> <code>img_grep</code> <code>str</code> <p>description</p> required <code>label_grep</code> <code>str</code> <p>description</p> required <code>means</code> <code>list[float]</code> <p>description</p> required <code>stds</code> <code>list[float]</code> <p>description</p> required <code>num_classes</code> <code>int</code> <p>description</p> required <code>train_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_label_data_root</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset. Naming must match that of dataset_bands. Defaults to None.</p> <code>None</code> <code>predict_dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Overwrites dataset_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>predict_output_bands</code> <code>list[HLSBands | int] | None</code> <p>Overwrites output_bands with this value at predict time. Defaults to None, which does not overwrite.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>description. Defaults to 1.</p> <code>1</code> <code>rgb_indices</code> <code>list[int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>val_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>test_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Drop the last batch if it is not complete. Defaults to True.</p> <code>True</code> <code>pin_memory</code> <code>bool</code> <p>If <code>True</code>, the data loader will copy Tensors</p> <code>False</code> Source code in <code>terratorch/datamodules/generic_pixel_wise_data_module.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    num_workers: int,\n    train_data_root: Path,\n    val_data_root: Path,\n    test_data_root: Path,\n    img_grep: str,\n    label_grep: str,\n    means: list[float] | str,\n    stds: list[float] | str,\n    num_classes: int,\n    predict_data_root: Path | None = None,\n    train_label_data_root: Path | None = None,\n    val_label_data_root: Path | None = None,\n    test_label_data_root: Path | None = None,\n    train_split: Path | None = None,\n    val_split: Path | None = None,\n    test_split: Path | None = None,\n    ignore_split_file_extensions: bool = True,\n    allow_substring_split_file: bool = True,\n    dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    predict_dataset_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n    predict_output_bands: list[HLSBands | int | tuple[int, int] | str ] | None = None,\n    constant_scale: float = 1,\n    rgb_indices: list[int] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = None,\n    drop_last: bool = True,\n    pin_memory: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        batch_size (int): _description_\n        num_workers (int): _description_\n        train_data_root (Path): _description_\n        val_data_root (Path): _description_\n        test_data_root (Path): _description_\n        predict_data_root (Path): _description_\n        img_grep (str): _description_\n        label_grep (str): _description_\n        means (list[float]): _description_\n        stds (list[float]): _description_\n        num_classes (int): _description_\n        train_label_data_root (Path | None, optional): _description_. Defaults to None.\n        val_label_data_root (Path | None, optional): _description_. Defaults to None.\n        test_label_data_root (Path | None, optional): _description_. Defaults to None.\n        train_split (Path | None, optional): _description_. Defaults to None.\n        val_split (Path | None, optional): _description_. Defaults to None.\n        test_split (Path | None, optional): _description_. Defaults to None.\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\". Defaults to True.\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        dataset_bands (list[HLSBands | int] | None): Bands present in the dataset. Defaults to None.\n        output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n            Naming must match that of dataset_bands. Defaults to None.\n        predict_dataset_bands (list[HLSBands | int] | None): Overwrites dataset_bands\n            with this value at predict time.\n            Defaults to None, which does not overwrite.\n        predict_output_bands (list[HLSBands | int] | None): Overwrites output_bands\n            with this value at predict time. Defaults to None, which does not overwrite.\n        constant_scale (float, optional): _description_. Defaults to 1.\n        rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n        train_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        val_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        test_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n        no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n        drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n        pin_memory (bool): If ``True``, the data loader will copy Tensors\n        into device/CUDA pinned memory before returning them. Defaults to False.\n    \"\"\"\n    super().__init__(GenericNonGeoSegmentationDataset, batch_size, num_workers, **kwargs)\n    self.num_classes = num_classes\n    self.img_grep = img_grep\n    self.label_grep = label_grep\n    self.train_root = train_data_root\n    self.val_root = val_data_root\n    self.test_root = test_data_root\n    self.predict_root = predict_data_root\n    self.train_split = train_split\n    self.val_split = val_split\n    self.test_split = test_split\n    self.ignore_split_file_extensions = ignore_split_file_extensions\n    self.allow_substring_split_file = allow_substring_split_file\n    self.constant_scale = constant_scale\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.drop_last = drop_last\n    self.pin_memory = pin_memory\n\n    self.train_label_data_root = train_label_data_root\n    self.val_label_data_root = val_label_data_root\n    self.test_label_data_root = test_label_data_root\n\n    self.dataset_bands = dataset_bands\n    self.predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands\n    self.predict_output_bands = predict_output_bands if predict_output_bands else output_bands\n    self.output_bands = output_bands\n    self.rgb_indices = rgb_indices\n    self.expand_temporal_dimension = expand_temporal_dimension\n    self.reduce_zero_label = reduce_zero_label\n\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n\n    # self.aug = AugmentationSequential(\n    #     K.Normalize(means, stds),\n    #     data_keys=[\"image\"],\n    # )\n    means = load_from_file_or_attribute(means)\n    stds = load_from_file_or_attribute(stds)\n\n    self.aug = Normalize(means, stds)\n</code></pre>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_scalar_label_data_module","title":"<code>terratorch.datamodules.generic_scalar_label_data_module</code>","text":"<p>This module contains generic data modules for instantiation at runtime.</p>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_scalar_label_data_module.GenericNonGeoClassificationDataModule","title":"<code>GenericNonGeoClassificationDataModule</code>","text":"<p>               Bases: <code>NonGeoDataModule</code></p> <p>This is a generic datamodule class for instantiating data modules at runtime. Composes several GenericNonGeoClassificationDatasets</p> Source code in <code>terratorch/datamodules/generic_scalar_label_data_module.py</code> <pre><code>class GenericNonGeoClassificationDataModule(NonGeoDataModule):\n    \"\"\"\n    This is a generic datamodule class for instantiating data modules at runtime.\n    Composes several [GenericNonGeoClassificationDatasets][terratorch.datasets.GenericNonGeoClassificationDataset]\n    \"\"\"\n\n    def __init__(\n        self,\n        batch_size: int,\n        num_workers: int,\n        train_data_root: Path,\n        val_data_root: Path,\n        test_data_root: Path,\n        means: list[float] | str,\n        stds: list[float] | str,\n        num_classes: int,\n        predict_data_root: Path | None = None,\n        train_split: Path | None = None,\n        val_split: Path | None = None,\n        test_split: Path | None = None,\n        ignore_split_file_extensions: bool = True,\n        allow_substring_split_file: bool = True,\n        dataset_bands: list[HLSBands | int] | None = None,\n        predict_dataset_bands: list[HLSBands | int] | None = None,\n        output_bands: list[HLSBands | int] | None = None,\n        constant_scale: float = 1,\n        rgb_indices: list[int] | None = None,\n        train_transform: A.Compose | None | list[A.BasicTransform] = None,\n        val_transform: A.Compose | None | list[A.BasicTransform] = None,\n        test_transform: A.Compose | None | list[A.BasicTransform] = None,\n        expand_temporal_dimension: bool = False,\n        no_data_replace: float = 0,\n        drop_last: bool = True,\n        check_stackability: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            batch_size (int): _description_\n            num_workers (int): _description_\n            train_data_root (Path): _description_\n            val_data_root (Path): _description_\n            test_data_root (Path): _description_\n            means (list[float]): _description_\n            stds (list[float]): _description_\n            num_classes (int): _description_\n            predict_data_root (Path): _description_\n            train_split (Path | None, optional): _description_. Defaults to None.\n            val_split (Path | None, optional): _description_. Defaults to None.\n            test_split (Path | None, optional): _description_. Defaults to None.\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\".\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None.\n            predict_dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None.\n            output_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None.\n            constant_scale (float, optional): _description_. Defaults to 1.\n            rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n            train_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            val_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            test_transform (Albumentations.Compose | None): Albumentations transform\n                to be applied to the train dataset.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float): Replace nan values in input images with this value. Defaults to 0.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n            drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n            check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked.\n        \"\"\"\n        super().__init__(GenericNonGeoClassificationDataset, batch_size, num_workers, **kwargs)\n        self.num_classes = num_classes\n        self.train_root = train_data_root\n        self.val_root = val_data_root\n        self.test_root = test_data_root\n        self.predict_root = predict_data_root\n        self.train_split = train_split\n        self.val_split = val_split\n        self.test_split = test_split\n        self.ignore_split_file_extensions = ignore_split_file_extensions\n        self.allow_substring_split_file = allow_substring_split_file\n        self.constant_scale = constant_scale\n        self.no_data_replace = no_data_replace\n        self.drop_last = drop_last\n\n        self.dataset_bands = dataset_bands\n        self.predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands\n        self.output_bands = output_bands\n        self.rgb_indices = rgb_indices\n        self.expand_temporal_dimension = expand_temporal_dimension\n\n        self.train_transform = wrap_in_compose_is_list(train_transform)\n        self.val_transform = wrap_in_compose_is_list(val_transform)\n        self.test_transform = wrap_in_compose_is_list(test_transform)\n\n        # self.aug = AugmentationSequential(\n        #     K.Normalize(means, stds),\n        #     data_keys=[\"image\"],\n        # )\n\n        means = load_from_file_or_attribute(means)\n        stds = load_from_file_or_attribute(stds)\n\n        self.aug = Normalize(means, stds)\n\n        # self.aug = Normalize(means, stds)\n        # self.collate_fn = collate_fn_list_dicts\n\n        self.check_stackability = check_stackability\n\n    def setup(self, stage: str) -&gt; None:\n        if stage in [\"fit\"]:\n            self.train_dataset = self.dataset_class(\n                self.train_root,\n                self.num_classes,\n                split=self.train_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.train_transform,\n                no_data_replace=self.no_data_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n            )\n        if stage in [\"fit\", \"validate\"]:\n            self.val_dataset = self.dataset_class(\n                self.val_root,\n                self.num_classes,\n                split=self.val_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.val_transform,\n                no_data_replace=self.no_data_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n            )\n        if stage in [\"test\"]:\n            self.test_dataset = self.dataset_class(\n                self.test_root,\n                self.num_classes,\n                split=self.test_split,\n                ignore_split_file_extensions=self.ignore_split_file_extensions,\n                allow_substring_split_file=self.allow_substring_split_file,\n                dataset_bands=self.dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n            )\n        if stage in [\"predict\"] and self.predict_root:\n            self.predict_dataset = self.dataset_class(\n                self.predict_root,\n                self.num_classes,\n                dataset_bands=self.predict_dataset_bands,\n                output_bands=self.output_bands,\n                constant_scale=self.constant_scale,\n                rgb_indices=self.rgb_indices,\n                transform=self.test_transform,\n                no_data_replace=self.no_data_replace,\n                expand_temporal_dimension=self.expand_temporal_dimension,\n            )\n\n    def _dataloader_factory(self, split: str) -&gt; DataLoader[dict[str, Tensor]]:\n        \"\"\"Implement one or more PyTorch DataLoaders.\n\n        Args:\n            split: Either 'train', 'val', 'test', or 'predict'.\n\n        Returns:\n            A collection of data loaders specifying samples.\n\n        Raises:\n            MisconfigurationException: If :meth:`setup` does not define a\n                dataset or sampler, or if the dataset or sampler has length 0.\n        \"\"\"\n        dataset = self._valid_attribute(f\"{split}_dataset\", \"dataset\")\n        batch_size = self._valid_attribute(f\"{split}_batch_size\", \"batch_size\")\n\n        if self.check_stackability:\n            print(\"Checking stackability.\")\n            batch_size = check_dataset_stackability(dataset, batch_size)\n\n        return DataLoader(\n            dataset=dataset,\n            batch_size=batch_size,\n            shuffle=split == \"train\",\n            num_workers=self.num_workers,\n            collate_fn=self.collate_fn,\n            drop_last=split == \"train\" and self.drop_last,\n        )\n</code></pre>"},{"location":"generic_datamodules/#terratorch.datamodules.generic_scalar_label_data_module.GenericNonGeoClassificationDataModule.__init__","title":"<code>__init__(batch_size, num_workers, train_data_root, val_data_root, test_data_root, means, stds, num_classes, predict_data_root=None, train_split=None, val_split=None, test_split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, dataset_bands=None, predict_dataset_bands=None, output_bands=None, constant_scale=1, rgb_indices=None, train_transform=None, val_transform=None, test_transform=None, expand_temporal_dimension=False, no_data_replace=0, drop_last=True, check_stackability=True, **kwargs)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>description</p> required <code>num_workers</code> <code>int</code> <p>description</p> required <code>train_data_root</code> <code>Path</code> <p>description</p> required <code>val_data_root</code> <code>Path</code> <p>description</p> required <code>test_data_root</code> <code>Path</code> <p>description</p> required <code>means</code> <code>list[float]</code> <p>description</p> required <code>stds</code> <code>list[float]</code> <p>description</p> required <code>num_classes</code> <code>int</code> <p>description</p> required <code>predict_data_root</code> <code>Path</code> <p>description</p> <code>None</code> <code>train_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>val_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>test_split</code> <code>Path | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\".</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>predict_dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>description. Defaults to 1.</p> <code>1</code> <code>rgb_indices</code> <code>list[int] | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>val_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>test_transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied to the train dataset. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float</code> <p>Replace nan values in input images with this value. Defaults to 0.</p> <code>0</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>drop_last</code> <code>bool</code> <p>Drop the last batch if it is not complete. Defaults to True.</p> <code>True</code> <code>check_stackability</code> <code>bool</code> <p>Check if all the files in the dataset has the same size and can be stacked.</p> <code>True</code> Source code in <code>terratorch/datamodules/generic_scalar_label_data_module.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    num_workers: int,\n    train_data_root: Path,\n    val_data_root: Path,\n    test_data_root: Path,\n    means: list[float] | str,\n    stds: list[float] | str,\n    num_classes: int,\n    predict_data_root: Path | None = None,\n    train_split: Path | None = None,\n    val_split: Path | None = None,\n    test_split: Path | None = None,\n    ignore_split_file_extensions: bool = True,\n    allow_substring_split_file: bool = True,\n    dataset_bands: list[HLSBands | int] | None = None,\n    predict_dataset_bands: list[HLSBands | int] | None = None,\n    output_bands: list[HLSBands | int] | None = None,\n    constant_scale: float = 1,\n    rgb_indices: list[int] | None = None,\n    train_transform: A.Compose | None | list[A.BasicTransform] = None,\n    val_transform: A.Compose | None | list[A.BasicTransform] = None,\n    test_transform: A.Compose | None | list[A.BasicTransform] = None,\n    expand_temporal_dimension: bool = False,\n    no_data_replace: float = 0,\n    drop_last: bool = True,\n    check_stackability: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        batch_size (int): _description_\n        num_workers (int): _description_\n        train_data_root (Path): _description_\n        val_data_root (Path): _description_\n        test_data_root (Path): _description_\n        means (list[float]): _description_\n        stds (list[float]): _description_\n        num_classes (int): _description_\n        predict_data_root (Path): _description_\n        train_split (Path | None, optional): _description_. Defaults to None.\n        val_split (Path | None, optional): _description_. Defaults to None.\n        test_split (Path | None, optional): _description_. Defaults to None.\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\".\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None.\n        predict_dataset_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None.\n        output_bands (list[HLSBands | int] | None, optional): _description_. Defaults to None.\n        constant_scale (float, optional): _description_. Defaults to 1.\n        rgb_indices (list[int] | None, optional): _description_. Defaults to None.\n        train_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        val_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        test_transform (Albumentations.Compose | None): Albumentations transform\n            to be applied to the train dataset.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float): Replace nan values in input images with this value. Defaults to 0.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n        drop_last (bool): Drop the last batch if it is not complete. Defaults to True.\n        check_stackability (bool): Check if all the files in the dataset has the same size and can be stacked.\n    \"\"\"\n    super().__init__(GenericNonGeoClassificationDataset, batch_size, num_workers, **kwargs)\n    self.num_classes = num_classes\n    self.train_root = train_data_root\n    self.val_root = val_data_root\n    self.test_root = test_data_root\n    self.predict_root = predict_data_root\n    self.train_split = train_split\n    self.val_split = val_split\n    self.test_split = test_split\n    self.ignore_split_file_extensions = ignore_split_file_extensions\n    self.allow_substring_split_file = allow_substring_split_file\n    self.constant_scale = constant_scale\n    self.no_data_replace = no_data_replace\n    self.drop_last = drop_last\n\n    self.dataset_bands = dataset_bands\n    self.predict_dataset_bands = predict_dataset_bands if predict_dataset_bands else dataset_bands\n    self.output_bands = output_bands\n    self.rgb_indices = rgb_indices\n    self.expand_temporal_dimension = expand_temporal_dimension\n\n    self.train_transform = wrap_in_compose_is_list(train_transform)\n    self.val_transform = wrap_in_compose_is_list(val_transform)\n    self.test_transform = wrap_in_compose_is_list(test_transform)\n\n    # self.aug = AugmentationSequential(\n    #     K.Normalize(means, stds),\n    #     data_keys=[\"image\"],\n    # )\n\n    means = load_from_file_or_attribute(means)\n    stds = load_from_file_or_attribute(stds)\n\n    self.aug = Normalize(means, stds)\n\n    # self.aug = Normalize(means, stds)\n    # self.collate_fn = collate_fn_list_dicts\n\n    self.check_stackability = check_stackability\n</code></pre>"},{"location":"generic_datasets/","title":"Generic datasets","text":""},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset","title":"<code>terratorch.datasets.generic_pixel_wise_dataset</code>","text":"<p>Module containing generic dataset classes</p>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoPixelwiseRegressionDataset","title":"<code>GenericNonGeoPixelwiseRegressionDataset</code>","text":"<p>               Bases: <code>GenericPixelWiseDataset</code></p> <p>GenericNonGeoPixelwiseRegressionDataset</p> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>class GenericNonGeoPixelwiseRegressionDataset(GenericPixelWiseDataset):\n    \"\"\"GenericNonGeoPixelwiseRegressionDataset\"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        label_data_root: Path | None = None,\n        image_grep: str | None = \"*\",\n        label_grep: str | None = \"*\",\n        split: Path | None = None,\n        ignore_split_file_extensions: bool = True,\n        allow_substring_split_file: bool = True,\n        rgb_indices: list[int] | None = None,\n        dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        constant_scale: float = 1,\n        transform: A.Compose | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = None,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (Path): Path to data root directory\n            label_data_root (Path, optional): Path to data root directory with labels.\n                If not specified, will use the same as for images.\n            image_grep (str, optional): Regular expression appended to data_root to find input images.\n                Defaults to \"*\".\n            label_grep (str, optional): Regular expression appended to data_root to find ground truth masks.\n                Defaults to \"*\".\n            split (Path, optional): Path to file containing files to be used for this split.\n                The file should be a new-line separated prefixes contained in the desired files.\n                Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\". Defaults to True.\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            dataset_bands (list[HLSBands | int] | None): Bands present in the dataset.\n            output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n            constant_scale (float): Factor to multiply image values by. Defaults to 1.\n            transform (Albumentations.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n            no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n        \"\"\"\n        super().__init__(\n            data_root,\n            label_data_root=label_data_root,\n            image_grep=image_grep,\n            label_grep=label_grep,\n            split=split,\n            ignore_split_file_extensions=ignore_split_file_extensions,\n            allow_substring_split_file=allow_substring_split_file,\n            rgb_indices=rgb_indices,\n            dataset_bands=dataset_bands,\n            output_bands=output_bands,\n            constant_scale=constant_scale,\n            transform=transform,\n            no_data_replace=no_data_replace,\n            no_label_replace=no_label_replace,\n            expand_temporal_dimension=expand_temporal_dimension,\n            reduce_zero_label=reduce_zero_label,\n        )\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        item = super().__getitem__(index)\n        item[\"mask\"] = item[\"mask\"].float()\n        return item\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__`\n            suptitle (str|None): optional string to use as a suptitle\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n\n        .. versionadded:: 0.2\n        \"\"\"\n        image = sample[\"image\"]\n        if len(image.shape) == 5:\n            return\n        if isinstance(image, Tensor):\n            image = image.numpy()\n        image = image.take(self.rgb_indices, axis=0)\n        image = np.transpose(image, (1, 2, 0))\n        image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n        image = np.clip(image, 0, 1)\n\n        label_mask = sample[\"mask\"]\n        if isinstance(label_mask, Tensor):\n            label_mask = label_mask.numpy()\n\n        showing_predictions = \"prediction\" in sample\n        if showing_predictions:\n            prediction_mask = sample[\"prediction\"]\n            if isinstance(prediction_mask, Tensor):\n                prediction_mask = prediction_mask.numpy()\n\n        return self._plot_sample(\n            image,\n            label_mask,\n            prediction=prediction_mask if showing_predictions else None,\n            suptitle=suptitle,\n        )\n\n    @staticmethod\n    def _plot_sample(image, label, prediction=None, suptitle=None):\n        num_images = 4 if prediction is not None else 3\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 10), layout=\"compressed\")\n\n        norm = mpl.colors.Normalize(vmin=label.min(), vmax=label.max())\n        ax[0].axis(\"off\")\n        ax[0].title.set_text(\"Image\")\n        ax[0].imshow(image)\n\n        ax[1].axis(\"off\")\n        ax[1].title.set_text(\"Ground Truth Mask\")\n        ax[1].imshow(label, cmap=\"Greens\", norm=norm)\n\n        ax[2].axis(\"off\")\n        ax[2].title.set_text(\"GT Mask on Image\")\n        ax[2].imshow(image)\n        ax[2].imshow(label, cmap=\"Greens\", alpha=0.3, norm=norm)\n        # ax[2].legend()\n\n        if prediction is not None:\n            ax[3].title.set_text(\"Predicted Mask\")\n            ax[3].imshow(prediction, cmap=\"Greens\", norm=norm)\n\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n        return fig\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoPixelwiseRegressionDataset.__init__","title":"<code>__init__(data_root, label_data_root=None, image_grep='*', label_grep='*', split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, constant_scale=1, transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=False, reduce_zero_label=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with labels. If not specified, will use the same as for images.</p> <code>None</code> <code>image_grep</code> <code>str</code> <p>Regular expression appended to data_root to find input images. Defaults to \"*\".</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to data_root to find ground truth masks. Defaults to \"*\".</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    label_data_root: Path | None = None,\n    image_grep: str | None = \"*\",\n    label_grep: str | None = \"*\",\n    split: Path | None = None,\n    ignore_split_file_extensions: bool = True,\n    allow_substring_split_file: bool = True,\n    rgb_indices: list[int] | None = None,\n    dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    constant_scale: float = 1,\n    transform: A.Compose | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = None,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (Path): Path to data root directory\n        label_data_root (Path, optional): Path to data root directory with labels.\n            If not specified, will use the same as for images.\n        image_grep (str, optional): Regular expression appended to data_root to find input images.\n            Defaults to \"*\".\n        label_grep (str, optional): Regular expression appended to data_root to find ground truth masks.\n            Defaults to \"*\".\n        split (Path, optional): Path to file containing files to be used for this split.\n            The file should be a new-line separated prefixes contained in the desired files.\n            Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\". Defaults to True.\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        dataset_bands (list[HLSBands | int] | None): Bands present in the dataset.\n        output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n        constant_scale (float): Factor to multiply image values by. Defaults to 1.\n        transform (Albumentations.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n        no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n    \"\"\"\n    super().__init__(\n        data_root,\n        label_data_root=label_data_root,\n        image_grep=image_grep,\n        label_grep=label_grep,\n        split=split,\n        ignore_split_file_extensions=ignore_split_file_extensions,\n        allow_substring_split_file=allow_substring_split_file,\n        rgb_indices=rgb_indices,\n        dataset_bands=dataset_bands,\n        output_bands=output_bands,\n        constant_scale=constant_scale,\n        transform=transform,\n        no_data_replace=no_data_replace,\n        no_label_replace=no_label_replace,\n        expand_temporal_dimension=expand_temporal_dimension,\n        reduce_zero_label=reduce_zero_label,\n    )\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoPixelwiseRegressionDataset.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> <p>.. versionadded:: 0.2</p> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample (dict[str, Tensor]): a sample returned by :meth:`__getitem__`\n        suptitle (str|None): optional string to use as a suptitle\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n\n    .. versionadded:: 0.2\n    \"\"\"\n    image = sample[\"image\"]\n    if len(image.shape) == 5:\n        return\n    if isinstance(image, Tensor):\n        image = image.numpy()\n    image = image.take(self.rgb_indices, axis=0)\n    image = np.transpose(image, (1, 2, 0))\n    image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n    image = np.clip(image, 0, 1)\n\n    label_mask = sample[\"mask\"]\n    if isinstance(label_mask, Tensor):\n        label_mask = label_mask.numpy()\n\n    showing_predictions = \"prediction\" in sample\n    if showing_predictions:\n        prediction_mask = sample[\"prediction\"]\n        if isinstance(prediction_mask, Tensor):\n            prediction_mask = prediction_mask.numpy()\n\n    return self._plot_sample(\n        image,\n        label_mask,\n        prediction=prediction_mask if showing_predictions else None,\n        suptitle=suptitle,\n    )\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoSegmentationDataset","title":"<code>GenericNonGeoSegmentationDataset</code>","text":"<p>               Bases: <code>GenericPixelWiseDataset</code></p> <p>GenericNonGeoSegmentationDataset</p> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>class GenericNonGeoSegmentationDataset(GenericPixelWiseDataset):\n    \"\"\"GenericNonGeoSegmentationDataset\"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        num_classes: int,\n        label_data_root: Path | None = None,\n        image_grep: str | None = \"*\",\n        label_grep: str | None = \"*\",\n        split: Path | None = None,\n        ignore_split_file_extensions: bool = True,\n        allow_substring_split_file: bool = True,\n        rgb_indices: list[str] | None = None,\n        dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        class_names: list[str] | None = None,\n        constant_scale: float = 1,\n        transform: A.Compose | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = None,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (Path): Path to data root directory\n            num_classes (int): Number of classes in the dataset\n            label_data_root (Path, optional): Path to data root directory with labels.\n                If not specified, will use the same as for images.\n            image_grep (str, optional): Regular expression appended to data_root to find input images.\n                Defaults to \"*\".\n            label_grep (str, optional): Regular expression appended to data_root to find ground truth masks.\n                Defaults to \"*\".\n            split (Path, optional): Path to file containing files to be used for this split.\n                The file should be a new-line separated prefixes contained in the desired files.\n                Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\". Defaults to True\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            dataset_bands (list[HLSBands | int] | None): Bands present in the dataset.\n            output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n            class_names (list[str], optional): Class names. Defaults to None.\n            constant_scale (float): Factor to multiply image values by. Defaults to 1.\n            transform (Albumentations.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n            no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n        \"\"\"\n        super().__init__(\n            data_root,\n            label_data_root=label_data_root,\n            image_grep=image_grep,\n            label_grep=label_grep,\n            split=split,\n            ignore_split_file_extensions=ignore_split_file_extensions,\n            allow_substring_split_file=allow_substring_split_file,\n            rgb_indices=rgb_indices,\n            dataset_bands=dataset_bands,\n            output_bands=output_bands,\n            constant_scale=constant_scale,\n            transform=transform,\n            no_data_replace=no_data_replace,\n            no_label_replace=no_label_replace,\n            expand_temporal_dimension=expand_temporal_dimension,\n            reduce_zero_label=reduce_zero_label,\n        )\n        self.num_classes = num_classes\n        self.class_names = class_names\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        item = super().__getitem__(index)\n        item[\"mask\"] = item[\"mask\"].long()\n        return item\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n        \"\"\"Plot a sample from the dataset.\n\n        Args:\n            sample: a sample returned by :meth:`__getitem__`\n            suptitle: optional string to use as a suptitle\n\n        Returns:\n            a matplotlib Figure with the rendered sample\n\n        .. versionadded:: 0.2\n        \"\"\"\n        image = sample[\"image\"]\n        if len(image.shape) == 5:\n            return\n        if isinstance(image, Tensor):\n            image = image.numpy()\n        image = image.take(self.rgb_indices, axis=0)\n        image = np.transpose(image, (1, 2, 0))\n        image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n        image = np.clip(image, 0, 1)\n\n        label_mask = sample[\"mask\"]\n        if isinstance(label_mask, Tensor):\n            label_mask = label_mask.numpy()\n\n        showing_predictions = \"prediction\" in sample\n        if showing_predictions:\n            prediction_mask = sample[\"prediction\"]\n            if isinstance(prediction_mask, Tensor):\n                prediction_mask = prediction_mask.numpy()\n\n        return self._plot_sample(\n            image,\n            label_mask,\n            self.num_classes,\n            prediction=prediction_mask if showing_predictions else None,\n            suptitle=suptitle,\n            class_names=self.class_names,\n        )\n\n    @staticmethod\n    def _plot_sample(image, label, num_classes, prediction=None, suptitle=None, class_names=None):\n        num_images = 5 if prediction is not None else 4\n        fig, ax = plt.subplots(1, num_images, figsize=(12, 10), layout=\"compressed\")\n\n        # for legend\n        ax[0].axis(\"off\")\n\n        norm = mpl.colors.Normalize(vmin=0, vmax=num_classes - 1)\n        ax[1].axis(\"off\")\n        ax[1].title.set_text(\"Image\")\n        ax[1].imshow(image)\n\n        ax[2].axis(\"off\")\n        ax[2].title.set_text(\"Ground Truth Mask\")\n        ax[2].imshow(label, cmap=\"jet\", norm=norm)\n\n        ax[3].axis(\"off\")\n        ax[3].title.set_text(\"GT Mask on Image\")\n        ax[3].imshow(image)\n        ax[3].imshow(label, cmap=\"jet\", alpha=0.3, norm=norm)\n\n        if prediction is not None:\n            ax[4].title.set_text(\"Predicted Mask\")\n            ax[4].imshow(prediction, cmap=\"jet\", norm=norm)\n\n        cmap = plt.get_cmap(\"jet\")\n        legend_data = []\n        for i, _ in enumerate(range(num_classes)):\n            class_name = class_names[i] if class_names else str(i)\n            data = [i, cmap(norm(i)), class_name]\n            legend_data.append(data)\n        handles = [Rectangle((0, 0), 1, 1, color=tuple(v for v in c)) for k, c, n in legend_data]\n        labels = [n for k, c, n in legend_data]\n        ax[0].legend(handles, labels, loc=\"center\")\n        if suptitle is not None:\n            plt.suptitle(suptitle)\n        return fig\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoSegmentationDataset.__init__","title":"<code>__init__(data_root, num_classes, label_data_root=None, image_grep='*', label_grep='*', split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, class_names=None, constant_scale=1, transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=False, reduce_zero_label=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>num_classes</code> <code>int</code> <p>Number of classes in the dataset</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with labels. If not specified, will use the same as for images.</p> <code>None</code> <code>image_grep</code> <code>str</code> <p>Regular expression appended to data_root to find input images. Defaults to \"*\".</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to data_root to find ground truth masks. Defaults to \"*\".</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset.</p> <code>None</code> <code>class_names</code> <code>list[str]</code> <p>Class names. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    num_classes: int,\n    label_data_root: Path | None = None,\n    image_grep: str | None = \"*\",\n    label_grep: str | None = \"*\",\n    split: Path | None = None,\n    ignore_split_file_extensions: bool = True,\n    allow_substring_split_file: bool = True,\n    rgb_indices: list[str] | None = None,\n    dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    class_names: list[str] | None = None,\n    constant_scale: float = 1,\n    transform: A.Compose | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = None,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (Path): Path to data root directory\n        num_classes (int): Number of classes in the dataset\n        label_data_root (Path, optional): Path to data root directory with labels.\n            If not specified, will use the same as for images.\n        image_grep (str, optional): Regular expression appended to data_root to find input images.\n            Defaults to \"*\".\n        label_grep (str, optional): Regular expression appended to data_root to find ground truth masks.\n            Defaults to \"*\".\n        split (Path, optional): Path to file containing files to be used for this split.\n            The file should be a new-line separated prefixes contained in the desired files.\n            Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\". Defaults to True\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        dataset_bands (list[HLSBands | int] | None): Bands present in the dataset.\n        output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n        class_names (list[str], optional): Class names. Defaults to None.\n        constant_scale (float): Factor to multiply image values by. Defaults to 1.\n        transform (Albumentations.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n        no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to None.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n    \"\"\"\n    super().__init__(\n        data_root,\n        label_data_root=label_data_root,\n        image_grep=image_grep,\n        label_grep=label_grep,\n        split=split,\n        ignore_split_file_extensions=ignore_split_file_extensions,\n        allow_substring_split_file=allow_substring_split_file,\n        rgb_indices=rgb_indices,\n        dataset_bands=dataset_bands,\n        output_bands=output_bands,\n        constant_scale=constant_scale,\n        transform=transform,\n        no_data_replace=no_data_replace,\n        no_label_replace=no_label_replace,\n        expand_temporal_dimension=expand_temporal_dimension,\n        reduce_zero_label=reduce_zero_label,\n    )\n    self.num_classes = num_classes\n    self.class_names = class_names\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericNonGeoSegmentationDataset.plot","title":"<code>plot(sample, suptitle=None)</code>","text":"<p>Plot a sample from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Tensor]</code> <p>a sample returned by :meth:<code>__getitem__</code></p> required <code>suptitle</code> <code>str | None</code> <p>optional string to use as a suptitle</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>a matplotlib Figure with the rendered sample</p> <p>.. versionadded:: 0.2</p> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n    \"\"\"Plot a sample from the dataset.\n\n    Args:\n        sample: a sample returned by :meth:`__getitem__`\n        suptitle: optional string to use as a suptitle\n\n    Returns:\n        a matplotlib Figure with the rendered sample\n\n    .. versionadded:: 0.2\n    \"\"\"\n    image = sample[\"image\"]\n    if len(image.shape) == 5:\n        return\n    if isinstance(image, Tensor):\n        image = image.numpy()\n    image = image.take(self.rgb_indices, axis=0)\n    image = np.transpose(image, (1, 2, 0))\n    image = (image - image.min(axis=(0, 1))) * (1 / image.max(axis=(0, 1)))\n    image = np.clip(image, 0, 1)\n\n    label_mask = sample[\"mask\"]\n    if isinstance(label_mask, Tensor):\n        label_mask = label_mask.numpy()\n\n    showing_predictions = \"prediction\" in sample\n    if showing_predictions:\n        prediction_mask = sample[\"prediction\"]\n        if isinstance(prediction_mask, Tensor):\n            prediction_mask = prediction_mask.numpy()\n\n    return self._plot_sample(\n        image,\n        label_mask,\n        self.num_classes,\n        prediction=prediction_mask if showing_predictions else None,\n        suptitle=suptitle,\n        class_names=self.class_names,\n    )\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericPixelWiseDataset","title":"<code>GenericPixelWiseDataset</code>","text":"<p>               Bases: <code>NonGeoDataset</code>, <code>ABC</code></p> <p>This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset.</p> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>class GenericPixelWiseDataset(NonGeoDataset, ABC):\n    \"\"\"\n    This is a generic dataset class to be used for instantiating datasets from arguments.\n    Ideally, one would create a dataset class specific to a dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        label_data_root: Path | None = None,\n        image_grep: str | None = \"*\",\n        label_grep: str | None = \"*\",\n        split: Path | None = None,\n        ignore_split_file_extensions: bool = True,\n        allow_substring_split_file: bool = True,\n        rgb_indices: list[int] | None = None,\n        dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        constant_scale: float = 1,\n        transform: A.Compose | None = None,\n        no_data_replace: float | None = None,\n        no_label_replace: int | None = None,\n        expand_temporal_dimension: bool = False,\n        reduce_zero_label: bool = False,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (Path): Path to data root directory\n            label_data_root (Path, optional): Path to data root directory with labels.\n                If not specified, will use the same as for images.\n            image_grep (str, optional): Regular expression appended to data_root to find input images.\n                Defaults to \"*\".\n            label_grep (str, optional): Regular expression appended to data_root to find ground truth masks.\n                Defaults to \"*\".\n            split (Path, optional): Path to file containing files to be used for this split.\n                The file should be a new-line separated prefixes contained in the desired files.\n                Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\". Defaults to True.\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be refered to by output_bands. Defaults to None.\n            output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the dataset as named by dataset_bands.\n            constant_scale (float): Factor to multiply image values by. Defaults to 1.\n            transform (Albumentations.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n            no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n            reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n                expected 0. Defaults to False.\n        \"\"\"\n        super().__init__()\n\n        self.split_file = split\n\n        label_data_root = label_data_root if label_data_root is not None else data_root\n        self.image_files = sorted(glob.glob(os.path.join(data_root, image_grep)))\n        self.constant_scale = constant_scale\n        self.no_data_replace = no_data_replace\n        self.no_label_replace = no_label_replace\n        self.segmentation_mask_files = sorted(glob.glob(os.path.join(label_data_root, label_grep)))\n        self.reduce_zero_label = reduce_zero_label\n        self.expand_temporal_dimension = expand_temporal_dimension\n\n        if self.expand_temporal_dimension and output_bands is None:\n            msg = \"Please provide output_bands when expand_temporal_dimension is True\"\n            raise Exception(msg)\n        if self.split_file is not None:\n            with open(self.split_file) as f:\n                split = f.readlines()\n            valid_files = {rf\"{substring.strip()}\" for substring in split}\n            self.image_files = filter_valid_files(\n                self.image_files,\n                valid_files=valid_files,\n                ignore_extensions=ignore_split_file_extensions,\n                allow_substring=allow_substring_split_file,\n            )\n            self.segmentation_mask_files = filter_valid_files(\n                self.segmentation_mask_files,\n                valid_files=valid_files,\n                ignore_extensions=ignore_split_file_extensions,\n                allow_substring=allow_substring_split_file,\n            )\n        self.rgb_indices = [0, 1, 2] if rgb_indices is None else rgb_indices\n\n        self.dataset_bands = generate_bands_intervals(dataset_bands)\n        self.output_bands = generate_bands_intervals(output_bands)\n\n        if self.output_bands and not self.dataset_bands:\n            msg = \"If output bands provided, dataset_bands must also be provided\"\n            return Exception(msg)  # noqa: PLE0101\n\n        # There is a special condition if the bands are defined as simple strings.\n        if self.output_bands:\n            if len(set(self.output_bands) &amp; set(self.dataset_bands)) != len(self.output_bands):\n                msg = \"Output bands must be a subset of dataset bands\"\n                raise Exception(msg)\n\n            self.filter_indices = [self.dataset_bands.index(band) for band in self.output_bands]\n\n        else:\n            self.filter_indices = None\n\n        # If no transform is given, apply only to transform to torch tensor\n        self.transform = transform if transform else default_transform\n        # self.transform = transform if transform else ToTensorV2()\n\n        import warnings\n\n        import rasterio\n\n        warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        image = self._load_file(self.image_files[index], nan_replace=self.no_data_replace).to_numpy()\n        # to channels last\n        if self.expand_temporal_dimension:\n            image = rearrange(image, \"(channels time) h w -&gt; channels time h w\", channels=len(self.output_bands))\n        image = np.moveaxis(image, 0, -1)\n\n        if self.filter_indices:\n            image = image[..., self.filter_indices]\n        output = {\n            \"image\": image.astype(np.float32) * self.constant_scale,\n            \"mask\": self._load_file(self.segmentation_mask_files[index], nan_replace=self.no_label_replace).to_numpy()[\n                0\n            ]\n        }\n\n        if self.reduce_zero_label:\n            output[\"mask\"] -= 1\n        if self.transform:\n            output = self.transform(**output)\n        output[\"filename\"] = self.image_files[index]\n\n        return output\n\n    def _load_file(self, path, nan_replace: int | float | None = None) -&gt; xr.DataArray:\n        data = rioxarray.open_rasterio(path, masked=True)\n        if nan_replace is not None:\n            data = data.fillna(nan_replace)\n        return data\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_pixel_wise_dataset.GenericPixelWiseDataset.__init__","title":"<code>__init__(data_root, label_data_root=None, image_grep='*', label_grep='*', split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, constant_scale=1, transform=None, no_data_replace=None, no_label_replace=None, expand_temporal_dimension=False, reduce_zero_label=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>label_data_root</code> <code>Path</code> <p>Path to data root directory with labels. If not specified, will use the same as for images.</p> <code>None</code> <code>image_grep</code> <code>str</code> <p>Regular expression appended to data_root to find input images. Defaults to \"*\".</p> <code>'*'</code> <code>label_grep</code> <code>str</code> <p>Regular expression appended to data_root to find ground truth masks. Defaults to \"*\".</p> <code>'*'</code> <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int | tuple[int, int] | str] | None</code> <p>Bands present in the dataset. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be refered to by output_bands. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int | tuple[int, int] | str] | None</code> <p>Bands that should be output by the dataset as named by dataset_bands.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float | None</code> <p>Replace nan values in input images with this value. If none, does no replacement. Defaults to None.</p> <code>None</code> <code>no_label_replace</code> <code>int | None</code> <p>Replace nan values in label with this value. If none, does no replacement. Defaults to -1.</p> <code>None</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> <code>reduce_zero_label</code> <code>bool</code> <p>Subtract 1 from all labels. Useful when labels start from 1 instead of the expected 0. Defaults to False.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_pixel_wise_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    label_data_root: Path | None = None,\n    image_grep: str | None = \"*\",\n    label_grep: str | None = \"*\",\n    split: Path | None = None,\n    ignore_split_file_extensions: bool = True,\n    allow_substring_split_file: bool = True,\n    rgb_indices: list[int] | None = None,\n    dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    constant_scale: float = 1,\n    transform: A.Compose | None = None,\n    no_data_replace: float | None = None,\n    no_label_replace: int | None = None,\n    expand_temporal_dimension: bool = False,\n    reduce_zero_label: bool = False,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (Path): Path to data root directory\n        label_data_root (Path, optional): Path to data root directory with labels.\n            If not specified, will use the same as for images.\n        image_grep (str, optional): Regular expression appended to data_root to find input images.\n            Defaults to \"*\".\n        label_grep (str, optional): Regular expression appended to data_root to find ground truth masks.\n            Defaults to \"*\".\n        split (Path, optional): Path to file containing files to be used for this split.\n            The file should be a new-line separated prefixes contained in the desired files.\n            Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\". Defaults to True.\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This parameter names input channels (bands) using HLSBands, ints, int ranges, or strings, so that they can then be refered to by output_bands. Defaults to None.\n        output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the dataset as named by dataset_bands.\n        constant_scale (float): Factor to multiply image values by. Defaults to 1.\n        transform (Albumentations.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float | None): Replace nan values in input images with this value. If none, does no replacement. Defaults to None.\n        no_label_replace (int | None): Replace nan values in label with this value. If none, does no replacement. Defaults to -1.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n        reduce_zero_label (bool): Subtract 1 from all labels. Useful when labels start from 1 instead of the\n            expected 0. Defaults to False.\n    \"\"\"\n    super().__init__()\n\n    self.split_file = split\n\n    label_data_root = label_data_root if label_data_root is not None else data_root\n    self.image_files = sorted(glob.glob(os.path.join(data_root, image_grep)))\n    self.constant_scale = constant_scale\n    self.no_data_replace = no_data_replace\n    self.no_label_replace = no_label_replace\n    self.segmentation_mask_files = sorted(glob.glob(os.path.join(label_data_root, label_grep)))\n    self.reduce_zero_label = reduce_zero_label\n    self.expand_temporal_dimension = expand_temporal_dimension\n\n    if self.expand_temporal_dimension and output_bands is None:\n        msg = \"Please provide output_bands when expand_temporal_dimension is True\"\n        raise Exception(msg)\n    if self.split_file is not None:\n        with open(self.split_file) as f:\n            split = f.readlines()\n        valid_files = {rf\"{substring.strip()}\" for substring in split}\n        self.image_files = filter_valid_files(\n            self.image_files,\n            valid_files=valid_files,\n            ignore_extensions=ignore_split_file_extensions,\n            allow_substring=allow_substring_split_file,\n        )\n        self.segmentation_mask_files = filter_valid_files(\n            self.segmentation_mask_files,\n            valid_files=valid_files,\n            ignore_extensions=ignore_split_file_extensions,\n            allow_substring=allow_substring_split_file,\n        )\n    self.rgb_indices = [0, 1, 2] if rgb_indices is None else rgb_indices\n\n    self.dataset_bands = generate_bands_intervals(dataset_bands)\n    self.output_bands = generate_bands_intervals(output_bands)\n\n    if self.output_bands and not self.dataset_bands:\n        msg = \"If output bands provided, dataset_bands must also be provided\"\n        return Exception(msg)  # noqa: PLE0101\n\n    # There is a special condition if the bands are defined as simple strings.\n    if self.output_bands:\n        if len(set(self.output_bands) &amp; set(self.dataset_bands)) != len(self.output_bands):\n            msg = \"Output bands must be a subset of dataset bands\"\n            raise Exception(msg)\n\n        self.filter_indices = [self.dataset_bands.index(band) for band in self.output_bands]\n\n    else:\n        self.filter_indices = None\n\n    # If no transform is given, apply only to transform to torch tensor\n    self.transform = transform if transform else default_transform\n    # self.transform = transform if transform else ToTensorV2()\n\n    import warnings\n\n    import rasterio\n\n    warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_scalar_label_dataset","title":"<code>terratorch.datasets.generic_scalar_label_dataset</code>","text":"<p>Module containing generic dataset classes</p>"},{"location":"generic_datasets/#terratorch.datasets.generic_scalar_label_dataset.GenericNonGeoClassificationDataset","title":"<code>GenericNonGeoClassificationDataset</code>","text":"<p>               Bases: <code>GenericScalarLabelDataset</code></p> <p>GenericNonGeoClassificationDataset</p> Source code in <code>terratorch/datasets/generic_scalar_label_dataset.py</code> <pre><code>class GenericNonGeoClassificationDataset(GenericScalarLabelDataset):\n    \"\"\"GenericNonGeoClassificationDataset\"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        num_classes: int,\n        split: Path | None = None,\n        ignore_split_file_extensions: bool = True,  # noqa: FBT001, FBT002\n        allow_substring_split_file: bool = True,  # noqa: FBT001, FBT002\n        rgb_indices: list[str] | None = None,\n        dataset_bands: list[HLSBands | int] | None = None,\n        output_bands: list[HLSBands | int] | None = None,\n        class_names: list[str] | None = None,\n        constant_scale: float = 1,\n        transform: A.Compose | None = None,\n        no_data_replace: float = 0,\n        expand_temporal_dimension: bool = False,  # noqa: FBT001, FBT002\n    ) -&gt; None:\n        \"\"\"A generic Non-Geo dataset for classification.\n\n        Args:\n            data_root (Path): Path to data root directory\n            num_classes (int): Number of classes in the dataset\n            split (Path, optional): Path to file containing files to be used for this split.\n                The file should be a new-line separated prefixes contained in the desired files.\n                Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\". Defaults to True.\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            dataset_bands (list[HLSBands | int] | None): Bands present in the dataset.\n            output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n            class_names (list[str], optional): Class names. Defaults to None.\n            constant_scale (float): Factor to multiply image values by. Defaults to 1.\n            transform (Albumentations.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float): Replace nan values in input images with this value. Defaults to 0.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n        \"\"\"\n        super().__init__(\n            data_root,\n            split=split,\n            ignore_split_file_extensions=ignore_split_file_extensions,\n            allow_substring_split_file=allow_substring_split_file,\n            rgb_indices=rgb_indices,\n            dataset_bands=dataset_bands,\n            output_bands=output_bands,\n            constant_scale=constant_scale,\n            transform=transform,\n            no_data_replace=no_data_replace,\n            expand_temporal_dimension=expand_temporal_dimension,\n        )\n        self.num_classes = num_classes\n        self.class_names = class_names\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        item = super().__getitem__(index)\n        item[\"label\"] = torch.tensor(item[\"label\"]).long()\n        return item\n\n    def plot(self, sample: dict[str, Tensor], suptitle: str | None = None) -&gt; Figure:\n        pass\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_scalar_label_dataset.GenericNonGeoClassificationDataset.__init__","title":"<code>__init__(data_root, num_classes, split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, class_names=None, constant_scale=1, transform=None, no_data_replace=0, expand_temporal_dimension=False)</code>","text":"<p>A generic Non-Geo dataset for classification.</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>num_classes</code> <code>int</code> <p>Number of classes in the dataset</p> required <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands present in the dataset.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int] | None</code> <p>Bands that should be output by the dataset.</p> <code>None</code> <code>class_names</code> <code>list[str]</code> <p>Class names. Defaults to None.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float</code> <p>Replace nan values in input images with this value. Defaults to 0.</p> <code>0</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_scalar_label_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    num_classes: int,\n    split: Path | None = None,\n    ignore_split_file_extensions: bool = True,  # noqa: FBT001, FBT002\n    allow_substring_split_file: bool = True,  # noqa: FBT001, FBT002\n    rgb_indices: list[str] | None = None,\n    dataset_bands: list[HLSBands | int] | None = None,\n    output_bands: list[HLSBands | int] | None = None,\n    class_names: list[str] | None = None,\n    constant_scale: float = 1,\n    transform: A.Compose | None = None,\n    no_data_replace: float = 0,\n    expand_temporal_dimension: bool = False,  # noqa: FBT001, FBT002\n) -&gt; None:\n    \"\"\"A generic Non-Geo dataset for classification.\n\n    Args:\n        data_root (Path): Path to data root directory\n        num_classes (int): Number of classes in the dataset\n        split (Path, optional): Path to file containing files to be used for this split.\n            The file should be a new-line separated prefixes contained in the desired files.\n            Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\". Defaults to True.\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        dataset_bands (list[HLSBands | int] | None): Bands present in the dataset.\n        output_bands (list[HLSBands | int] | None): Bands that should be output by the dataset.\n        class_names (list[str], optional): Class names. Defaults to None.\n        constant_scale (float): Factor to multiply image values by. Defaults to 1.\n        transform (Albumentations.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float): Replace nan values in input images with this value. Defaults to 0.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n    \"\"\"\n    super().__init__(\n        data_root,\n        split=split,\n        ignore_split_file_extensions=ignore_split_file_extensions,\n        allow_substring_split_file=allow_substring_split_file,\n        rgb_indices=rgb_indices,\n        dataset_bands=dataset_bands,\n        output_bands=output_bands,\n        constant_scale=constant_scale,\n        transform=transform,\n        no_data_replace=no_data_replace,\n        expand_temporal_dimension=expand_temporal_dimension,\n    )\n    self.num_classes = num_classes\n    self.class_names = class_names\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_scalar_label_dataset.GenericScalarLabelDataset","title":"<code>GenericScalarLabelDataset</code>","text":"<p>               Bases: <code>NonGeoDataset</code>, <code>ImageFolder</code>, <code>ABC</code></p> <p>This is a generic dataset class to be used for instantiating datasets from arguments. Ideally, one would create a dataset class specific to a dataset.</p> Source code in <code>terratorch/datasets/generic_scalar_label_dataset.py</code> <pre><code>class GenericScalarLabelDataset(NonGeoDataset, ImageFolder, ABC):\n    \"\"\"\n    This is a generic dataset class to be used for instantiating datasets from arguments.\n    Ideally, one would create a dataset class specific to a dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_root: Path,\n        split: Path | None = None,\n        ignore_split_file_extensions: bool = True,  # noqa: FBT001, FBT002\n        allow_substring_split_file: bool = True,  # noqa: FBT001, FBT002\n        rgb_indices: list[int] | None = None,\n        dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n        constant_scale: float = 1,\n        transform: A.Compose | None = None,\n        no_data_replace: float = 0,\n        expand_temporal_dimension: bool = False,  # noqa: FBT001, FBT002\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            data_root (Path): Path to data root directory\n            split (Path, optional): Path to file containing files to be used for this split.\n                The file should be a new-line separated prefixes contained in the desired files.\n                Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n            ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n                file to determine which files to include in the dataset.\n                E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n                actually \".jpg\". Defaults to True.\n            allow_substring_split_file (bool, optional): Whether the split files contain substrings\n                that must be present in file names to be included (as in mmsegmentation), or exact\n                matches (e.g. eurosat). Defaults to True.\n            rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n            dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This\n                parameter gives identifiers to input channels (bands) so that they can then be refered to by\n                output_bands. Can use the HLSBands enum, ints, int ranges, or strings. Defaults to None.\n            output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the\n                dataset as named by dataset_bands.\n            constant_scale (float): Factor to multiply image values by. Defaults to 1.\n            transform (Albumentations.Compose | None): Albumentations transform to be applied.\n                Should end with ToTensorV2(). If used through the generic_data_module,\n                should not include normalization. Not supported for multi-temporal data.\n                Defaults to None, which simply applies ToTensorV2().\n            no_data_replace (float): Replace nan values in input images with this value. Defaults to 0.\n            expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n                Defaults to False.\n        \"\"\"\n        self.split_file = split\n\n        self.image_files = sorted(glob.glob(os.path.join(data_root, \"**\"), recursive=True))\n        self.image_files = [f for f in self.image_files if not os.path.isdir(f)]\n        self.constant_scale = constant_scale\n        self.no_data_replace = no_data_replace\n        self.expand_temporal_dimension = expand_temporal_dimension\n        if self.expand_temporal_dimension and output_bands is None:\n            msg = \"Please provide output_bands when expand_temporal_dimension is True\"\n            raise Exception(msg)\n        if self.split_file is not None:\n            with open(self.split_file) as f:\n                split = f.readlines()\n            valid_files = {rf\"{substring.strip()}\" for substring in split}\n            self.image_files = filter_valid_files(\n                self.image_files,\n                valid_files=valid_files,\n                ignore_extensions=ignore_split_file_extensions,\n                allow_substring=allow_substring_split_file,\n            )\n\n            def is_valid_file(x):\n                return x in self.image_files\n\n        else:\n\n            def is_valid_file(x):\n                return True\n\n        super().__init__(\n            root=data_root, transform=None, target_transform=None, loader=rasterio_loader, is_valid_file=is_valid_file\n        )\n\n        self.rgb_indices = [0, 1, 2] if rgb_indices is None else rgb_indices\n\n        self.dataset_bands = generate_bands_intervals(dataset_bands)\n        self.output_bands = generate_bands_intervals(output_bands)\n\n        if self.output_bands and not self.dataset_bands:\n            msg = \"If output bands provided, dataset_bands must also be provided\"\n            return Exception(msg)  # noqa: PLE0101\n\n        # There is a special condition if the bands are defined as simple strings.\n        if self.output_bands:\n            if len(set(self.output_bands) &amp; set(self.dataset_bands)) != len(self.output_bands):\n                msg = \"Output bands must be a subset of dataset bands\"\n                raise Exception(msg)\n\n            self.filter_indices = [self.dataset_bands.index(band) for band in self.output_bands]\n\n        else:\n            self.filter_indices = None\n        # If no transform is given, apply only to transform to torch tensor\n        self.transforms = transform if transform else default_transform\n        # self.transform = transform if transform else ToTensorV2()\n\n        import warnings\n\n        import rasterio\n        warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n\n    def __len__(self) -&gt; int:\n        return len(self.image_files)\n\n    def __getitem__(self, index: int) -&gt; dict[str, Any]:\n        image, label = ImageFolder.__getitem__(self, index)\n        if self.expand_temporal_dimension:\n            image = rearrange(image, \"h w (channels time) -&gt; time h w channels\", channels=len(self.output_bands))\n        if self.filter_indices:\n            image = image[..., self.filter_indices]\n\n        image = image.astype(np.float32) * self.constant_scale\n\n        if self.transforms:\n            image = self.transforms(image=image)[\"image\"]  # albumentations returns dict\n\n        output = {\n            \"image\": image,\n            \"label\": label,  # samples is an attribute of ImageFolder. Contains a tuple of (Path, Target)\n            \"filename\": self.image_files[index]\n        }\n\n        return output\n\n    def _generate_bands_intervals(self, bands_intervals: list[int | str | HLSBands | tuple[int]] | None = None):\n        if bands_intervals is None:\n            return None\n        bands = []\n        for element in bands_intervals:\n            # if its an interval\n            if isinstance(element, tuple):\n                if len(element) != 2:  # noqa: PLR2004\n                    msg = \"When defining an interval, a tuple of two integers should be passed,\\\n                    defining start and end indices inclusive\"\n                    raise Exception(msg)\n                expanded_element = list(range(element[0], element[1] + 1))\n                bands.extend(expanded_element)\n            else:\n                bands.append(element)\n        return bands\n\n    def _load_file(self, path) -&gt; xr.DataArray:\n        data = rioxarray.open_rasterio(path, masked=True)\n        data = data.fillna(self.no_data_replace)\n        return data\n</code></pre>"},{"location":"generic_datasets/#terratorch.datasets.generic_scalar_label_dataset.GenericScalarLabelDataset.__init__","title":"<code>__init__(data_root, split=None, ignore_split_file_extensions=True, allow_substring_split_file=True, rgb_indices=None, dataset_bands=None, output_bands=None, constant_scale=1, transform=None, no_data_replace=0, expand_temporal_dimension=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>data_root</code> <code>Path</code> <p>Path to data root directory</p> required <code>split</code> <code>Path</code> <p>Path to file containing files to be used for this split. The file should be a new-line separated prefixes contained in the desired files. Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])</p> <code>None</code> <code>ignore_split_file_extensions</code> <code>bool</code> <p>Whether to disregard extensions when using the split file to determine which files to include in the dataset. E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are actually \".jpg\". Defaults to True.</p> <code>True</code> <code>allow_substring_split_file</code> <code>bool</code> <p>Whether the split files contain substrings that must be present in file names to be included (as in mmsegmentation), or exact matches (e.g. eurosat). Defaults to True.</p> <code>True</code> <code>rgb_indices</code> <code>list[str]</code> <p>Indices of RGB channels. Defaults to [0, 1, 2].</p> <code>None</code> <code>dataset_bands</code> <code>list[HLSBands | int | tuple[int, int] | str] | None</code> <p>Bands present in the dataset. This parameter gives identifiers to input channels (bands) so that they can then be refered to by output_bands. Can use the HLSBands enum, ints, int ranges, or strings. Defaults to None.</p> <code>None</code> <code>output_bands</code> <code>list[HLSBands | int | tuple[int, int] | str] | None</code> <p>Bands that should be output by the dataset as named by dataset_bands.</p> <code>None</code> <code>constant_scale</code> <code>float</code> <p>Factor to multiply image values by. Defaults to 1.</p> <code>1</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform to be applied. Should end with ToTensorV2(). If used through the generic_data_module, should not include normalization. Not supported for multi-temporal data. Defaults to None, which simply applies ToTensorV2().</p> <code>None</code> <code>no_data_replace</code> <code>float</code> <p>Replace nan values in input images with this value. Defaults to 0.</p> <code>0</code> <code>expand_temporal_dimension</code> <code>bool</code> <p>Go from shape (time*channels, h, w) to (channels, time, h, w). Defaults to False.</p> <code>False</code> Source code in <code>terratorch/datasets/generic_scalar_label_dataset.py</code> <pre><code>def __init__(\n    self,\n    data_root: Path,\n    split: Path | None = None,\n    ignore_split_file_extensions: bool = True,  # noqa: FBT001, FBT002\n    allow_substring_split_file: bool = True,  # noqa: FBT001, FBT002\n    rgb_indices: list[int] | None = None,\n    dataset_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    output_bands: list[HLSBands | int | tuple[int, int] | str] | None = None,\n    constant_scale: float = 1,\n    transform: A.Compose | None = None,\n    no_data_replace: float = 0,\n    expand_temporal_dimension: bool = False,  # noqa: FBT001, FBT002\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        data_root (Path): Path to data root directory\n        split (Path, optional): Path to file containing files to be used for this split.\n            The file should be a new-line separated prefixes contained in the desired files.\n            Files will be seached using glob with the form Path(data_root).glob(prefix + [image or label grep])\n        ignore_split_file_extensions (bool, optional): Whether to disregard extensions when using the split\n            file to determine which files to include in the dataset.\n            E.g. necessary for Eurosat, since the split files specify \".jpg\" but files are\n            actually \".jpg\". Defaults to True.\n        allow_substring_split_file (bool, optional): Whether the split files contain substrings\n            that must be present in file names to be included (as in mmsegmentation), or exact\n            matches (e.g. eurosat). Defaults to True.\n        rgb_indices (list[str], optional): Indices of RGB channels. Defaults to [0, 1, 2].\n        dataset_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands present in the dataset. This\n            parameter gives identifiers to input channels (bands) so that they can then be refered to by\n            output_bands. Can use the HLSBands enum, ints, int ranges, or strings. Defaults to None.\n        output_bands (list[HLSBands | int | tuple[int, int] | str] | None): Bands that should be output by the\n            dataset as named by dataset_bands.\n        constant_scale (float): Factor to multiply image values by. Defaults to 1.\n        transform (Albumentations.Compose | None): Albumentations transform to be applied.\n            Should end with ToTensorV2(). If used through the generic_data_module,\n            should not include normalization. Not supported for multi-temporal data.\n            Defaults to None, which simply applies ToTensorV2().\n        no_data_replace (float): Replace nan values in input images with this value. Defaults to 0.\n        expand_temporal_dimension (bool): Go from shape (time*channels, h, w) to (channels, time, h, w).\n            Defaults to False.\n    \"\"\"\n    self.split_file = split\n\n    self.image_files = sorted(glob.glob(os.path.join(data_root, \"**\"), recursive=True))\n    self.image_files = [f for f in self.image_files if not os.path.isdir(f)]\n    self.constant_scale = constant_scale\n    self.no_data_replace = no_data_replace\n    self.expand_temporal_dimension = expand_temporal_dimension\n    if self.expand_temporal_dimension and output_bands is None:\n        msg = \"Please provide output_bands when expand_temporal_dimension is True\"\n        raise Exception(msg)\n    if self.split_file is not None:\n        with open(self.split_file) as f:\n            split = f.readlines()\n        valid_files = {rf\"{substring.strip()}\" for substring in split}\n        self.image_files = filter_valid_files(\n            self.image_files,\n            valid_files=valid_files,\n            ignore_extensions=ignore_split_file_extensions,\n            allow_substring=allow_substring_split_file,\n        )\n\n        def is_valid_file(x):\n            return x in self.image_files\n\n    else:\n\n        def is_valid_file(x):\n            return True\n\n    super().__init__(\n        root=data_root, transform=None, target_transform=None, loader=rasterio_loader, is_valid_file=is_valid_file\n    )\n\n    self.rgb_indices = [0, 1, 2] if rgb_indices is None else rgb_indices\n\n    self.dataset_bands = generate_bands_intervals(dataset_bands)\n    self.output_bands = generate_bands_intervals(output_bands)\n\n    if self.output_bands and not self.dataset_bands:\n        msg = \"If output bands provided, dataset_bands must also be provided\"\n        return Exception(msg)  # noqa: PLE0101\n\n    # There is a special condition if the bands are defined as simple strings.\n    if self.output_bands:\n        if len(set(self.output_bands) &amp; set(self.dataset_bands)) != len(self.output_bands):\n            msg = \"Output bands must be a subset of dataset bands\"\n            raise Exception(msg)\n\n        self.filter_indices = [self.dataset_bands.index(band) for band in self.output_bands]\n\n    else:\n        self.filter_indices = None\n    # If no transform is given, apply only to transform to torch tensor\n    self.transforms = transform if transform else default_transform\n    # self.transform = transform if transform else ToTensorV2()\n\n    import warnings\n\n    import rasterio\n    warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n</code></pre>"},{"location":"glossary/","title":"Glossary of terms used in this Documentation and in the Geospatial AI area","text":""},{"location":"glossary/#encoder","title":"Encoder","text":"<p>The neural network used to map between the inputs and the intermdiary stage (usually referred as embedding or sometimes as latent space) of the forward step. The encoder is also frequently called backbone and, for  finetuning tasks, it is usually the part of the model which is not updated/trained. </p>"},{"location":"glossary/#decoder","title":"Decoder","text":"<p>The neural network employed to map between the intermediary stage (embedding/latent space) and the target output. For finetuning tasks, the decoder is the most essential part, since it is trained to map the embedding produced by a previoulsy trained encoder to a new task. </p>"},{"location":"glossary/#head","title":"Head","text":"<p>A network, usually very small when compared to the encoder and decoder, which is used as final step to adapt the decoder output to a specific task, for example, by applying a determined activation to it. </p>"},{"location":"glossary/#neck","title":"Neck","text":"<p>Necks are operations placed between the encoder and the decoder stages aimed at adjusting possible discrepancies, as incompatible shapes, or applying some specific transform, as a normalization required for the task being executed. </p>"},{"location":"glossary/#factory","title":"Factory","text":"<p>A Factory is a class which organizes the instantiation of a complete model, as a backbone-neck-decoder-head architecture. A class is intended to receive lists and dictionaries containing the required arguments used to build the model and returns a new instance already ready to be used. </p>"},{"location":"heads/","title":"Heads","text":""},{"location":"heads/#terratorch.models.heads.regression_head","title":"<code>terratorch.models.heads.regression_head</code>","text":""},{"location":"heads/#terratorch.models.heads.regression_head.RegressionHead","title":"<code>RegressionHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Regression head</p> Source code in <code>terratorch/models/heads/regression_head.py</code> <pre><code>class RegressionHead(nn.Module):\n    \"\"\"Regression head\"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        final_act: nn.Module | str | None = None,\n        learned_upscale_layers: int = 0,\n        channel_list: list[int] | None = None,\n        batch_norm: bool = True,\n        dropout: float = 0,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            in_channels (int): Number of input channels\n            final_act (nn.Module | None, optional): Final activation to be applied. Defaults to None.\n            learned_upscale_layers (int, optional): Number of Pixelshuffle layers to create. Each upscales 2x.\n                Defaults to 0.\n            channel_list (list[int] | None, optional): List with number of channels for each Conv\n                layer to be created. Defaults to None.\n            batch_norm (bool, optional): Whether to apply batch norm. Defaults to True.\n            dropout (float, optional): Dropout value to apply. Defaults to 0.\n\n        \"\"\"\n        super().__init__()\n        self.learned_upscale_layers = learned_upscale_layers\n        self.final_act = final_act if final_act else nn.Identity()\n        if isinstance(final_act, str):\n            module_name, class_name = final_act.rsplit(\".\", 1)\n            target_class = getattr(importlib.import_module(module_name), class_name)\n            self.final_act = target_class()\n        pre_layers = []\n        if learned_upscale_layers != 0:\n            learned_upscale = nn.Sequential(\n                *[PixelShuffleUpscale(in_channels) for _ in range(self.learned_upscale_layers)]\n            )\n            pre_layers.append(learned_upscale)\n\n        if channel_list is None:\n            pre_head = nn.Identity()\n        else:\n\n            def block(in_channels, out_channels):\n                return nn.Sequential(\n                    nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=False),\n                    nn.BatchNorm2d(out_channels),\n                    nn.ReLU(inplace=True),\n                )\n\n            channel_list = [in_channels, *channel_list]\n            pre_head = nn.Sequential(\n                *[block(channel_list[i], channel_list[i + 1]) for i in range(len(channel_list) - 1)]\n            )\n            in_channels = channel_list[-1]\n            pre_layers.append(pre_head)\n        dropout = nn.Dropout2d(dropout)\n        final_layer = nn.Conv2d(in_channels=in_channels, out_channels=1, kernel_size=1)\n        self.head = nn.Sequential(*[*pre_layers, dropout, final_layer])\n\n    def forward(self, x):\n        output = self.head(x)\n        return self.final_act(output)\n</code></pre>"},{"location":"heads/#terratorch.models.heads.regression_head.RegressionHead.__init__","title":"<code>__init__(in_channels, final_act=None, learned_upscale_layers=0, channel_list=None, batch_norm=True, dropout=0)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>final_act</code> <code>Module | None</code> <p>Final activation to be applied. Defaults to None.</p> <code>None</code> <code>learned_upscale_layers</code> <code>int</code> <p>Number of Pixelshuffle layers to create. Each upscales 2x. Defaults to 0.</p> <code>0</code> <code>channel_list</code> <code>list[int] | None</code> <p>List with number of channels for each Conv layer to be created. Defaults to None.</p> <code>None</code> <code>batch_norm</code> <code>bool</code> <p>Whether to apply batch norm. Defaults to True.</p> <code>True</code> <code>dropout</code> <code>float</code> <p>Dropout value to apply. Defaults to 0.</p> <code>0</code> Source code in <code>terratorch/models/heads/regression_head.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    final_act: nn.Module | str | None = None,\n    learned_upscale_layers: int = 0,\n    channel_list: list[int] | None = None,\n    batch_norm: bool = True,\n    dropout: float = 0,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        in_channels (int): Number of input channels\n        final_act (nn.Module | None, optional): Final activation to be applied. Defaults to None.\n        learned_upscale_layers (int, optional): Number of Pixelshuffle layers to create. Each upscales 2x.\n            Defaults to 0.\n        channel_list (list[int] | None, optional): List with number of channels for each Conv\n            layer to be created. Defaults to None.\n        batch_norm (bool, optional): Whether to apply batch norm. Defaults to True.\n        dropout (float, optional): Dropout value to apply. Defaults to 0.\n\n    \"\"\"\n    super().__init__()\n    self.learned_upscale_layers = learned_upscale_layers\n    self.final_act = final_act if final_act else nn.Identity()\n    if isinstance(final_act, str):\n        module_name, class_name = final_act.rsplit(\".\", 1)\n        target_class = getattr(importlib.import_module(module_name), class_name)\n        self.final_act = target_class()\n    pre_layers = []\n    if learned_upscale_layers != 0:\n        learned_upscale = nn.Sequential(\n            *[PixelShuffleUpscale(in_channels) for _ in range(self.learned_upscale_layers)]\n        )\n        pre_layers.append(learned_upscale)\n\n    if channel_list is None:\n        pre_head = nn.Identity()\n    else:\n\n        def block(in_channels, out_channels):\n            return nn.Sequential(\n                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True),\n            )\n\n        channel_list = [in_channels, *channel_list]\n        pre_head = nn.Sequential(\n            *[block(channel_list[i], channel_list[i + 1]) for i in range(len(channel_list) - 1)]\n        )\n        in_channels = channel_list[-1]\n        pre_layers.append(pre_head)\n    dropout = nn.Dropout2d(dropout)\n    final_layer = nn.Conv2d(in_channels=in_channels, out_channels=1, kernel_size=1)\n    self.head = nn.Sequential(*[*pre_layers, dropout, final_layer])\n</code></pre>"},{"location":"heads/#terratorch.models.heads.segmentation_head","title":"<code>terratorch.models.heads.segmentation_head</code>","text":""},{"location":"heads/#terratorch.models.heads.segmentation_head.SegmentationHead","title":"<code>SegmentationHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Segmentation head</p> Source code in <code>terratorch/models/heads/segmentation_head.py</code> <pre><code>class SegmentationHead(nn.Module):\n    \"\"\"Segmentation head\"\"\"\n\n    def __init__(\n        self, in_channels: int, num_classes: int, channel_list: list[int] | None = None, dropout: float = 0\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            in_channels (int): Number of input channels\n            num_classes (int): Number of output classes\n            channel_list (list[int] | None, optional):  List with number of channels for each Conv\n                layer to be created. Defaults to None.\n            dropout (float, optional): Dropout value to apply. Defaults to 0.\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        if channel_list is None:\n            pre_head = nn.Identity()\n        else:\n\n            def block(in_channels, out_channels):\n                return nn.Sequential(\n                    nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1), nn.ReLU()\n                )\n\n            channel_list = [in_channels, *channel_list]\n            pre_head = nn.Sequential(\n                *[block(channel_list[i], channel_list[i + 1]) for i in range(len(channel_list) - 1)]\n            )\n            in_channels = channel_list[-1]\n        dropout = nn.Identity() if dropout == 0 else nn.Dropout(dropout)\n        self.head = nn.Sequential(\n            pre_head,\n            dropout,\n            nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=num_classes,\n                kernel_size=1,\n            ),\n        )\n\n    def forward(self, x):\n        return self.head(x)\n</code></pre>"},{"location":"heads/#terratorch.models.heads.segmentation_head.SegmentationHead.__init__","title":"<code>__init__(in_channels, num_classes, channel_list=None, dropout=0)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes</p> required <code>channel_list</code> <code>list[int] | None</code> <p>List with number of channels for each Conv layer to be created. Defaults to None.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout value to apply. Defaults to 0.</p> <code>0</code> Source code in <code>terratorch/models/heads/segmentation_head.py</code> <pre><code>def __init__(\n    self, in_channels: int, num_classes: int, channel_list: list[int] | None = None, dropout: float = 0\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        in_channels (int): Number of input channels\n        num_classes (int): Number of output classes\n        channel_list (list[int] | None, optional):  List with number of channels for each Conv\n            layer to be created. Defaults to None.\n        dropout (float, optional): Dropout value to apply. Defaults to 0.\n    \"\"\"\n    super().__init__()\n    self.num_classes = num_classes\n    if channel_list is None:\n        pre_head = nn.Identity()\n    else:\n\n        def block(in_channels, out_channels):\n            return nn.Sequential(\n                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1), nn.ReLU()\n            )\n\n        channel_list = [in_channels, *channel_list]\n        pre_head = nn.Sequential(\n            *[block(channel_list[i], channel_list[i + 1]) for i in range(len(channel_list) - 1)]\n        )\n        in_channels = channel_list[-1]\n    dropout = nn.Identity() if dropout == 0 else nn.Dropout(dropout)\n    self.head = nn.Sequential(\n        pre_head,\n        dropout,\n        nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=num_classes,\n            kernel_size=1,\n        ),\n    )\n</code></pre>"},{"location":"heads/#terratorch.models.heads.classification_head","title":"<code>terratorch.models.heads.classification_head</code>","text":""},{"location":"heads/#terratorch.models.heads.classification_head.ClassificationHead","title":"<code>ClassificationHead</code>","text":"<p>               Bases: <code>Module</code></p> <p>Classification head</p> Source code in <code>terratorch/models/heads/classification_head.py</code> <pre><code>class ClassificationHead(nn.Module):\n    \"\"\"Classification head\"\"\"\n\n    # how to allow cls token?\n    def __init__(\n        self,\n        in_dim: int,\n        num_classes: int,\n        dim_list: list[int] | None = None,\n        dropout: float = 0,\n        linear_after_pool: bool = False,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            in_dim (int): Input dimensionality\n            num_classes (int): Number of output classes\n            dim_list (list[int] | None, optional):  List with number of dimensions for each Linear\n                layer to be created. Defaults to None.\n            dropout (float, optional): Dropout value to apply. Defaults to 0.\n            linear_after_pool (bool, optional): Apply pooling first, then apply the linear layer. Defaults to False\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.linear_after_pool = linear_after_pool\n        if dim_list is None:\n            pre_head = nn.Identity()\n        else:\n\n            def block(in_dim, out_dim):\n                return nn.Sequential(nn.Linear(in_features=in_dim, out_features=out_dim), nn.ReLU())\n\n            dim_list = [in_dim, *dim_list]\n            pre_head = nn.Sequential(*[block(dim_list[i], dim_list[i + 1]) for i in range(len(dim_list) - 1)])\n            in_dim = dim_list[-1]\n        dropout = nn.Identity() if dropout == 0 else nn.Dropout(dropout)\n        self.head = nn.Sequential(\n            pre_head,\n            dropout,\n            nn.Linear(in_features=in_dim, out_features=num_classes),\n        )\n\n    def forward(self, x: Tensor):\n        x = x.reshape(x.shape[0], x.shape[1], -1).permute(0, 2, 1)\n\n        if self.linear_after_pool:\n            x = x.mean(axis=1)\n            out = self.head(x)\n        else:\n            x = self.head(x)\n            out = x.mean(axis=1)\n        return out\n</code></pre>"},{"location":"heads/#terratorch.models.heads.classification_head.ClassificationHead.__init__","title":"<code>__init__(in_dim, num_classes, dim_list=None, dropout=0, linear_after_pool=False)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>in_dim</code> <code>int</code> <p>Input dimensionality</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes</p> required <code>dim_list</code> <code>list[int] | None</code> <p>List with number of dimensions for each Linear layer to be created. Defaults to None.</p> <code>None</code> <code>dropout</code> <code>float</code> <p>Dropout value to apply. Defaults to 0.</p> <code>0</code> <code>linear_after_pool</code> <code>bool</code> <p>Apply pooling first, then apply the linear layer. Defaults to False</p> <code>False</code> Source code in <code>terratorch/models/heads/classification_head.py</code> <pre><code>def __init__(\n    self,\n    in_dim: int,\n    num_classes: int,\n    dim_list: list[int] | None = None,\n    dropout: float = 0,\n    linear_after_pool: bool = False,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        in_dim (int): Input dimensionality\n        num_classes (int): Number of output classes\n        dim_list (list[int] | None, optional):  List with number of dimensions for each Linear\n            layer to be created. Defaults to None.\n        dropout (float, optional): Dropout value to apply. Defaults to 0.\n        linear_after_pool (bool, optional): Apply pooling first, then apply the linear layer. Defaults to False\n    \"\"\"\n    super().__init__()\n    self.num_classes = num_classes\n    self.linear_after_pool = linear_after_pool\n    if dim_list is None:\n        pre_head = nn.Identity()\n    else:\n\n        def block(in_dim, out_dim):\n            return nn.Sequential(nn.Linear(in_features=in_dim, out_features=out_dim), nn.ReLU())\n\n        dim_list = [in_dim, *dim_list]\n        pre_head = nn.Sequential(*[block(dim_list[i], dim_list[i + 1]) for i in range(len(dim_list) - 1)])\n        in_dim = dim_list[-1]\n    dropout = nn.Identity() if dropout == 0 else nn.Dropout(dropout)\n    self.head = nn.Sequential(\n        pre_head,\n        dropout,\n        nn.Linear(in_features=in_dim, out_features=num_classes),\n    )\n</code></pre>"},{"location":"license/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright yyyy</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"loss/","title":"Loss","text":""},{"location":"loss/#terratorch.tasks.loss_handler","title":"<code>terratorch.tasks.loss_handler</code>","text":""},{"location":"loss/#terratorch.tasks.loss_handler.LossHandler","title":"<code>LossHandler</code>","text":"<p>Class to help handle the computation and logging of loss</p> Source code in <code>terratorch/tasks/loss_handler.py</code> <pre><code>class LossHandler:\n    \"\"\"Class to help handle the computation and logging of loss\"\"\"\n\n    def __init__(self, loss_prefix: str) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            loss_prefix (str): Prefix to be prepended to all the metrics (e.g. training).\n        \"\"\"\n        self.loss_prefix = loss_prefix\n\n    def compute_loss(\n        self,\n        model_output: ModelOutput,\n        ground_truth: Tensor,\n        criterion: Callable,\n        aux_loss_weights: dict[str, float] | None,\n    ) -&gt; dict[str, Tensor]:\n        \"\"\"Compute the loss for the mean decode head as well as other heads\n\n        Args:\n            model_output (ModelOutput): Output from the model\n            ground_truth (Tensor): Tensor with labels\n            criterion (Callable): Loss function to be applied\n            aux_loss_weights (Union[dict[str, float], None]): Dictionary of names of model auxiliary\n                heads and their weights\n\n        Raises:\n            Exception: If the keys in aux_loss_weights and the model output do not match, will raise an exception.\n\n        Returns:\n            dict[str, Tensor]: Dictionary of computed losses. Total loss is returned under the key \"loss\".\n                If there are auxiliary heads, the main decode head is returned under the key \"decode_head\".\n                All other heads are returned with the same key as their name.\n        \"\"\"\n\n        loss = self._compute_loss(model_output.output, ground_truth, criterion)\n        if not model_output.auxiliary_heads:\n            return {\"loss\": loss}\n\n        if aux_loss_weights is None:\n            msg = \"Auxiliary heads given with no aux_loss_weights\"\n            raise Exception(msg)\n        all_losses = {}\n        all_losses[\"decode_head\"] = loss\n        total_loss = loss.clone()\n        # incorporate aux heads\n        model_output_names = set(model_output.auxiliary_heads.keys())\n        aux_loss_names = set(aux_loss_weights.keys())\n        if aux_loss_names != model_output_names:\n            msg = f\"Found difference in declared auxiliary losses and model outputs.\\n \\\n                Found in declared losses but not in model output: {aux_loss_names - model_output_names}. \\n \\\n                Found in model output but not in delcared losses: {model_output_names - aux_loss_names}\"\n            raise Exception(msg)\n\n        for loss_name, loss_weight in aux_loss_weights.items():\n            output = model_output.auxiliary_heads[loss_name]\n            loss_value: Tensor = self._compute_loss(output, ground_truth, criterion)\n            all_losses[loss_name] = loss_value\n            total_loss = total_loss + loss_value * loss_weight\n\n        all_losses[\"loss\"] = total_loss\n        return all_losses\n\n    def _compute_loss(self, y_hat: Tensor, ground_truth: Tensor, criterion: Callable):\n        loss: Tensor = criterion(y_hat, ground_truth)\n        return loss\n\n    def log_loss(\n        self, log_function: Callable, loss_dict: dict[str, Tensor] | None = None, batch_size: int | None = None\n    ) -&gt; None:\n        \"\"\"Log the loss. If auxiliary heads exist, log the full loss suffix \"loss\", and then all other losses.\n\n        Args:\n            log_function (Callable): _description_\n            loss_dict (dict[str, Tensor], optional): _description_. Defaults to None.\n        \"\"\"\n\n        # dont alter passed dict\n        all_losses = dict(loss_dict)\n        full_loss = all_losses.pop(\"loss\")\n        log_function(f\"{self.loss_prefix}loss\", full_loss, sync_dist=True, batch_size=batch_size)\n\n        for loss_name, loss_value in all_losses.items():\n            log_function(\n                f\"{self.loss_prefix}{loss_name}\",\n                loss_value,\n                on_epoch=True,\n                on_step=True,\n                sync_dist=True,\n                batch_size=batch_size,\n            )\n</code></pre>"},{"location":"loss/#terratorch.tasks.loss_handler.LossHandler.__init__","title":"<code>__init__(loss_prefix)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>loss_prefix</code> <code>str</code> <p>Prefix to be prepended to all the metrics (e.g. training).</p> required Source code in <code>terratorch/tasks/loss_handler.py</code> <pre><code>def __init__(self, loss_prefix: str) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        loss_prefix (str): Prefix to be prepended to all the metrics (e.g. training).\n    \"\"\"\n    self.loss_prefix = loss_prefix\n</code></pre>"},{"location":"loss/#terratorch.tasks.loss_handler.LossHandler.compute_loss","title":"<code>compute_loss(model_output, ground_truth, criterion, aux_loss_weights)</code>","text":"<p>Compute the loss for the mean decode head as well as other heads</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>ModelOutput</code> <p>Output from the model</p> required <code>ground_truth</code> <code>Tensor</code> <p>Tensor with labels</p> required <code>criterion</code> <code>Callable</code> <p>Loss function to be applied</p> required <code>aux_loss_weights</code> <code>Union[dict[str, float], None]</code> <p>Dictionary of names of model auxiliary heads and their weights</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If the keys in aux_loss_weights and the model output do not match, will raise an exception.</p> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>dict[str, Tensor]: Dictionary of computed losses. Total loss is returned under the key \"loss\". If there are auxiliary heads, the main decode head is returned under the key \"decode_head\". All other heads are returned with the same key as their name.</p> Source code in <code>terratorch/tasks/loss_handler.py</code> <pre><code>def compute_loss(\n    self,\n    model_output: ModelOutput,\n    ground_truth: Tensor,\n    criterion: Callable,\n    aux_loss_weights: dict[str, float] | None,\n) -&gt; dict[str, Tensor]:\n    \"\"\"Compute the loss for the mean decode head as well as other heads\n\n    Args:\n        model_output (ModelOutput): Output from the model\n        ground_truth (Tensor): Tensor with labels\n        criterion (Callable): Loss function to be applied\n        aux_loss_weights (Union[dict[str, float], None]): Dictionary of names of model auxiliary\n            heads and their weights\n\n    Raises:\n        Exception: If the keys in aux_loss_weights and the model output do not match, will raise an exception.\n\n    Returns:\n        dict[str, Tensor]: Dictionary of computed losses. Total loss is returned under the key \"loss\".\n            If there are auxiliary heads, the main decode head is returned under the key \"decode_head\".\n            All other heads are returned with the same key as their name.\n    \"\"\"\n\n    loss = self._compute_loss(model_output.output, ground_truth, criterion)\n    if not model_output.auxiliary_heads:\n        return {\"loss\": loss}\n\n    if aux_loss_weights is None:\n        msg = \"Auxiliary heads given with no aux_loss_weights\"\n        raise Exception(msg)\n    all_losses = {}\n    all_losses[\"decode_head\"] = loss\n    total_loss = loss.clone()\n    # incorporate aux heads\n    model_output_names = set(model_output.auxiliary_heads.keys())\n    aux_loss_names = set(aux_loss_weights.keys())\n    if aux_loss_names != model_output_names:\n        msg = f\"Found difference in declared auxiliary losses and model outputs.\\n \\\n            Found in declared losses but not in model output: {aux_loss_names - model_output_names}. \\n \\\n            Found in model output but not in delcared losses: {model_output_names - aux_loss_names}\"\n        raise Exception(msg)\n\n    for loss_name, loss_weight in aux_loss_weights.items():\n        output = model_output.auxiliary_heads[loss_name]\n        loss_value: Tensor = self._compute_loss(output, ground_truth, criterion)\n        all_losses[loss_name] = loss_value\n        total_loss = total_loss + loss_value * loss_weight\n\n    all_losses[\"loss\"] = total_loss\n    return all_losses\n</code></pre>"},{"location":"loss/#terratorch.tasks.loss_handler.LossHandler.log_loss","title":"<code>log_loss(log_function, loss_dict=None, batch_size=None)</code>","text":"<p>Log the loss. If auxiliary heads exist, log the full loss suffix \"loss\", and then all other losses.</p> <p>Parameters:</p> Name Type Description Default <code>log_function</code> <code>Callable</code> <p>description</p> required <code>loss_dict</code> <code>dict[str, Tensor]</code> <p>description. Defaults to None.</p> <code>None</code> Source code in <code>terratorch/tasks/loss_handler.py</code> <pre><code>def log_loss(\n    self, log_function: Callable, loss_dict: dict[str, Tensor] | None = None, batch_size: int | None = None\n) -&gt; None:\n    \"\"\"Log the loss. If auxiliary heads exist, log the full loss suffix \"loss\", and then all other losses.\n\n    Args:\n        log_function (Callable): _description_\n        loss_dict (dict[str, Tensor], optional): _description_. Defaults to None.\n    \"\"\"\n\n    # dont alter passed dict\n    all_losses = dict(loss_dict)\n    full_loss = all_losses.pop(\"loss\")\n    log_function(f\"{self.loss_prefix}loss\", full_loss, sync_dist=True, batch_size=batch_size)\n\n    for loss_name, loss_value in all_losses.items():\n        log_function(\n            f\"{self.loss_prefix}{loss_name}\",\n            loss_value,\n            on_epoch=True,\n            on_step=True,\n            sync_dist=True,\n            batch_size=batch_size,\n        )\n</code></pre>"},{"location":"meta_models/","title":"Meta models","text":""},{"location":"meta_models/#terratorch.models.pixel_wise_model.PixelWiseModel","title":"<code>terratorch.models.pixel_wise_model.PixelWiseModel</code>","text":"<p>               Bases: <code>Model</code>, <code>SegmentationModel</code></p> <p>Model that encapsulates encoder and decoder and heads Expects decoder to have a \"forward_features\" method, an embed_dims property and optionally a \"prepare_features_for_image_model\" method.</p> Source code in <code>terratorch/models/pixel_wise_model.py</code> <pre><code>class PixelWiseModel(Model, SegmentationModel):\n    \"\"\"Model that encapsulates encoder and decoder and heads\n    Expects decoder to have a \"forward_features\" method, an embed_dims property\n    and optionally a \"prepare_features_for_image_model\" method.\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        encoder: nn.Module,\n        decoder: nn.Module,\n        head_kwargs: dict,\n        patch_size: int = None, \n        padding: str = None,\n        decoder_includes_head: bool = False,\n        auxiliary_heads: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None = None,\n        neck: nn.Module | None = None,\n        rescale: bool = True,  # noqa: FBT002, FBT001\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            task (str): Task to be performed. One of segmentation or regression.\n            encoder (nn.Module): Encoder to be used\n            decoder (nn.Module): Decoder to be used\n            head_kwargs (dict): Arguments to be passed at instantiation of the head.\n            decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.\n            auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of\n                AuxiliaryHeads with heads to be instantiated. Defaults to None.\n            neck (nn.Module | None): Module applied between backbone and decoder.\n                Defaults to None, which applies the identity.\n            rescale (bool, optional): Rescale the output of the model if it has a different size than the ground truth.\n                Uses bilinear interpolation. Defaults to True.\n        \"\"\"\n        super().__init__()\n\n        self.task = task\n        self.encoder = encoder\n        self.decoder = decoder\n        self.head = (\n            self._get_head(task, decoder.out_channels, head_kwargs) if not decoder_includes_head else nn.Identity()\n        )\n\n        if auxiliary_heads is not None:\n            aux_heads = {}\n            for aux_head_to_be_instantiated in auxiliary_heads:\n                aux_head: nn.Module = self._get_head(\n                    task, aux_head_to_be_instantiated.decoder.out_channels, head_kwargs\n                ) if not aux_head_to_be_instantiated.decoder_includes_head else nn.Identity()\n                aux_head = nn.Sequential(aux_head_to_be_instantiated.decoder, aux_head)\n                aux_heads[aux_head_to_be_instantiated.name] = aux_head\n        else:\n            aux_heads = {}\n        self.aux_heads = nn.ModuleDict(aux_heads)\n\n        self.neck = neck\n        self.rescale = rescale\n        self.patch_size = patch_size\n        self.padding = padding\n\n    def freeze_encoder(self):\n        freeze_module(self.encoder)\n\n    def freeze_decoder(self):\n        freeze_module(self.decoder)\n\n    def freeze_head(self):\n        freeze_module(self.head)\n\n    @staticmethod\n    def _check_for_single_channel_and_squeeze(x):\n        if x.shape[1] == 1:\n            x = x.squeeze(1)\n        return x\n\n    def forward(self, x: torch.Tensor, **kwargs) -&gt; ModelOutput:\n        \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\"\n\n        def _get_size(x):\n            if isinstance(x, torch.Tensor):\n                return x.shape[-2:]\n            elif isinstance(x, dict):\n                # Multimodal input in passed as dict (Assuming first modality to be an image)\n                return list(x.values())[0].shape[-2:]\n            elif hasattr(kwargs, 'image_size'):\n                return kwargs['image_size']\n            else:\n                ValueError('Could not infer image shape.')\n\n        image_size = _get_size(x)\n        if isinstance(x, torch.Tensor) and self.patch_size:\n            # Only works for single image modalities\n            x = pad_images(x, self.patch_size, self.padding)\n        input_size = _get_size(x)\n\n        features = self.encoder(x, **kwargs)\n\n        # only for backwards compatibility with pre-neck times.\n        if self.neck:\n            prepare = self.neck\n        else:\n            # for backwards compatibility, if this is defined in the encoder, use it\n            prepare = getattr(self.encoder, \"prepare_features_for_image_model\", lambda x: x)\n        print(f\"neck: {self.neck}\")\n        features = prepare(features)\n        print([f.shape for f in features])\n        decoder_output = self.decoder([f.clone() for f in features])\n        mask = self.head(decoder_output)\n        if self.rescale and mask.shape[-2:] != input_size:\n            mask = F.interpolate(mask, size=input_size, mode=\"bilinear\")\n        mask = self._check_for_single_channel_and_squeeze(mask)\n        mask = mask[..., :image_size[0], :image_size[1]]\n\n        aux_outputs = {}\n        for name, decoder in self.aux_heads.items():\n            aux_output = decoder([f.clone() for f in features])\n            if self.rescale and aux_output.shape[-2:] != input_size:\n                aux_output = F.interpolate(aux_output, size=input_size, mode=\"bilinear\")\n            aux_output = self._check_for_single_channel_and_squeeze(aux_output)\n            aux_output = aux_output[..., :image_size[0], :image_size[1]]\n            aux_outputs[name] = aux_output\n\n\n        return ModelOutput(output=mask, auxiliary_heads=aux_outputs)\n\n    def _get_head(self, task: str, input_embed_dim: int, head_kwargs):\n        if task == \"segmentation\":\n            if \"num_classes\" not in head_kwargs:\n                msg = \"num_classes must be defined for segmentation task\"\n                raise Exception(msg)\n            return SegmentationHead(input_embed_dim, **head_kwargs)\n        if task == \"regression\":\n            return RegressionHead(input_embed_dim, **head_kwargs)\n        msg = \"Task must be one of segmentation or regression.\"\n        raise Exception(msg)\n</code></pre>"},{"location":"meta_models/#terratorch.models.pixel_wise_model.PixelWiseModel.__init__","title":"<code>__init__(task, encoder, decoder, head_kwargs, patch_size=None, padding=None, decoder_includes_head=False, auxiliary_heads=None, neck=None, rescale=True)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. One of segmentation or regression.</p> required <code>encoder</code> <code>Module</code> <p>Encoder to be used</p> required <code>decoder</code> <code>Module</code> <p>Decoder to be used</p> required <code>head_kwargs</code> <code>dict</code> <p>Arguments to be passed at instantiation of the head.</p> required <code>decoder_includes_head</code> <code>bool</code> <p>Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.</p> <code>False</code> <code>auxiliary_heads</code> <code>list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None</code> <p>List of AuxiliaryHeads with heads to be instantiated. Defaults to None.</p> <code>None</code> <code>neck</code> <code>Module | None</code> <p>Module applied between backbone and decoder. Defaults to None, which applies the identity.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Rescale the output of the model if it has a different size than the ground truth. Uses bilinear interpolation. Defaults to True.</p> <code>True</code> Source code in <code>terratorch/models/pixel_wise_model.py</code> <pre><code>def __init__(\n    self,\n    task: str,\n    encoder: nn.Module,\n    decoder: nn.Module,\n    head_kwargs: dict,\n    patch_size: int = None, \n    padding: str = None,\n    decoder_includes_head: bool = False,\n    auxiliary_heads: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None = None,\n    neck: nn.Module | None = None,\n    rescale: bool = True,  # noqa: FBT002, FBT001\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        task (str): Task to be performed. One of segmentation or regression.\n        encoder (nn.Module): Encoder to be used\n        decoder (nn.Module): Decoder to be used\n        head_kwargs (dict): Arguments to be passed at instantiation of the head.\n        decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.\n        auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of\n            AuxiliaryHeads with heads to be instantiated. Defaults to None.\n        neck (nn.Module | None): Module applied between backbone and decoder.\n            Defaults to None, which applies the identity.\n        rescale (bool, optional): Rescale the output of the model if it has a different size than the ground truth.\n            Uses bilinear interpolation. Defaults to True.\n    \"\"\"\n    super().__init__()\n\n    self.task = task\n    self.encoder = encoder\n    self.decoder = decoder\n    self.head = (\n        self._get_head(task, decoder.out_channels, head_kwargs) if not decoder_includes_head else nn.Identity()\n    )\n\n    if auxiliary_heads is not None:\n        aux_heads = {}\n        for aux_head_to_be_instantiated in auxiliary_heads:\n            aux_head: nn.Module = self._get_head(\n                task, aux_head_to_be_instantiated.decoder.out_channels, head_kwargs\n            ) if not aux_head_to_be_instantiated.decoder_includes_head else nn.Identity()\n            aux_head = nn.Sequential(aux_head_to_be_instantiated.decoder, aux_head)\n            aux_heads[aux_head_to_be_instantiated.name] = aux_head\n    else:\n        aux_heads = {}\n    self.aux_heads = nn.ModuleDict(aux_heads)\n\n    self.neck = neck\n    self.rescale = rescale\n    self.patch_size = patch_size\n    self.padding = padding\n</code></pre>"},{"location":"meta_models/#terratorch.models.pixel_wise_model.PixelWiseModel.forward","title":"<code>forward(x, **kwargs)</code>","text":"<p>Sequentially pass <code>x</code> through model`s encoder, decoder and heads</p> Source code in <code>terratorch/models/pixel_wise_model.py</code> <pre><code>def forward(self, x: torch.Tensor, **kwargs) -&gt; ModelOutput:\n    \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\"\n\n    def _get_size(x):\n        if isinstance(x, torch.Tensor):\n            return x.shape[-2:]\n        elif isinstance(x, dict):\n            # Multimodal input in passed as dict (Assuming first modality to be an image)\n            return list(x.values())[0].shape[-2:]\n        elif hasattr(kwargs, 'image_size'):\n            return kwargs['image_size']\n        else:\n            ValueError('Could not infer image shape.')\n\n    image_size = _get_size(x)\n    if isinstance(x, torch.Tensor) and self.patch_size:\n        # Only works for single image modalities\n        x = pad_images(x, self.patch_size, self.padding)\n    input_size = _get_size(x)\n\n    features = self.encoder(x, **kwargs)\n\n    # only for backwards compatibility with pre-neck times.\n    if self.neck:\n        prepare = self.neck\n    else:\n        # for backwards compatibility, if this is defined in the encoder, use it\n        prepare = getattr(self.encoder, \"prepare_features_for_image_model\", lambda x: x)\n    print(f\"neck: {self.neck}\")\n    features = prepare(features)\n    print([f.shape for f in features])\n    decoder_output = self.decoder([f.clone() for f in features])\n    mask = self.head(decoder_output)\n    if self.rescale and mask.shape[-2:] != input_size:\n        mask = F.interpolate(mask, size=input_size, mode=\"bilinear\")\n    mask = self._check_for_single_channel_and_squeeze(mask)\n    mask = mask[..., :image_size[0], :image_size[1]]\n\n    aux_outputs = {}\n    for name, decoder in self.aux_heads.items():\n        aux_output = decoder([f.clone() for f in features])\n        if self.rescale and aux_output.shape[-2:] != input_size:\n            aux_output = F.interpolate(aux_output, size=input_size, mode=\"bilinear\")\n        aux_output = self._check_for_single_channel_and_squeeze(aux_output)\n        aux_output = aux_output[..., :image_size[0], :image_size[1]]\n        aux_outputs[name] = aux_output\n\n\n    return ModelOutput(output=mask, auxiliary_heads=aux_outputs)\n</code></pre>"},{"location":"meta_models/#terratorch.models.scalar_output_model.ScalarOutputModel","title":"<code>terratorch.models.scalar_output_model.ScalarOutputModel</code>","text":"<p>               Bases: <code>Model</code>, <code>SegmentationModel</code></p> <p>Model that encapsulates encoder and decoder and heads for a scalar output Expects decoder to have a \"forward_features\" method, an embed_dims property and optionally a \"prepare_features_for_image_model\" method.</p> Source code in <code>terratorch/models/scalar_output_model.py</code> <pre><code>class ScalarOutputModel(Model, SegmentationModel):\n    \"\"\"Model that encapsulates encoder and decoder and heads for a scalar output\n    Expects decoder to have a \"forward_features\" method, an embed_dims property\n    and optionally a \"prepare_features_for_image_model\" method.\n    \"\"\"\n\n    def __init__(\n        self,\n        task: str,\n        encoder: nn.Module,\n        decoder: nn.Module,\n        head_kwargs: dict,\n        patch_size: int = None,\n        padding: str = None,\n        decoder_includes_head: bool = False,\n        auxiliary_heads: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None = None,\n        neck: nn.Module | None = None,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            task (str): Task to be performed. Must be \"classification\".\n            encoder (nn.Module): Encoder to be used\n            decoder (nn.Module): Decoder to be used\n            head_kwargs (dict): Arguments to be passed at instantiation of the head.\n            decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.\n            auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of\n                AuxiliaryHeads with heads to be instantiated. Defaults to None.\n            neck (nn.Module | None): Module applied between backbone and decoder.\n                Defaults to None, which applies the identity.\n        \"\"\"\n        super().__init__()\n        self.task = task\n        self.encoder = encoder\n        self.decoder = decoder\n        self.head = (\n            self._get_head(task, decoder.out_channels, head_kwargs) if not decoder_includes_head else nn.Identity()\n        )\n\n        if auxiliary_heads is not None:\n            aux_heads = {}\n            for aux_head_to_be_instantiated in auxiliary_heads:\n                aux_head: nn.Module = self._get_head(\n                    task, aux_head_to_be_instantiated.decoder.out_channels, head_kwargs\n                ) if not aux_head_to_be_instantiated.decoder_includes_head else nn.Identity()\n                aux_head = nn.Sequential(aux_head_to_be_instantiated.decoder, aux_head)\n                aux_heads[aux_head_to_be_instantiated.name] = aux_head\n        else:\n            aux_heads = {}\n        self.aux_heads = nn.ModuleDict(aux_heads)\n\n        self.neck = neck\n        self.patch_size = patch_size\n        self.padding = padding\n\n    def freeze_encoder(self):\n        freeze_module(self.encoder)\n\n    def freeze_decoder(self):\n        freeze_module(self.decoder)\n\n    def freeze_head(self):\n        freeze_module(self.head)\n\n    def forward(self, x: torch.Tensor, **kwargs) -&gt; ModelOutput:\n        \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\"\n\n        if isinstance(x, torch.Tensor) and self.patch_size:\n            # Only works for single image modalities\n            x = pad_images(x, self.patch_size, self.padding)\n        features = self.encoder(x, **kwargs)\n\n        # only for backwards compatibility with pre-neck times.\n        if self.neck:\n            prepare = self.neck\n        else:\n            # for backwards compatibility, if this is defined in the encoder, use it\n            prepare = getattr(self.encoder, \"prepare_features_for_image_model\", lambda x: x)\n\n        features = prepare(features)\n\n        decoder_output = self.decoder([f.clone() for f in features])\n        mask = self.head(decoder_output)\n\n        aux_outputs = {}\n        for name, decoder in self.aux_heads.items():\n            aux_output = decoder([f.clone() for f in features])\n            aux_outputs[name] = aux_output\n\n        return ModelOutput(output=mask, auxiliary_heads=aux_outputs)\n\n    def _get_head(self, task: str, input_embed_dim: int, head_kwargs: dict):\n        if task == \"classification\":\n            if \"num_classes\" not in head_kwargs:\n                msg = \"num_classes must be defined for classification task\"\n                raise Exception(msg)\n            return ClassificationHead(input_embed_dim, **head_kwargs)\n        msg = \"Task must be classification.\"\n        raise Exception(msg)\n</code></pre>"},{"location":"meta_models/#terratorch.models.scalar_output_model.ScalarOutputModel.__init__","title":"<code>__init__(task, encoder, decoder, head_kwargs, patch_size=None, padding=None, decoder_includes_head=False, auxiliary_heads=None, neck=None)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Must be \"classification\".</p> required <code>encoder</code> <code>Module</code> <p>Encoder to be used</p> required <code>decoder</code> <code>Module</code> <p>Decoder to be used</p> required <code>head_kwargs</code> <code>dict</code> <p>Arguments to be passed at instantiation of the head.</p> required <code>decoder_includes_head</code> <code>bool</code> <p>Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.</p> <code>False</code> <code>auxiliary_heads</code> <code>list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None</code> <p>List of AuxiliaryHeads with heads to be instantiated. Defaults to None.</p> <code>None</code> <code>neck</code> <code>Module | None</code> <p>Module applied between backbone and decoder. Defaults to None, which applies the identity.</p> <code>None</code> Source code in <code>terratorch/models/scalar_output_model.py</code> <pre><code>def __init__(\n    self,\n    task: str,\n    encoder: nn.Module,\n    decoder: nn.Module,\n    head_kwargs: dict,\n    patch_size: int = None,\n    padding: str = None,\n    decoder_includes_head: bool = False,\n    auxiliary_heads: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None = None,\n    neck: nn.Module | None = None,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        task (str): Task to be performed. Must be \"classification\".\n        encoder (nn.Module): Encoder to be used\n        decoder (nn.Module): Decoder to be used\n        head_kwargs (dict): Arguments to be passed at instantiation of the head.\n        decoder_includes_head (bool): Whether the decoder already incldes a head. If true, a head will not be added. Defaults to False.\n        auxiliary_heads (list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] | None, optional): List of\n            AuxiliaryHeads with heads to be instantiated. Defaults to None.\n        neck (nn.Module | None): Module applied between backbone and decoder.\n            Defaults to None, which applies the identity.\n    \"\"\"\n    super().__init__()\n    self.task = task\n    self.encoder = encoder\n    self.decoder = decoder\n    self.head = (\n        self._get_head(task, decoder.out_channels, head_kwargs) if not decoder_includes_head else nn.Identity()\n    )\n\n    if auxiliary_heads is not None:\n        aux_heads = {}\n        for aux_head_to_be_instantiated in auxiliary_heads:\n            aux_head: nn.Module = self._get_head(\n                task, aux_head_to_be_instantiated.decoder.out_channels, head_kwargs\n            ) if not aux_head_to_be_instantiated.decoder_includes_head else nn.Identity()\n            aux_head = nn.Sequential(aux_head_to_be_instantiated.decoder, aux_head)\n            aux_heads[aux_head_to_be_instantiated.name] = aux_head\n    else:\n        aux_heads = {}\n    self.aux_heads = nn.ModuleDict(aux_heads)\n\n    self.neck = neck\n    self.patch_size = patch_size\n    self.padding = padding\n</code></pre>"},{"location":"meta_models/#terratorch.models.scalar_output_model.ScalarOutputModel.forward","title":"<code>forward(x, **kwargs)</code>","text":"<p>Sequentially pass <code>x</code> through model`s encoder, decoder and heads</p> Source code in <code>terratorch/models/scalar_output_model.py</code> <pre><code>def forward(self, x: torch.Tensor, **kwargs) -&gt; ModelOutput:\n    \"\"\"Sequentially pass `x` through model`s encoder, decoder and heads\"\"\"\n\n    if isinstance(x, torch.Tensor) and self.patch_size:\n        # Only works for single image modalities\n        x = pad_images(x, self.patch_size, self.padding)\n    features = self.encoder(x, **kwargs)\n\n    # only for backwards compatibility with pre-neck times.\n    if self.neck:\n        prepare = self.neck\n    else:\n        # for backwards compatibility, if this is defined in the encoder, use it\n        prepare = getattr(self.encoder, \"prepare_features_for_image_model\", lambda x: x)\n\n    features = prepare(features)\n\n    decoder_output = self.decoder([f.clone() for f in features])\n    mask = self.head(decoder_output)\n\n    aux_outputs = {}\n    for name, decoder in self.aux_heads.items():\n        aux_output = decoder([f.clone() for f in features])\n        aux_outputs[name] = aux_output\n\n    return ModelOutput(output=mask, auxiliary_heads=aux_outputs)\n</code></pre>"},{"location":"model_factories/","title":"Model factories","text":""},{"location":"model_factories/#terratorch.models.model.ModelFactory","title":"<code>terratorch.models.model.ModelFactory</code>","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>terratorch/models/model.py</code> <pre><code>class ModelFactory(typing.Protocol):\n    def build_model(self, *args, **kwargs) -&gt; Model:...\n</code></pre>"},{"location":"model_factories/#terratorch.models.clay_model_factory.ClayModelFactory","title":"<code>terratorch.models.clay_model_factory.ClayModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/clay_model_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass ClayModelFactory(ModelFactory):\n    def build_model(\n        self,\n        task: str,\n        backbone: str | nn.Module,\n        decoder: str | nn.Module,\n        in_channels: int,\n        bands: list[int] = [],\n        num_classes: int | None = None,\n        pretrained: bool = True,  # noqa: FBT001, FBT002\n        num_frames: int = 1,\n        prepare_features_for_image_model: Callable | None = None,\n        aux_decoders: list[AuxiliaryHead] | None = None,\n        rescale: bool = True,  # noqa: FBT002, FBT001\n        checkpoint_path: str = None,\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"Model factory for Clay models.\n\n        Further arguments to be passed to the backbone, decoder or head. They should be prefixed with\n        `backbone_`, `decoder_` and `head_` respectively.\n\n        Args:\n            task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\".\n            backbone (str, nn.Module): Backbone to be used. If string, should be able to be parsed\n                by the specified factory. Defaults to \"prithvi_100\".\n            decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model.\n                    If a string, it will be created from a class exposed in decoder.__init__.py with the same name.\n                    If an nn.Module, we expect it to expose a property `decoder.out_channels`.\n                    Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".\n            in_channels (int, optional): Number of input channels. Defaults to 3.\n            num_classes (int, optional): Number of classes. None for regression tasks.\n            pretrained (Union[bool, Path], optional): Whether to load pretrained weights for the backbone, if available.\n                Defaults to True.\n            num_frames (int, optional): Number of timesteps for the model to handle. Defaults to 1.\n            prepare_features_for_image_model (Callable | None): Function to be called on encoder features\n                before passing them to the decoder. Defaults to None, which applies the identity function.\n            aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead deciders to be added to the model.\n                These decoders take the input from the encoder as well.\n            rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n                is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.\n\n        Raises:\n            NotImplementedError: _description_\n            DecoderNotFoundException: _description_\n\n        Returns:\n            nn.Module: _description_\n        \"\"\"\n        if not torch.cuda.is_available():\n            self.CPU_ONLY = True\n        else:\n            self.CPU_ONLY = False\n\n        # Path for accessing the model source code.\n        self.syspath_kwarg = \"model_sys_path\"\n        backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n\n        # TODO: support auxiliary heads\n        if not isinstance(backbone, nn.Module):\n            if not \"clay\" in backbone:\n                msg = \"This class only handles models for `Clay` encoders\"\n                raise NotImplementedError(msg)\n\n            task = task.lower()\n            if task not in SUPPORTED_TASKS:\n                msg = f\"Task {task} not supported. Please choose one of {SUPPORTED_TASKS}\"\n                raise NotImplementedError(msg)\n\n            # Trying to find the model on HuggingFace.\n            try:\n                backbone: nn.Module = timm.create_model(\n                    backbone,\n                    pretrained=pretrained,\n                    in_chans=in_channels,\n                    bands=bands,\n                    num_frames=num_frames,\n                    features_only=True,\n                    **backbone_kwargs,\n                )\n            except Exception as e:\n                print(e, \"Error loading from HF. Trying to instantiate locally ...\")\n\n        else:\n            if checkpoint_path is None:\n                raise ValueError(\"A checkpoint (checkpoint_path) must be provided to restore the model.\")\n\n            backbone: nn.Module = Embedder(ckpt_path=checkpoint_path, **backbone_kwargs)\n            print(\"Model Clay was successfully restored.\")\n\n        # If patch size is not provided in the config or by the model, it might lead to errors due to irregular images.\n        patch_size = backbone_kwargs.get(\"patch_size\", None)\n        if patch_size is None:\n            # Infer patch size from model by checking all backbone modules\n            for module in backbone.modules():\n                if hasattr(module, \"patch_size\"):\n                    patch_size = module.patch_size\n                    break\n        padding = backbone_kwargs.get(\"padding\", \"reflect\")\n\n        # allow decoder to be a module passed directly\n        decoder_cls = _get_decoder(decoder)\n        decoder_kwargs, kwargs = extract_prefix_keys(kwargs, \"decoder_\")\n\n        # TODO: remove this\n        decoder: nn.Module = decoder_cls(\n            backbone.feature_info.channels(), **decoder_kwargs)\n        # decoder: nn.Module = decoder_cls([128, 256, 512, 1024], **decoder_kwargs)\n\n        head_kwargs, kwargs = extract_prefix_keys(kwargs, \"head_\")\n        if num_classes:\n            head_kwargs[\"num_classes\"] = num_classes\n        if aux_decoders is None:\n            return _build_appropriate_model(\n                task, backbone, decoder, head_kwargs, prepare_features_for_image_model, patch_size=patch_size, padding=padding, rescale=rescale\n            )\n\n        to_be_aux_decoders: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] = []\n\n        for aux_decoder in aux_decoders:\n            args = aux_decoder.decoder_args if aux_decoder.decoder_args else {}\n            aux_decoder_cls: nn.Module = _get_decoder(aux_decoder.decoder)\n\n            aux_decoder_kwargs, kwargs = extract_prefix_keys(args, \"decoder_\")\n            aux_decoder_instance = aux_decoder_cls(backbone.feature_info.channels(), **aux_decoder_kwargs)\n            # aux_decoder_instance = aux_decoder_cls([128, 256, 512, 1024], **decoder_kwargs)\n\n            aux_head_kwargs, kwargs = extract_prefix_keys(args, \"head_\")\n            if num_classes:\n                aux_head_kwargs[\"num_classes\"] = num_classes\n            # aux_head: nn.Module = _get_head(task, aux_decoder_instance, num_classes=num_classes, **head_kwargs)\n            # aux_decoder.decoder = nn.Sequential(aux_decoder_instance, aux_head)\n            to_be_aux_decoders.append(\n                AuxiliaryHeadWithDecoderWithoutInstantiatedHead(\n                    aux_decoder.name, aux_decoder_instance, aux_head_kwargs)\n            )\n\n        return _build_appropriate_model(\n            task,\n            backbone,\n            decoder,\n            head_kwargs,\n            prepare_features_for_image_model,\n            patch_size=patch_size,\n            padding=padding,\n            rescale=rescale,\n            auxiliary_heads=to_be_aux_decoders,\n        )\n</code></pre>"},{"location":"model_factories/#terratorch.models.clay_model_factory.ClayModelFactory.build_model","title":"<code>build_model(task, backbone, decoder, in_channels, bands=[], num_classes=None, pretrained=True, num_frames=1, prepare_features_for_image_model=None, aux_decoders=None, rescale=True, checkpoint_path=None, **kwargs)</code>","text":"<p>Model factory for Clay models.</p> <p>Further arguments to be passed to the backbone, decoder or head. They should be prefixed with <code>backbone_</code>, <code>decoder_</code> and <code>head_</code> respectively.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Currently supports \"segmentation\" and \"regression\".</p> required <code>backbone</code> <code>(str, Module)</code> <p>Backbone to be used. If string, should be able to be parsed by the specified factory. Defaults to \"prithvi_100\".</p> required <code>decoder</code> <code>Union[str, Module]</code> <p>Decoder to be used for the segmentation model.     If a string, it will be created from a class exposed in decoder.init.py with the same name.     If an nn.Module, we expect it to expose a property <code>decoder.out_channels</code>.     Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes. None for regression tasks.</p> <code>None</code> <code>pretrained</code> <code>Union[bool, Path]</code> <p>Whether to load pretrained weights for the backbone, if available. Defaults to True.</p> <code>True</code> <code>num_frames</code> <code>int</code> <p>Number of timesteps for the model to handle. Defaults to 1.</p> <code>1</code> <code>prepare_features_for_image_model</code> <code>Callable | None</code> <p>Function to be called on encoder features before passing them to the decoder. Defaults to None, which applies the identity function.</p> <code>None</code> <code>aux_decoders</code> <code>list[AuxiliaryHead] | None</code> <p>List of AuxiliaryHead deciders to be added to the model. These decoders take the input from the encoder as well.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> <code>DecoderNotFoundException</code> <p>description</p> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: description</p> Source code in <code>terratorch/models/clay_model_factory.py</code> <pre><code>def build_model(\n    self,\n    task: str,\n    backbone: str | nn.Module,\n    decoder: str | nn.Module,\n    in_channels: int,\n    bands: list[int] = [],\n    num_classes: int | None = None,\n    pretrained: bool = True,  # noqa: FBT001, FBT002\n    num_frames: int = 1,\n    prepare_features_for_image_model: Callable | None = None,\n    aux_decoders: list[AuxiliaryHead] | None = None,\n    rescale: bool = True,  # noqa: FBT002, FBT001\n    checkpoint_path: str = None,\n    **kwargs,\n) -&gt; Model:\n    \"\"\"Model factory for Clay models.\n\n    Further arguments to be passed to the backbone, decoder or head. They should be prefixed with\n    `backbone_`, `decoder_` and `head_` respectively.\n\n    Args:\n        task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\".\n        backbone (str, nn.Module): Backbone to be used. If string, should be able to be parsed\n            by the specified factory. Defaults to \"prithvi_100\".\n        decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model.\n                If a string, it will be created from a class exposed in decoder.__init__.py with the same name.\n                If an nn.Module, we expect it to expose a property `decoder.out_channels`.\n                Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".\n        in_channels (int, optional): Number of input channels. Defaults to 3.\n        num_classes (int, optional): Number of classes. None for regression tasks.\n        pretrained (Union[bool, Path], optional): Whether to load pretrained weights for the backbone, if available.\n            Defaults to True.\n        num_frames (int, optional): Number of timesteps for the model to handle. Defaults to 1.\n        prepare_features_for_image_model (Callable | None): Function to be called on encoder features\n            before passing them to the decoder. Defaults to None, which applies the identity function.\n        aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead deciders to be added to the model.\n            These decoders take the input from the encoder as well.\n        rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n            is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.\n\n    Raises:\n        NotImplementedError: _description_\n        DecoderNotFoundException: _description_\n\n    Returns:\n        nn.Module: _description_\n    \"\"\"\n    if not torch.cuda.is_available():\n        self.CPU_ONLY = True\n    else:\n        self.CPU_ONLY = False\n\n    # Path for accessing the model source code.\n    self.syspath_kwarg = \"model_sys_path\"\n    backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n\n    # TODO: support auxiliary heads\n    if not isinstance(backbone, nn.Module):\n        if not \"clay\" in backbone:\n            msg = \"This class only handles models for `Clay` encoders\"\n            raise NotImplementedError(msg)\n\n        task = task.lower()\n        if task not in SUPPORTED_TASKS:\n            msg = f\"Task {task} not supported. Please choose one of {SUPPORTED_TASKS}\"\n            raise NotImplementedError(msg)\n\n        # Trying to find the model on HuggingFace.\n        try:\n            backbone: nn.Module = timm.create_model(\n                backbone,\n                pretrained=pretrained,\n                in_chans=in_channels,\n                bands=bands,\n                num_frames=num_frames,\n                features_only=True,\n                **backbone_kwargs,\n            )\n        except Exception as e:\n            print(e, \"Error loading from HF. Trying to instantiate locally ...\")\n\n    else:\n        if checkpoint_path is None:\n            raise ValueError(\"A checkpoint (checkpoint_path) must be provided to restore the model.\")\n\n        backbone: nn.Module = Embedder(ckpt_path=checkpoint_path, **backbone_kwargs)\n        print(\"Model Clay was successfully restored.\")\n\n    # If patch size is not provided in the config or by the model, it might lead to errors due to irregular images.\n    patch_size = backbone_kwargs.get(\"patch_size\", None)\n    if patch_size is None:\n        # Infer patch size from model by checking all backbone modules\n        for module in backbone.modules():\n            if hasattr(module, \"patch_size\"):\n                patch_size = module.patch_size\n                break\n    padding = backbone_kwargs.get(\"padding\", \"reflect\")\n\n    # allow decoder to be a module passed directly\n    decoder_cls = _get_decoder(decoder)\n    decoder_kwargs, kwargs = extract_prefix_keys(kwargs, \"decoder_\")\n\n    # TODO: remove this\n    decoder: nn.Module = decoder_cls(\n        backbone.feature_info.channels(), **decoder_kwargs)\n    # decoder: nn.Module = decoder_cls([128, 256, 512, 1024], **decoder_kwargs)\n\n    head_kwargs, kwargs = extract_prefix_keys(kwargs, \"head_\")\n    if num_classes:\n        head_kwargs[\"num_classes\"] = num_classes\n    if aux_decoders is None:\n        return _build_appropriate_model(\n            task, backbone, decoder, head_kwargs, prepare_features_for_image_model, patch_size=patch_size, padding=padding, rescale=rescale\n        )\n\n    to_be_aux_decoders: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] = []\n\n    for aux_decoder in aux_decoders:\n        args = aux_decoder.decoder_args if aux_decoder.decoder_args else {}\n        aux_decoder_cls: nn.Module = _get_decoder(aux_decoder.decoder)\n\n        aux_decoder_kwargs, kwargs = extract_prefix_keys(args, \"decoder_\")\n        aux_decoder_instance = aux_decoder_cls(backbone.feature_info.channels(), **aux_decoder_kwargs)\n        # aux_decoder_instance = aux_decoder_cls([128, 256, 512, 1024], **decoder_kwargs)\n\n        aux_head_kwargs, kwargs = extract_prefix_keys(args, \"head_\")\n        if num_classes:\n            aux_head_kwargs[\"num_classes\"] = num_classes\n        # aux_head: nn.Module = _get_head(task, aux_decoder_instance, num_classes=num_classes, **head_kwargs)\n        # aux_decoder.decoder = nn.Sequential(aux_decoder_instance, aux_head)\n        to_be_aux_decoders.append(\n            AuxiliaryHeadWithDecoderWithoutInstantiatedHead(\n                aux_decoder.name, aux_decoder_instance, aux_head_kwargs)\n        )\n\n    return _build_appropriate_model(\n        task,\n        backbone,\n        decoder,\n        head_kwargs,\n        prepare_features_for_image_model,\n        patch_size=patch_size,\n        padding=padding,\n        rescale=rescale,\n        auxiliary_heads=to_be_aux_decoders,\n    )\n</code></pre>"},{"location":"model_factories/#terratorch.models.generic_unet_model_factory.GenericUnetModelFactory","title":"<code>terratorch.models.generic_unet_model_factory.GenericUnetModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/generic_unet_model_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass GenericUnetModelFactory(ModelFactory):\n    def _check_model_availability(self, model, builtin_engine, engine, **model_kwargs):\n\n        try:\n            print(f\"Using module {model} from terratorch.\")\n            model_class = getattr(builtin_engine, model)\n        except:\n            if _has_mmseg:\n                print(\"Module not available on terratorch.\")\n                print(f\"Using module {model} from mmseg.\")\n                model_class = getattr(engine, model)\n            else:\n                raise Exception(\"mmseg is not installed.\")\n\n        model = model_class(\n           **model_kwargs,\n        )\n\n        return model \n\n    def build_model(\n        self,\n        task: str = \"segmentation\",\n        backbone: str | None = None,\n        decoder: str | None = None,\n        dilations: tuple[int] = (1, 6, 12, 18),\n        in_channels: int = 6,\n        pretrained: str | bool | None = True,\n        num_classes: int = 1,\n        regression_relu: bool = False,\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"Factory to create model based on SMP.\n\n        Args:\n            task (str): Must be \"segmentation\".\n            model (str): Decoder architecture. Currently only supports \"unet\".\n            in_channels (int): Number of input channels.\n            pretrained(str | bool): Which weights to use for the backbone. If true, will use \"imagenet\". If false or None, random weights. Defaults to True.\n            num_classes (int): Number of classes.\n            regression_relu (bool). Whether to apply a ReLU if task is regression. Defaults to False.\n\n        Returns:\n            Model: SMP model wrapped in SMPModelWrapper.\n        \"\"\"\n        if task not in [\"segmentation\", \"regression\"]:\n            msg = f\"SMP models can only perform pixel wise tasks, but got task {task}\"\n            raise Exception(msg)\n\n        builtin_engine_decoders = importlib.import_module(\"terratorch.models.decoders\")\n        builtin_engine_encoders = importlib.import_module(\"terratorch.models.backbones\")\n\n        try:\n            engine_decoders = importlib.import_module(\"mmseg.models.decode_heads\")\n            engine_encoders = importlib.import_module(\"mmseg.models.backbones\")\n            _has_mmseg = True\n        except:\n            engine_decoders = None\n            engine_encoders = None\n            _has_mmseg = False\n            print(\"mmseg is not installed.\")\n\n        if backbone:\n            backbone_kwargs = _extract_prefix_keys(kwargs, \"backbone_\")\n            backbone_model_kwargs = backbone_kwargs\n            backbone_engine = engine_encoders\n            backbone_builtin_engine = builtin_engine_encoders\n        else:\n            backbone=None\n\n        if decoder: \n            decoder_kwargs = _extract_prefix_keys(kwargs, \"decoder_\")\n            decoder_model_kwargs = decoder_kwargs\n            decoder_engine = engine_decoders\n            decoder_builtin_engine = builtin_engine_decoders\n        else:\n            decoder = None \n\n        if not backbone and not decoder:\n            print(\"It is necessary to define a backbone and/or a decoder.\")\n\n        # Instantianting backbone and decoder \n        backbone = self._check_model_availability(backbone, backbone_builtin_engine, backbone_engine, **backbone_model_kwargs) \n        decoder = self._check_model_availability(decoder, decoder_builtin_engine, decoder_engine, **decoder_model_kwargs) \n\n        return GenericUnetModelWrapper(\n            backbone, decoder=decoder, relu=task == \"regression\" and regression_relu, squeeze_single_class=task == \"regression\"\n        )\n</code></pre>"},{"location":"model_factories/#terratorch.models.generic_unet_model_factory.GenericUnetModelFactory.build_model","title":"<code>build_model(task='segmentation', backbone=None, decoder=None, dilations=(1, 6, 12, 18), in_channels=6, pretrained=True, num_classes=1, regression_relu=False, **kwargs)</code>","text":"<p>Factory to create model based on SMP.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Must be \"segmentation\".</p> <code>'segmentation'</code> <code>model</code> <code>str</code> <p>Decoder architecture. Currently only supports \"unet\".</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> <code>6</code> <code>pretrained(str</code> <code>| bool</code> <p>Which weights to use for the backbone. If true, will use \"imagenet\". If false or None, random weights. Defaults to True.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>SMP model wrapped in SMPModelWrapper.</p> Source code in <code>terratorch/models/generic_unet_model_factory.py</code> <pre><code>def build_model(\n    self,\n    task: str = \"segmentation\",\n    backbone: str | None = None,\n    decoder: str | None = None,\n    dilations: tuple[int] = (1, 6, 12, 18),\n    in_channels: int = 6,\n    pretrained: str | bool | None = True,\n    num_classes: int = 1,\n    regression_relu: bool = False,\n    **kwargs,\n) -&gt; Model:\n    \"\"\"Factory to create model based on SMP.\n\n    Args:\n        task (str): Must be \"segmentation\".\n        model (str): Decoder architecture. Currently only supports \"unet\".\n        in_channels (int): Number of input channels.\n        pretrained(str | bool): Which weights to use for the backbone. If true, will use \"imagenet\". If false or None, random weights. Defaults to True.\n        num_classes (int): Number of classes.\n        regression_relu (bool). Whether to apply a ReLU if task is regression. Defaults to False.\n\n    Returns:\n        Model: SMP model wrapped in SMPModelWrapper.\n    \"\"\"\n    if task not in [\"segmentation\", \"regression\"]:\n        msg = f\"SMP models can only perform pixel wise tasks, but got task {task}\"\n        raise Exception(msg)\n\n    builtin_engine_decoders = importlib.import_module(\"terratorch.models.decoders\")\n    builtin_engine_encoders = importlib.import_module(\"terratorch.models.backbones\")\n\n    try:\n        engine_decoders = importlib.import_module(\"mmseg.models.decode_heads\")\n        engine_encoders = importlib.import_module(\"mmseg.models.backbones\")\n        _has_mmseg = True\n    except:\n        engine_decoders = None\n        engine_encoders = None\n        _has_mmseg = False\n        print(\"mmseg is not installed.\")\n\n    if backbone:\n        backbone_kwargs = _extract_prefix_keys(kwargs, \"backbone_\")\n        backbone_model_kwargs = backbone_kwargs\n        backbone_engine = engine_encoders\n        backbone_builtin_engine = builtin_engine_encoders\n    else:\n        backbone=None\n\n    if decoder: \n        decoder_kwargs = _extract_prefix_keys(kwargs, \"decoder_\")\n        decoder_model_kwargs = decoder_kwargs\n        decoder_engine = engine_decoders\n        decoder_builtin_engine = builtin_engine_decoders\n    else:\n        decoder = None \n\n    if not backbone and not decoder:\n        print(\"It is necessary to define a backbone and/or a decoder.\")\n\n    # Instantianting backbone and decoder \n    backbone = self._check_model_availability(backbone, backbone_builtin_engine, backbone_engine, **backbone_model_kwargs) \n    decoder = self._check_model_availability(decoder, decoder_builtin_engine, decoder_engine, **decoder_model_kwargs) \n\n    return GenericUnetModelWrapper(\n        backbone, decoder=decoder, relu=task == \"regression\" and regression_relu, squeeze_single_class=task == \"regression\"\n    )\n</code></pre>"},{"location":"model_factories/#terratorch.models.prithvi_model_factory.PrithviModelFactory","title":"<code>terratorch.models.prithvi_model_factory.PrithviModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/prithvi_model_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass PrithviModelFactory(ModelFactory):\n    def __init__(self) -&gt; None:\n        self._factory: EncoderDecoderFactory = EncoderDecoderFactory()\n    def build_model(\n        self,\n        task: str,\n        backbone: str | nn.Module,\n        decoder: str | nn.Module,\n        bands: list[HLSBands | int],\n        in_channels: int\n        | None = None,  # this should be removed, can be derived from bands. But it is a breaking change\n        num_classes: int | None = None,\n        pretrained: bool = True,  # noqa: FBT001, FBT002\n        num_frames: int = 1,\n        prepare_features_for_image_model: Callable | None = None,\n        aux_decoders: list[AuxiliaryHead] | None = None,\n        rescale: bool = True,  # noqa: FBT002, FBT001\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"Model factory for prithvi models.\n\n        Further arguments to be passed to the backbone, decoder or head. They should be prefixed with\n        `backbone_`, `decoder_` and `head_` respectively.\n\n        Args:\n            task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\".\n            backbone (str, nn.Module): Backbone to be used. If string, should be able to be parsed\n                by the specified factory. Defaults to \"prithvi_100\".\n            decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model.\n                    If a string, it will be created from a class exposed in decoder.__init__.py with the same name.\n                    If an nn.Module, we expect it to expose a property `decoder.out_channels`.\n                    Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".\n            in_channels (int, optional): Number of input channels. Defaults to 3.\n            bands (list[terratorch.datasets.HLSBands], optional): Bands the model will be trained on.\n                    Should be a list of terratorch.datasets.HLSBands.\n                    Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE].\n            num_classes (int, optional): Number of classes. None for regression tasks.\n            pretrained (Union[bool, Path], optional): Whether to load pretrained weights for the backbone, if available.\n                Defaults to True.\n            num_frames (int, optional): Number of timesteps for the model to handle. Defaults to 1.\n            prepare_features_for_image_model (Callable | None): Function to be called on encoder features\n                before passing them to the decoder. Defaults to None, which applies the identity function.\n            aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead deciders to be added to the model.\n                These decoders take the input from the encoder as well.\n            rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n                is different from the ground truth. Only applicable to pixel wise models\n                (e.g. segmentation, pixel wise regression). Defaults to True.\n\n\n        Returns:\n            nn.Module: Full model with encoder, decoder and head.\n        \"\"\"\n        warnings.warn(\"PrithviModelFactory is deprecated. Please switch to EncoderDecoderFactory.\", stacklevel=1)\n        if in_channels is None:\n            in_channels = len(bands)\n        # TODO: support auxiliary heads\n        kwargs[\"backbone_bands\"] = bands\n        kwargs[\"backbone_in_chans\"] = in_channels\n        kwargs[\"backbone_pretrained\"] = pretrained\n        kwargs[\"backbone_num_frames\"] = num_frames\n        if prepare_features_for_image_model:\n            msg = (\n                \"This functionality is no longer supported. Please migrate to EncoderDecoderFactory\\\n                         and use necks.\"\n            )\n            raise RuntimeError(msg)\n\n\n        if not isinstance(backbone, nn.Module):\n            if not backbone.startswith(\"prithvi_\"):\n                msg = \"This class only handles models for `prithvi` encoders\"\n                raise NotImplementedError(msg)\n\n        return self._factory.build_model(task,\n                                         backbone,\n                                         decoder,\n                                         num_classes=num_classes,\n                                         necks=None,\n                                         aux_decoders=aux_decoders,\n                                         rescale=rescale,\n                                         **kwargs)\n</code></pre>"},{"location":"model_factories/#terratorch.models.prithvi_model_factory.PrithviModelFactory.build_model","title":"<code>build_model(task, backbone, decoder, bands, in_channels=None, num_classes=None, pretrained=True, num_frames=1, prepare_features_for_image_model=None, aux_decoders=None, rescale=True, **kwargs)</code>","text":"<p>Model factory for prithvi models.</p> <p>Further arguments to be passed to the backbone, decoder or head. They should be prefixed with <code>backbone_</code>, <code>decoder_</code> and <code>head_</code> respectively.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Currently supports \"segmentation\" and \"regression\".</p> required <code>backbone</code> <code>(str, Module)</code> <p>Backbone to be used. If string, should be able to be parsed by the specified factory. Defaults to \"prithvi_100\".</p> required <code>decoder</code> <code>Union[str, Module]</code> <p>Decoder to be used for the segmentation model.     If a string, it will be created from a class exposed in decoder.init.py with the same name.     If an nn.Module, we expect it to expose a property <code>decoder.out_channels</code>.     Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> <code>None</code> <code>bands</code> <code>list[HLSBands]</code> <p>Bands the model will be trained on.     Should be a list of terratorch.datasets.HLSBands.     Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE].</p> required <code>num_classes</code> <code>int</code> <p>Number of classes. None for regression tasks.</p> <code>None</code> <code>pretrained</code> <code>Union[bool, Path]</code> <p>Whether to load pretrained weights for the backbone, if available. Defaults to True.</p> <code>True</code> <code>num_frames</code> <code>int</code> <p>Number of timesteps for the model to handle. Defaults to 1.</p> <code>1</code> <code>prepare_features_for_image_model</code> <code>Callable | None</code> <p>Function to be called on encoder features before passing them to the decoder. Defaults to None, which applies the identity function.</p> <code>None</code> <code>aux_decoders</code> <code>list[AuxiliaryHead] | None</code> <p>List of AuxiliaryHead deciders to be added to the model. These decoders take the input from the encoder as well.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: Full model with encoder, decoder and head.</p> Source code in <code>terratorch/models/prithvi_model_factory.py</code> <pre><code>def build_model(\n    self,\n    task: str,\n    backbone: str | nn.Module,\n    decoder: str | nn.Module,\n    bands: list[HLSBands | int],\n    in_channels: int\n    | None = None,  # this should be removed, can be derived from bands. But it is a breaking change\n    num_classes: int | None = None,\n    pretrained: bool = True,  # noqa: FBT001, FBT002\n    num_frames: int = 1,\n    prepare_features_for_image_model: Callable | None = None,\n    aux_decoders: list[AuxiliaryHead] | None = None,\n    rescale: bool = True,  # noqa: FBT002, FBT001\n    **kwargs,\n) -&gt; Model:\n    \"\"\"Model factory for prithvi models.\n\n    Further arguments to be passed to the backbone, decoder or head. They should be prefixed with\n    `backbone_`, `decoder_` and `head_` respectively.\n\n    Args:\n        task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\".\n        backbone (str, nn.Module): Backbone to be used. If string, should be able to be parsed\n            by the specified factory. Defaults to \"prithvi_100\".\n        decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model.\n                If a string, it will be created from a class exposed in decoder.__init__.py with the same name.\n                If an nn.Module, we expect it to expose a property `decoder.out_channels`.\n                Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".\n        in_channels (int, optional): Number of input channels. Defaults to 3.\n        bands (list[terratorch.datasets.HLSBands], optional): Bands the model will be trained on.\n                Should be a list of terratorch.datasets.HLSBands.\n                Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE].\n        num_classes (int, optional): Number of classes. None for regression tasks.\n        pretrained (Union[bool, Path], optional): Whether to load pretrained weights for the backbone, if available.\n            Defaults to True.\n        num_frames (int, optional): Number of timesteps for the model to handle. Defaults to 1.\n        prepare_features_for_image_model (Callable | None): Function to be called on encoder features\n            before passing them to the decoder. Defaults to None, which applies the identity function.\n        aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead deciders to be added to the model.\n            These decoders take the input from the encoder as well.\n        rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n            is different from the ground truth. Only applicable to pixel wise models\n            (e.g. segmentation, pixel wise regression). Defaults to True.\n\n\n    Returns:\n        nn.Module: Full model with encoder, decoder and head.\n    \"\"\"\n    warnings.warn(\"PrithviModelFactory is deprecated. Please switch to EncoderDecoderFactory.\", stacklevel=1)\n    if in_channels is None:\n        in_channels = len(bands)\n    # TODO: support auxiliary heads\n    kwargs[\"backbone_bands\"] = bands\n    kwargs[\"backbone_in_chans\"] = in_channels\n    kwargs[\"backbone_pretrained\"] = pretrained\n    kwargs[\"backbone_num_frames\"] = num_frames\n    if prepare_features_for_image_model:\n        msg = (\n            \"This functionality is no longer supported. Please migrate to EncoderDecoderFactory\\\n                     and use necks.\"\n        )\n        raise RuntimeError(msg)\n\n\n    if not isinstance(backbone, nn.Module):\n        if not backbone.startswith(\"prithvi_\"):\n            msg = \"This class only handles models for `prithvi` encoders\"\n            raise NotImplementedError(msg)\n\n    return self._factory.build_model(task,\n                                     backbone,\n                                     decoder,\n                                     num_classes=num_classes,\n                                     necks=None,\n                                     aux_decoders=aux_decoders,\n                                     rescale=rescale,\n                                     **kwargs)\n</code></pre>"},{"location":"model_factories/#terratorch.models.satmae_model_factory.SatMAEModelFactory","title":"<code>terratorch.models.satmae_model_factory.SatMAEModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/satmae_model_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass SatMAEModelFactory(ModelFactory):\n    def build_model(\n        self,\n        task: str,\n        backbone: str | nn.Module,\n        decoder: str | nn.Module,\n        in_channels: int,\n        bands: list[HLSBands | int],\n        num_classes: int | None = None,\n        pretrained: bool = True,  # noqa: FBT001, FBT002\n        num_frames: int = 1,\n        prepare_features_for_image_model: Callable | None = None,\n        aux_decoders: list[AuxiliaryHead] | None = None,\n        rescale: bool = True,  # noqa: FBT002, FBT001\n        checkpoint_path: str = None,\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"Model factory for SatMAE  models.\n\n        Further arguments to be passed to the backbone, decoder or head. They should be prefixed with\n        `backbone_`, `decoder_` and `head_` respectively.\n\n        Args:\n            task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\".\n            backbone (str, nn.Module): Backbone to be used. If string, should be able to be parsed\n                by the specified factory. Defaults to \"prithvi_100\".\n            decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model.\n                    If a string, it will be created from a class exposed in decoder.__init__.py with the same name.\n                    If an nn.Module, we expect it to expose a property `decoder.out_channels`.\n                    Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".\n            in_channels (int, optional): Number of input channels. Defaults to 3.\n            bands (list[terratorch.datasets.HLSBands], optional): Bands the model will be trained on.\n                    Should be a list of terratorch.datasets.HLSBands.\n                    Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE].\n            num_classes (int, optional): Number of classes. None for regression tasks.\n            pretrained (Union[bool, Path], optional): Whether to load pretrained weights for the backbone, if available.\n                Defaults to True.\n            num_frames (int, optional): Number of timesteps for the model to handle. Defaults to 1.\n            prepare_features_for_image_model (Callable | None): Function to be called on encoder features\n                before passing them to the decoder. Defaults to None, which applies the identity function.\n            aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead deciders to be added to the model.\n                These decoders take the input from the encoder as well.\n            rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n                is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.\n\n        Raises:\n            NotImplementedError: _description_\n            DecoderNotFoundException: _description_\n\n        Returns:\n            nn.Module: _description_\n        \"\"\"\n\n        self.possible_modules = None \n\n        if not torch.cuda.is_available():\n            self.CPU_ONLY = True\n        else:\n            self.CPU_ONLY = False\n\n        # Path for accessing the model source code.\n        self.syspath_kwarg = \"model_sys_path\"\n\n        bands = [HLSBands.try_convert_to_hls_bands_enum(b) for b in bands]\n\n        # TODO: support auxiliary heads\n        if not isinstance(backbone, nn.Module):\n            if not 'SatMAE' in kwargs[self.syspath_kwarg]:\n                msg = \"This class only handles models for `SatMAE` encoders\"\n                raise NotImplementedError(msg)\n\n\n            task = task.lower()\n            if task not in SUPPORTED_TASKS:\n                msg = f\"Task {task} not supported. Please choose one of {SUPPORTED_TASKS}\"\n                raise NotImplementedError(msg)\n\n            backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n            backbone_name = backbone\n\n            # Trying to find the model on HuggingFace.\n            try:\n                backbone: nn.Module = timm.create_model(\n                    backbone,\n                    pretrained=pretrained,\n                    in_chans=in_channels,\n                    num_frames=num_frames,\n                    bands=bands,\n                    features_only=True,\n                    **backbone_kwargs,\n                )\n            except Exception:\n\n                # When the model is not on HG, it needs be restored locally.\n                print(\"This model is not available on HuggingFace. Trying to instantiate locally ...\")\n\n                assert checkpoint_path, \"A checkpoint must be provided to restore the model.\"\n\n                # The SatMAE source code must be installed or available via PYTHONPATH.\n                try:  \n                    if self.syspath_kwarg in kwargs:\n                        syspath_value = kwargs.get(self.syspath_kwarg)\n\n                    else:\n\n                        Exception(f\"It is necessary to define the variable {self.syspath_kwarg} on yaml\"\n                                                           \"config for restoring local model.\")\n\n                    sys.path.insert(0, syspath_value)\n\n                    # There are dozens of classes in the SatMAE repo, but it seems to be the right open_generic_torch_model\n                    backbone_template = None\n\n                    self.possible_modules = [importlib.import_module(mod) for mod in [\"models_mae\", \"models_vit\"]]\n\n                    for backbone_module in self.possible_modules:\n\n                        backbone_template_ = getattr(backbone_module, backbone_name, None)\n                        if not backbone_template_ :\n                            pass\n                        else:\n                            backbone_template = backbone_template_\n\n                except ModuleNotFoundError:\n\n                    print(f\"It is better to review the field {self.syspath_kwarg} in the yaml file.\")\n\n                # Is it a ViT or a ViT-MAE ?\n                backbone_kind = check_the_kind_of_vit(name=backbone_name)\n\n                backbone: nn.Module = ModelWrapper(model=backbone_template(**backbone_kwargs), kind=backbone_kind)\n\n                if self.CPU_ONLY:\n                    model_dict = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n                else:\n                    model_dict = torch.load(checkpoint_path, weights_only=True)\n\n\n                # Filtering parameters from the model state_dict (when necessary)\n                model_dict = filter_cefficients_when_necessary(model_state_dict=model_dict, kind=backbone_kind)\n\n                if backbone_kind == \"vit\":\n                    backbone.model.fc_norm = nn.Identity()\n                    backbone.model.head_drop = nn.Identity()\n                    backbone.model.head = nn.Identity()\n                    backbone.model.pos_embed = None # TODO It needs be corrected from source\n\n                # Load saved model when it exists\n                if  pretrained: \n                    backbone.model.load_state_dict(model_dict['model'], strict=False)\n\n                # Print the general architecture\n                backbone.summary()\n\n                print(\"Model SatMAE was successfully restored.\")\n\n        # allow decoder to be a module passed directly\n        decoder_cls = _get_decoder(decoder)\n\n        decoder_kwargs, kwargs = extract_prefix_keys(kwargs, \"decoder_\")\n\n        # If backabone is a ViT-MAE, the attribute \"num_patches\" will be necessary\n        if hasattr(backbone, \"num_patches\"):\n            decoder_kwargs[\"num_patches\"] = backbone.num_patches\n\n        # TODO: remove this\n        if \"SatMAEHead\" in decoder:\n            decoder: nn.Module = decoder_cls(**decoder_kwargs)\n        else:\n            decoder: nn.Module = decoder_cls(backbone.channels(), **decoder_kwargs)\n\n        head_kwargs, kwargs = extract_prefix_keys(kwargs, \"head_\")\n        if num_classes:\n            head_kwargs[\"num_classes\"] = num_classes\n        if aux_decoders is None:\n            return _build_appropriate_model(\n                task, backbone, decoder, head_kwargs, prepare_features_for_image_model, rescale=rescale\n            )\n\n        to_be_aux_decoders: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] = []\n        for aux_decoder in aux_decoders:\n            args = aux_decoder.decoder_args if aux_decoder.decoder_args else {}\n            aux_decoder_cls: nn.Module = _get_decoder(aux_decoder.decoder)\n            aux_decoder_kwargs, kwargs = extract_prefix_keys(args, \"decoder_\")\n            aux_decoder_instance = aux_decoder_cls(backbone.feature_info.channels(), **aux_decoder_kwargs)\n            # aux_decoder_instance = aux_decoder_cls([128, 256, 512, 1024], **decoder_kwargs)\n\n            aux_head_kwargs, kwargs = extract_prefix_keys(args, \"head_\")\n            if num_classes:\n                aux_head_kwargs[\"num_classes\"] = num_classes\n            # aux_head: nn.Module = _get_head(task, aux_decoder_instance, num_classes=num_classes, **head_kwargs)\n            # aux_decoder.decoder = nn.Sequential(aux_decoder_instance, aux_head)\n            to_be_aux_decoders.append(\n                AuxiliaryHeadWithDecoderWithoutInstantiatedHead(aux_decoder.name, aux_decoder_instance, aux_head_kwargs)\n            )\n\n        return _build_appropriate_model(\n            task,\n            backbone,\n            decoder,\n            head_kwargs,\n            prepare_features_for_image_model,\n            rescale=rescale,\n            auxiliary_heads=to_be_aux_decoders,\n        )\n</code></pre>"},{"location":"model_factories/#terratorch.models.satmae_model_factory.SatMAEModelFactory.build_model","title":"<code>build_model(task, backbone, decoder, in_channels, bands, num_classes=None, pretrained=True, num_frames=1, prepare_features_for_image_model=None, aux_decoders=None, rescale=True, checkpoint_path=None, **kwargs)</code>","text":"<p>Model factory for SatMAE  models.</p> <p>Further arguments to be passed to the backbone, decoder or head. They should be prefixed with <code>backbone_</code>, <code>decoder_</code> and <code>head_</code> respectively.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>Task to be performed. Currently supports \"segmentation\" and \"regression\".</p> required <code>backbone</code> <code>(str, Module)</code> <p>Backbone to be used. If string, should be able to be parsed by the specified factory. Defaults to \"prithvi_100\".</p> required <code>decoder</code> <code>Union[str, Module]</code> <p>Decoder to be used for the segmentation model.     If a string, it will be created from a class exposed in decoder.init.py with the same name.     If an nn.Module, we expect it to expose a property <code>decoder.out_channels</code>.     Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".</p> required <code>in_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> required <code>bands</code> <code>list[HLSBands]</code> <p>Bands the model will be trained on.     Should be a list of terratorch.datasets.HLSBands.     Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE].</p> required <code>num_classes</code> <code>int</code> <p>Number of classes. None for regression tasks.</p> <code>None</code> <code>pretrained</code> <code>Union[bool, Path]</code> <p>Whether to load pretrained weights for the backbone, if available. Defaults to True.</p> <code>True</code> <code>num_frames</code> <code>int</code> <p>Number of timesteps for the model to handle. Defaults to 1.</p> <code>1</code> <code>prepare_features_for_image_model</code> <code>Callable | None</code> <p>Function to be called on encoder features before passing them to the decoder. Defaults to None, which applies the identity function.</p> <code>None</code> <code>aux_decoders</code> <code>list[AuxiliaryHead] | None</code> <p>List of AuxiliaryHead deciders to be added to the model. These decoders take the input from the encoder as well.</p> <code>None</code> <code>rescale</code> <code>bool</code> <p>Whether to apply bilinear interpolation to rescale the model output if its size is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> <code>DecoderNotFoundException</code> <p>description</p> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: description</p> Source code in <code>terratorch/models/satmae_model_factory.py</code> <pre><code>def build_model(\n    self,\n    task: str,\n    backbone: str | nn.Module,\n    decoder: str | nn.Module,\n    in_channels: int,\n    bands: list[HLSBands | int],\n    num_classes: int | None = None,\n    pretrained: bool = True,  # noqa: FBT001, FBT002\n    num_frames: int = 1,\n    prepare_features_for_image_model: Callable | None = None,\n    aux_decoders: list[AuxiliaryHead] | None = None,\n    rescale: bool = True,  # noqa: FBT002, FBT001\n    checkpoint_path: str = None,\n    **kwargs,\n) -&gt; Model:\n    \"\"\"Model factory for SatMAE  models.\n\n    Further arguments to be passed to the backbone, decoder or head. They should be prefixed with\n    `backbone_`, `decoder_` and `head_` respectively.\n\n    Args:\n        task (str): Task to be performed. Currently supports \"segmentation\" and \"regression\".\n        backbone (str, nn.Module): Backbone to be used. If string, should be able to be parsed\n            by the specified factory. Defaults to \"prithvi_100\".\n        decoder (Union[str, nn.Module], optional): Decoder to be used for the segmentation model.\n                If a string, it will be created from a class exposed in decoder.__init__.py with the same name.\n                If an nn.Module, we expect it to expose a property `decoder.out_channels`.\n                Will be concatenated with a Conv2d for the final convolution. Defaults to \"FCNDecoder\".\n        in_channels (int, optional): Number of input channels. Defaults to 3.\n        bands (list[terratorch.datasets.HLSBands], optional): Bands the model will be trained on.\n                Should be a list of terratorch.datasets.HLSBands.\n                Defaults to [HLSBands.RED, HLSBands.GREEN, HLSBands.BLUE].\n        num_classes (int, optional): Number of classes. None for regression tasks.\n        pretrained (Union[bool, Path], optional): Whether to load pretrained weights for the backbone, if available.\n            Defaults to True.\n        num_frames (int, optional): Number of timesteps for the model to handle. Defaults to 1.\n        prepare_features_for_image_model (Callable | None): Function to be called on encoder features\n            before passing them to the decoder. Defaults to None, which applies the identity function.\n        aux_decoders (list[AuxiliaryHead] | None): List of AuxiliaryHead deciders to be added to the model.\n            These decoders take the input from the encoder as well.\n        rescale (bool): Whether to apply bilinear interpolation to rescale the model output if its size\n            is different from the ground truth. Only applicable to pixel wise models (e.g. segmentation, pixel wise regression). Defaults to True.\n\n    Raises:\n        NotImplementedError: _description_\n        DecoderNotFoundException: _description_\n\n    Returns:\n        nn.Module: _description_\n    \"\"\"\n\n    self.possible_modules = None \n\n    if not torch.cuda.is_available():\n        self.CPU_ONLY = True\n    else:\n        self.CPU_ONLY = False\n\n    # Path for accessing the model source code.\n    self.syspath_kwarg = \"model_sys_path\"\n\n    bands = [HLSBands.try_convert_to_hls_bands_enum(b) for b in bands]\n\n    # TODO: support auxiliary heads\n    if not isinstance(backbone, nn.Module):\n        if not 'SatMAE' in kwargs[self.syspath_kwarg]:\n            msg = \"This class only handles models for `SatMAE` encoders\"\n            raise NotImplementedError(msg)\n\n\n        task = task.lower()\n        if task not in SUPPORTED_TASKS:\n            msg = f\"Task {task} not supported. Please choose one of {SUPPORTED_TASKS}\"\n            raise NotImplementedError(msg)\n\n        backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")\n        backbone_name = backbone\n\n        # Trying to find the model on HuggingFace.\n        try:\n            backbone: nn.Module = timm.create_model(\n                backbone,\n                pretrained=pretrained,\n                in_chans=in_channels,\n                num_frames=num_frames,\n                bands=bands,\n                features_only=True,\n                **backbone_kwargs,\n            )\n        except Exception:\n\n            # When the model is not on HG, it needs be restored locally.\n            print(\"This model is not available on HuggingFace. Trying to instantiate locally ...\")\n\n            assert checkpoint_path, \"A checkpoint must be provided to restore the model.\"\n\n            # The SatMAE source code must be installed or available via PYTHONPATH.\n            try:  \n                if self.syspath_kwarg in kwargs:\n                    syspath_value = kwargs.get(self.syspath_kwarg)\n\n                else:\n\n                    Exception(f\"It is necessary to define the variable {self.syspath_kwarg} on yaml\"\n                                                       \"config for restoring local model.\")\n\n                sys.path.insert(0, syspath_value)\n\n                # There are dozens of classes in the SatMAE repo, but it seems to be the right open_generic_torch_model\n                backbone_template = None\n\n                self.possible_modules = [importlib.import_module(mod) for mod in [\"models_mae\", \"models_vit\"]]\n\n                for backbone_module in self.possible_modules:\n\n                    backbone_template_ = getattr(backbone_module, backbone_name, None)\n                    if not backbone_template_ :\n                        pass\n                    else:\n                        backbone_template = backbone_template_\n\n            except ModuleNotFoundError:\n\n                print(f\"It is better to review the field {self.syspath_kwarg} in the yaml file.\")\n\n            # Is it a ViT or a ViT-MAE ?\n            backbone_kind = check_the_kind_of_vit(name=backbone_name)\n\n            backbone: nn.Module = ModelWrapper(model=backbone_template(**backbone_kwargs), kind=backbone_kind)\n\n            if self.CPU_ONLY:\n                model_dict = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n            else:\n                model_dict = torch.load(checkpoint_path, weights_only=True)\n\n\n            # Filtering parameters from the model state_dict (when necessary)\n            model_dict = filter_cefficients_when_necessary(model_state_dict=model_dict, kind=backbone_kind)\n\n            if backbone_kind == \"vit\":\n                backbone.model.fc_norm = nn.Identity()\n                backbone.model.head_drop = nn.Identity()\n                backbone.model.head = nn.Identity()\n                backbone.model.pos_embed = None # TODO It needs be corrected from source\n\n            # Load saved model when it exists\n            if  pretrained: \n                backbone.model.load_state_dict(model_dict['model'], strict=False)\n\n            # Print the general architecture\n            backbone.summary()\n\n            print(\"Model SatMAE was successfully restored.\")\n\n    # allow decoder to be a module passed directly\n    decoder_cls = _get_decoder(decoder)\n\n    decoder_kwargs, kwargs = extract_prefix_keys(kwargs, \"decoder_\")\n\n    # If backabone is a ViT-MAE, the attribute \"num_patches\" will be necessary\n    if hasattr(backbone, \"num_patches\"):\n        decoder_kwargs[\"num_patches\"] = backbone.num_patches\n\n    # TODO: remove this\n    if \"SatMAEHead\" in decoder:\n        decoder: nn.Module = decoder_cls(**decoder_kwargs)\n    else:\n        decoder: nn.Module = decoder_cls(backbone.channels(), **decoder_kwargs)\n\n    head_kwargs, kwargs = extract_prefix_keys(kwargs, \"head_\")\n    if num_classes:\n        head_kwargs[\"num_classes\"] = num_classes\n    if aux_decoders is None:\n        return _build_appropriate_model(\n            task, backbone, decoder, head_kwargs, prepare_features_for_image_model, rescale=rescale\n        )\n\n    to_be_aux_decoders: list[AuxiliaryHeadWithDecoderWithoutInstantiatedHead] = []\n    for aux_decoder in aux_decoders:\n        args = aux_decoder.decoder_args if aux_decoder.decoder_args else {}\n        aux_decoder_cls: nn.Module = _get_decoder(aux_decoder.decoder)\n        aux_decoder_kwargs, kwargs = extract_prefix_keys(args, \"decoder_\")\n        aux_decoder_instance = aux_decoder_cls(backbone.feature_info.channels(), **aux_decoder_kwargs)\n        # aux_decoder_instance = aux_decoder_cls([128, 256, 512, 1024], **decoder_kwargs)\n\n        aux_head_kwargs, kwargs = extract_prefix_keys(args, \"head_\")\n        if num_classes:\n            aux_head_kwargs[\"num_classes\"] = num_classes\n        # aux_head: nn.Module = _get_head(task, aux_decoder_instance, num_classes=num_classes, **head_kwargs)\n        # aux_decoder.decoder = nn.Sequential(aux_decoder_instance, aux_head)\n        to_be_aux_decoders.append(\n            AuxiliaryHeadWithDecoderWithoutInstantiatedHead(aux_decoder.name, aux_decoder_instance, aux_head_kwargs)\n        )\n\n    return _build_appropriate_model(\n        task,\n        backbone,\n        decoder,\n        head_kwargs,\n        prepare_features_for_image_model,\n        rescale=rescale,\n        auxiliary_heads=to_be_aux_decoders,\n    )\n</code></pre>"},{"location":"model_factories/#terratorch.models.smp_model_factory.SMPModelFactory","title":"<code>terratorch.models.smp_model_factory.SMPModelFactory</code>","text":"<p>               Bases: <code>ModelFactory</code></p> Source code in <code>terratorch/models/smp_model_factory.py</code> <pre><code>@MODEL_FACTORY_REGISTRY.register\nclass SMPModelFactory(ModelFactory):\n    def build_model(\n        self,\n        task: str,\n        backbone: str,\n        model: str,\n        bands: list[HLSBands | int],\n        in_channels: int | None = None,\n        num_classes: int = 1,\n        pretrained: str | bool | None = True,  # noqa: FBT002\n        prepare_features_for_image_model: Callable | None = None,\n        regression_relu: bool = False,  # noqa: FBT001, FBT002\n        **kwargs,\n    ) -&gt; Model:\n        \"\"\"\n        Factory class for creating SMP (Segmentation Models Pytorch) based models with optional customization.\n\n        This factory handles the instantiation of segmentation and regression models using specified\n        encoders and decoders from the SMP library, along with custom modifications and extensions such\n        as auxiliary decoders or modified encoders.\n\n        Attributes:\n            task (str): Specifies the task for which the model is being built. Supported tasks are\n                        \"segmentation\".\n            backbone (str): Specifies the backbone model to be used.\n            decoder (str): Specifies the decoder to be used for constructing the\n                        segmentation model.\n            bands (list[terratorch.datasets.HLSBands | int]): A list specifying the bands that the model\n                        will operate on. These are expected to be from terratorch.datasets.HLSBands.\n            in_channels (int, optional): Specifies the number of input channels. Defaults to None.\n            num_classes (int, optional): The number of output classes for the model.\n            pretrained (bool | Path, optional): Indicates whether to load pretrained weights for the\n                        backbone. Can also specify a path to weights. Defaults to True.\n            num_frames (int, optional): Specifies the number of timesteps the model should handle. Useful\n                        for temporal models.\n            regression_relu (bool): Whether to apply ReLU activation in the case of regression tasks.\n            **kwargs: Additional arguments that might be passed to further customize the backbone, decoder,\n                        or any auxiliary heads. These should be prefixed appropriately\n\n        Raises:\n            ValueError: If the specified decoder is not supported by SMP.\n            Exception: If the specified task is not \"segmentation\"\n\n        Returns:\n            nn.Module: A model instance wrapped in SMPModelWrapper configured according to the specified\n                    parameters and tasks.\n        \"\"\"\n        if task != \"segmentation\":\n            msg = f\"SMP models can only perform segmentation, but got task {task}\"\n            raise Exception(msg)\n\n        bands = [HLSBands.try_convert_to_hls_bands_enum(b) for b in bands]\n        if in_channels is None:\n            in_channels = len(bands)\n\n        # Gets decoder module.\n        model_module = getattr(smp, model, None)\n        if model_module is None:\n            msg = f\"Decoder {model} is not supported in SMP.\"\n            raise ValueError(msg)\n\n        backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")  # Encoder params should be prefixed backbone_\n        smp_kwargs, kwargs = extract_prefix_keys(backbone_kwargs, \"smp_\")  # Smp model params should be prefixed smp_\n        aux_params, kwargs = extract_prefix_keys(backbone_kwargs, \"aux_\")  # Auxiliary head params should be prefixed aux_\n        aux_params = None if aux_params == {} else aux_params\n\n        if isinstance(pretrained, bool):\n            if pretrained:\n                pretrained = \"imagenet\"\n            else:\n                pretrained = None\n\n        # If encoder not currently supported by SMP (custom encoder).\n        if backbone not in smp_encoders:\n            # These params must be included in the config file with appropriate prefix.\n            required_params = {\n                \"encoder_depth\": smp_kwargs,\n                \"out_channels\": backbone_kwargs,\n                \"output_stride\": backbone_kwargs,\n            }\n\n            for param, config_dict in required_params.items():\n                if param not in config_dict:\n                    msg = f\"Config must include the '{param}' parameter\"\n                    raise ValueError(msg)\n\n            # Using new encoder.\n            backbone_class = make_smp_encoder(backbone)\n            backbone_kwargs[\"prepare_features_for_image_model\"] = prepare_features_for_image_model\n            # Registering custom encoder into SMP.\n            register_custom_encoder(backbone_class, backbone_kwargs, pretrained)\n\n            model_args = {\n                \"encoder_name\": \"SMPEncoderWrapperWithPFFIM\",\n                \"encoder_weights\": pretrained,\n                \"in_channels\": in_channels,\n                \"classes\": num_classes,\n                **smp_kwargs,\n            }\n        # Using SMP encoder.\n        else:\n            model_args = {\n                \"encoder_name\": backbone,\n                \"encoder_weights\": pretrained,\n                \"in_channels\": in_channels,\n                \"classes\": num_classes,\n                **smp_kwargs,\n            }\n\n        model = model_module(**model_args, aux_params=aux_params)\n\n        return SMPModelWrapper(\n            model, relu=task == \"regression\" and regression_relu, squeeze_single_class=task == \"regression\"\n        )\n</code></pre>"},{"location":"model_factories/#terratorch.models.smp_model_factory.SMPModelFactory.build_model","title":"<code>build_model(task, backbone, model, bands, in_channels=None, num_classes=1, pretrained=True, prepare_features_for_image_model=None, regression_relu=False, **kwargs)</code>","text":"<p>Factory class for creating SMP (Segmentation Models Pytorch) based models with optional customization.</p> <p>This factory handles the instantiation of segmentation and regression models using specified encoders and decoders from the SMP library, along with custom modifications and extensions such as auxiliary decoders or modified encoders.</p> <p>Attributes:</p> Name Type Description <code>task</code> <code>str</code> <p>Specifies the task for which the model is being built. Supported tasks are         \"segmentation\".</p> <code>backbone</code> <code>str</code> <p>Specifies the backbone model to be used.</p> <code>decoder</code> <code>str</code> <p>Specifies the decoder to be used for constructing the         segmentation model.</p> <code>bands</code> <code>list[HLSBands | int]</code> <p>A list specifying the bands that the model         will operate on. These are expected to be from terratorch.datasets.HLSBands.</p> <code>in_channels</code> <code>int</code> <p>Specifies the number of input channels. Defaults to None.</p> <code>num_classes</code> <code>int</code> <p>The number of output classes for the model.</p> <code>pretrained</code> <code>bool | Path</code> <p>Indicates whether to load pretrained weights for the         backbone. Can also specify a path to weights. Defaults to True.</p> <code>num_frames</code> <code>int</code> <p>Specifies the number of timesteps the model should handle. Useful         for temporal models.</p> <code>regression_relu</code> <code>bool</code> <p>Whether to apply ReLU activation in the case of regression tasks.</p> <code>**kwargs</code> <code>bool</code> <p>Additional arguments that might be passed to further customize the backbone, decoder,         or any auxiliary heads. These should be prefixed appropriately</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified decoder is not supported by SMP.</p> <code>Exception</code> <p>If the specified task is not \"segmentation\"</p> <p>Returns:</p> Type Description <code>Model</code> <p>nn.Module: A model instance wrapped in SMPModelWrapper configured according to the specified     parameters and tasks.</p> Source code in <code>terratorch/models/smp_model_factory.py</code> <pre><code>def build_model(\n    self,\n    task: str,\n    backbone: str,\n    model: str,\n    bands: list[HLSBands | int],\n    in_channels: int | None = None,\n    num_classes: int = 1,\n    pretrained: str | bool | None = True,  # noqa: FBT002\n    prepare_features_for_image_model: Callable | None = None,\n    regression_relu: bool = False,  # noqa: FBT001, FBT002\n    **kwargs,\n) -&gt; Model:\n    \"\"\"\n    Factory class for creating SMP (Segmentation Models Pytorch) based models with optional customization.\n\n    This factory handles the instantiation of segmentation and regression models using specified\n    encoders and decoders from the SMP library, along with custom modifications and extensions such\n    as auxiliary decoders or modified encoders.\n\n    Attributes:\n        task (str): Specifies the task for which the model is being built. Supported tasks are\n                    \"segmentation\".\n        backbone (str): Specifies the backbone model to be used.\n        decoder (str): Specifies the decoder to be used for constructing the\n                    segmentation model.\n        bands (list[terratorch.datasets.HLSBands | int]): A list specifying the bands that the model\n                    will operate on. These are expected to be from terratorch.datasets.HLSBands.\n        in_channels (int, optional): Specifies the number of input channels. Defaults to None.\n        num_classes (int, optional): The number of output classes for the model.\n        pretrained (bool | Path, optional): Indicates whether to load pretrained weights for the\n                    backbone. Can also specify a path to weights. Defaults to True.\n        num_frames (int, optional): Specifies the number of timesteps the model should handle. Useful\n                    for temporal models.\n        regression_relu (bool): Whether to apply ReLU activation in the case of regression tasks.\n        **kwargs: Additional arguments that might be passed to further customize the backbone, decoder,\n                    or any auxiliary heads. These should be prefixed appropriately\n\n    Raises:\n        ValueError: If the specified decoder is not supported by SMP.\n        Exception: If the specified task is not \"segmentation\"\n\n    Returns:\n        nn.Module: A model instance wrapped in SMPModelWrapper configured according to the specified\n                parameters and tasks.\n    \"\"\"\n    if task != \"segmentation\":\n        msg = f\"SMP models can only perform segmentation, but got task {task}\"\n        raise Exception(msg)\n\n    bands = [HLSBands.try_convert_to_hls_bands_enum(b) for b in bands]\n    if in_channels is None:\n        in_channels = len(bands)\n\n    # Gets decoder module.\n    model_module = getattr(smp, model, None)\n    if model_module is None:\n        msg = f\"Decoder {model} is not supported in SMP.\"\n        raise ValueError(msg)\n\n    backbone_kwargs, kwargs = extract_prefix_keys(kwargs, \"backbone_\")  # Encoder params should be prefixed backbone_\n    smp_kwargs, kwargs = extract_prefix_keys(backbone_kwargs, \"smp_\")  # Smp model params should be prefixed smp_\n    aux_params, kwargs = extract_prefix_keys(backbone_kwargs, \"aux_\")  # Auxiliary head params should be prefixed aux_\n    aux_params = None if aux_params == {} else aux_params\n\n    if isinstance(pretrained, bool):\n        if pretrained:\n            pretrained = \"imagenet\"\n        else:\n            pretrained = None\n\n    # If encoder not currently supported by SMP (custom encoder).\n    if backbone not in smp_encoders:\n        # These params must be included in the config file with appropriate prefix.\n        required_params = {\n            \"encoder_depth\": smp_kwargs,\n            \"out_channels\": backbone_kwargs,\n            \"output_stride\": backbone_kwargs,\n        }\n\n        for param, config_dict in required_params.items():\n            if param not in config_dict:\n                msg = f\"Config must include the '{param}' parameter\"\n                raise ValueError(msg)\n\n        # Using new encoder.\n        backbone_class = make_smp_encoder(backbone)\n        backbone_kwargs[\"prepare_features_for_image_model\"] = prepare_features_for_image_model\n        # Registering custom encoder into SMP.\n        register_custom_encoder(backbone_class, backbone_kwargs, pretrained)\n\n        model_args = {\n            \"encoder_name\": \"SMPEncoderWrapperWithPFFIM\",\n            \"encoder_weights\": pretrained,\n            \"in_channels\": in_channels,\n            \"classes\": num_classes,\n            **smp_kwargs,\n        }\n    # Using SMP encoder.\n    else:\n        model_args = {\n            \"encoder_name\": backbone,\n            \"encoder_weights\": pretrained,\n            \"in_channels\": in_channels,\n            \"classes\": num_classes,\n            **smp_kwargs,\n        }\n\n    model = model_module(**model_args, aux_params=aux_params)\n\n    return SMPModelWrapper(\n        model, relu=task == \"regression\" and regression_relu, squeeze_single_class=task == \"regression\"\n    )\n</code></pre>"},{"location":"models/","title":"Models","text":"<p>To interface with terratorch tasks correctly, models must inherit from the Model parent class and have a forward method which returns an object ModelOutput:</p>"},{"location":"models/#model-factories","title":"Model Factories","text":"<p>In order to be used by tasks, models must have a Model Factory which builds them. Factories must conform to the ModelFactory parent class. </p> <p>You most likely do not need to implement your own model factory, unless you are wrapping another library which generates full models.</p> <p>For most cases, the encoder decoder factory can be used to combine a backbone with a decoder.</p> <p>To add new backbones or decoders, to be used with the encoder decoder factory they should be registered. </p> <p>To add a new model factory, it should be registered in the <code>MODEL_FACTORY_REGISTRY</code>.</p>"},{"location":"models/#adding-a-new-model","title":"Adding a new model","text":"<p>To add a new backbone, simply create a class and annotate it (or a constructor function that instantiates it) with <code>@TERRATORCH_BACKBONE_FACTORY.register</code>. </p> <p>The model will be registered with the same name as the function. To create many model variants from the same class, the reccomended approach is to annotate a constructor function from each with a fully descriptive name.</p> <pre><code>from terratorch.registry import TERRATORCH_BACKBONE_REGISTRY, BACKBONE_REGISTRY\n\nfrom torch import nn\n\n# make sure this is in the import path for terratorch\n@TERRATORCH_BACKBONE_REGISTRY.register\nclass BasicBackbone(nn.Module):\n    def __init__(self, out_channels=64):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.layer = nn.Linear(224*224, out_channels)\n        self.out_channels = [out_channels]\n\n    def forward(self, x):\n        return self.layer(self.flatten(x))\n\n# you can build directly with the TERRATORCH_BACKBONE_REGISTRY\n# but typically this will be accessed from the BACKBONE_REGISTRY\n&gt;&gt;&gt; BACKBONE_REGISTRY.build(\"BasicBackbone\", out_channels=64)\nBasicBackbone(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (layer): Linear(in_features=50176, out_features=64, bias=True)\n)\n\n@TERRATORCH_BACKBONE_REGISTRY.register\ndef basic_backbone_128():\n    return BasicBackbone(out_channels=128)\n\n&gt;&gt;&gt; BACKBONE_REGISTRY.build(\"basic_backbone_128\")\nBasicBackbone(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (layer): Linear(in_features=50176, out_features=128, bias=True)\n)\n</code></pre> <p>Adding a new decoder can be done in the same way with the <code>TERRATORCH_DECODER_REGISTRY</code>.</p> <p>Info</p> <p>All decoders will be passed the channel_list as the first argument for initialization.</p> <p>To pass your own path from where to load the weights with the PrithviModelFactory, you can make use of timm's <code>pretrained_cfg_overlay</code>. E.g. to pass a local path, you can pass the parameter <code>backbone_pretrained_cfg_overlay = {\"file\": \"&lt;local_path&gt;\"}</code> to the model factory.</p> <p>Besides <code>file</code>, you can also pass <code>url</code>, <code>hf_hub_id</code>, amongst others. Check timm's documentation for full details.</p>"},{"location":"models/#adding-new-model-types","title":"Adding new model types","text":"<p>Adding new model types is as simple as creating a new factory that produces models. See for instance the example below for a potential <code>SMPModelFactory</code> <pre><code>from terratorch.models.model import register_factory\n\n@register_factory\nclass SMPModelFactory(ModelFactory):\n    def build_model(\n        self,\n        task: str,\n        backbone: str | nn.Module,\n        decoder: str | nn.Module,\n        in_channels: int,\n        **kwargs,\n    ) -&gt; Model:\n\n        model = smp.Unet(encoder_name=\"resnet34\", encoder_weights=None, in_channels=in_channels, classes=1)\n        return SMPModelWrapper(model)\n\n@register_factory\nclass SMPModelWrapper(Model, nn.Module):\n    def __init__(self, smp_model) -&gt; None:\n        super().__init__()\n        self.smp_model = smp_model\n\n    def forward(self, *args, **kwargs):\n        return ModelOutput(self.smp_model(*args, **kwargs).squeeze(1))\n\n    def freeze_encoder(self):\n        pass\n\n    def freeze_decoder(self):\n        pass\n</code></pre></p>"},{"location":"models/#custom-modules-with-cli","title":"Custom modules with CLI","text":"<p>Custom modules must be in the import path in order to be registered in the appropriate registries. </p> <p>In order to do this without modifying the code when using the CLI, you may place your modules under a <code>custom_modules</code> directory. This must be in the directory from which you execute terratorch.</p>"},{"location":"necks/","title":"Necks","text":""},{"location":"necks/#terratorch.models.necks.Neck","title":"<code>terratorch.models.necks.Neck</code>","text":"<p>               Bases: <code>ABC</code>, <code>Module</code></p> <p>Base class for Neck</p> <p>A neck must must implement <code>self.process_channel_list</code> which returns the new channel list.</p> Source code in <code>terratorch/models/necks.py</code> <pre><code>class Neck(ABC, nn.Module):\n    \"\"\"Base class for Neck\n\n    A neck must must implement `self.process_channel_list` which returns the new channel list.\n    \"\"\"\n\n    def __init__(self, channel_list: list[int]) -&gt; None:\n        super().__init__()\n        self.channel_list = channel_list\n\n    @abstractmethod\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        return channel_list\n\n    @abstractmethod\n    def forward(self, channel_list: list[torch.Tensor]) -&gt; list[torch.Tensor]: ...\n</code></pre>"},{"location":"necks/#terratorch.models.necks.SelectIndices","title":"<code>terratorch.models.necks.SelectIndices</code>","text":"<p>               Bases: <code>Neck</code></p> Source code in <code>terratorch/models/necks.py</code> <pre><code>@TERRATORCH_NECK_REGISTRY.register\nclass SelectIndices(Neck):\n    def __init__(self, channel_list: list[int], indices: list[int]):\n        \"\"\"Select indices from the embedding list\n\n        Args:\n            indices (list[int]): list of indices to select.\n        \"\"\"\n        super().__init__(channel_list)\n        self.indices = indices\n\n    def forward(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        features = [features[i] for i in self.indices]\n        return features\n\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        channel_list = [channel_list[i] for i in self.indices]\n        return channel_list\n</code></pre>"},{"location":"necks/#terratorch.models.necks.SelectIndices.__init__","title":"<code>__init__(channel_list, indices)</code>","text":"<p>Select indices from the embedding list</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>list[int]</code> <p>list of indices to select.</p> required Source code in <code>terratorch/models/necks.py</code> <pre><code>def __init__(self, channel_list: list[int], indices: list[int]):\n    \"\"\"Select indices from the embedding list\n\n    Args:\n        indices (list[int]): list of indices to select.\n    \"\"\"\n    super().__init__(channel_list)\n    self.indices = indices\n</code></pre>"},{"location":"necks/#terratorch.models.necks.PermuteDims","title":"<code>terratorch.models.necks.PermuteDims</code>","text":"<p>               Bases: <code>Neck</code></p> Source code in <code>terratorch/models/necks.py</code> <pre><code>@TERRATORCH_NECK_REGISTRY.register\nclass PermuteDims(Neck):\n    def __init__(self, channel_list: list[int], new_order: list[int]):\n        \"\"\"Permute dimensions of each element in the embedding list\n\n        Args:\n            new_order (list[int]): list of indices to be passed to tensor.permute()\n        \"\"\"\n        super().__init__(channel_list)\n        self.new_order = new_order\n\n    def forward(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        features = [feat.permute(*self.new_order).contiguous() for feat in features]\n        return features\n\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        return super().process_channel_list(channel_list)\n</code></pre>"},{"location":"necks/#terratorch.models.necks.PermuteDims.__init__","title":"<code>__init__(channel_list, new_order)</code>","text":"<p>Permute dimensions of each element in the embedding list</p> <p>Parameters:</p> Name Type Description Default <code>new_order</code> <code>list[int]</code> <p>list of indices to be passed to tensor.permute()</p> required Source code in <code>terratorch/models/necks.py</code> <pre><code>def __init__(self, channel_list: list[int], new_order: list[int]):\n    \"\"\"Permute dimensions of each element in the embedding list\n\n    Args:\n        new_order (list[int]): list of indices to be passed to tensor.permute()\n    \"\"\"\n    super().__init__(channel_list)\n    self.new_order = new_order\n</code></pre>"},{"location":"necks/#terratorch.models.necks.InterpolateToPyramidal","title":"<code>terratorch.models.necks.InterpolateToPyramidal</code>","text":"<p>               Bases: <code>Neck</code></p> Source code in <code>terratorch/models/necks.py</code> <pre><code>@TERRATORCH_NECK_REGISTRY.register\nclass InterpolateToPyramidal(Neck):\n    def __init__(self, channel_list: list[int], scale_factor: int = 2, mode: str = \"nearest\"):\n        \"\"\"Spatially interpolate embeddings so that embedding[i - 1] is scale_factor times larger than embedding[i]\n\n        Useful to make non-pyramidal backbones compatible with hierarachical ones\n        Args:\n            scale_factor (int): Amount to scale embeddings by each layer. Defaults to 2.\n            mode (str): Interpolation mode to be passed to torch.nn.functional.interpolate. Defaults to 'nearest'.\n        \"\"\"\n        super().__init__(channel_list)\n        self.scale_factor = scale_factor\n        self.mode = mode\n\n    def forward(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        out = []\n        scale_exponents = list(range(len(features), 0, -1))\n        for x, exponent in zip(features, scale_exponents, strict=True):\n            out.append(F.interpolate(x, scale_factor=self.scale_factor**exponent, mode=self.mode))\n\n        return out\n\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        return super().process_channel_list(channel_list)\n</code></pre>"},{"location":"necks/#terratorch.models.necks.InterpolateToPyramidal.__init__","title":"<code>__init__(channel_list, scale_factor=2, mode='nearest')</code>","text":"<p>Spatially interpolate embeddings so that embedding[i - 1] is scale_factor times larger than embedding[i]</p> <p>Useful to make non-pyramidal backbones compatible with hierarachical ones Args:     scale_factor (int): Amount to scale embeddings by each layer. Defaults to 2.     mode (str): Interpolation mode to be passed to torch.nn.functional.interpolate. Defaults to 'nearest'.</p> Source code in <code>terratorch/models/necks.py</code> <pre><code>def __init__(self, channel_list: list[int], scale_factor: int = 2, mode: str = \"nearest\"):\n    \"\"\"Spatially interpolate embeddings so that embedding[i - 1] is scale_factor times larger than embedding[i]\n\n    Useful to make non-pyramidal backbones compatible with hierarachical ones\n    Args:\n        scale_factor (int): Amount to scale embeddings by each layer. Defaults to 2.\n        mode (str): Interpolation mode to be passed to torch.nn.functional.interpolate. Defaults to 'nearest'.\n    \"\"\"\n    super().__init__(channel_list)\n    self.scale_factor = scale_factor\n    self.mode = mode\n</code></pre>"},{"location":"necks/#terratorch.models.necks.MaxpoolToPyramidal","title":"<code>terratorch.models.necks.MaxpoolToPyramidal</code>","text":"<p>               Bases: <code>Neck</code></p> Source code in <code>terratorch/models/necks.py</code> <pre><code>@TERRATORCH_NECK_REGISTRY.register\nclass MaxpoolToPyramidal(Neck):\n    def __init__(self, channel_list: list[int], kernel_size: int = 2):\n        \"\"\"Spatially downsample embeddings so that embedding[i - 1] is scale_factor times smaller than embedding[i]\n\n        Useful to make non-pyramidal backbones compatible with hierarachical ones\n        Args:\n            kernel_size (int). Base kernel size to use for maxpool. Defaults to 2.\n        \"\"\"\n        super().__init__(channel_list)\n        self.kernel_size = kernel_size\n\n    def forward(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        out = []\n        scale_exponents = list(range(len(features)))\n        for x, exponent in zip(features, scale_exponents, strict=True):\n            if exponent == 0:\n                out.append(x.clone())\n            else:\n                out.append(F.max_pool2d(x, kernel_size=self.kernel_size**exponent))\n\n        return out\n\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        return super().process_channel_list(channel_list)\n</code></pre>"},{"location":"necks/#terratorch.models.necks.MaxpoolToPyramidal.__init__","title":"<code>__init__(channel_list, kernel_size=2)</code>","text":"<p>Spatially downsample embeddings so that embedding[i - 1] is scale_factor times smaller than embedding[i]</p> <p>Useful to make non-pyramidal backbones compatible with hierarachical ones Args:     kernel_size (int). Base kernel size to use for maxpool. Defaults to 2.</p> Source code in <code>terratorch/models/necks.py</code> <pre><code>def __init__(self, channel_list: list[int], kernel_size: int = 2):\n    \"\"\"Spatially downsample embeddings so that embedding[i - 1] is scale_factor times smaller than embedding[i]\n\n    Useful to make non-pyramidal backbones compatible with hierarachical ones\n    Args:\n        kernel_size (int). Base kernel size to use for maxpool. Defaults to 2.\n    \"\"\"\n    super().__init__(channel_list)\n    self.kernel_size = kernel_size\n</code></pre>"},{"location":"necks/#terratorch.models.necks.ReshapeTokensToImage","title":"<code>terratorch.models.necks.ReshapeTokensToImage</code>","text":"<p>               Bases: <code>Neck</code></p> Source code in <code>terratorch/models/necks.py</code> <pre><code>@TERRATORCH_NECK_REGISTRY.register\nclass ReshapeTokensToImage(Neck):\n    def __init__(self, channel_list: list[int], remove_cls_token=True, effective_time_dim: int = 1):  # noqa: FBT002\n        \"\"\"Reshape output of transformer encoder so it can be passed to a conv net.\n\n        Args:\n            remove_cls_token (bool, optional): Whether to remove the cls token from the first position.\n                Defaults to True.\n            effective_time_dim (int, optional): The effective temporal dimension the transformer processes.\n                For a ViT, his will be given by `num_frames // tubelet size`. This is used to determine\n                the temporal dimension of the embedding, which is concatenated with the embedding dimension.\n                For example:\n                - A model which processes 1 frame with a tubelet size of 1 has an effective_time_dim of 1.\n                    The embedding produced by this model has embedding size embed_dim * 1.\n                - A model which processes 3 frames with a tubelet size of 1 has an effective_time_dim of 3.\n                    The embedding produced by this model has embedding size embed_dim * 3.\n                - A model which processes 12 frames with a tubelet size of 4 has an effective_time_dim of 3.\n                    The embedding produced by this model has an embedding size embed_dim * 3.\n                Defaults to 1.\n        \"\"\"\n        super().__init__(channel_list)\n        self.remove_cls_token = remove_cls_token\n        self.effective_time_dim = effective_time_dim\n\n    def collapse_dims(self, x):\n        \"\"\"\n        When the encoder output has more than 3 dimensions, is necessary to \n        reshape it. \n        \"\"\"\n        shape = x.shape\n        batch = x.shape[0]\n        e = x.shape[-1]\n        collapsed_dim = np.prod(x.shape[1:-1])\n\n        return x.reshape(batch, collapsed_dim, e)\n\n    def forward(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        out = []\n        for x in features:\n            if self.remove_cls_token:\n                x_no_token = x[:, 1:, :]\n            else:\n                x_no_token = x\n            x_no_token = self.collapse_dims(x_no_token)\n            number_of_tokens = x_no_token.shape[1]\n            tokens_per_timestep = number_of_tokens // self.effective_time_dim\n            h = int(math.sqrt(tokens_per_timestep))\n\n            encoded = rearrange(\n                x_no_token,\n                \"batch (t h w) e -&gt; batch (t e) h w\",\n                batch=x_no_token.shape[0],\n                t=self.effective_time_dim,\n                h=h,\n            )\n            out.append(encoded)\n        return out\n\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        return super().process_channel_list(channel_list)\n</code></pre>"},{"location":"necks/#terratorch.models.necks.ReshapeTokensToImage.__init__","title":"<code>__init__(channel_list, remove_cls_token=True, effective_time_dim=1)</code>","text":"<p>Reshape output of transformer encoder so it can be passed to a conv net.</p> <p>Parameters:</p> Name Type Description Default <code>remove_cls_token</code> <code>bool</code> <p>Whether to remove the cls token from the first position. Defaults to True.</p> <code>True</code> <code>effective_time_dim</code> <code>int</code> <p>The effective temporal dimension the transformer processes. For a ViT, his will be given by <code>num_frames // tubelet size</code>. This is used to determine the temporal dimension of the embedding, which is concatenated with the embedding dimension. For example: - A model which processes 1 frame with a tubelet size of 1 has an effective_time_dim of 1.     The embedding produced by this model has embedding size embed_dim * 1. - A model which processes 3 frames with a tubelet size of 1 has an effective_time_dim of 3.     The embedding produced by this model has embedding size embed_dim * 3. - A model which processes 12 frames with a tubelet size of 4 has an effective_time_dim of 3.     The embedding produced by this model has an embedding size embed_dim * 3. Defaults to 1.</p> <code>1</code> Source code in <code>terratorch/models/necks.py</code> <pre><code>def __init__(self, channel_list: list[int], remove_cls_token=True, effective_time_dim: int = 1):  # noqa: FBT002\n    \"\"\"Reshape output of transformer encoder so it can be passed to a conv net.\n\n    Args:\n        remove_cls_token (bool, optional): Whether to remove the cls token from the first position.\n            Defaults to True.\n        effective_time_dim (int, optional): The effective temporal dimension the transformer processes.\n            For a ViT, his will be given by `num_frames // tubelet size`. This is used to determine\n            the temporal dimension of the embedding, which is concatenated with the embedding dimension.\n            For example:\n            - A model which processes 1 frame with a tubelet size of 1 has an effective_time_dim of 1.\n                The embedding produced by this model has embedding size embed_dim * 1.\n            - A model which processes 3 frames with a tubelet size of 1 has an effective_time_dim of 3.\n                The embedding produced by this model has embedding size embed_dim * 3.\n            - A model which processes 12 frames with a tubelet size of 4 has an effective_time_dim of 3.\n                The embedding produced by this model has an embedding size embed_dim * 3.\n            Defaults to 1.\n    \"\"\"\n    super().__init__(channel_list)\n    self.remove_cls_token = remove_cls_token\n    self.effective_time_dim = effective_time_dim\n</code></pre>"},{"location":"necks/#terratorch.models.necks.ReshapeTokensToImage.collapse_dims","title":"<code>collapse_dims(x)</code>","text":"<p>When the encoder output has more than 3 dimensions, is necessary to  reshape it.</p> Source code in <code>terratorch/models/necks.py</code> <pre><code>def collapse_dims(self, x):\n    \"\"\"\n    When the encoder output has more than 3 dimensions, is necessary to \n    reshape it. \n    \"\"\"\n    shape = x.shape\n    batch = x.shape[0]\n    e = x.shape[-1]\n    collapsed_dim = np.prod(x.shape[1:-1])\n\n    return x.reshape(batch, collapsed_dim, e)\n</code></pre>"},{"location":"necks/#terratorch.models.necks.AddBottleneckLayer","title":"<code>terratorch.models.necks.AddBottleneckLayer</code>","text":"<p>               Bases: <code>Neck</code></p> <p>Add a layer that reduces the channel dimension of the final embedding by half, and concatenates it</p> <p>Useful for compatibility with some smp decoders.</p> Source code in <code>terratorch/models/necks.py</code> <pre><code>@TERRATORCH_NECK_REGISTRY.register\nclass AddBottleneckLayer(Neck):\n    \"\"\"Add a layer that reduces the channel dimension of the final embedding by half, and concatenates it\n\n    Useful for compatibility with some smp decoders.\n    \"\"\"\n\n    def __init__(self, channel_list: list[int]):\n        super().__init__(channel_list)\n        self.bottleneck = nn.Conv2d(channel_list[-1], channel_list[-1]//2, kernel_size=1)\n\n    def forward(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        new_embedding = self.bottleneck(features[-1])\n        features.append(new_embedding)\n        return features\n\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        return [*channel_list, channel_list[-1] // 2]\n</code></pre>"},{"location":"necks/#terratorch.models.necks.LearnedInterpolateToPyramidal","title":"<code>terratorch.models.necks.LearnedInterpolateToPyramidal</code>","text":"<p>               Bases: <code>Neck</code></p> <p>Use learned convolutions to transform the output of a non-pyramidal encoder into pyramidal ones</p> <p>Always requires exactly 4 embeddings</p> Source code in <code>terratorch/models/necks.py</code> <pre><code>@TERRATORCH_NECK_REGISTRY.register\nclass LearnedInterpolateToPyramidal(Neck):\n    \"\"\"Use learned convolutions to transform the output of a non-pyramidal encoder into pyramidal ones\n\n    Always requires exactly 4 embeddings\n    \"\"\"\n\n    def __init__(self, channel_list: list[int]):\n        super().__init__(channel_list)\n        if len(channel_list) != 4:\n            msg = \"This class can only handle exactly 4 input embeddings\"\n            raise Exception(msg)\n        self.fpn1 = nn.Sequential(\n            nn.ConvTranspose2d(channel_list[0], channel_list[0] // 2, 2, 2),\n            nn.BatchNorm2d(channel_list[0] // 2),\n            nn.GELU(),\n            nn.ConvTranspose2d(channel_list[0] // 2, channel_list[0] // 4, 2, 2),\n        )\n        self.fpn2 = nn.Sequential(nn.ConvTranspose2d(channel_list[1], channel_list[1] // 2, 2, 2))\n        self.fpn3 = nn.Sequential(nn.Identity())\n        self.fpn4 = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2))\n        self.embedding_dim = [channel_list[0] // 4, channel_list[1] // 2, channel_list[2], channel_list[3]]\n\n    def forward(self, features: list[torch.Tensor]) -&gt; list[torch.Tensor]:\n        scaled_inputs = []\n        scaled_inputs.append(self.fpn1(features[0]))\n        scaled_inputs.append(self.fpn2(features[1]))\n        scaled_inputs.append(self.fpn3(features[2]))\n        scaled_inputs.append(self.fpn4(features[3]))\n        return scaled_inputs\n\n    def process_channel_list(self, channel_list: list[int]) -&gt; list[int]:\n        return [channel_list[0] // 4, channel_list[1] // 2, channel_list[2], channel_list[3]]\n</code></pre>"},{"location":"quick_start/","title":"Quick start","text":""},{"location":"quick_start/#configuring-the-environment","title":"Configuring the environment","text":""},{"location":"quick_start/#python","title":"Python","text":"<p>TerraTorch is currently tested for Python in <code>3.10 &lt;= Python &lt;= 3.12</code>. </p>"},{"location":"quick_start/#gdal","title":"GDAL","text":"<p>GDAL is required  to read and write TIFF images. It is usually easy to install in Unix/Linux systems, but if it is not your case  we reccomend using a conda environment and installing it with <code>conda install -c conda-forge gdal</code>.</p>"},{"location":"quick_start/#installing-terratorch","title":"Installing TerraTorch","text":"<p>For a stable point-release, use <code>pip install terratorch</code>. If you prefer to get the most recent version of the main branch, install the library with <code>pip install git+https://github.com/IBM/terratorch.git</code>.</p> <p>To install as a developer (e.g. to extend the library) clone this repo, and run <code>pip install -e .</code> .</p>"},{"location":"quick_start/#creating-backbones","title":"Creating Backbones","text":"<p>You can interact with the library at several levels of abstraction. Each deeper level of abstraction trades off some amount of flexibility for ease of use and configuration. In the simplest case, we might only want access a backbone and code all the rest ourselves. In this case, we can simply use the library as a backbone factory:</p> Instantiating a prithvi backbone<pre><code>from terratorch import BACKBONE_REGISTRY\n\n# find available prithvi models\nprint([model_name for model_name in BACKBONE_REGISTRY if \"terratorch_prithvi\" in model_name])\n&gt;&gt;&gt; ['terratorch_prithvi_eo_tiny', 'terratorch_prithvi_eo_v1_100', 'terratorch_prithvi_eo_v2_300', 'terratorch_prithvi_eo_v2_600', 'terratorch_prithvi_eo_v2_300_tl', 'terratorch_prithvi_eo_v2_600_tl']\n\n# show all models with list(BACKBONE_REGISTRY)\n\n# check a model is in the registry\n\"terratorch_prithvi_eo_v2_300\" in BACKBONE_REGISTRY\n&gt;&gt;&gt; True\n\n# without the prefix, all internal registries will be searched until the first match is found\n\"prithvi_eo_v1_100\" in BACKBONE_REGISTRY\n&gt;&gt;&gt; True\n\n# instantiate your desired model\n# the backbone registry prefix (e.g. `terratorch` or `timm`) is optional\n# in this case, the underlying registry is terratorch.\nmodel = BACKBONE_REGISTRY.build(\"prithvi_eo_v1_100\", pretrained=True)\n\n# instantiate your model with more options, for instance, passing weights from your own file\nmodel = BACKBONE_REGISTRY.build(\n    \"prithvi_eo_v2_300\", num_frames=1, ckpt_path='path/to/model.pt'\n)\n# Rest of your PyTorch / PyTorchLightning code\n</code></pre> <p>Internally, terratorch maintains several registries for components such as backbones or decoders. The top-level <code>BACKBONE_REGISTRY</code> collects all of them.</p> <p>The name passed to <code>build</code> is used to find the appropriate model constructor, which will be the first model from the first registry found with that name.</p> <p>To explicitly determine the registry that will build the model, you may prepend a prefix such as <code>timm_</code> to the model name. In this case, the <code>timm</code> model registry will be exclusively searched for the model.</p>"},{"location":"quick_start/#directly-creating-a-full-model","title":"Directly creating a full model","text":"<p>We also provide a model factory for a task specific model built on one a backbones:</p> Building a full model, with task specific decoder<pre><code>import terratorch # even though we don't use the import directly, we need it so that the models are available in the timm registry\nfrom terratorch.models import EncoderDecoderFactory\nfrom terratorch.datasets import HLSBands\n\nmodel_factory = EncoderDecoderFactory()\n\n# Let's build a segmentation model\n# Parameters prefixed with backbone_ get passed to the backbone\n# Parameters prefixed with decoder_ get passed to the decoder\n# Parameters prefixed with head_ get passed to the head\n\nmodel = model_factory.build_model(\n    task=\"segmentation\",\n    backbone=\"prithvi_eo_v2_300\",\n    backbone_pretrained=True,\n    backbone_bands=[\n        HLSBands.BLUE,\n        HLSBands.GREEN,\n        HLSBands.RED,\n        HLSBands.NIR_NARROW,\n        HLSBands.SWIR_1,\n        HLSBands.SWIR_2,\n    ],\n    necks=[{\"name\": \"SelectIndices\", \"indices\": [-1]},\n           {\"name\": \"ReshapeTokensToImage\"}],\n    decoder=\"FCNDecoder\",\n    decoder_channels=128,\n    head_dropout=0.1,\n    num_classes=4,\n)\n\n# Rest of your PyTorch / PyTorchLightning code\n.\n.\n.\n</code></pre>"},{"location":"quick_start/#training-with-lightning-tasks","title":"Training with Lightning Tasks","text":"<p>At the highest level of abstraction, you can directly obtain a LightningModule ready to be trained.</p> <p>Building a full Pixel-Wise Regression task<pre><code>model_args = dict(\n  backbone=\"prithvi_eo_v2_300\",\n  backbone_pretrained=True,\n  backbone_num_frames=1,\n  backbone_bands=[\n      HLSBands.BLUE,\n      HLSBands.GREEN,\n      HLSBands.RED,\n      HLSBands.NIR_NARROW,\n      HLSBands.SWIR_1,\n      HLSBands.SWIR_2,\n  ],\n  necks=[{\"name\": \"SelectIndices\", \"indices\": [-1]},\n               {\"name\": \"ReshapeTokensToImage\"}],\n  decoder=\"FCNDecoder\",\n  decoder_channels=128,\n  head_dropout=0.1\n)\n\ntask = PixelwiseRegressionTask(\n    model_args,\n    \"EncoderDecoderFactory\",\n    loss=\"rmse\",\n    lr=lr,\n    ignore_index=-1,\n    optimizer=\"AdamW\",\n    optimizer_hparams={\"weight_decay\": 0.05},\n)\n\n# Pass this LightningModule to a Lightning Trainer, together with some LightningDataModule\n</code></pre> Alternatively, all the process can be summarized in configuration files written in YAML format, as seen below. Configuration file for a Semantic Segmentation Task<pre><code># lightning.pytorch==2.1.1\nseed_everything: 0\ntrainer:\n  accelerator: auto\n  strategy: auto\n  devices: auto\n  num_nodes: 1\n  precision: bf16\n  logger:\n    class_path: TensorBoardLogger\n    init_args:\n      save_dir: &lt;path_to_experiment_dir&gt;\n      name: &lt;experiment_name&gt;\n  callbacks:\n    - class_path: RichProgressBar\n    - class_path: LearningRateMonitor\n      init_args:\n        logging_interval: epoch\n\n  max_epochs: 200\n  check_val_every_n_epoch: 1\n  log_every_n_steps: 50\n  enable_checkpointing: true\n  default_root_dir: &lt;path_to_experiment_dir&gt;\ndata:\n  class_path: terratorch.datamodules.sen1floods11.Sen1Floods11NonGeoDataModule\n  init_args:\n    batch_size: 16\n    num_workers: 8\n  dict_kwargs:\n    data_root: &lt;path_to_data_root&gt;\n    bands:\n      - 1\n      - 2\n      - 3\n      - 8\n      - 11\n      - 12\nmodel:\n  class_path: terratorch.tasks.SemanticSegmentationTask\n  init_args:\n    model_factory: EncoderDecoderFactory\n    model_args:\n      backbone: prithvi_eo_v2_300\n      backbone_img_size: 512\n      backbone_pretrained: True\n      backbone_bands:\n        - BLUE\n        - GREEN\n        - RED\n        - NIR_NARROW\n        - SWIR_1\n        - SWIR_2\n      necks:\n        - name: SelectIndices\n          indices: [5, 11, 17, 23]\n        - name: ReshapeTokensToImage\n        - name: LearnedInterpolateToPyramidal\n      decoder: UperNetDecoder\n      decoder_channels: 256\n      head_channel_list: [256]\n      head_dropout: 0.1\n      num_classes: 2\n    loss: dice\n    ignore_index: -1\n    freeze_backbone: false\n    freeze_decoder: false    \noptimizer:\n  class_path: torch.optim.AdamW\n  init_args:\n    lr: 1.e-4\n    weight_decay: 0.1\nlr_scheduler:\n  class_path: ReduceLROnPlateau\n  init_args:\n    monitor: val/loss\n</code></pre></p> <p>To run this training task using the YAML, simply execute: <pre><code>terratorch fit --config &lt;path_to_config_file&gt;\n</code></pre></p> <p>To test your model on the test set, execute: <pre><code>terratorch test --config  &lt;path_to_config_file&gt; --ckpt_path &lt;path_to_checkpoint_file&gt;\n</code></pre></p> <p>For inference, execute: <pre><code>terratorch predict -c &lt;path_to_config_file&gt; --ckpt_path&lt;path_to_checkpoint&gt; --predict_output_dir &lt;path_to_output_dir&gt; --data.init_args.predict_data_root &lt;path_to_input_dir&gt; --data.init_args.predict_dataset_bands &lt;all bands in the predicted dataset, e.g. [BLUE,GREEN,RED,NIR_NARROW,SWIR_1,SWIR_2,0]&gt;\n</code></pre></p> <p>Experimental feature: Users that want to optimize hyperparameters or repeat best experiment might be interest in in terratorch-iterate, a terratorch's plugin. For instance, to run terratorch-iterate to optimize hyperparameters, one can run:  <pre><code>terratorch iterate --hpo --config &lt;path_to_config_file&gt; \n</code></pre> Please see how to install terratorch-iterate on this link and how to use it on this link.</p>"},{"location":"registry/","title":"Registries","text":"<p>TerraTorch keeps a set of registries which map strings to instances of those strings. They can be imported from <code>terratorch.registry</code>.</p> <p>Info</p> <p>If you are using tasks with existing models, you may never have to interact with registries directly. The model factory will handle interactions with registries.</p> <p>Registries behave like python sets, exposing the usual <code>contains</code> and <code>iter</code> operations. This means you can easily operate on them in a pythonic way, such as  <code>\"model\" in registry</code> or <code>list(registry)</code>.</p> <p>To create the desired instance, registries expose a <code>build</code> method, which accepts the name and the arguments to be passed to the constructor.</p> Using registries<pre><code>from terratorch import BACKBONE_REGISTRY\n\n# find available prithvi models\nprint([model_name for model_name in BACKBONE_REGISTRY if \"terratorch_prithvi\" in model_name])\n&gt;&gt;&gt; ['terratorch_prithvi_eo_tiny', 'terratorch_prithvi_eo_v1_100', 'terratorch_prithvi_eo_v2_300', 'terratorch_prithvi_eo_v2_600', 'terratorch_prithvi_eo_v2_300_tl', 'terratorch_prithvi_eo_v2_600_tl']\n\n# show all models with list(BACKBONE_REGISTRY)\n\n# check a model is in the registry\n\"terratorch_prithvi_eo_v2_300\" in BACKBONE_REGISTRY\n&gt;&gt;&gt; True\n\n# without the prefix, all internal registries will be searched until the first match is found\n\"prithvi_eo_v1_100\" in BACKBONE_REGISTRY\n&gt;&gt;&gt; True\n\n# instantiate your desired model\n# the backbone registry prefix (e.g. `terratorch` or `timm`) is optional\n# in this case, the underlying registry is terratorch.\nmodel = BACKBONE_REGISTRY.build(\"prithvi_eo_v1_100\", pretrained=True)\n\n# instantiate your model with more options, for instance, passing weights from your own file\nmodel = BACKBONE_REGISTRY.build(\n    \"prithvi_eo_v2_300\", num_frames=1, ckpt_path='path/to/model.pt'\n)\n# Rest of your PyTorch / PyTorchLightning code\n</code></pre>"},{"location":"registry/#multisourceregistries","title":"MultiSourceRegistries","text":"<p><code>BACKBONE_REGISTRY</code> and <code>DECODER_REGISTRY</code> are special registries which dynamically aggregate multiple registries. They behave as if they were a single large registry by searching over multiple registries.</p> <p>For instance, the <code>DECODER_REGISTRY</code> holds the <code>TERRATORCH_DECODER_REGISTRY</code>, which is responsible for decoders implemented in terratorch, as well as the <code>SMP_DECODER_REGISTRY</code> and the <code>MMSEG_DECODER_REGISTRY</code> (if mmseg is installed).</p> <p>To make sure you access the object from a particular registry, you may prepend your string with the prefix from that registry.</p> <pre><code>from terratorch import DECODER_REGISTRY\n\n# decoder registries always take at least one extra argument, the channel list with the channel dimension of each embedding passed to it\nDECODER_REGISTRY.build(\"FCNDecoder\", [32, 64, 128])\n\nDECODER_REGISTRY.build(\"terratorch_FCNDecoder\", [32, 64, 128])\n\n# Find all prefixes\nDECODER_REGISTRY.keys()\n&gt;&gt;&gt; odict_keys(['terratorch', 'smp', 'mmseg'])\n</code></pre> <p>If a prefix is not added, the <code>MultiSourceRegistry</code> will search each registry in the order it was added (starting with the <code>TERRATORCH_</code> registry) until it finds the first match.</p> <p>For both of these registries, only <code>TERRATORCH_X_REGISTRY</code> is mutable. To register backbones or decoders to terratorch, you should decorate the constructor function (or the model class itself) with <code>@TERRATORCH_DECODER_REGISTRY.register</code> or <code>@TERRATORCH_BACKBONE_REGISTRY.register</code>.</p> <p>To add a new registry to these top level registries, you should use the <code>.register</code> method, taking the register and the prefix that will be used for it.</p>"},{"location":"registry/#terratorch.registry.registry.MultiSourceRegistry","title":"<code>terratorch.registry.registry.MultiSourceRegistry</code>","text":"<p>               Bases: <code>Mapping[str, T]</code>, <code>Generic[T]</code></p> <p>Registry that searches in multiple sources</p> <p>Correct functioning of this class depends on registries raising a KeyError when the model is not found.</p> Source code in <code>terratorch/registry/registry.py</code> <pre><code>class MultiSourceRegistry(Mapping[str, T], typing.Generic[T]):\n    \"\"\"Registry that searches in multiple sources\n\n        Correct functioning of this class depends on registries raising a KeyError when the model is not found.\n    \"\"\"\n    def __init__(self, **sources) -&gt; None:\n        self._sources: OrderedDict[str, T] = OrderedDict(sources)\n\n    def _parse_prefix(self, name) -&gt; tuple[str, str] | None:\n        split = name.split(\"_\")\n        if len(split) &gt; 1 and split[0] in self._sources:\n            prefix = split[0]\n            name_without_prefix = \"_\".join(split[1:])\n            return prefix, name_without_prefix\n        return None\n\n    def find_registry(self, name: str) -&gt; T:\n        parsed_prefix = self._parse_prefix(name)\n        if parsed_prefix:\n            prefix, name_without_prefix = parsed_prefix\n            registry = self._sources[prefix]\n            return registry\n\n        # if no prefix is given, go through all sources in order\n        for registry in self._sources.values():\n            if name in registry:\n                return registry\n        msg = f\"Model {name} not found in any registry\"\n        raise KeyError(msg)\n\n    def build(self, name: str, *constructor_args, **constructor_kwargs):\n        parsed_prefix = self._parse_prefix(name)\n        if parsed_prefix:\n            prefix, name_without_prefix = parsed_prefix\n            registry = self._sources[prefix]\n            return registry.build(name_without_prefix, *constructor_args, **constructor_kwargs)\n\n        # if no prefix, try to build in order\n        for source in self._sources.values():\n            with suppress(KeyError):\n                return source.build(name, *constructor_args, **constructor_kwargs)\n\n        msg = f\"Could not instantiate model {name} not from any source.\"\n        raise KeyError(msg)\n\n    def register_source(self, prefix: str, registry: T) -&gt; None:\n        \"\"\"Register a source in the registry\"\"\"\n        if prefix in self._sources:\n            msg = f\"Source for prefix {prefix} already exists.\"\n            raise KeyError(msg)\n        self._sources[prefix] = registry\n\n    def __iter__(self):\n        for prefix in self._sources:\n            for element in self._sources[prefix]:\n                yield prefix + \"_\" + element\n\n    def __len__(self):\n        return sum(len(source) for source in self._sources.values())\n\n    def __getitem__(self, name):\n        return self._sources[name]\n\n    def __contains__(self, name):\n        parsed_prefix = self._parse_prefix(name)\n        if parsed_prefix:\n            prefix, name_without_prefix = parsed_prefix\n            return name_without_prefix in self._sources[prefix]\n        return any(name in source for source in self._sources.values())\n\n    @_recursive_repr()\n    def __repr__(self):\n        args = [f\"{name}={source!r}\" for name, source in self._sources.items()]\n        return f'{self.__class__.__name__}({\", \".join(args)})'\n\n    def __str__(self):\n        sources_str = str(\" | \".join([f\"{prefix}: {source!s}\" for prefix, source in self._sources.items()]))\n        return f\"Multi source registry with {len(self)} items: {sources_str}\"\n\n    def keys(self):\n        return self._sources.keys()\n</code></pre>"},{"location":"registry/#terratorch.registry.registry.MultiSourceRegistry.register_source","title":"<code>register_source(prefix, registry)</code>","text":"<p>Register a source in the registry</p> Source code in <code>terratorch/registry/registry.py</code> <pre><code>def register_source(self, prefix: str, registry: T) -&gt; None:\n    \"\"\"Register a source in the registry\"\"\"\n    if prefix in self._sources:\n        msg = f\"Source for prefix {prefix} already exists.\"\n        raise KeyError(msg)\n    self._sources[prefix] = registry\n</code></pre>"},{"location":"registry/#terratorch.registry.registry.Registry","title":"<code>terratorch.registry.registry.Registry</code>","text":"<p>               Bases: <code>Set</code></p> <p>Registry holding model constructors and multiple additional sources.</p> <p>This registry behaves as a set of strings, which are model names, to model classes or functions which instantiate model classes.</p> <p>In addition, it can instantiate models with the build method.</p> <p>Add constructors to the registry by annotating them with @registry.register. <pre><code>&gt;&gt;&gt; registry = Registry()\n&gt;&gt;&gt; @registry.register\n... def model(*args, **kwargs):\n...     return object()\n&gt;&gt;&gt; \"model\" in registry\nTrue\n&gt;&gt;&gt; model_instance = registry.build(\"model\")\n</code></pre></p> Source code in <code>terratorch/registry/registry.py</code> <pre><code>class Registry(Set):\n    \"\"\"Registry holding model constructors and multiple additional sources.\n\n    This registry behaves as a set of strings, which are model names,\n    to model classes or functions which instantiate model classes.\n\n    In addition, it can instantiate models with the build method.\n\n    Add constructors to the registry by annotating them with @registry.register.\n    ```\n    &gt;&gt;&gt; registry = Registry()\n    &gt;&gt;&gt; @registry.register\n    ... def model(*args, **kwargs):\n    ...     return object()\n    &gt;&gt;&gt; \"model\" in registry\n    True\n    &gt;&gt;&gt; model_instance = registry.build(\"model\")\n    ```\n    \"\"\"\n\n    def __init__(self, **elements) -&gt; None:\n        self._registry: dict[str, Callable] = dict(elements)\n\n    def register(self, constructor: Callable | type) -&gt; Callable:\n        \"\"\"Register a component in the registry. Used as a decorator.\n\n        Args:\n            constructor (Callable | type): Function or class to be decorated with @register.\n        \"\"\"\n        if not callable(constructor):\n            msg = f\"Invalid argument. Decorate a function or class with @{self.__class__.__name__}.register\"\n            raise TypeError(msg)\n        self._registry[constructor.__name__] = constructor\n        return constructor\n\n    def build(self, name: str, *constructor_args, **constructor_kwargs):\n        \"\"\"Build and return the component.\n        Use prefixes ending with _ to forward to a specific source\n        \"\"\"\n        return self._registry[name](*constructor_args, **constructor_kwargs)\n\n    def __iter__(self):\n        return iter(self._registry)\n\n    # def __getitem__(self, key):\n    #     return self._registry[key]\n\n    def __len__(self):\n        return len(self._registry)\n\n    def __contains__(self, key):\n        return key in self._registry\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self._registry!r})\"\n\n    def __str__(self):\n        return f\"Registry with {len(self)} registered items\"\n</code></pre>"},{"location":"registry/#terratorch.registry.registry.Registry.build","title":"<code>build(name, *constructor_args, **constructor_kwargs)</code>","text":"<p>Build and return the component. Use prefixes ending with _ to forward to a specific source</p> Source code in <code>terratorch/registry/registry.py</code> <pre><code>def build(self, name: str, *constructor_args, **constructor_kwargs):\n    \"\"\"Build and return the component.\n    Use prefixes ending with _ to forward to a specific source\n    \"\"\"\n    return self._registry[name](*constructor_args, **constructor_kwargs)\n</code></pre>"},{"location":"registry/#terratorch.registry.registry.Registry.register","title":"<code>register(constructor)</code>","text":"<p>Register a component in the registry. Used as a decorator.</p> <p>Parameters:</p> Name Type Description Default <code>constructor</code> <code>Callable | type</code> <p>Function or class to be decorated with @register.</p> required Source code in <code>terratorch/registry/registry.py</code> <pre><code>def register(self, constructor: Callable | type) -&gt; Callable:\n    \"\"\"Register a component in the registry. Used as a decorator.\n\n    Args:\n        constructor (Callable | type): Function or class to be decorated with @register.\n    \"\"\"\n    if not callable(constructor):\n        msg = f\"Invalid argument. Decorate a function or class with @{self.__class__.__name__}.register\"\n        raise TypeError(msg)\n    self._registry[constructor.__name__] = constructor\n    return constructor\n</code></pre>"},{"location":"registry/#other-registries","title":"Other Registries","text":"<p>Additionally, terratorch has the <code>NECK_REGISTRY</code>, where all necks must be registered, and the <code>MODEL_FACTORY_REGISTRY</code>, where all model factories must be registered.</p>"},{"location":"tasks/","title":"Tasks","text":"<p>Tasks provide a convenient abstraction over the training of a model for a specific downstream task.  They encapsulate the model, optimizer, metrics, loss as well as training, validation and testing steps. The task expects to be passed a model factory, to which the model_args arguments are passed to instantiate the model that will be trained. The models produced by this model factory should output ModelOutput instances and conform to the Model ABC. Tasks are best leveraged using config files, where they are specified in the <code>model</code> section under <code>class_path</code>. You can check out some examples of config files here. Below are the details of the tasks currently implemented in TerraTorch (Pixelwise Regression, Semantic Segmentation and Classification). </p>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask","title":"<code>terratorch.tasks.segmentation_tasks.SemanticSegmentationTask</code>","text":"<p>               Bases: <code>TerraTorchTask</code></p> <p>Semantic Segmentation Task that accepts models from a range of sources.</p> <p>This class is analog in functionality to class SemanticSegmentationTask defined by torchgeo. However, it has some important differences:     - Accepts the specification of a model factory     - Logs metrics per class     - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)     - Allows the setting of optimizers in the constructor     - Allows to evaluate on multiple test dataloaders</p> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>class SemanticSegmentationTask(TerraTorchTask):\n    \"\"\"Semantic Segmentation Task that accepts models from a range of sources.\n\n    This class is analog in functionality to class SemanticSegmentationTask defined by torchgeo.\n    However, it has some important differences:\n        - Accepts the specification of a model factory\n        - Logs metrics per class\n        - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)\n        - Allows the setting of optimizers in the constructor\n        - Allows to evaluate on multiple test dataloaders\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: dict,\n        model_factory: str | None = None,\n        model: torch.nn.Module | None = None,\n        loss: str = \"ce\",\n        aux_heads: list[AuxiliaryHead] | None = None,\n        aux_loss: dict[str, float] | None = None,\n        class_weights: list[float] | None = None,\n        ignore_index: int | None = None,\n        lr: float = 0.001,\n        # the following are optional so CLI doesnt need to pass them\n        optimizer: str | None = None,\n        optimizer_hparams: dict | None = None,\n        scheduler: str | None = None,\n        scheduler_hparams: dict | None = None,\n        #\n        freeze_backbone: bool = False,  # noqa: FBT001, FBT002\n        freeze_decoder: bool = False,  # noqa: FBT002, FBT001\n        freeze_head: bool = False, \n        plot_on_val: bool | int = 10,\n        class_names: list[str] | None = None,\n        tiled_inference_parameters: TiledInferenceParameters = None,\n        test_dataloaders_names: list[str] | None = None,\n        lr_overrides: dict[str, float] | None = None,\n        output_most_probable: bool = True,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            Defaults to None.\n            model_args (Dict): Arguments passed to the model factory.\n            model_factory (str, optional): ModelFactory class to be used to instantiate the model.\n                Is ignored when model is provided.\n            model (torch.nn.Module, optional): Custom model.\n            loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss.\n                Defaults to \"ce\".\n            aux_loss (dict[str, float] | None, optional): Auxiliary loss weights.\n                Should be a dictionary where the key is the name given to the loss\n                and the value is the weight to be applied to that loss.\n                The name of the loss should match the key in the dictionary output by the model's forward\n                method containing that output. Defaults to None.\n            class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss.\n            class_weights (list[float] | None, optional): List of class weights to be applied to the loss.\n                Defaults to None.\n            ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None.\n            lr (float, optional): Learning rate to be used. Defaults to 0.001.\n            optimizer (str | None, optional): Name of optimizer class from torch.optim to be used.\n            If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.\n            optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer.\n                Overriden by config / cli specification through LightningCLI.\n            scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler\n                to be used (e.g. ReduceLROnPlateau). Defaults to None.\n                Overriden by config / cli specification through LightningCLI.\n            scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler.\n                Overriden by config / cli specification through LightningCLI.\n            freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False.\n            freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False.\n            freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False.\n            plot_on_val (bool | int, optional): Whether to plot visualizations on validation.\n            If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs.\n            class_names (list[str] | None, optional): List of class names passed to metrics for better naming.\n                Defaults to numeric ordering.\n            tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters\n                used to determine if inference is done on the whole image or through tiling.\n            test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when\n                multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None,\n                which assumes only one test dataloader is used.\n            lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific\n                parameters. The key should be a substring of the parameter names (it will check the substring is\n                contained in the parameter name)and the value should be the new lr. Defaults to None.\n            output_most_probable (bool): A boolean to define if the output during the inference will be just\n                for the most probable class or if it will include all of them. \n        \"\"\"\n        self.tiled_inference_parameters = tiled_inference_parameters\n        self.aux_loss = aux_loss\n        self.aux_heads = aux_heads\n\n        if model is not None and model_factory is not None:\n            logger.warning(\"A model_factory and a model was provided. The model_factory is ignored.\")\n        if model is None and model_factory is None:\n            raise ValueError(\"A model_factory or a model (torch.nn.Module) must be provided.\")\n\n        if model_factory and model is None:\n            self.model_factory = MODEL_FACTORY_REGISTRY.build(model_factory)\n\n        super().__init__(task=\"segmentation\")\n\n        if model is not None:\n            # Custom model\n            self.model = model\n\n        self.train_loss_handler = LossHandler(self.train_metrics.prefix)\n        self.test_loss_handler: list[LossHandler] = []\n        for metrics in self.test_metrics:\n            self.test_loss_handler.append(LossHandler(metrics.prefix))\n        self.val_loss_handler = LossHandler(self.val_metrics.prefix)\n        self.monitor = f\"{self.val_metrics.prefix}loss\"\n        self.plot_on_val = int(plot_on_val)\n        self.output_most_probable = output_most_probable\n\n        if output_most_probable:\n            self.select_classes = lambda y: y.argmax(dim=1) \n        else:\n            self.select_classes = lambda y: y\n\n    def configure_losses(self) -&gt; None:\n        \"\"\"Initialize the loss criterion.\n\n        Raises:\n            ValueError: If *loss* is invalid.\n        \"\"\"\n        loss: str = self.hparams[\"loss\"]\n        ignore_index = self.hparams[\"ignore_index\"]\n\n        class_weights = (\n            torch.Tensor(self.hparams[\"class_weights\"]) if self.hparams[\"class_weights\"] is not None else None\n        )\n        if loss == \"ce\":\n            ignore_value = -100 if ignore_index is None else ignore_index\n            self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_value, weight=class_weights)\n        elif loss == \"jaccard\":\n            if ignore_index is not None:\n                exception_message = (\n                    f\"Jaccard loss does not support ignore_index, but found non-None value of {ignore_index}.\"\n                )\n                raise RuntimeError(exception_message)\n            self.criterion = smp.losses.JaccardLoss(mode=\"multiclass\")\n        elif loss == \"focal\":\n            self.criterion = smp.losses.FocalLoss(\"multiclass\", ignore_index=ignore_index, normalized=True)\n        elif loss == \"dice\":\n            self.criterion = smp.losses.DiceLoss(\"multiclass\", ignore_index=ignore_index)\n        else:\n            exception_message = (\n                f\"Loss type '{loss}' is not valid. Currently, supports 'ce', 'jaccard', 'dice' or 'focal' loss.\"\n            )\n            raise ValueError(exception_message)\n\n    def configure_metrics(self) -&gt; None:\n        \"\"\"Initialize the performance metrics.\"\"\"\n        num_classes: int = self.hparams[\"model_args\"][\"num_classes\"]\n        ignore_index: int = self.hparams[\"ignore_index\"]\n        class_names = self.hparams[\"class_names\"]\n        metrics = MetricCollection(\n            {\n                \"Multiclass_Accuracy\": MulticlassAccuracy(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                    multidim_average=\"global\",\n                    average=\"micro\",\n                ),\n                \"Multiclass_Accuracy_Class\": ClasswiseWrapper(\n                    MulticlassAccuracy(\n                        num_classes=num_classes,\n                        ignore_index=ignore_index,\n                        multidim_average=\"global\",\n                        average=None,\n                    ),\n                    labels=class_names,\n                ),\n                \"Multiclass_Jaccard_Index_Micro\": MulticlassJaccardIndex(\n                    num_classes=num_classes, ignore_index=ignore_index, average=\"micro\"\n                ),\n                \"Multiclass_Jaccard_Index\": MulticlassJaccardIndex(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                ),\n                \"Multiclass_Jaccard_Index_Class\": ClasswiseWrapper(\n                    MulticlassJaccardIndex(num_classes=num_classes, ignore_index=ignore_index, average=None),\n                    labels=class_names,\n                ),\n                \"Multiclass_F1_Score\": MulticlassF1Score(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                    multidim_average=\"global\",\n                    average=\"micro\",\n                ),\n            }\n        )\n        self.train_metrics = metrics.clone(prefix=\"train/\")\n        self.val_metrics = metrics.clone(prefix=\"val/\")\n        if self.hparams[\"test_dataloaders_names\"] is not None:\n            self.test_metrics = nn.ModuleList(\n                [metrics.clone(prefix=f\"test/{dl_name}/\") for dl_name in self.hparams[\"test_dataloaders_names\"]]\n            )\n        else:\n            self.test_metrics = nn.ModuleList([metrics.clone(prefix=\"test/\")])\n\n    def training_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n        \"\"\"Compute the train loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        loss = self.train_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.train_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n        y_hat_hard = to_segmentation_prediction(model_output)\n        self.train_metrics.update(y_hat_hard, y)\n\n        return loss[\"loss\"]\n\n    def test_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        \"\"\"Compute the test loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        if dataloader_idx &gt;= len(self.test_loss_handler):\n            msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\"\n            raise ValueError(msg)\n        loss = self.test_loss_handler[dataloader_idx].compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.test_loss_handler[dataloader_idx].log_loss(\n            partial(self.log, add_dataloader_idx=False),  # We don't need the dataloader idx as prefixes are different\n            loss_dict=loss,\n            batch_size=y.shape[0],\n        )\n        y_hat_hard = to_segmentation_prediction(model_output)\n        self.test_metrics[dataloader_idx].update(y_hat_hard, y)\n\n    def validation_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        \"\"\"Compute the validation loss and additional metrics.\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        loss = self.val_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.val_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n        y_hat_hard = to_segmentation_prediction(model_output)\n        self.val_metrics.update(y_hat_hard, y)\n\n        if self._do_plot_samples(batch_idx):\n            try:\n                datamodule = self.trainer.datamodule\n                batch[\"prediction\"] = y_hat_hard\n\n                if isinstance(batch[\"image\"], dict):\n                    if hasattr(datamodule, \"rgb_modality\"):\n                        # Generic multimodal dataset\n                        batch[\"image\"] = batch[\"image\"][datamodule.rgb_modality]\n                    else:\n                        # Multimodal dataset. Assuming first item to be the modality to visualize.\n                        batch[\"image\"] = batch[\"image\"][list(batch[\"image\"].keys())[0]]\n\n                for key in [\"image\", \"mask\", \"prediction\"]:\n                    batch[key] = batch[key].cpu()\n                sample = unbind_samples(batch)[0]\n                fig = datamodule.val_dataset.plot(sample)\n                if fig:\n                    summary_writer = self.logger.experiment\n                    if hasattr(summary_writer, \"add_figure\"):\n                        summary_writer.add_figure(f\"image/{batch_idx}\", fig, global_step=self.global_step)\n                    elif hasattr(summary_writer, \"log_figure\"):\n                        summary_writer.log_figure(\n                            self.logger.run_id, fig, f\"epoch_{self.current_epoch}_{batch_idx}.png\"\n                        )\n            except ValueError:\n                pass\n            finally:\n                plt.close()\n\n    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n        \"\"\"Compute the predicted class probabilities.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n\n        Returns:\n            Output predicted probabilities.\n        \"\"\"\n        x = batch[\"image\"]\n        file_names = batch[\"filename\"] if \"filename\" in batch else None\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n\n        rest = {k: batch[k] for k in other_keys}\n\n        def model_forward(x,  **kwargs):\n            return self(x, **kwargs).output\n\n        if self.tiled_inference_parameters:\n            y_hat: Tensor = tiled_inference(\n                model_forward,\n                x,\n                self.hparams[\"model_args\"][\"num_classes\"],\n                self.tiled_inference_parameters,\n                **rest,\n            )\n        else:\n            y_hat: Tensor = self(x, **rest).output\n\n        y_hat = self.select_classes(y_hat)\n\n        return y_hat, file_names\n</code></pre>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.__init__","title":"<code>__init__(model_args, model_factory=None, model=None, loss='ce', aux_heads=None, aux_loss=None, class_weights=None, ignore_index=None, lr=0.001, optimizer=None, optimizer_hparams=None, scheduler=None, scheduler_hparams=None, freeze_backbone=False, freeze_decoder=False, freeze_head=False, plot_on_val=10, class_names=None, tiled_inference_parameters=None, test_dataloaders_names=None, lr_overrides=None, output_most_probable=True)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict</code> <p>Arguments passed to the model factory.</p> required <code>model_factory</code> <code>str</code> <p>ModelFactory class to be used to instantiate the model. Is ignored when model is provided.</p> <code>None</code> <code>model</code> <code>Module</code> <p>Custom model.</p> <code>None</code> <code>loss</code> <code>str</code> <p>Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\".</p> <code>'ce'</code> <code>aux_loss</code> <code>dict[str, float] | None</code> <p>Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None.</p> <code>None</code> <code>class_weights</code> <code>Union[list[float], None]</code> <p>List of class weights to be applied to the loss.</p> <code>None</code> <code>class_weights</code> <code>list[float] | None</code> <p>List of class weights to be applied to the loss. Defaults to None.</p> <code>None</code> <code>ignore_index</code> <code>int | None</code> <p>Label to ignore in the loss computation. Defaults to None.</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate to be used. Defaults to 0.001.</p> <code>0.001</code> <code>optimizer</code> <code>str | None</code> <p>Name of optimizer class from torch.optim to be used.</p> <code>None</code> <code>optimizer_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler</code> <code>str</code> <p>Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>freeze_backbone</code> <code>bool</code> <p>Whether to freeze the backbone. Defaults to False.</p> <code>False</code> <code>freeze_decoder</code> <code>bool</code> <p>Whether to freeze the decoder. Defaults to False.</p> <code>False</code> <code>freeze_head</code> <code>bool</code> <p>Whether to freeze the segmentation head. Defaults to False.</p> <code>False</code> <code>plot_on_val</code> <code>bool | int</code> <p>Whether to plot visualizations on validation.</p> <code>10</code> <code>class_names</code> <code>list[str] | None</code> <p>List of class names passed to metrics for better naming. Defaults to numeric ordering.</p> <code>None</code> <code>tiled_inference_parameters</code> <code>TiledInferenceParameters | None</code> <p>Inference parameters used to determine if inference is done on the whole image or through tiling.</p> <code>None</code> <code>test_dataloaders_names</code> <code>list[str] | None</code> <p>Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used.</p> <code>None</code> <code>lr_overrides</code> <code>dict[str, float] | None</code> <p>Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None.</p> <code>None</code> <code>output_most_probable</code> <code>bool</code> <p>A boolean to define if the output during the inference will be just for the most probable class or if it will include all of them.</p> <code>True</code> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>def __init__(\n    self,\n    model_args: dict,\n    model_factory: str | None = None,\n    model: torch.nn.Module | None = None,\n    loss: str = \"ce\",\n    aux_heads: list[AuxiliaryHead] | None = None,\n    aux_loss: dict[str, float] | None = None,\n    class_weights: list[float] | None = None,\n    ignore_index: int | None = None,\n    lr: float = 0.001,\n    # the following are optional so CLI doesnt need to pass them\n    optimizer: str | None = None,\n    optimizer_hparams: dict | None = None,\n    scheduler: str | None = None,\n    scheduler_hparams: dict | None = None,\n    #\n    freeze_backbone: bool = False,  # noqa: FBT001, FBT002\n    freeze_decoder: bool = False,  # noqa: FBT002, FBT001\n    freeze_head: bool = False, \n    plot_on_val: bool | int = 10,\n    class_names: list[str] | None = None,\n    tiled_inference_parameters: TiledInferenceParameters = None,\n    test_dataloaders_names: list[str] | None = None,\n    lr_overrides: dict[str, float] | None = None,\n    output_most_probable: bool = True,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        Defaults to None.\n        model_args (Dict): Arguments passed to the model factory.\n        model_factory (str, optional): ModelFactory class to be used to instantiate the model.\n            Is ignored when model is provided.\n        model (torch.nn.Module, optional): Custom model.\n        loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss.\n            Defaults to \"ce\".\n        aux_loss (dict[str, float] | None, optional): Auxiliary loss weights.\n            Should be a dictionary where the key is the name given to the loss\n            and the value is the weight to be applied to that loss.\n            The name of the loss should match the key in the dictionary output by the model's forward\n            method containing that output. Defaults to None.\n        class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss.\n        class_weights (list[float] | None, optional): List of class weights to be applied to the loss.\n            Defaults to None.\n        ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None.\n        lr (float, optional): Learning rate to be used. Defaults to 0.001.\n        optimizer (str | None, optional): Name of optimizer class from torch.optim to be used.\n        If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.\n        optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer.\n            Overriden by config / cli specification through LightningCLI.\n        scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler\n            to be used (e.g. ReduceLROnPlateau). Defaults to None.\n            Overriden by config / cli specification through LightningCLI.\n        scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler.\n            Overriden by config / cli specification through LightningCLI.\n        freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False.\n        freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False.\n        freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False.\n        plot_on_val (bool | int, optional): Whether to plot visualizations on validation.\n        If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs.\n        class_names (list[str] | None, optional): List of class names passed to metrics for better naming.\n            Defaults to numeric ordering.\n        tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters\n            used to determine if inference is done on the whole image or through tiling.\n        test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when\n            multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None,\n            which assumes only one test dataloader is used.\n        lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific\n            parameters. The key should be a substring of the parameter names (it will check the substring is\n            contained in the parameter name)and the value should be the new lr. Defaults to None.\n        output_most_probable (bool): A boolean to define if the output during the inference will be just\n            for the most probable class or if it will include all of them. \n    \"\"\"\n    self.tiled_inference_parameters = tiled_inference_parameters\n    self.aux_loss = aux_loss\n    self.aux_heads = aux_heads\n\n    if model is not None and model_factory is not None:\n        logger.warning(\"A model_factory and a model was provided. The model_factory is ignored.\")\n    if model is None and model_factory is None:\n        raise ValueError(\"A model_factory or a model (torch.nn.Module) must be provided.\")\n\n    if model_factory and model is None:\n        self.model_factory = MODEL_FACTORY_REGISTRY.build(model_factory)\n\n    super().__init__(task=\"segmentation\")\n\n    if model is not None:\n        # Custom model\n        self.model = model\n\n    self.train_loss_handler = LossHandler(self.train_metrics.prefix)\n    self.test_loss_handler: list[LossHandler] = []\n    for metrics in self.test_metrics:\n        self.test_loss_handler.append(LossHandler(metrics.prefix))\n    self.val_loss_handler = LossHandler(self.val_metrics.prefix)\n    self.monitor = f\"{self.val_metrics.prefix}loss\"\n    self.plot_on_val = int(plot_on_val)\n    self.output_most_probable = output_most_probable\n\n    if output_most_probable:\n        self.select_classes = lambda y: y.argmax(dim=1) \n    else:\n        self.select_classes = lambda y: y\n</code></pre>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.configure_losses","title":"<code>configure_losses()</code>","text":"<p>Initialize the loss criterion.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If loss is invalid.</p> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>def configure_losses(self) -&gt; None:\n    \"\"\"Initialize the loss criterion.\n\n    Raises:\n        ValueError: If *loss* is invalid.\n    \"\"\"\n    loss: str = self.hparams[\"loss\"]\n    ignore_index = self.hparams[\"ignore_index\"]\n\n    class_weights = (\n        torch.Tensor(self.hparams[\"class_weights\"]) if self.hparams[\"class_weights\"] is not None else None\n    )\n    if loss == \"ce\":\n        ignore_value = -100 if ignore_index is None else ignore_index\n        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_value, weight=class_weights)\n    elif loss == \"jaccard\":\n        if ignore_index is not None:\n            exception_message = (\n                f\"Jaccard loss does not support ignore_index, but found non-None value of {ignore_index}.\"\n            )\n            raise RuntimeError(exception_message)\n        self.criterion = smp.losses.JaccardLoss(mode=\"multiclass\")\n    elif loss == \"focal\":\n        self.criterion = smp.losses.FocalLoss(\"multiclass\", ignore_index=ignore_index, normalized=True)\n    elif loss == \"dice\":\n        self.criterion = smp.losses.DiceLoss(\"multiclass\", ignore_index=ignore_index)\n    else:\n        exception_message = (\n            f\"Loss type '{loss}' is not valid. Currently, supports 'ce', 'jaccard', 'dice' or 'focal' loss.\"\n        )\n        raise ValueError(exception_message)\n</code></pre>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.configure_metrics","title":"<code>configure_metrics()</code>","text":"<p>Initialize the performance metrics.</p> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>def configure_metrics(self) -&gt; None:\n    \"\"\"Initialize the performance metrics.\"\"\"\n    num_classes: int = self.hparams[\"model_args\"][\"num_classes\"]\n    ignore_index: int = self.hparams[\"ignore_index\"]\n    class_names = self.hparams[\"class_names\"]\n    metrics = MetricCollection(\n        {\n            \"Multiclass_Accuracy\": MulticlassAccuracy(\n                num_classes=num_classes,\n                ignore_index=ignore_index,\n                multidim_average=\"global\",\n                average=\"micro\",\n            ),\n            \"Multiclass_Accuracy_Class\": ClasswiseWrapper(\n                MulticlassAccuracy(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                    multidim_average=\"global\",\n                    average=None,\n                ),\n                labels=class_names,\n            ),\n            \"Multiclass_Jaccard_Index_Micro\": MulticlassJaccardIndex(\n                num_classes=num_classes, ignore_index=ignore_index, average=\"micro\"\n            ),\n            \"Multiclass_Jaccard_Index\": MulticlassJaccardIndex(\n                num_classes=num_classes,\n                ignore_index=ignore_index,\n            ),\n            \"Multiclass_Jaccard_Index_Class\": ClasswiseWrapper(\n                MulticlassJaccardIndex(num_classes=num_classes, ignore_index=ignore_index, average=None),\n                labels=class_names,\n            ),\n            \"Multiclass_F1_Score\": MulticlassF1Score(\n                num_classes=num_classes,\n                ignore_index=ignore_index,\n                multidim_average=\"global\",\n                average=\"micro\",\n            ),\n        }\n    )\n    self.train_metrics = metrics.clone(prefix=\"train/\")\n    self.val_metrics = metrics.clone(prefix=\"val/\")\n    if self.hparams[\"test_dataloaders_names\"] is not None:\n        self.test_metrics = nn.ModuleList(\n            [metrics.clone(prefix=f\"test/{dl_name}/\") for dl_name in self.hparams[\"test_dataloaders_names\"]]\n        )\n    else:\n        self.test_metrics = nn.ModuleList([metrics.clone(prefix=\"test/\")])\n</code></pre>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the predicted class probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output predicted probabilities.</p> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n    \"\"\"Compute the predicted class probabilities.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n\n    Returns:\n        Output predicted probabilities.\n    \"\"\"\n    x = batch[\"image\"]\n    file_names = batch[\"filename\"] if \"filename\" in batch else None\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n\n    rest = {k: batch[k] for k in other_keys}\n\n    def model_forward(x,  **kwargs):\n        return self(x, **kwargs).output\n\n    if self.tiled_inference_parameters:\n        y_hat: Tensor = tiled_inference(\n            model_forward,\n            x,\n            self.hparams[\"model_args\"][\"num_classes\"],\n            self.tiled_inference_parameters,\n            **rest,\n        )\n    else:\n        y_hat: Tensor = self(x, **rest).output\n\n    y_hat = self.select_classes(y_hat)\n\n    return y_hat, file_names\n</code></pre>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the test loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>def test_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n    \"\"\"Compute the test loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    if dataloader_idx &gt;= len(self.test_loss_handler):\n        msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\"\n        raise ValueError(msg)\n    loss = self.test_loss_handler[dataloader_idx].compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.test_loss_handler[dataloader_idx].log_loss(\n        partial(self.log, add_dataloader_idx=False),  # We don't need the dataloader idx as prefixes are different\n        loss_dict=loss,\n        batch_size=y.shape[0],\n    )\n    y_hat_hard = to_segmentation_prediction(model_output)\n    self.test_metrics[dataloader_idx].update(y_hat_hard, y)\n</code></pre>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.training_step","title":"<code>training_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the train loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>def training_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n    \"\"\"Compute the train loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    loss = self.train_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.train_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n    y_hat_hard = to_segmentation_prediction(model_output)\n    self.train_metrics.update(y_hat_hard, y)\n\n    return loss[\"loss\"]\n</code></pre>"},{"location":"tasks/#terratorch.tasks.segmentation_tasks.SemanticSegmentationTask.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the validation loss and additional metrics. Args:     batch: The output of your DataLoader.     batch_idx: Integer displaying index of this batch.     dataloader_idx: Index of the current dataloader.</p> Source code in <code>terratorch/tasks/segmentation_tasks.py</code> <pre><code>def validation_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n    \"\"\"Compute the validation loss and additional metrics.\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    loss = self.val_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.val_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n    y_hat_hard = to_segmentation_prediction(model_output)\n    self.val_metrics.update(y_hat_hard, y)\n\n    if self._do_plot_samples(batch_idx):\n        try:\n            datamodule = self.trainer.datamodule\n            batch[\"prediction\"] = y_hat_hard\n\n            if isinstance(batch[\"image\"], dict):\n                if hasattr(datamodule, \"rgb_modality\"):\n                    # Generic multimodal dataset\n                    batch[\"image\"] = batch[\"image\"][datamodule.rgb_modality]\n                else:\n                    # Multimodal dataset. Assuming first item to be the modality to visualize.\n                    batch[\"image\"] = batch[\"image\"][list(batch[\"image\"].keys())[0]]\n\n            for key in [\"image\", \"mask\", \"prediction\"]:\n                batch[key] = batch[key].cpu()\n            sample = unbind_samples(batch)[0]\n            fig = datamodule.val_dataset.plot(sample)\n            if fig:\n                summary_writer = self.logger.experiment\n                if hasattr(summary_writer, \"add_figure\"):\n                    summary_writer.add_figure(f\"image/{batch_idx}\", fig, global_step=self.global_step)\n                elif hasattr(summary_writer, \"log_figure\"):\n                    summary_writer.log_figure(\n                        self.logger.run_id, fig, f\"epoch_{self.current_epoch}_{batch_idx}.png\"\n                    )\n        except ValueError:\n            pass\n        finally:\n            plt.close()\n</code></pre>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask","title":"<code>terratorch.tasks.regression_tasks.PixelwiseRegressionTask</code>","text":"<p>               Bases: <code>TerraTorchTask</code></p> <p>Pixelwise Regression Task that accepts models from a range of sources.</p> <p>This class is analog in functionality to PixelwiseRegressionTask defined by torchgeo. However, it has some important differences:     - Accepts the specification of a model factory     - Logs metrics per class     - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)     - Allows the setting of optimizers in the constructor     - Allows to evaluate on multiple test dataloaders</p> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>class PixelwiseRegressionTask(TerraTorchTask):\n    \"\"\"Pixelwise Regression Task that accepts models from a range of sources.\n\n    This class is analog in functionality to PixelwiseRegressionTask defined by torchgeo.\n    However, it has some important differences:\n        - Accepts the specification of a model factory\n        - Logs metrics per class\n        - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)\n        - Allows the setting of optimizers in the constructor\n        - Allows to evaluate on multiple test dataloaders\"\"\"\n\n    def __init__(\n        self,\n        model_args: dict,\n        model_factory: str | None = None,\n        model: torch.nn.Module | None = None,\n        loss: str = \"mse\",\n        aux_heads: list[AuxiliaryHead] | None = None,\n        aux_loss: dict[str, float] | None = None,\n        class_weights: list[float] | None = None,\n        ignore_index: int | None = None,\n        lr: float = 0.001,\n        # the following are optional so CLI doesnt need to pass them\n        optimizer: str | None = None,\n        optimizer_hparams: dict | None = None,\n        scheduler: str | None = None,\n        scheduler_hparams: dict | None = None,\n        #\n        freeze_backbone: bool = False,  # noqa: FBT001, FBT002\n        freeze_decoder: bool = False,  # noqa: FBT001, FBT002\n        freeze_head: bool = False,  # noqa: FBT001, FBT002\n        plot_on_val: bool | int = 10,\n        tiled_inference_parameters: TiledInferenceParameters | None = None,\n        test_dataloaders_names: list[str] | None = None,\n        lr_overrides: dict[str, float] | None = None,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            model_args (Dict): Arguments passed to the model factory.\n            model_factory (str, optional): Name of ModelFactory class to be used to instantiate the model.\n                Is ignored when model is provided.\n            model (torch.nn.Module, optional): Custom model.\n            loss (str, optional): Loss to be used. Currently, supports 'mse', 'rmse', 'mae' or 'huber' loss.\n                Defaults to \"mse\".\n            aux_loss (dict[str, float] | None, optional): Auxiliary loss weights.\n                Should be a dictionary where the key is the name given to the loss\n                and the value is the weight to be applied to that loss.\n                The name of the loss should match the key in the dictionary output by the model's forward\n                method containing that output. Defaults to None.\n            class_weights (list[float] | None, optional): List of class weights to be applied to the loss.\n                Defaults to None.\n            ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None.\n            lr (float, optional): Learning rate to be used. Defaults to 0.001.\n            optimizer (str | None, optional): Name of optimizer class from torch.optim to be used.\n                If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.\n            optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer.\n                Overriden by config / cli specification through LightningCLI.\n            scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler\n                to be used (e.g. ReduceLROnPlateau). Defaults to None.\n                Overriden by config / cli specification through LightningCLI.\n            scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler.\n                Overriden by config / cli specification through LightningCLI.\n            freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False.\n            freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False.\n            freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False.\n            plot_on_val (bool | int, optional): Whether to plot visualizations on validation.\n                If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs.\n            tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters\n                used to determine if inference is done on the whole image or through tiling.\n            test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when\n                multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None,\n                which assumes only one test dataloader is used.\n            lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific\n                parameters. The key should be a substring of the parameter names (it will check the substring is\n                contained in the parameter name)and the value should be the new lr. Defaults to None.\n        \"\"\"\n        self.tiled_inference_parameters = tiled_inference_parameters\n        self.aux_loss = aux_loss\n        self.aux_heads = aux_heads\n\n        if model is not None and model_factory is not None:\n            logger.warning(\"A model_factory and a model was provided. The model_factory is ignored.\")\n        if model is None and model_factory is None:\n            raise ValueError(\"A model_factory or a model (torch.nn.Module) must be provided.\")\n\n        if model_factory and model is None:\n            self.model_factory = MODEL_FACTORY_REGISTRY.build(model_factory)\n\n        super().__init__(task=\"regression\")\n\n        if model:\n            # Custom_model\n            self.model = model\n\n        self.train_loss_handler = LossHandler(self.train_metrics.prefix)\n        self.test_loss_handler: list[LossHandler] = []\n        for metrics in self.test_metrics:\n            self.test_loss_handler.append(LossHandler(metrics.prefix))\n        self.val_loss_handler = LossHandler(self.val_metrics.prefix)\n        self.monitor = f\"{self.val_metrics.prefix}loss\"\n        self.plot_on_val = int(plot_on_val)\n\n    def configure_losses(self) -&gt; None:\n        \"\"\"Initialize the loss criterion.\n\n        Raises:\n            ValueError: If *loss* is invalid.\n        \"\"\"\n        loss: str = self.hparams[\"loss\"].lower()\n        if loss == \"mse\":\n            self.criterion: nn.Module = IgnoreIndexLossWrapper(\n                nn.MSELoss(reduction=\"none\"), self.hparams[\"ignore_index\"]\n            )\n        elif loss == \"mae\":\n            self.criterion = IgnoreIndexLossWrapper(nn.L1Loss(reduction=\"none\"), self.hparams[\"ignore_index\"])\n        elif loss == \"rmse\":\n            # IMPORTANT! Root is done only after ignore index! Otherwise the mean taken is incorrect\n            self.criterion = RootLossWrapper(\n                IgnoreIndexLossWrapper(nn.MSELoss(reduction=\"none\"), self.hparams[\"ignore_index\"]), reduction=None\n            )\n        elif loss == \"huber\":\n            self.criterion = IgnoreIndexLossWrapper(nn.HuberLoss(reduction=\"none\"), self.hparams[\"ignore_index\"])\n        else:\n            exception_message = f\"Loss type '{loss}' is not valid. Currently, supports 'mse', 'rmse' or 'mae' loss.\"\n            raise ValueError(exception_message)\n\n    def configure_metrics(self) -&gt; None:\n        \"\"\"Initialize the performance metrics.\"\"\"\n\n        def instantiate_metrics():\n            return {\n                \"RMSE\": MeanSquaredError(squared=False),\n                \"MSE\": MeanSquaredError(squared=True),\n                \"MAE\": MeanAbsoluteError(),\n            }\n\n        def wrap_metrics_with_ignore_index(metrics):\n            return {\n                name: IgnoreIndexMetricWrapper(metric, ignore_index=self.hparams[\"ignore_index\"])\n                for name, metric in metrics.items()\n            }\n\n        self.train_metrics = MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=\"train/\")\n        self.val_metrics = MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=\"val/\")\n        if self.hparams[\"test_dataloaders_names\"] is not None:\n            self.test_metrics = nn.ModuleList(\n                [\n                    MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=f\"test/{dl_name}/\")\n                    for dl_name in self.hparams[\"test_dataloaders_names\"]\n                ]\n            )\n        else:\n            self.test_metrics = nn.ModuleList(\n                [MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=\"test/\")]\n            )\n\n    def training_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n        \"\"\"Compute the train loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        loss = self.train_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.train_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=x.shape[0])\n        y_hat = model_output.output\n        self.train_metrics.update(y_hat, y)\n\n        return loss[\"loss\"]\n\n    def validation_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        \"\"\"Compute the validation loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        loss = self.val_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.val_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n        y_hat = model_output.output\n        self.val_metrics.update(y_hat, y)\n\n        if self._do_plot_samples(batch_idx):\n            try:\n                datamodule = self.trainer.datamodule\n                batch[\"prediction\"] = y_hat\n                if isinstance(batch[\"image\"], dict):\n                    # Multimodal input\n                    batch[\"image\"] = batch[\"image\"][self.trainer.datamodule.rgb_modality]\n                for key in [\"image\", \"mask\", \"prediction\"]:\n                    batch[key] = batch[key].cpu()\n                sample = unbind_samples(batch)[0]\n                fig = datamodule.val_dataset.plot(sample)\n                if fig:\n                    summary_writer = self.logger.experiment\n                    if hasattr(summary_writer, \"add_figure\"):\n                        summary_writer.add_figure(f\"image/{batch_idx}\", fig, global_step=self.global_step)\n                    elif hasattr(summary_writer, \"log_figure\"):\n                        summary_writer.log_figure(\n                            self.logger.run_id, fig, f\"epoch_{self.current_epoch}_{batch_idx}.png\"\n                        )\n            except ValueError:\n                pass\n            finally:\n                plt.close()\n\n    def test_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        \"\"\"Compute the test loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"mask\"]\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        if dataloader_idx &gt;= len(self.test_loss_handler):\n            msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\"\n            raise ValueError(msg)\n        loss = self.test_loss_handler[dataloader_idx].compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.test_loss_handler[dataloader_idx].log_loss(\n            partial(self.log, add_dataloader_idx=False),  # We don't need the dataloader idx as prefixes are different\n            loss_dict=loss,\n            batch_size=x.shape[0],\n        )\n        y_hat = model_output.output\n        self.test_metrics[dataloader_idx].update(y_hat, y)\n\n    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n        \"\"\"Compute the predicted class probabilities.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n\n        Returns:\n            Output predicted probabilities.\n        \"\"\"\n        x = batch[\"image\"]\n        file_names = batch[\"filename\"] if \"filename\" in batch else None\n        other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n\n        def model_forward(x, **kwargs):\n            return self(x).output\n\n        if self.tiled_inference_parameters:\n            # TODO: tiled inference does not work with additional input data (**rest)\n            y_hat: Tensor = tiled_inference(model_forward, x, 1, self.tiled_inference_parameters, **rest)\n        else:\n            y_hat: Tensor = self(x, **rest).output\n        return y_hat, file_names\n</code></pre>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.__init__","title":"<code>__init__(model_args, model_factory=None, model=None, loss='mse', aux_heads=None, aux_loss=None, class_weights=None, ignore_index=None, lr=0.001, optimizer=None, optimizer_hparams=None, scheduler=None, scheduler_hparams=None, freeze_backbone=False, freeze_decoder=False, freeze_head=False, plot_on_val=10, tiled_inference_parameters=None, test_dataloaders_names=None, lr_overrides=None)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict</code> <p>Arguments passed to the model factory.</p> required <code>model_factory</code> <code>str</code> <p>Name of ModelFactory class to be used to instantiate the model. Is ignored when model is provided.</p> <code>None</code> <code>model</code> <code>Module</code> <p>Custom model.</p> <code>None</code> <code>loss</code> <code>str</code> <p>Loss to be used. Currently, supports 'mse', 'rmse', 'mae' or 'huber' loss. Defaults to \"mse\".</p> <code>'mse'</code> <code>aux_loss</code> <code>dict[str, float] | None</code> <p>Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None.</p> <code>None</code> <code>class_weights</code> <code>list[float] | None</code> <p>List of class weights to be applied to the loss. Defaults to None.</p> <code>None</code> <code>ignore_index</code> <code>int | None</code> <p>Label to ignore in the loss computation. Defaults to None.</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate to be used. Defaults to 0.001.</p> <code>0.001</code> <code>optimizer</code> <code>str | None</code> <p>Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>optimizer_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler</code> <code>str</code> <p>Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>freeze_backbone</code> <code>bool</code> <p>Whether to freeze the backbone. Defaults to False.</p> <code>False</code> <code>freeze_decoder</code> <code>bool</code> <p>Whether to freeze the decoder. Defaults to False.</p> <code>False</code> <code>freeze_head</code> <code>bool</code> <p>Whether to freeze the segmentation head. Defaults to False.</p> <code>False</code> <code>plot_on_val</code> <code>bool | int</code> <p>Whether to plot visualizations on validation. If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs.</p> <code>10</code> <code>tiled_inference_parameters</code> <code>TiledInferenceParameters | None</code> <p>Inference parameters used to determine if inference is done on the whole image or through tiling.</p> <code>None</code> <code>test_dataloaders_names</code> <code>list[str] | None</code> <p>Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used.</p> <code>None</code> <code>lr_overrides</code> <code>dict[str, float] | None</code> <p>Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None.</p> <code>None</code> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>def __init__(\n    self,\n    model_args: dict,\n    model_factory: str | None = None,\n    model: torch.nn.Module | None = None,\n    loss: str = \"mse\",\n    aux_heads: list[AuxiliaryHead] | None = None,\n    aux_loss: dict[str, float] | None = None,\n    class_weights: list[float] | None = None,\n    ignore_index: int | None = None,\n    lr: float = 0.001,\n    # the following are optional so CLI doesnt need to pass them\n    optimizer: str | None = None,\n    optimizer_hparams: dict | None = None,\n    scheduler: str | None = None,\n    scheduler_hparams: dict | None = None,\n    #\n    freeze_backbone: bool = False,  # noqa: FBT001, FBT002\n    freeze_decoder: bool = False,  # noqa: FBT001, FBT002\n    freeze_head: bool = False,  # noqa: FBT001, FBT002\n    plot_on_val: bool | int = 10,\n    tiled_inference_parameters: TiledInferenceParameters | None = None,\n    test_dataloaders_names: list[str] | None = None,\n    lr_overrides: dict[str, float] | None = None,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        model_args (Dict): Arguments passed to the model factory.\n        model_factory (str, optional): Name of ModelFactory class to be used to instantiate the model.\n            Is ignored when model is provided.\n        model (torch.nn.Module, optional): Custom model.\n        loss (str, optional): Loss to be used. Currently, supports 'mse', 'rmse', 'mae' or 'huber' loss.\n            Defaults to \"mse\".\n        aux_loss (dict[str, float] | None, optional): Auxiliary loss weights.\n            Should be a dictionary where the key is the name given to the loss\n            and the value is the weight to be applied to that loss.\n            The name of the loss should match the key in the dictionary output by the model's forward\n            method containing that output. Defaults to None.\n        class_weights (list[float] | None, optional): List of class weights to be applied to the loss.\n            Defaults to None.\n        ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None.\n        lr (float, optional): Learning rate to be used. Defaults to 0.001.\n        optimizer (str | None, optional): Name of optimizer class from torch.optim to be used.\n            If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.\n        optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer.\n            Overriden by config / cli specification through LightningCLI.\n        scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler\n            to be used (e.g. ReduceLROnPlateau). Defaults to None.\n            Overriden by config / cli specification through LightningCLI.\n        scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler.\n            Overriden by config / cli specification through LightningCLI.\n        freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False.\n        freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False.\n        freeze_head (bool, optional): Whether to freeze the segmentation head. Defaults to False.\n        plot_on_val (bool | int, optional): Whether to plot visualizations on validation.\n            If true, log every epoch. Defaults to 10. If int, will plot every plot_on_val epochs.\n        tiled_inference_parameters (TiledInferenceParameters | None, optional): Inference parameters\n            used to determine if inference is done on the whole image or through tiling.\n        test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when\n            multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None,\n            which assumes only one test dataloader is used.\n        lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific\n            parameters. The key should be a substring of the parameter names (it will check the substring is\n            contained in the parameter name)and the value should be the new lr. Defaults to None.\n    \"\"\"\n    self.tiled_inference_parameters = tiled_inference_parameters\n    self.aux_loss = aux_loss\n    self.aux_heads = aux_heads\n\n    if model is not None and model_factory is not None:\n        logger.warning(\"A model_factory and a model was provided. The model_factory is ignored.\")\n    if model is None and model_factory is None:\n        raise ValueError(\"A model_factory or a model (torch.nn.Module) must be provided.\")\n\n    if model_factory and model is None:\n        self.model_factory = MODEL_FACTORY_REGISTRY.build(model_factory)\n\n    super().__init__(task=\"regression\")\n\n    if model:\n        # Custom_model\n        self.model = model\n\n    self.train_loss_handler = LossHandler(self.train_metrics.prefix)\n    self.test_loss_handler: list[LossHandler] = []\n    for metrics in self.test_metrics:\n        self.test_loss_handler.append(LossHandler(metrics.prefix))\n    self.val_loss_handler = LossHandler(self.val_metrics.prefix)\n    self.monitor = f\"{self.val_metrics.prefix}loss\"\n    self.plot_on_val = int(plot_on_val)\n</code></pre>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.configure_losses","title":"<code>configure_losses()</code>","text":"<p>Initialize the loss criterion.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If loss is invalid.</p> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>def configure_losses(self) -&gt; None:\n    \"\"\"Initialize the loss criterion.\n\n    Raises:\n        ValueError: If *loss* is invalid.\n    \"\"\"\n    loss: str = self.hparams[\"loss\"].lower()\n    if loss == \"mse\":\n        self.criterion: nn.Module = IgnoreIndexLossWrapper(\n            nn.MSELoss(reduction=\"none\"), self.hparams[\"ignore_index\"]\n        )\n    elif loss == \"mae\":\n        self.criterion = IgnoreIndexLossWrapper(nn.L1Loss(reduction=\"none\"), self.hparams[\"ignore_index\"])\n    elif loss == \"rmse\":\n        # IMPORTANT! Root is done only after ignore index! Otherwise the mean taken is incorrect\n        self.criterion = RootLossWrapper(\n            IgnoreIndexLossWrapper(nn.MSELoss(reduction=\"none\"), self.hparams[\"ignore_index\"]), reduction=None\n        )\n    elif loss == \"huber\":\n        self.criterion = IgnoreIndexLossWrapper(nn.HuberLoss(reduction=\"none\"), self.hparams[\"ignore_index\"])\n    else:\n        exception_message = f\"Loss type '{loss}' is not valid. Currently, supports 'mse', 'rmse' or 'mae' loss.\"\n        raise ValueError(exception_message)\n</code></pre>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.configure_metrics","title":"<code>configure_metrics()</code>","text":"<p>Initialize the performance metrics.</p> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>def configure_metrics(self) -&gt; None:\n    \"\"\"Initialize the performance metrics.\"\"\"\n\n    def instantiate_metrics():\n        return {\n            \"RMSE\": MeanSquaredError(squared=False),\n            \"MSE\": MeanSquaredError(squared=True),\n            \"MAE\": MeanAbsoluteError(),\n        }\n\n    def wrap_metrics_with_ignore_index(metrics):\n        return {\n            name: IgnoreIndexMetricWrapper(metric, ignore_index=self.hparams[\"ignore_index\"])\n            for name, metric in metrics.items()\n        }\n\n    self.train_metrics = MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=\"train/\")\n    self.val_metrics = MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=\"val/\")\n    if self.hparams[\"test_dataloaders_names\"] is not None:\n        self.test_metrics = nn.ModuleList(\n            [\n                MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=f\"test/{dl_name}/\")\n                for dl_name in self.hparams[\"test_dataloaders_names\"]\n            ]\n        )\n    else:\n        self.test_metrics = nn.ModuleList(\n            [MetricCollection(wrap_metrics_with_ignore_index(instantiate_metrics()), prefix=\"test/\")]\n        )\n</code></pre>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the predicted class probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output predicted probabilities.</p> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n    \"\"\"Compute the predicted class probabilities.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n\n    Returns:\n        Output predicted probabilities.\n    \"\"\"\n    x = batch[\"image\"]\n    file_names = batch[\"filename\"] if \"filename\" in batch else None\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n\n    def model_forward(x, **kwargs):\n        return self(x).output\n\n    if self.tiled_inference_parameters:\n        # TODO: tiled inference does not work with additional input data (**rest)\n        y_hat: Tensor = tiled_inference(model_forward, x, 1, self.tiled_inference_parameters, **rest)\n    else:\n        y_hat: Tensor = self(x, **rest).output\n    return y_hat, file_names\n</code></pre>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the test loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>def test_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n    \"\"\"Compute the test loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    if dataloader_idx &gt;= len(self.test_loss_handler):\n        msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\"\n        raise ValueError(msg)\n    loss = self.test_loss_handler[dataloader_idx].compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.test_loss_handler[dataloader_idx].log_loss(\n        partial(self.log, add_dataloader_idx=False),  # We don't need the dataloader idx as prefixes are different\n        loss_dict=loss,\n        batch_size=x.shape[0],\n    )\n    y_hat = model_output.output\n    self.test_metrics[dataloader_idx].update(y_hat, y)\n</code></pre>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.training_step","title":"<code>training_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the train loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>def training_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n    \"\"\"Compute the train loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    loss = self.train_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.train_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=x.shape[0])\n    y_hat = model_output.output\n    self.train_metrics.update(y_hat, y)\n\n    return loss[\"loss\"]\n</code></pre>"},{"location":"tasks/#terratorch.tasks.regression_tasks.PixelwiseRegressionTask.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the validation loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/regression_tasks.py</code> <pre><code>def validation_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n    \"\"\"Compute the validation loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"mask\"]\n    other_keys = batch.keys() - {\"image\", \"mask\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    loss = self.val_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.val_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=y.shape[0])\n    y_hat = model_output.output\n    self.val_metrics.update(y_hat, y)\n\n    if self._do_plot_samples(batch_idx):\n        try:\n            datamodule = self.trainer.datamodule\n            batch[\"prediction\"] = y_hat\n            if isinstance(batch[\"image\"], dict):\n                # Multimodal input\n                batch[\"image\"] = batch[\"image\"][self.trainer.datamodule.rgb_modality]\n            for key in [\"image\", \"mask\", \"prediction\"]:\n                batch[key] = batch[key].cpu()\n            sample = unbind_samples(batch)[0]\n            fig = datamodule.val_dataset.plot(sample)\n            if fig:\n                summary_writer = self.logger.experiment\n                if hasattr(summary_writer, \"add_figure\"):\n                    summary_writer.add_figure(f\"image/{batch_idx}\", fig, global_step=self.global_step)\n                elif hasattr(summary_writer, \"log_figure\"):\n                    summary_writer.log_figure(\n                        self.logger.run_id, fig, f\"epoch_{self.current_epoch}_{batch_idx}.png\"\n                    )\n        except ValueError:\n            pass\n        finally:\n            plt.close()\n</code></pre>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask","title":"<code>terratorch.tasks.classification_tasks.ClassificationTask</code>","text":"<p>               Bases: <code>TerraTorchTask</code></p> <p>Classification Task that accepts models from a range of sources.</p> <p>This class is analog in functionality to the class ClassificationTask defined by torchgeo. However, it has some important differences:     - Accepts the specification of a model factory     - Logs metrics per class     - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)     - Allows the setting of optimizers in the constructor     - It provides mIoU with both Micro and Macro averaging     - Allows to evaluate on multiple test dataloaders</p> <p>.. note::        * 'Micro' averaging suits overall performance evaluation but may not reflect          minority class accuracy.        * 'Macro' averaging gives equal weight to each class, useful          for balanced performance assessment across imbalanced classes.</p> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>class ClassificationTask(TerraTorchTask):\n    \"\"\"Classification Task that accepts models from a range of sources.\n\n    This class is analog in functionality to the class ClassificationTask defined by torchgeo.\n    However, it has some important differences:\n        - Accepts the specification of a model factory\n        - Logs metrics per class\n        - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)\n        - Allows the setting of optimizers in the constructor\n        - It provides mIoU with both Micro and Macro averaging\n        - Allows to evaluate on multiple test dataloaders\n\n    .. note::\n           * 'Micro' averaging suits overall performance evaluation but may not reflect\n             minority class accuracy.\n           * 'Macro' averaging gives equal weight to each class, useful\n             for balanced performance assessment across imbalanced classes.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: dict,\n        model_factory: str | None = None,\n        model: torch.nn.Module | None = None,\n        loss: str = \"ce\",\n        aux_heads: list[AuxiliaryHead] | None = None,\n        aux_loss: dict[str, float] | None = None,\n        class_weights: list[float] | None = None,\n        ignore_index: int | None = None,\n        lr: float = 0.001,\n        # the following are optional so CLI doesnt need to pass them\n        optimizer: str | None = None,\n        optimizer_hparams: dict | None = None,\n        scheduler: str | None = None,\n        scheduler_hparams: dict | None = None,\n        #\n        #\n        freeze_backbone: bool = False,  # noqa: FBT001, FBT002\n        freeze_decoder: bool = False,  # noqa: FBT002, FBT001\n        freeze_head: bool = False,  # noqa: FBT002, FBT001\n        class_names: list[str] | None = None,\n        test_dataloaders_names: list[str] | None = None,\n        lr_overrides: dict[str, float] | None = None,\n    ) -&gt; None:\n        \"\"\"Constructor\n\n        Args:\n            Defaults to None.\n            model_args (Dict): Arguments passed to the model factory.\n            model_factory (str, optional): ModelFactory class to be used to instantiate the model.\n                Is ignored when model is provided.\n            model (torch.nn.Module, optional): Custom model.\n            loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss.\n                Defaults to \"ce\".\n            aux_loss (dict[str, float] | None, optional): Auxiliary loss weights.\n                Should be a dictionary where the key is the name given to the loss\n                and the value is the weight to be applied to that loss.\n                The name of the loss should match the key in the dictionary output by the model's forward\n                method containing that output. Defaults to None.\n            class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss.\n            class_weights (list[float] | None, optional): List of class weights to be applied to the loss.\n                Defaults to None.\n            ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None.\n            lr (float, optional): Learning rate to be used. Defaults to 0.001.\n            optimizer (str | None, optional): Name of optimizer class from torch.optim to be used.\n                If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.\n            optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer.\n                Overriden by config / cli specification through LightningCLI.\n            scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler\n                to be used (e.g. ReduceLROnPlateau). Defaults to None.\n                Overriden by config / cli specification through LightningCLI.\n            scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler.\n                Overriden by config / cli specification through LightningCLI.\n            freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False.\n            freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False.\n            freeze_head (bool, optional): Whether to freeze the segmentation_head. Defaults to False.\n            class_names (list[str] | None, optional): List of class names passed to metrics for better naming.\n                Defaults to numeric ordering.\n            test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when\n                multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None,\n                which assumes only one test dataloader is used.\n            lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific\n                parameters. The key should be a substring of the parameter names (it will check the substring is\n                contained in the parameter name)and the value should be the new lr. Defaults to None.\n        \"\"\"\n        self.aux_loss = aux_loss\n        self.aux_heads = aux_heads\n\n        if model is not None and model_factory is not None:\n            logger.warning(\"A model_factory and a model was provided. The model_factory is ignored.\")\n        if model is None and model_factory is None:\n            raise ValueError(\"A model_factory or a model (torch.nn.Module) must be provided.\")\n\n        if model_factory and model is None:\n            self.model_factory = MODEL_FACTORY_REGISTRY.build(model_factory)\n\n        super().__init__(task=\"classification\")\n\n        if model:\n            # Custom model\n            self.model = model\n\n        self.train_loss_handler = LossHandler(self.train_metrics.prefix)\n        self.test_loss_handler: list[LossHandler] = []\n        for metrics in self.test_metrics:\n            self.test_loss_handler.append(LossHandler(metrics.prefix))\n        self.val_loss_handler = LossHandler(self.val_metrics.prefix)\n        self.monitor = f\"{self.val_metrics.prefix}loss\"\n\n    def configure_losses(self) -&gt; None:\n        \"\"\"Initialize the loss criterion.\n\n        Raises:\n            ValueError: If *loss* is invalid.\n        \"\"\"\n        loss: str = self.hparams[\"loss\"]\n        ignore_index = self.hparams[\"ignore_index\"]\n\n        class_weights = (\n            torch.Tensor(self.hparams[\"class_weights\"]) if self.hparams[\"class_weights\"] is not None else None\n        )\n        if loss == \"ce\":\n            ignore_value = -100 if ignore_index is None else ignore_index\n            self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_value, weight=class_weights)\n        elif loss == \"bce\":\n            self.criterion = nn.BCEWithLogitsLoss()\n        elif loss == \"jaccard\":\n            self.criterion = JaccardLoss(mode=\"multiclass\")\n        elif loss == \"focal\":\n            self.criterion = FocalLoss(mode=\"multiclass\", normalized=True)\n        else:\n            msg = f\"Loss type '{loss}' is not valid.\"\n            raise ValueError(msg)\n\n    def configure_metrics(self) -&gt; None:\n        \"\"\"Initialize the performance metrics.\"\"\"\n        num_classes: int = self.hparams[\"model_args\"][\"num_classes\"]\n        ignore_index: int = self.hparams[\"ignore_index\"]\n        class_names = self.hparams[\"class_names\"]\n        metrics = MetricCollection(\n            {\n                \"Overall_Accuracy\": MulticlassAccuracy(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                    average=\"micro\",\n                ),\n                \"Average_Accuracy\": MulticlassAccuracy(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                    average=\"macro\",\n                ),\n                \"Multiclass_Accuracy_Class\": ClasswiseWrapper(\n                    MulticlassAccuracy(\n                        num_classes=num_classes,\n                        ignore_index=ignore_index,\n                        average=None,\n                    ),\n                    labels=class_names,\n                ),\n                \"Multiclass_Jaccard_Index\": MulticlassJaccardIndex(num_classes=num_classes, ignore_index=ignore_index),\n                \"Multiclass_Jaccard_Index_Class\": ClasswiseWrapper(\n                    MulticlassJaccardIndex(num_classes=num_classes, ignore_index=ignore_index, average=None),\n                    labels=class_names,\n                ),\n                # why FBetaScore\n                \"Multiclass_F1_Score\": MulticlassFBetaScore(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                    beta=1.0,\n                    average=\"micro\",\n                ),\n            }\n        )\n        self.train_metrics = metrics.clone(prefix=\"train/\")\n        self.val_metrics = metrics.clone(prefix=\"val/\")\n        if self.hparams[\"test_dataloaders_names\"] is not None:\n            self.test_metrics = nn.ModuleList(\n                [metrics.clone(prefix=f\"test/{dl_name}/\") for dl_name in self.hparams[\"test_dataloaders_names\"]]\n            )\n        else:\n            self.test_metrics = nn.ModuleList([metrics.clone(prefix=\"test/\")])\n\n    def training_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n        \"\"\"Compute the train loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"label\"]\n        other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        loss = self.train_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.train_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=x.shape[0])\n        y_hat_hard = to_class_prediction(model_output)\n        self.train_metrics.update(y_hat_hard, y)\n\n        return loss[\"loss\"]\n\n    def validation_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        \"\"\"Compute the validation loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"label\"]\n        other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        loss = self.val_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.val_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=x.shape[0])\n        y_hat_hard = to_class_prediction(model_output)\n        self.val_metrics.update(y_hat_hard, y)\n\n    def test_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n        \"\"\"Compute the test loss and additional metrics.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n        \"\"\"\n        x = batch[\"image\"]\n        y = batch[\"label\"]\n        other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n        if dataloader_idx &gt;= len(self.test_loss_handler):\n            msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\"\n            raise ValueError(msg)\n        loss = self.test_loss_handler[dataloader_idx].compute_loss(model_output, y, self.criterion, self.aux_loss)\n        self.test_loss_handler[dataloader_idx].log_loss(\n            partial(self.log, add_dataloader_idx=False),  # We don't need the dataloader idx as prefixes are different\n            loss_dict=loss,\n            batch_size=x.shape[0],\n        )\n        y_hat_hard = to_class_prediction(model_output)\n        self.test_metrics[dataloader_idx].update(y_hat_hard, y)\n\n    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n        \"\"\"Compute the predicted class probabilities.\n\n        Args:\n            batch: The output of your DataLoader.\n            batch_idx: Integer displaying index of this batch.\n            dataloader_idx: Index of the current dataloader.\n\n        Returns:\n            Output predicted probabilities.\n        \"\"\"\n        x = batch[\"image\"]\n        file_names = batch[\"filename\"] if \"filename\" in batch else None\n        other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n        rest = {k: batch[k] for k in other_keys}\n        model_output: ModelOutput = self(x, **rest)\n\n        y_hat = self(x).output\n        y_hat = y_hat.argmax(dim=1)\n        return y_hat, file_names\n</code></pre>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.__init__","title":"<code>__init__(model_args, model_factory=None, model=None, loss='ce', aux_heads=None, aux_loss=None, class_weights=None, ignore_index=None, lr=0.001, optimizer=None, optimizer_hparams=None, scheduler=None, scheduler_hparams=None, freeze_backbone=False, freeze_decoder=False, freeze_head=False, class_names=None, test_dataloaders_names=None, lr_overrides=None)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Dict</code> <p>Arguments passed to the model factory.</p> required <code>model_factory</code> <code>str</code> <p>ModelFactory class to be used to instantiate the model. Is ignored when model is provided.</p> <code>None</code> <code>model</code> <code>Module</code> <p>Custom model.</p> <code>None</code> <code>loss</code> <code>str</code> <p>Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss. Defaults to \"ce\".</p> <code>'ce'</code> <code>aux_loss</code> <code>dict[str, float] | None</code> <p>Auxiliary loss weights. Should be a dictionary where the key is the name given to the loss and the value is the weight to be applied to that loss. The name of the loss should match the key in the dictionary output by the model's forward method containing that output. Defaults to None.</p> <code>None</code> <code>class_weights</code> <code>Union[list[float], None]</code> <p>List of class weights to be applied to the loss.</p> <code>None</code> <code>class_weights</code> <code>list[float] | None</code> <p>List of class weights to be applied to the loss. Defaults to None.</p> <code>None</code> <code>ignore_index</code> <code>int | None</code> <p>Label to ignore in the loss computation. Defaults to None.</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate to be used. Defaults to 0.001.</p> <code>0.001</code> <code>optimizer</code> <code>str | None</code> <p>Name of optimizer class from torch.optim to be used. If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>optimizer_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the optimizer. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler</code> <code>str</code> <p>Name of Torch scheduler class from torch.optim.lr_scheduler to be used (e.g. ReduceLROnPlateau). Defaults to None. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>scheduler_hparams</code> <code>dict | None</code> <p>Parameters to be passed for instantiation of the scheduler. Overriden by config / cli specification through LightningCLI.</p> <code>None</code> <code>freeze_backbone</code> <code>bool</code> <p>Whether to freeze the backbone. Defaults to False.</p> <code>False</code> <code>freeze_decoder</code> <code>bool</code> <p>Whether to freeze the decoder. Defaults to False.</p> <code>False</code> <code>freeze_head</code> <code>bool</code> <p>Whether to freeze the segmentation_head. Defaults to False.</p> <code>False</code> <code>class_names</code> <code>list[str] | None</code> <p>List of class names passed to metrics for better naming. Defaults to numeric ordering.</p> <code>None</code> <code>test_dataloaders_names</code> <code>list[str] | None</code> <p>Names used to differentiate metrics when multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None, which assumes only one test dataloader is used.</p> <code>None</code> <code>lr_overrides</code> <code>dict[str, float] | None</code> <p>Dictionary to override the default lr in specific parameters. The key should be a substring of the parameter names (it will check the substring is contained in the parameter name)and the value should be the new lr. Defaults to None.</p> <code>None</code> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>def __init__(\n    self,\n    model_args: dict,\n    model_factory: str | None = None,\n    model: torch.nn.Module | None = None,\n    loss: str = \"ce\",\n    aux_heads: list[AuxiliaryHead] | None = None,\n    aux_loss: dict[str, float] | None = None,\n    class_weights: list[float] | None = None,\n    ignore_index: int | None = None,\n    lr: float = 0.001,\n    # the following are optional so CLI doesnt need to pass them\n    optimizer: str | None = None,\n    optimizer_hparams: dict | None = None,\n    scheduler: str | None = None,\n    scheduler_hparams: dict | None = None,\n    #\n    #\n    freeze_backbone: bool = False,  # noqa: FBT001, FBT002\n    freeze_decoder: bool = False,  # noqa: FBT002, FBT001\n    freeze_head: bool = False,  # noqa: FBT002, FBT001\n    class_names: list[str] | None = None,\n    test_dataloaders_names: list[str] | None = None,\n    lr_overrides: dict[str, float] | None = None,\n) -&gt; None:\n    \"\"\"Constructor\n\n    Args:\n        Defaults to None.\n        model_args (Dict): Arguments passed to the model factory.\n        model_factory (str, optional): ModelFactory class to be used to instantiate the model.\n            Is ignored when model is provided.\n        model (torch.nn.Module, optional): Custom model.\n        loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss.\n            Defaults to \"ce\".\n        aux_loss (dict[str, float] | None, optional): Auxiliary loss weights.\n            Should be a dictionary where the key is the name given to the loss\n            and the value is the weight to be applied to that loss.\n            The name of the loss should match the key in the dictionary output by the model's forward\n            method containing that output. Defaults to None.\n        class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss.\n        class_weights (list[float] | None, optional): List of class weights to be applied to the loss.\n            Defaults to None.\n        ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None.\n        lr (float, optional): Learning rate to be used. Defaults to 0.001.\n        optimizer (str | None, optional): Name of optimizer class from torch.optim to be used.\n            If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.\n        optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer.\n            Overriden by config / cli specification through LightningCLI.\n        scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler\n            to be used (e.g. ReduceLROnPlateau). Defaults to None.\n            Overriden by config / cli specification through LightningCLI.\n        scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler.\n            Overriden by config / cli specification through LightningCLI.\n        freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False.\n        freeze_decoder (bool, optional): Whether to freeze the decoder. Defaults to False.\n        freeze_head (bool, optional): Whether to freeze the segmentation_head. Defaults to False.\n        class_names (list[str] | None, optional): List of class names passed to metrics for better naming.\n            Defaults to numeric ordering.\n        test_dataloaders_names (list[str] | None, optional): Names used to differentiate metrics when\n            multiple dataloaders are returned by test_dataloader in the datamodule. Defaults to None,\n            which assumes only one test dataloader is used.\n        lr_overrides (dict[str, float] | None, optional): Dictionary to override the default lr in specific\n            parameters. The key should be a substring of the parameter names (it will check the substring is\n            contained in the parameter name)and the value should be the new lr. Defaults to None.\n    \"\"\"\n    self.aux_loss = aux_loss\n    self.aux_heads = aux_heads\n\n    if model is not None and model_factory is not None:\n        logger.warning(\"A model_factory and a model was provided. The model_factory is ignored.\")\n    if model is None and model_factory is None:\n        raise ValueError(\"A model_factory or a model (torch.nn.Module) must be provided.\")\n\n    if model_factory and model is None:\n        self.model_factory = MODEL_FACTORY_REGISTRY.build(model_factory)\n\n    super().__init__(task=\"classification\")\n\n    if model:\n        # Custom model\n        self.model = model\n\n    self.train_loss_handler = LossHandler(self.train_metrics.prefix)\n    self.test_loss_handler: list[LossHandler] = []\n    for metrics in self.test_metrics:\n        self.test_loss_handler.append(LossHandler(metrics.prefix))\n    self.val_loss_handler = LossHandler(self.val_metrics.prefix)\n    self.monitor = f\"{self.val_metrics.prefix}loss\"\n</code></pre>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.configure_losses","title":"<code>configure_losses()</code>","text":"<p>Initialize the loss criterion.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If loss is invalid.</p> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>def configure_losses(self) -&gt; None:\n    \"\"\"Initialize the loss criterion.\n\n    Raises:\n        ValueError: If *loss* is invalid.\n    \"\"\"\n    loss: str = self.hparams[\"loss\"]\n    ignore_index = self.hparams[\"ignore_index\"]\n\n    class_weights = (\n        torch.Tensor(self.hparams[\"class_weights\"]) if self.hparams[\"class_weights\"] is not None else None\n    )\n    if loss == \"ce\":\n        ignore_value = -100 if ignore_index is None else ignore_index\n        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_value, weight=class_weights)\n    elif loss == \"bce\":\n        self.criterion = nn.BCEWithLogitsLoss()\n    elif loss == \"jaccard\":\n        self.criterion = JaccardLoss(mode=\"multiclass\")\n    elif loss == \"focal\":\n        self.criterion = FocalLoss(mode=\"multiclass\", normalized=True)\n    else:\n        msg = f\"Loss type '{loss}' is not valid.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.configure_metrics","title":"<code>configure_metrics()</code>","text":"<p>Initialize the performance metrics.</p> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>def configure_metrics(self) -&gt; None:\n    \"\"\"Initialize the performance metrics.\"\"\"\n    num_classes: int = self.hparams[\"model_args\"][\"num_classes\"]\n    ignore_index: int = self.hparams[\"ignore_index\"]\n    class_names = self.hparams[\"class_names\"]\n    metrics = MetricCollection(\n        {\n            \"Overall_Accuracy\": MulticlassAccuracy(\n                num_classes=num_classes,\n                ignore_index=ignore_index,\n                average=\"micro\",\n            ),\n            \"Average_Accuracy\": MulticlassAccuracy(\n                num_classes=num_classes,\n                ignore_index=ignore_index,\n                average=\"macro\",\n            ),\n            \"Multiclass_Accuracy_Class\": ClasswiseWrapper(\n                MulticlassAccuracy(\n                    num_classes=num_classes,\n                    ignore_index=ignore_index,\n                    average=None,\n                ),\n                labels=class_names,\n            ),\n            \"Multiclass_Jaccard_Index\": MulticlassJaccardIndex(num_classes=num_classes, ignore_index=ignore_index),\n            \"Multiclass_Jaccard_Index_Class\": ClasswiseWrapper(\n                MulticlassJaccardIndex(num_classes=num_classes, ignore_index=ignore_index, average=None),\n                labels=class_names,\n            ),\n            # why FBetaScore\n            \"Multiclass_F1_Score\": MulticlassFBetaScore(\n                num_classes=num_classes,\n                ignore_index=ignore_index,\n                beta=1.0,\n                average=\"micro\",\n            ),\n        }\n    )\n    self.train_metrics = metrics.clone(prefix=\"train/\")\n    self.val_metrics = metrics.clone(prefix=\"val/\")\n    if self.hparams[\"test_dataloaders_names\"] is not None:\n        self.test_metrics = nn.ModuleList(\n            [metrics.clone(prefix=f\"test/{dl_name}/\") for dl_name in self.hparams[\"test_dataloaders_names\"]]\n        )\n    else:\n        self.test_metrics = nn.ModuleList([metrics.clone(prefix=\"test/\")])\n</code></pre>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the predicted class probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output predicted probabilities.</p> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n    \"\"\"Compute the predicted class probabilities.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n\n    Returns:\n        Output predicted probabilities.\n    \"\"\"\n    x = batch[\"image\"]\n    file_names = batch[\"filename\"] if \"filename\" in batch else None\n    other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n\n    y_hat = self(x).output\n    y_hat = y_hat.argmax(dim=1)\n    return y_hat, file_names\n</code></pre>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.test_step","title":"<code>test_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the test loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>def test_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n    \"\"\"Compute the test loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"label\"]\n    other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    if dataloader_idx &gt;= len(self.test_loss_handler):\n        msg = \"You are returning more than one test dataloader but not defining enough test_dataloaders_names.\"\n        raise ValueError(msg)\n    loss = self.test_loss_handler[dataloader_idx].compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.test_loss_handler[dataloader_idx].log_loss(\n        partial(self.log, add_dataloader_idx=False),  # We don't need the dataloader idx as prefixes are different\n        loss_dict=loss,\n        batch_size=x.shape[0],\n    )\n    y_hat_hard = to_class_prediction(model_output)\n    self.test_metrics[dataloader_idx].update(y_hat_hard, y)\n</code></pre>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.training_step","title":"<code>training_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the train loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>def training_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Tensor:\n    \"\"\"Compute the train loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"label\"]\n    other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    loss = self.train_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.train_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=x.shape[0])\n    y_hat_hard = to_class_prediction(model_output)\n    self.train_metrics.update(y_hat_hard, y)\n\n    return loss[\"loss\"]\n</code></pre>"},{"location":"tasks/#terratorch.tasks.classification_tasks.ClassificationTask.validation_step","title":"<code>validation_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Compute the validation loss and additional metrics.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Any</code> <p>The output of your DataLoader.</p> required <code>batch_idx</code> <code>int</code> <p>Integer displaying index of this batch.</p> required <code>dataloader_idx</code> <code>int</code> <p>Index of the current dataloader.</p> <code>0</code> Source code in <code>terratorch/tasks/classification_tasks.py</code> <pre><code>def validation_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; None:\n    \"\"\"Compute the validation loss and additional metrics.\n\n    Args:\n        batch: The output of your DataLoader.\n        batch_idx: Integer displaying index of this batch.\n        dataloader_idx: Index of the current dataloader.\n    \"\"\"\n    x = batch[\"image\"]\n    y = batch[\"label\"]\n    other_keys = batch.keys() - {\"image\", \"label\", \"filename\"}\n    rest = {k: batch[k] for k in other_keys}\n    model_output: ModelOutput = self(x, **rest)\n    loss = self.val_loss_handler.compute_loss(model_output, y, self.criterion, self.aux_loss)\n    self.val_loss_handler.log_loss(self.log, loss_dict=loss, batch_size=x.shape[0])\n    y_hat_hard = to_class_prediction(model_output)\n    self.val_metrics.update(y_hat_hard, y)\n</code></pre>"},{"location":"transforms/","title":"Transforms","text":""},{"location":"transforms/#terratorch.datasets.transforms","title":"<code>terratorch.datasets.transforms</code>","text":""},{"location":"transforms/#terratorch.datasets.transforms.FlattenSamplesIntoChannels","title":"<code>FlattenSamplesIntoChannels</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>FlattenSamplesIntoChannels is an image transformation that merges the sample (and optionally temporal) dimensions into the channel dimension.</p> <p>This transform rearranges an input tensor by flattening the sample dimension, and if specified, also the temporal dimension, thereby concatenating these dimensions into a single channel dimension.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class FlattenSamplesIntoChannels(ImageOnlyTransform):\n    \"\"\"\n    FlattenSamplesIntoChannels is an image transformation that merges the sample (and optionally temporal) dimensions into the channel dimension.\n\n    This transform rearranges an input tensor by flattening the sample dimension, and if specified, also the temporal dimension,\n    thereby concatenating these dimensions into a single channel dimension.\n    \"\"\"\n    def __init__(self, time_dim: bool = True):\n        \"\"\"\n        Initialize the FlattenSamplesIntoChannels transform.\n\n        Args:\n            time_dim (bool): If True, the temporal dimension is included in the flattening process. Default is True.\n        \"\"\"\n        super().__init__(True, 1)\n        self.time_dim = time_dim\n\n    def apply(self, img, **params):\n        if self.time_dim:\n            rearranged = rearrange(img,\n                                   \"samples time height width channels -&gt; height width (samples time channels)\")\n        else:\n            rearranged = rearrange(img, \"samples height width channels -&gt; height width (samples channels)\")\n        return rearranged\n\n    def get_transform_init_args_names(self):\n        return ()\n</code></pre>"},{"location":"transforms/#terratorch.datasets.transforms.FlattenSamplesIntoChannels.__init__","title":"<code>__init__(time_dim=True)</code>","text":"<p>Initialize the FlattenSamplesIntoChannels transform.</p> <p>Parameters:</p> Name Type Description Default <code>time_dim</code> <code>bool</code> <p>If True, the temporal dimension is included in the flattening process. Default is True.</p> <code>True</code> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>def __init__(self, time_dim: bool = True):\n    \"\"\"\n    Initialize the FlattenSamplesIntoChannels transform.\n\n    Args:\n        time_dim (bool): If True, the temporal dimension is included in the flattening process. Default is True.\n    \"\"\"\n    super().__init__(True, 1)\n    self.time_dim = time_dim\n</code></pre>"},{"location":"transforms/#terratorch.datasets.transforms.FlattenTemporalIntoChannels","title":"<code>FlattenTemporalIntoChannels</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>FlattenTemporalIntoChannels is an image transformation that flattens the temporal dimension into the channel dimension.</p> <p>This transform rearranges an input tensor with a temporal dimension into one where the time and channel dimensions are merged. It expects the input to have a fixed number of dimensions defined by N_DIMS_FOR_TEMPORAL.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class FlattenTemporalIntoChannels(ImageOnlyTransform):\n    \"\"\"\n    FlattenTemporalIntoChannels is an image transformation that flattens the temporal dimension into the channel dimension.\n\n    This transform rearranges an input tensor with a temporal dimension into one where the time and channel dimensions\n    are merged. It expects the input to have a fixed number of dimensions defined by N_DIMS_FOR_TEMPORAL.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the FlattenTemporalIntoChannels transform.\n        \"\"\"\n        super().__init__(True, 1)\n\n    def apply(self, img, **params):\n        if len(img.shape) != N_DIMS_FOR_TEMPORAL:\n            msg = f\"Expected input temporal sequence to have {N_DIMS_FOR_TEMPORAL} dimensions, but got {len(img.shape)}\"\n            raise Exception(msg)\n        rearranged = rearrange(img, \"time height width channels -&gt; height width (time channels)\")\n        return rearranged\n\n    def get_transform_init_args_names(self):\n        return ()\n</code></pre>"},{"location":"transforms/#terratorch.datasets.transforms.FlattenTemporalIntoChannels.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the FlattenTemporalIntoChannels transform.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the FlattenTemporalIntoChannels transform.\n    \"\"\"\n    super().__init__(True, 1)\n</code></pre>"},{"location":"transforms/#terratorch.datasets.transforms.MultimodalTransforms","title":"<code>MultimodalTransforms</code>","text":"<p>MultimodalTransforms applies albumentations transforms to multiple image modalities.</p> <p>This class supports both shared transformations across modalities and separate transformations for each modality. It also handles non-image modalities by applying a specified non-image transform.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class MultimodalTransforms:\n    \"\"\"\n    MultimodalTransforms applies albumentations transforms to multiple image modalities.\n\n    This class supports both shared transformations across modalities and separate transformations for each modality.\n    It also handles non-image modalities by applying a specified non-image transform.\n    \"\"\"\n    def __init__(\n            self,\n            transforms: dict | A.Compose,\n            shared : bool = True,\n            non_image_modalities: list[str] | None = None,\n            non_image_transform: object | None = None,\n    ):\n        \"\"\"\n        Initialize the MultimodalTransforms.\n\n        Args:\n            transforms (dict or A.Compose): The transformation(s) to apply to the data.\n            shared (bool): If True, the same transform is applied to all modalities; if False, separate transforms are used.\n            non_image_modalities (list[str] | None): List of keys corresponding to non-image modalities.\n            non_image_transform (object | None): A transform to apply to non-image modalities. If None, a default transform is used.\n        \"\"\"\n        self.transforms = transforms\n        self.shared = shared\n        self.non_image_modalities = non_image_modalities\n        self.non_image_transform = non_image_transform or default_non_image_transform\n\n    def __call__(self, data: dict):\n        if self.shared:\n            # albumentations requires a key 'image' and treats all other keys as additional targets\n            image_modality = list(set(data.keys()) - set(self.non_image_modalities))[0]\n            data['image'] = data.pop(image_modality)\n            data = self.transforms(**data)\n            data[image_modality] = data.pop('image')\n\n            # Process sequence data which is ignored by albumentations as 'global_label'\n            for modality in self.non_image_modalities:\n                data[modality] = self.non_image_transform(data[modality])\n        else:\n            # Applies transformations for each modality separate\n            for key, value in data.items():\n                data[key] = self.transforms[key](image=value)['image']  # Only works with image modalities\n\n        return data\n</code></pre>"},{"location":"transforms/#terratorch.datasets.transforms.MultimodalTransforms.__init__","title":"<code>__init__(transforms, shared=True, non_image_modalities=None, non_image_transform=None)</code>","text":"<p>Initialize the MultimodalTransforms.</p> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>dict or Compose</code> <p>The transformation(s) to apply to the data.</p> required <code>shared</code> <code>bool</code> <p>If True, the same transform is applied to all modalities; if False, separate transforms are used.</p> <code>True</code> <code>non_image_modalities</code> <code>list[str] | None</code> <p>List of keys corresponding to non-image modalities.</p> <code>None</code> <code>non_image_transform</code> <code>object | None</code> <p>A transform to apply to non-image modalities. If None, a default transform is used.</p> <code>None</code> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>def __init__(\n        self,\n        transforms: dict | A.Compose,\n        shared : bool = True,\n        non_image_modalities: list[str] | None = None,\n        non_image_transform: object | None = None,\n):\n    \"\"\"\n    Initialize the MultimodalTransforms.\n\n    Args:\n        transforms (dict or A.Compose): The transformation(s) to apply to the data.\n        shared (bool): If True, the same transform is applied to all modalities; if False, separate transforms are used.\n        non_image_modalities (list[str] | None): List of keys corresponding to non-image modalities.\n        non_image_transform (object | None): A transform to apply to non-image modalities. If None, a default transform is used.\n    \"\"\"\n    self.transforms = transforms\n    self.shared = shared\n    self.non_image_modalities = non_image_modalities\n    self.non_image_transform = non_image_transform or default_non_image_transform\n</code></pre>"},{"location":"transforms/#terratorch.datasets.transforms.Padding","title":"<code>Padding</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>Padding to adjust (slight) discrepancies between input images</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class Padding(ImageOnlyTransform):\n    \"\"\"Padding to adjust (slight) discrepancies between input images\"\"\"\n\n    def __init__(self, input_shape: list=None):\n        super().__init__(True, 1)\n        self.input_shape = input_shape\n\n    def apply(self, img, **params):\n\n        shape = img.shape[-2:]\n        pad_values_ = [j - i for i,j in zip(shape, self.input_shape)]\n\n        if all([i%2==0 for i in pad_values_]):\n            pad_values = sum([[int(j/2), int(j/2)] for j in  pad_values_], [])\n        else:\n            pad_values = sum([[0, j] for j in  pad_values_], [])\n\n        return F.pad(img, pad_values)\n\n    def get_transform_init_args_names(self):\n        return ()\n</code></pre>"},{"location":"transforms/#terratorch.datasets.transforms.Rearrange","title":"<code>Rearrange</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>Rearrange is a generic image transformation that reshapes an input tensor using a custom einops pattern.</p> <p>This transform allows flexible reordering of tensor dimensions based on the provided pattern and arguments.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class Rearrange(ImageOnlyTransform):\n    \"\"\"\n    Rearrange is a generic image transformation that reshapes an input tensor using a custom einops pattern.\n\n    This transform allows flexible reordering of tensor dimensions based on the provided pattern and arguments.\n    \"\"\"\n\n    def __init__(\n        self, rearrange: str, rearrange_args: dict[str, int] | None = None, always_apply: bool = True, p: float = 1\n    ):\n        \"\"\"\n        Initialize the Rearrange transform.\n\n        Args:\n            rearrange (str): The einops rearrangement pattern to apply.\n            rearrange_args (dict[str, int] | None): Additional arguments for the rearrangement pattern.\n            always_apply (bool): Whether to always apply this transform. Default is True.\n            p (float): The probability of applying the transform. Default is 1.\n        \"\"\"\n        super().__init__(always_apply, p)\n        self.rearrange = rearrange\n        self.vars = rearrange_args if rearrange_args else {}\n\n    def apply(self, img, **params):\n        return rearrange(img, self.rearrange, **self.vars)\n\n    def get_transform_init_args_names(self):\n        return \"rearrange\"\n</code></pre>"},{"location":"transforms/#terratorch.datasets.transforms.Rearrange.__init__","title":"<code>__init__(rearrange, rearrange_args=None, always_apply=True, p=1)</code>","text":"<p>Initialize the Rearrange transform.</p> <p>Parameters:</p> Name Type Description Default <code>rearrange</code> <code>str</code> <p>The einops rearrangement pattern to apply.</p> required <code>rearrange_args</code> <code>dict[str, int] | None</code> <p>Additional arguments for the rearrangement pattern.</p> <code>None</code> <code>always_apply</code> <code>bool</code> <p>Whether to always apply this transform. Default is True.</p> <code>True</code> <code>p</code> <code>float</code> <p>The probability of applying the transform. Default is 1.</p> <code>1</code> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>def __init__(\n    self, rearrange: str, rearrange_args: dict[str, int] | None = None, always_apply: bool = True, p: float = 1\n):\n    \"\"\"\n    Initialize the Rearrange transform.\n\n    Args:\n        rearrange (str): The einops rearrangement pattern to apply.\n        rearrange_args (dict[str, int] | None): Additional arguments for the rearrangement pattern.\n        always_apply (bool): Whether to always apply this transform. Default is True.\n        p (float): The probability of applying the transform. Default is 1.\n    \"\"\"\n    super().__init__(always_apply, p)\n    self.rearrange = rearrange\n    self.vars = rearrange_args if rearrange_args else {}\n</code></pre>"},{"location":"transforms/#terratorch.datasets.transforms.SelectBands","title":"<code>SelectBands</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>SelectBands is an image transformation that selects a subset of bands (channels) from an input image.</p> <p>This transform uses specified band indices to filter and output only the desired channels from the image tensor.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class SelectBands(ImageOnlyTransform):\n    \"\"\"\n    SelectBands is an image transformation that selects a subset of bands (channels) from an input image.\n\n    This transform uses specified band indices to filter and output only the desired channels from the image tensor.\n    \"\"\"\n\n    def __init__(self, band_indices: list[int]):\n        \"\"\"\n        Initialize the SelectBands transform.\n\n        Args:\n            band_indices (list[int]): A list of indices specifying which bands to select.\n        \"\"\"\n        super().__init__(True, 1)\n        self.band_indices = band_indices\n\n    def apply(self, img, **params):\n        return img[..., self.band_indices]\n\n    def get_transform_init_args_names(self):\n        return \"band_indices\"\n</code></pre>"},{"location":"transforms/#terratorch.datasets.transforms.SelectBands.__init__","title":"<code>__init__(band_indices)</code>","text":"<p>Initialize the SelectBands transform.</p> <p>Parameters:</p> Name Type Description Default <code>band_indices</code> <code>list[int]</code> <p>A list of indices specifying which bands to select.</p> required Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>def __init__(self, band_indices: list[int]):\n    \"\"\"\n    Initialize the SelectBands transform.\n\n    Args:\n        band_indices (list[int]): A list of indices specifying which bands to select.\n    \"\"\"\n    super().__init__(True, 1)\n    self.band_indices = band_indices\n</code></pre>"},{"location":"transforms/#terratorch.datasets.transforms.UnflattenSamplesFromChannels","title":"<code>UnflattenSamplesFromChannels</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>UnflattenSamplesFromChannels is an image transformation that restores the sample (and optionally temporal) dimensions from the channel dimension.</p> <p>This transform is designed to reverse the flattening performed by FlattenSamplesIntoChannels and is typically applied after converting images to a channels-first format.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class UnflattenSamplesFromChannels(ImageOnlyTransform):\n    \"\"\"\n    UnflattenSamplesFromChannels is an image transformation that restores the sample (and optionally temporal) dimensions from the channel dimension.\n\n    This transform is designed to reverse the flattening performed by FlattenSamplesIntoChannels and is typically applied\n    after converting images to a channels-first format.\n    \"\"\"\n    def __init__(\n            self,\n            time_dim: bool = True,\n            n_samples: int | None = None,\n            n_timesteps: int | None = None,\n            n_channels: int | None = None\n    ):\n        \"\"\"\n        Initialize the UnflattenSamplesFromChannels transform.\n\n        Args:\n            time_dim (bool): If True, the temporal dimension is considered during unflattening.\n            n_samples (int | None): The number of samples.\n            n_timesteps (int | None): The number of time steps.\n            n_channels (int | None): The number of channels per time step.\n\n        Raises:\n            Exception: If time_dim is True and fewer than two of n_channels, n_timesteps, and n_samples are provided.\n            Exception: If time_dim is False and neither n_channels nor n_samples is provided.\n        \"\"\"\n        super().__init__(True, 1)\n\n        self.time_dim = time_dim\n        if self.time_dim:\n            if bool(n_channels) + bool(n_timesteps) + bool(n_samples) &lt; 2:\n                msg = \"Two of n_channels, n_timesteps, and n_channels must be provided\"\n                raise Exception(msg)\n            if n_timesteps and n_channels:\n                self.additional_info = {\"channels\": n_channels, \"time\": n_timesteps}\n            elif n_timesteps and n_samples:\n                self.additional_info = {\"time\": n_timesteps, \"samples\": n_samples}\n            else:\n                self.additional_info = {\"channels\": n_channels, \"samples\": n_samples}\n        else:\n            if n_channels is None and n_samples is None:\n                msg = \"One of n_channels or n_samples must be provided\"\n                raise Exception(msg)\n            self.additional_info = {\"channels\": n_channels} if n_channels else {\"samples\": n_samples}\n\n    def apply(self, img, **params):\n        if self.time_dim:\n            rearranged = rearrange(\n                img, \"(samples time channels) height width -&gt; samples channels time height width\",\n                **self.additional_info\n            )\n        else:\n            rearranged = rearrange(\n                img, \"(samples channels) height width -&gt; samples channels height width\", **self.additional_info\n            )\n        return rearranged\n\n    def get_transform_init_args_names(self):\n        return (\"n_timesteps\", \"n_channels\")\n</code></pre>"},{"location":"transforms/#terratorch.datasets.transforms.UnflattenSamplesFromChannels.__init__","title":"<code>__init__(time_dim=True, n_samples=None, n_timesteps=None, n_channels=None)</code>","text":"<p>Initialize the UnflattenSamplesFromChannels transform.</p> <p>Parameters:</p> Name Type Description Default <code>time_dim</code> <code>bool</code> <p>If True, the temporal dimension is considered during unflattening.</p> <code>True</code> <code>n_samples</code> <code>int | None</code> <p>The number of samples.</p> <code>None</code> <code>n_timesteps</code> <code>int | None</code> <p>The number of time steps.</p> <code>None</code> <code>n_channels</code> <code>int | None</code> <p>The number of channels per time step.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If time_dim is True and fewer than two of n_channels, n_timesteps, and n_samples are provided.</p> <code>Exception</code> <p>If time_dim is False and neither n_channels nor n_samples is provided.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>def __init__(\n        self,\n        time_dim: bool = True,\n        n_samples: int | None = None,\n        n_timesteps: int | None = None,\n        n_channels: int | None = None\n):\n    \"\"\"\n    Initialize the UnflattenSamplesFromChannels transform.\n\n    Args:\n        time_dim (bool): If True, the temporal dimension is considered during unflattening.\n        n_samples (int | None): The number of samples.\n        n_timesteps (int | None): The number of time steps.\n        n_channels (int | None): The number of channels per time step.\n\n    Raises:\n        Exception: If time_dim is True and fewer than two of n_channels, n_timesteps, and n_samples are provided.\n        Exception: If time_dim is False and neither n_channels nor n_samples is provided.\n    \"\"\"\n    super().__init__(True, 1)\n\n    self.time_dim = time_dim\n    if self.time_dim:\n        if bool(n_channels) + bool(n_timesteps) + bool(n_samples) &lt; 2:\n            msg = \"Two of n_channels, n_timesteps, and n_channels must be provided\"\n            raise Exception(msg)\n        if n_timesteps and n_channels:\n            self.additional_info = {\"channels\": n_channels, \"time\": n_timesteps}\n        elif n_timesteps and n_samples:\n            self.additional_info = {\"time\": n_timesteps, \"samples\": n_samples}\n        else:\n            self.additional_info = {\"channels\": n_channels, \"samples\": n_samples}\n    else:\n        if n_channels is None and n_samples is None:\n            msg = \"One of n_channels or n_samples must be provided\"\n            raise Exception(msg)\n        self.additional_info = {\"channels\": n_channels} if n_channels else {\"samples\": n_samples}\n</code></pre>"},{"location":"transforms/#terratorch.datasets.transforms.UnflattenTemporalFromChannels","title":"<code>UnflattenTemporalFromChannels</code>","text":"<p>               Bases: <code>ImageOnlyTransform</code></p> <p>UnflattenTemporalFromChannels is an image transformation that restores the temporal dimension from the channel dimension.</p> <p>This transform is typically applied after converting images to a channels-first format (e.g., after ToTensorV2) and rearranges the flattened temporal information back into separate time and channel dimensions.</p> Source code in <code>terratorch/datasets/transforms.py</code> <pre><code>class UnflattenTemporalFromChannels(ImageOnlyTransform):\n    \"\"\"\n    UnflattenTemporalFromChannels is an image transformation that restores the temporal dimension from the channel dimension.\n\n    This transform is typically applied after converting images to a channels-first format (e.g., after ToTensorV2)\n    and rearranges the flattened temporal information back into separate time and channel dimensions.\n    \"\"\"\n\n    def __init__(self, n_timesteps: int | None = None, n_channels: int | None = None):\n        super().__init__(True, 1)\n        \"\"\"\n        Initialize the UnflattenTemporalFromChannels transform.\n\n        Args:\n            n_timesteps (int | None): The number of time steps. Must be provided if n_channels is not provided.\n            n_channels (int | None): The number of channels per time step. Must be provided if n_timesteps is not provided.\n\n        Raises:\n            Exception: If neither n_timesteps nor n_channels is provided.\n        \"\"\"\n        if n_timesteps is None and n_channels is None:\n            msg = \"One of n_timesteps or n_channels must be provided\"\n            raise Exception(msg)\n        self.additional_info = {\"channels\": n_channels} if n_channels else {\"time\": n_timesteps}\n\n    def apply(self, img, **params):\n        if len(img.shape) != N_DIMS_FLATTENED_TEMPORAL:\n            msg = f\"Expected input temporal sequence to have {N_DIMS_FLATTENED_TEMPORAL} dimensions\\\n                , but got {len(img.shape)}\"\n            raise Exception(msg)\n\n        rearranged = rearrange(\n            img, \"(time channels) height width -&gt; channels time height width\", **self.additional_info\n        )\n        return rearranged\n\n    def get_transform_init_args_names(self):\n        return (\"n_timesteps\", \"n_channels\")\n</code></pre>"}]}