{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import CenterCrop, ToTensor\n",
    "from terratorch.datasets.m_chesapeake_landcover import MChesapeakeLandcoverNonGeo\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# ------------------- Config -------------------\n",
    "root = \"./chesapeake\"          # local cache\n",
    "splits = [\"de-test\"]           # or [\"pa-test\"], [\"md-train\"], etc.\n",
    "patch_size = 224\n",
    "num_samples = 12\n",
    "out_dir = Path(\"./chesapeake_samples\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------- Class mapping -------------------\n",
    "class_map = {\n",
    "    0: \"Water\",\n",
    "    1: \"Tree canopy\",\n",
    "    2: \"Low vegetation\",\n",
    "    3: \"Barren\",\n",
    "    4: \"Impervious\",\n",
    "    5: \"Wetlands\",\n",
    "    6: \"Crops\",\n",
    "}\n",
    "\n",
    "# fixed colors for visualization\n",
    "class_colors = {\n",
    "    0: (0, 0, 255),       # blue for water\n",
    "    1: (34, 139, 34),     # green for tree canopy\n",
    "    2: (124, 252, 0),     # light green for low vegetation\n",
    "    3: (210, 180, 140),   # tan for barren\n",
    "    4: (128, 128, 128),   # gray for impervious\n",
    "    5: (0, 255, 255),     # cyan for wetlands\n",
    "    6: (255, 255, 0),     # yellow for crops\n",
    "}\n",
    "\n",
    "# ------------------- Transforms -------------------\n",
    "def chesapeake_transform(sample):\n",
    "    image, mask = sample[\"image\"], sample[\"mask\"]\n",
    "\n",
    "    crop = CenterCrop(patch_size)\n",
    "\n",
    "    # convert both to tensor first\n",
    "    image = ToTensor()(image)   # (C,H,W), float in [0,1] if 8-bit\n",
    "    mask = torch.as_tensor(np.array(mask), dtype=torch.long)  # (H,W)\n",
    "\n",
    "    # apply same crop\n",
    "    image = crop(image)\n",
    "    mask = crop(mask.unsqueeze(0)).squeeze(0)  # keep it 2D (H,W)\n",
    "\n",
    "    return {\"image\": image, \"mask\": mask}\n",
    "\n",
    "# ------------------- Dataset -------------------\n",
    "dataset = MChesapeakeLandcoverNonGeo(\n",
    "    data_root='.',\n",
    ")\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# ------------------- Save samples -------------------\n",
    "for i, sample in enumerate(loader):\n",
    "    image = sample[\"image\"].squeeze(0)  # (C,H,W), usually 4 channels\n",
    "    mask = sample[\"mask\"].squeeze(0)    # (H,W)\n",
    "\n",
    "    # ---- Save 4-channel image for inference ----\n",
    "    img4 = (image.numpy().transpose(1, 2, 0) * 255).astype(np.uint8)  # (H,W,4)\n",
    "    img4_pil = Image.fromarray(img4)\n",
    "    img4_resized = img4_pil.resize((224, 224), resample=Image.BILINEAR)  # or Image.LANCZOS for even higher quality\n",
    "    img4_resized.save(out_dir / f\"sample_{i}_x.png\")\n",
    "\n",
    "    # ---- Save RGB visualization (natural look) ----\n",
    "    # Remove batch dimension for each key in the sample dict\n",
    "    single_sample = {k: v.squeeze(0) for k, v in sample.items()}  # Now single image/mask\n",
    "\n",
    "    fig = dataset.plot(single_sample)\n",
    "    fig.savefig(out_dir / f\"sample_{i}_z.png\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Save mask with colors ----\n",
    "    mask_np = mask.numpy()\n",
    "    color_mask = np.zeros((mask_np.shape[0], mask_np.shape[1], 3), dtype=np.uint8)\n",
    "    for cls_id, color in class_colors.items():\n",
    "        color_mask[mask_np == cls_id] = color\n",
    "    Image.fromarray(color_mask).save(out_dir / f\"sample_{i}_y.png\")\n",
    "\n",
    "    # Print info\n",
    "    unique_classes = torch.unique(mask).tolist()\n",
    "    class_names = [class_map[c] for c in unique_classes]\n",
    "    print(f\"Saved sample {i}: classes {unique_classes} → {class_names}\")\n",
    "\n",
    "    if i + 1 >= num_samples:\n",
    "        break\n",
    "\n",
    "print(f\"Samples stored in: {out_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model_path = \"/home/romeokienzler/gitco/tmtinyonnxwebdemo/src/model_chesapeake.onnx\"\n",
    "session = ort.InferenceSession(model_path)\n",
    "\n",
    "# Print model input and output info\n",
    "print(\"Model Input Info:\")\n",
    "for input_meta in session.get_inputs():\n",
    "    print(f\"  Name: {input_meta.name}, Shape: {input_meta.shape}, Type: {input_meta.type}\")\n",
    "\n",
    "print(\"\\nModel Output Info:\")\n",
    "for output_meta in session.get_outputs():\n",
    "    print(f\"  Name: {output_meta.name}, Shape: {output_meta.shape}, Type: {output_meta.type}\")\n",
    "\n",
    "# Define the expected input shape based on your web app code (224x224 with 4 channels)\n",
    "input_shape = [1, 4, 224, 224]\n",
    "\n",
    "# Create a random input tensor with the correct shape and data type\n",
    "# Using a normal distribution to get a variety of floating-point values\n",
    "# Scale them to be in the [0, 1] range like the image data\n",
    "random_input = np.random.randn(*input_shape).astype(np.float32)\n",
    "random_input = (random_input - random_input.min()) / (random_input.max() - random_input.min())\n",
    "\n",
    "# Run inference with the random input\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "outputs = session.run([output_name], {input_name: random_input})\n",
    "\n",
    "# Get the logits (raw output) from the model\n",
    "logits = outputs[0]\n",
    "\n",
    "# Print the logits to see if they're still constant\n",
    "print(\"\\nLogits from Random Input (first 100 values):\")\n",
    "print(logits.flatten()[:100])\n",
    "\n",
    "# Perform an argmax to find the predicted class for each pixel\n",
    "# This is a good way to see if there's any pattern in the output\n",
    "num_classes = logits.shape[1]\n",
    "num_pixels = logits.shape[2] * logits.shape[3]\n",
    "logits_reshaped = np.transpose(logits, (0, 2, 3, 1)).reshape(-1, num_classes)\n",
    "predicted_classes = np.argmax(logits_reshaped, axis=1)\n",
    "\n",
    "print(\"\\nPredicted Classes from Random Input (first 100 pixels):\")\n",
    "print(predicted_classes[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d61794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Decoder UperNetDecoder does not have an `includes_head` attribute. Falling back to the value of the registry.\n",
      "/home/romeokienzler/gitco/terratorch.add-aed-elephant-2/terratorch/models/decoders/upernet_decoder.py:37: UserWarning: DeprecationWarning: scale_modules is deprecated and will be removed in future versions. Use LearnedInterpolateToPyramidal neck instead.\n",
      "  warnings.warn(\n",
      "WARNING:root:Decoder UperNetDecoder does not have an `includes_head` attribute. Falling back to the value of the registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([1, 4, 224, 224])\n",
      "<class 'torch.Tensor'> torch.Size([1, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from terratorch.tasks import SemanticSegmentationTask\n",
    "from terratorch.datasets.m_chesapeake_landcover import MChesapeakeLandcoverNonGeo\n",
    "from terratorch.datamodules.m_chesapeake_landcover import MChesapeakeLandcoverNonGeoDataModule\n",
    "from jsonargparse import ArgumentParser\n",
    "\n",
    "def load_dm_from_config(config_path: str) -> MChesapeakeLandcoverNonGeoDataModule:\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_class_arguments(MChesapeakeLandcoverNonGeoDataModule, \"data\")\n",
    "    cfg = parser.parse_path(config_path)\n",
    "    namespace = parser.instantiate_classes(cfg)\n",
    "    return namespace.data\n",
    "\n",
    "def load_task_from_config(config_path: str) -> SemanticSegmentationTask:\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_class_arguments(SemanticSegmentationTask, \"model\")\n",
    "    cfg = parser.parse_path(config_path)\n",
    "    namespace = parser.instantiate_classes(cfg)\n",
    "    return namespace.model\n",
    "\n",
    "dm = load_dm_from_config('/home/romeokienzler/Downloads/chesapeake_best_iterate_data.yaml')\n",
    "task = load_task_from_config('/home/romeokienzler/Downloads/chesapeake_best_iterate.yaml')\n",
    "# restore weights\n",
    "task = SemanticSegmentationTask.load_from_checkpoint('/home/romeokienzler/Downloads/best-epoch=89-val=0.0000.ckpt', **task.hparams)\n",
    "\n",
    "model = task.model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "dm.setup(\"fit\")\n",
    "dm.setup(\"test\")    \n",
    "sample = dm.test_dataset[0]\n",
    "sample = dm.aug(sample)\n",
    "x = sample[\"image\"]   # tensor (C,H,W)\n",
    "y = sample[\"mask\"]    # tensor (H,W)\n",
    "\n",
    "print(type(x), x.shape)\n",
    "print(type(y), y.shape)\n",
    "\n",
    "#x = x.unsqueeze(0).unsqueeze(2)   # [1, C, 1, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8612ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "\n",
    "predicted_classes = torch.argmax(output.output, dim=1)\n",
    "print(\"Predicted classes for random input:\")\n",
    "print(predicted_classes.flatten()[:10000])\n",
    "print((predicted_classes.flatten() == 3).all())\n",
    "hist = torch.histc(predicted_classes.flatten().float(), bins=7, min=-10, max=10)\n",
    "print(hist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe5e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Save samples -------------------\n",
    "for i, sample in enumerate(loader):\n",
    "    image = sample[\"image\"].squeeze(0)  # (C,H,W), usually 4 channels\n",
    "    mask = sample[\"mask\"].squeeze(0)    # (H,W)\n",
    "\n",
    "    # ---- Save 4-channel image for inference ----\n",
    "    img4 = (image.numpy().transpose(1, 2, 0) * 255).astype(np.uint8)  # (H,W,4)\n",
    "    img4_pil = Image.fromarray(img4)\n",
    "    img4_resized = img4_pil.resize((224, 224), resample=Image.BILINEAR)  # or Image.LANCZOS for even higher quality\n",
    "    img4_resized.save(out_dir / f\"sample_{i}_x.png\")\n",
    "\n",
    "    # ---- Save RGB visualization (natural look) ----\n",
    "    # Remove batch dimension for each key in the sample dict\n",
    "    single_sample = {k: v.squeeze(0) for k, v in sample.items()}  # Now single image/mask\n",
    "\n",
    "    fig = dataset.plot(single_sample)\n",
    "    fig.savefig(out_dir / f\"sample_{i}_z.png\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Save mask with colors ----\n",
    "    mask_np = mask.numpy()\n",
    "    color_mask = np.zeros((mask_np.shape[0], mask_np.shape[1], 3), dtype=np.uint8)\n",
    "    for cls_id, color in class_colors.items():\n",
    "        color_mask[mask_np == cls_id] = color\n",
    "    Image.fromarray(color_mask).save(out_dir / f\"sample_{i}_y.png\")\n",
    "\n",
    "    # Print info\n",
    "    unique_classes = torch.unique(mask).tolist()\n",
    "    class_names = [class_map[c] for c in unique_classes]\n",
    "    print(f\"Saved sample {i}: classes {unique_classes} → {class_names}\")\n",
    "\n",
    "    if i + 1 >= num_samples:\n",
    "        break\n",
    "\n",
    "print(f\"Samples stored in: {out_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277dbd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_lightning.utilities.cloud_io import atomic_save\n",
    "from terratorch.tasks import SemanticSegmentationTask\n",
    "\n",
    "# Paths\n",
    "pt_path = \"Prithvi_EO_V2_tiny_TL.pt\"\n",
    "ckpt_path = \"Prithvi_EO_V2_tiny_TL.ckpt\"\n",
    "\n",
    "# 1. Load raw state dict from .pt\n",
    "raw = torch.load(pt_path, map_location=\"cpu\")\n",
    "if \"state_dict\" in raw:\n",
    "    state_dict = raw[\"state_dict\"]\n",
    "else:\n",
    "    state_dict = raw\n",
    "\n",
    "# 3. Load backbone weights into task (ignore missing heads if needed)\n",
    "task.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# 4. Build checkpoint dict\n",
    "checkpoint = {\n",
    "    \"state_dict\": task.state_dict(),\n",
    "    \"pytorch-lightning_version\": \"2.3.0\",  # or your installed version\n",
    "    \"hyper_parameters\": task.hparams,\n",
    "    \"epoch\": 0,\n",
    "    \"global_step\": 0,\n",
    "}\n",
    "\n",
    "# 5. Save as Lightning checkpoint\n",
    "atomic_save(checkpoint, ckpt_path)\n",
    "print(f\"Converted {pt_path} → {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d954a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0, Channel RED histogram:\n",
      "  -1.727 - -1.344: 490\n",
      "  -1.344 - -0.962: 2735\n",
      "  -0.962 - -0.579: 11475\n",
      "  -0.579 - -0.197: 71303\n",
      "  -0.197 - 0.185: 20685\n",
      "  0.185 - 0.568: 33698\n",
      "  0.568 - 0.950: 33098\n",
      "  0.950 - 1.333: 15570\n",
      "  1.333 - 1.715: 7840\n",
      "  1.715 - 2.097: 2095\n",
      "  2.097 - 2.480: 1128\n",
      "  2.480 - 2.862: 587\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 0: classes [2, 3, 5, 6] → ['Low vegetation', 'Barren', 'Wetlands', 'Crops']\n",
      "Sample 1, Channel RED histogram:\n",
      "  -1.865 - -1.584: 1096\n",
      "  -1.584 - -1.303: 3932\n",
      "  -1.303 - -1.022: 7490\n",
      "  -1.022 - -0.741: 15484\n",
      "  -0.741 - -0.460: 29006\n",
      "  -0.460 - -0.179: 39698\n",
      "  -0.179 - 0.102: 31131\n",
      "  0.102 - 0.383: 18155\n",
      "  0.383 - 0.664: 11149\n",
      "  0.664 - 0.945: 16451\n",
      "  0.945 - 1.226: 26824\n",
      "  1.226 - 1.507: 288\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 1: classes [2] → ['Low vegetation']\n",
      "Sample 2, Channel RED histogram:\n",
      "  -1.535 - -1.178: 3995\n",
      "  -1.178 - -0.820: 12973\n",
      "  -0.820 - -0.463: 23094\n",
      "  -0.463 - -0.106: 27349\n",
      "  -0.106 - 0.251: 59099\n",
      "  0.251 - 0.609: 57900\n",
      "  0.609 - 0.966: 8639\n",
      "  0.966 - 1.323: 3791\n",
      "  1.323 - 1.680: 1979\n",
      "  1.680 - 2.038: 826\n",
      "  2.038 - 2.395: 774\n",
      "  2.395 - 2.752: 285\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 2: classes [2, 3, 5, 6] → ['Low vegetation', 'Barren', 'Wetlands', 'Crops']\n",
      "Sample 3, Channel RED histogram:\n",
      "  -1.754 - -1.363: 3239\n",
      "  -1.363 - -0.972: 12390\n",
      "  -0.972 - -0.581: 24186\n",
      "  -0.581 - -0.190: 33204\n",
      "  -0.190 - 0.201: 39831\n",
      "  0.201 - 0.592: 31307\n",
      "  0.592 - 0.983: 25341\n",
      "  0.983 - 1.374: 25345\n",
      "  1.374 - 1.765: 4237\n",
      "  1.765 - 2.156: 1208\n",
      "  2.156 - 2.547: 296\n",
      "  2.547 - 2.938: 120\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 3: classes [2, 3, 5, 6] → ['Low vegetation', 'Barren', 'Wetlands', 'Crops']\n",
      "Sample 4, Channel RED histogram:\n",
      "  -1.311 - -0.982: 221\n",
      "  -0.982 - -0.653: 1655\n",
      "  -0.653 - -0.323: 2305\n",
      "  -0.323 - 0.006: 4703\n",
      "  0.006 - 0.335: 50778\n",
      "  0.335 - 0.664: 86120\n",
      "  0.664 - 0.993: 43745\n",
      "  0.993 - 1.323: 5831\n",
      "  1.323 - 1.652: 1885\n",
      "  1.652 - 1.981: 1299\n",
      "  1.981 - 2.310: 1500\n",
      "  2.310 - 2.640: 662\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 4: classes [2, 3, 5, 6] → ['Low vegetation', 'Barren', 'Wetlands', 'Crops']\n",
      "Sample 5, Channel RED histogram:\n",
      "  -1.777 - -1.383: 551\n",
      "  -1.383 - -0.988: 2660\n",
      "  -0.988 - -0.594: 5939\n",
      "  -0.594 - -0.199: 10958\n",
      "  -0.199 - 0.195: 14428\n",
      "  0.195 - 0.589: 32147\n",
      "  0.589 - 0.984: 57968\n",
      "  0.984 - 1.378: 33744\n",
      "  1.378 - 1.772: 28234\n",
      "  1.772 - 2.167: 12872\n",
      "  2.167 - 2.561: 1138\n",
      "  2.561 - 2.956: 65\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 5: classes [2, 3, 5, 6] → ['Low vegetation', 'Barren', 'Wetlands', 'Crops']\n",
      "Sample 6, Channel RED histogram:\n",
      "  -1.472 - -1.269: 2397\n",
      "  -1.269 - -1.066: 1793\n",
      "  -1.066 - -0.863: 596\n",
      "  -0.863 - -0.660: 4104\n",
      "  -0.660 - -0.457: 39934\n",
      "  -0.457 - -0.254: 80793\n",
      "  -0.254 - -0.051: 51937\n",
      "  -0.051 - 0.152: 16025\n",
      "  0.152 - 0.355: 2068\n",
      "  0.355 - 0.559: 754\n",
      "  0.559 - 0.762: 279\n",
      "  0.762 - 0.965: 24\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 6: classes [1, 3] → ['Tree canopy', 'Barren']\n",
      "Sample 7, Channel RED histogram:\n",
      "  -1.752 - -1.567: 50119\n",
      "  -1.567 - -1.381: 57\n",
      "  -1.381 - -1.196: 0\n",
      "  -1.196 - -1.011: 0\n",
      "  -1.011 - -0.826: 0\n",
      "  -0.826 - -0.641: 0\n",
      "  -0.641 - -0.456: 2\n",
      "  -0.456 - -0.271: 44897\n",
      "  -0.271 - -0.086: 104120\n",
      "  -0.086 - 0.100: 1480\n",
      "  0.100 - 0.285: 26\n",
      "  0.285 - 0.470: 3\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 7: classes [1] → ['Tree canopy']\n",
      "Sample 8, Channel RED histogram:\n",
      "  -1.832 - -1.511: 8297\n",
      "  -1.511 - -1.190: 19741\n",
      "  -1.190 - -0.869: 29490\n",
      "  -0.869 - -0.548: 37522\n",
      "  -0.548 - -0.227: 32987\n",
      "  -0.227 - 0.094: 18667\n",
      "  0.094 - 0.415: 11972\n",
      "  0.415 - 0.735: 11266\n",
      "  0.735 - 1.056: 16905\n",
      "  1.056 - 1.377: 13739\n",
      "  1.377 - 1.698: 101\n",
      "  1.698 - 2.019: 17\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 8: classes [2] → ['Low vegetation']\n",
      "Sample 9, Channel RED histogram:\n",
      "  -1.579 - -1.201: 911\n",
      "  -1.201 - -0.822: 1481\n",
      "  -0.822 - -0.444: 4783\n",
      "  -0.444 - -0.065: 8645\n",
      "  -0.065 - 0.313: 43595\n",
      "  0.313 - 0.692: 63404\n",
      "  0.692 - 1.070: 49422\n",
      "  1.070 - 1.449: 19181\n",
      "  1.449 - 1.828: 4966\n",
      "  1.828 - 2.206: 1790\n",
      "  2.206 - 2.585: 1866\n",
      "  2.585 - 2.963: 660\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 9: classes [2, 3, 4, 5, 6] → ['Low vegetation', 'Barren', 'Impervious', 'Wetlands', 'Crops']\n",
      "Sample 10, Channel RED histogram:\n",
      "  -1.440 - -1.099: 213\n",
      "  -1.099 - -0.758: 760\n",
      "  -0.758 - -0.417: 2252\n",
      "  -0.417 - -0.076: 14616\n",
      "  -0.076 - 0.265: 25980\n",
      "  0.265 - 0.605: 30630\n",
      "  0.605 - 0.946: 34015\n",
      "  0.946 - 1.287: 35051\n",
      "  1.287 - 1.628: 15889\n",
      "  1.628 - 1.969: 24602\n",
      "  1.969 - 2.310: 13941\n",
      "  2.310 - 2.651: 2755\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 10: classes [2, 3, 4, 5, 6] → ['Low vegetation', 'Barren', 'Impervious', 'Wetlands', 'Crops']\n",
      "Sample 11, Channel RED histogram:\n",
      "  -1.648 - -1.268: 3653\n",
      "  -1.268 - -0.888: 10403\n",
      "  -0.888 - -0.507: 17001\n",
      "  -0.507 - -0.127: 23282\n",
      "  -0.127 - 0.254: 36420\n",
      "  0.254 - 0.634: 35059\n",
      "  0.634 - 1.015: 23357\n",
      "  1.015 - 1.395: 10356\n",
      "  1.395 - 1.776: 12287\n",
      "  1.776 - 2.156: 15953\n",
      "  2.156 - 2.537: 10325\n",
      "  2.537 - 2.917: 2608\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 11: classes [2, 3, 5, 6] → ['Low vegetation', 'Barren', 'Wetlands', 'Crops']\n",
      "Sample 12, Channel RED histogram:\n",
      "  -1.366 - -1.153: 4732\n",
      "  -1.153 - -0.939: 14149\n",
      "  -0.939 - -0.726: 19660\n",
      "  -0.726 - -0.512: 24924\n",
      "  -0.512 - -0.299: 29540\n",
      "  -0.299 - -0.085: 27122\n",
      "  -0.085 - 0.128: 18481\n",
      "  0.128 - 0.342: 13611\n",
      "  0.342 - 0.555: 12996\n",
      "  0.555 - 0.769: 16138\n",
      "  0.769 - 0.982: 16879\n",
      "  0.982 - 1.196: 2472\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 12: classes [2] → ['Low vegetation']\n",
      "Sample 13, Channel RED histogram:\n",
      "  -2.331 - -1.949: 20\n",
      "  -1.949 - -1.568: 2149\n",
      "  -1.568 - -1.186: 3068\n",
      "  -1.186 - -0.805: 5500\n",
      "  -0.805 - -0.423: 11432\n",
      "  -0.423 - -0.042: 23658\n",
      "  -0.042 - 0.340: 24788\n",
      "  0.340 - 0.721: 29179\n",
      "  0.721 - 1.103: 39921\n",
      "  1.103 - 1.484: 43860\n",
      "  1.484 - 1.866: 13840\n",
      "  1.866 - 2.247: 3289\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 13: classes [2, 3] → ['Low vegetation', 'Barren']\n",
      "Sample 14, Channel RED histogram:\n",
      "  -1.890 - -1.669: 642\n",
      "  -1.669 - -1.447: 1941\n",
      "  -1.447 - -1.226: 6403\n",
      "  -1.226 - -1.005: 13407\n",
      "  -1.005 - -0.783: 25806\n",
      "  -0.783 - -0.562: 35771\n",
      "  -0.562 - -0.341: 38854\n",
      "  -0.341 - -0.119: 32134\n",
      "  -0.119 - 0.102: 27034\n",
      "  0.102 - 0.323: 15982\n",
      "  0.323 - 0.545: 2608\n",
      "  0.545 - 0.766: 122\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 14: classes [2, 5] → ['Low vegetation', 'Wetlands']\n",
      "Sample 15, Channel RED histogram:\n",
      "  -1.732 - -1.470: 2751\n",
      "  -1.470 - -1.209: 10164\n",
      "  -1.209 - -0.947: 20541\n",
      "  -0.947 - -0.686: 20852\n",
      "  -0.686 - -0.425: 23764\n",
      "  -0.425 - -0.163: 27422\n",
      "  -0.163 - 0.098: 25070\n",
      "  0.098 - 0.360: 18938\n",
      "  0.360 - 0.621: 14696\n",
      "  0.621 - 0.883: 14162\n",
      "  0.883 - 1.144: 18775\n",
      "  1.144 - 1.406: 3569\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 15: classes [2] → ['Low vegetation']\n",
      "Sample 16, Channel RED histogram:\n",
      "  -1.387 - -1.031: 405\n",
      "  -1.031 - -0.675: 987\n",
      "  -0.675 - -0.319: 3808\n",
      "  -0.319 - 0.037: 6693\n",
      "  0.037 - 0.394: 9462\n",
      "  0.394 - 0.750: 10612\n",
      "  0.750 - 1.106: 49827\n",
      "  1.106 - 1.462: 24341\n",
      "  1.462 - 1.818: 58418\n",
      "  1.818 - 2.175: 31788\n",
      "  2.175 - 2.531: 3495\n",
      "  2.531 - 2.887: 868\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 16: classes [2, 3, 4, 5] → ['Low vegetation', 'Barren', 'Impervious', 'Wetlands']\n",
      "Sample 17, Channel RED histogram:\n",
      "  -1.442 - -1.129: 3502\n",
      "  -1.129 - -0.816: 11329\n",
      "  -0.816 - -0.504: 27346\n",
      "  -0.504 - -0.191: 67834\n",
      "  -0.191 - 0.121: 27980\n",
      "  0.121 - 0.434: 16931\n",
      "  0.434 - 0.747: 27702\n",
      "  0.747 - 1.059: 15677\n",
      "  1.059 - 1.372: 1337\n",
      "  1.372 - 1.684: 708\n",
      "  1.684 - 1.997: 276\n",
      "  1.997 - 2.310: 82\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 17: classes [2, 3, 5, 6] → ['Low vegetation', 'Barren', 'Wetlands', 'Crops']\n",
      "Sample 18, Channel RED histogram:\n",
      "  -1.994 - -1.595: 10254\n",
      "  -1.595 - -1.196: 1135\n",
      "  -1.196 - -0.797: 28552\n",
      "  -0.797 - -0.398: 12675\n",
      "  -0.398 - 0.001: 31793\n",
      "  0.001 - 0.401: 34430\n",
      "  0.401 - 0.800: 44665\n",
      "  0.800 - 1.199: 23100\n",
      "  1.199 - 1.598: 7935\n",
      "  1.598 - 1.997: 4813\n",
      "  1.997 - 2.396: 1243\n",
      "  2.396 - 2.795: 109\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 18: classes [1, 2, 3, 4, 5, 6] → ['Tree canopy', 'Low vegetation', 'Barren', 'Impervious', 'Wetlands', 'Crops']\n",
      "Sample 19, Channel RED histogram:\n",
      "  -2.757 - -2.313: 22400\n",
      "  -2.313 - -1.870: 22182\n",
      "  -1.870 - -1.427: 307\n",
      "  -1.427 - -0.984: 5663\n",
      "  -0.984 - -0.540: 22974\n",
      "  -0.540 - -0.097: 41395\n",
      "  -0.097 - 0.346: 34588\n",
      "  0.346 - 0.789: 18788\n",
      "  0.789 - 1.232: 29791\n",
      "  1.232 - 1.676: 2568\n",
      "  1.676 - 2.119: 31\n",
      "  2.119 - 2.562: 17\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 19: classes [0, 2] → ['Water', 'Low vegetation']\n",
      "Sample 20, Channel RED histogram:\n",
      "  -1.412 - -1.076: 485\n",
      "  -1.076 - -0.740: 2613\n",
      "  -0.740 - -0.405: 1536\n",
      "  -0.405 - -0.069: 5234\n",
      "  -0.069 - 0.267: 45299\n",
      "  0.267 - 0.602: 61048\n",
      "  0.602 - 0.938: 23667\n",
      "  0.938 - 1.274: 28625\n",
      "  1.274 - 1.609: 14855\n",
      "  1.609 - 1.945: 6759\n",
      "  1.945 - 2.281: 5119\n",
      "  2.281 - 2.617: 5464\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 20: classes [2, 3, 5] → ['Low vegetation', 'Barren', 'Wetlands']\n",
      "Sample 21, Channel RED histogram:\n",
      "  -1.813 - -1.423: 820\n",
      "  -1.423 - -1.033: 3788\n",
      "  -1.033 - -0.643: 5431\n",
      "  -0.643 - -0.253: 6620\n",
      "  -0.253 - 0.137: 24315\n",
      "  0.137 - 0.527: 57472\n",
      "  0.527 - 0.916: 56709\n",
      "  0.916 - 1.306: 30885\n",
      "  1.306 - 1.696: 12202\n",
      "  1.696 - 2.086: 1784\n",
      "  2.086 - 2.476: 581\n",
      "  2.476 - 2.866: 97\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 21: classes [2, 3, 5, 6] → ['Low vegetation', 'Barren', 'Wetlands', 'Crops']\n",
      "Sample 22, Channel RED histogram:\n",
      "  -1.619 - -1.309: 1332\n",
      "  -1.309 - -1.000: 2999\n",
      "  -1.000 - -0.690: 3725\n",
      "  -0.690 - -0.380: 3808\n",
      "  -0.380 - -0.070: 4793\n",
      "  -0.070 - 0.240: 36145\n",
      "  0.240 - 0.550: 85865\n",
      "  0.550 - 0.860: 44170\n",
      "  0.860 - 1.170: 12469\n",
      "  1.170 - 1.480: 4723\n",
      "  1.480 - 1.790: 517\n",
      "  1.790 - 2.100: 158\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 22: classes [2, 3, 5] → ['Low vegetation', 'Barren', 'Wetlands']\n",
      "Sample 23, Channel RED histogram:\n",
      "  -1.760 - -1.430: 24032\n",
      "  -1.430 - -1.100: 19480\n",
      "  -1.100 - -0.771: 68584\n",
      "  -0.771 - -0.441: 29599\n",
      "  -0.441 - -0.111: 12462\n",
      "  -0.111 - 0.219: 5694\n",
      "  0.219 - 0.549: 6139\n",
      "  0.549 - 0.879: 7481\n",
      "  0.879 - 1.208: 13252\n",
      "  1.208 - 1.538: 9114\n",
      "  1.538 - 1.868: 3596\n",
      "  1.868 - 2.198: 1271\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 23: classes [1, 2, 3, 4] → ['Tree canopy', 'Low vegetation', 'Barren', 'Impervious']\n",
      "Sample 24, Channel RED histogram:\n",
      "  -2.012 - -1.723: 25309\n",
      "  -1.723 - -1.434: 19625\n",
      "  -1.434 - -1.145: 18453\n",
      "  -1.145 - -0.856: 25006\n",
      "  -0.856 - -0.568: 49477\n",
      "  -0.568 - -0.279: 23550\n",
      "  -0.279 - 0.010: 13577\n",
      "  0.010 - 0.299: 8509\n",
      "  0.299 - 0.588: 5781\n",
      "  0.588 - 0.877: 5731\n",
      "  0.877 - 1.166: 5354\n",
      "  1.166 - 1.455: 332\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 24: classes [1, 2, 3, 5, 6] → ['Tree canopy', 'Low vegetation', 'Barren', 'Wetlands', 'Crops']\n",
      "Sample 25, Channel RED histogram:\n",
      "  -1.939 - -1.529: 2372\n",
      "  -1.529 - -1.120: 10909\n",
      "  -1.120 - -0.710: 26926\n",
      "  -0.710 - -0.301: 25008\n",
      "  -0.301 - 0.109: 29996\n",
      "  0.109 - 0.518: 31231\n",
      "  0.518 - 0.928: 35420\n",
      "  0.928 - 1.337: 26023\n",
      "  1.337 - 1.747: 7913\n",
      "  1.747 - 2.156: 3947\n",
      "  2.156 - 2.566: 858\n",
      "  2.566 - 2.975: 101\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 25: classes [2, 3, 5, 6] → ['Low vegetation', 'Barren', 'Wetlands', 'Crops']\n",
      "Sample 26, Channel RED histogram:\n",
      "  -1.790 - -1.406: 1231\n",
      "  -1.406 - -1.022: 3470\n",
      "  -1.022 - -0.637: 7217\n",
      "  -0.637 - -0.253: 21379\n",
      "  -0.253 - 0.131: 23064\n",
      "  0.131 - 0.515: 18811\n",
      "  0.515 - 0.899: 21367\n",
      "  0.899 - 1.283: 24340\n",
      "  1.283 - 1.667: 30960\n",
      "  1.667 - 2.051: 37745\n",
      "  2.051 - 2.435: 9000\n",
      "  2.435 - 2.819: 2120\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 26: classes [2, 3, 4, 5] → ['Low vegetation', 'Barren', 'Impervious', 'Wetlands']\n",
      "Sample 27, Channel RED histogram:\n",
      "  -1.831 - -1.487: 649\n",
      "  -1.487 - -1.142: 6154\n",
      "  -1.142 - -0.798: 17008\n",
      "  -0.798 - -0.453: 22294\n",
      "  -0.453 - -0.109: 21802\n",
      "  -0.109 - 0.236: 30063\n",
      "  0.236 - 0.580: 19971\n",
      "  0.580 - 0.925: 17142\n",
      "  0.925 - 1.269: 16035\n",
      "  1.269 - 1.614: 18944\n",
      "  1.614 - 1.958: 18251\n",
      "  1.958 - 2.303: 12391\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 27: classes [2, 3, 4, 5, 6] → ['Low vegetation', 'Barren', 'Impervious', 'Wetlands', 'Crops']\n",
      "Sample 28, Channel RED histogram:\n",
      "  -1.423 - -1.154: 3163\n",
      "  -1.154 - -0.886: 45387\n",
      "  -0.886 - -0.617: 1622\n",
      "  -0.617 - -0.348: 4\n",
      "  -0.348 - -0.080: 0\n",
      "  -0.080 - 0.189: 0\n",
      "  0.189 - 0.457: 11\n",
      "  0.457 - 0.726: 1699\n",
      "  0.726 - 0.994: 56562\n",
      "  0.994 - 1.263: 41710\n",
      "  1.263 - 1.531: 45391\n",
      "  1.531 - 1.800: 5155\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 28: classes [1] → ['Tree canopy']\n",
      "Sample 29, Channel RED histogram:\n",
      "  -1.555 - -1.219: 887\n",
      "  -1.219 - -0.883: 6334\n",
      "  -0.883 - -0.547: 11587\n",
      "  -0.547 - -0.212: 15124\n",
      "  -0.212 - 0.124: 26028\n",
      "  0.124 - 0.460: 51383\n",
      "  0.460 - 0.796: 51985\n",
      "  0.796 - 1.132: 20857\n",
      "  1.132 - 1.468: 10474\n",
      "  1.468 - 1.804: 5818\n",
      "  1.804 - 2.140: 185\n",
      "  2.140 - 2.476: 42\n",
      "torch.Size([1, 4, 1, 224, 224])\n",
      "Saved sample 29: classes [2, 3, 5, 6] → ['Low vegetation', 'Barren', 'Wetlands', 'Crops']\n",
      "Samples stored in: /home/romeokienzler/gitco/terratorch.add-aed-elephant-2/examples/notebooks/chesapeake_samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import CenterCrop\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from terratorch.datamodules.m_chesapeake_landcover import MChesapeakeLandcoverNonGeoDataModule\n",
    "from jsonargparse import ArgumentParser\n",
    "\n",
    "# ------------------- Config -------------------\n",
    "config_path = '/home/romeokienzler/Downloads/chesapeake_best_iterate_data.yaml'\n",
    "patch_size = 224\n",
    "num_samples = 30\n",
    "out_dir = Path(\"./chesapeake_samples\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------- Class mapping -------------------\n",
    "class_map = {\n",
    "    0: \"Water\",\n",
    "    1: \"Tree canopy\",\n",
    "    2: \"Low vegetation\",\n",
    "    3: \"Barren\",\n",
    "    4: \"Impervious\",\n",
    "    5: \"Wetlands\",\n",
    "    6: \"Crops\",\n",
    "}\n",
    "\n",
    "class_colors = {\n",
    "    0: (0, 0, 255),\n",
    "    1: (34, 139, 34),\n",
    "    2: (124, 252, 0),\n",
    "    3: (210, 180, 140),\n",
    "    4: (128, 128, 128),\n",
    "    5: (0, 255, 255),\n",
    "    6: (255, 255, 0),\n",
    "}\n",
    "\n",
    "# ------------------- Load DataModule -------------------\n",
    "def load_dm_from_config(config_path: str) -> MChesapeakeLandcoverNonGeoDataModule:\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_class_arguments(MChesapeakeLandcoverNonGeoDataModule, \"data\")\n",
    "    cfg = parser.parse_path(config_path)\n",
    "    namespace = parser.instantiate_classes(cfg)\n",
    "    return namespace.data\n",
    "\n",
    "dm = load_dm_from_config(config_path)\n",
    "dm.setup(\"test\")\n",
    "loader = DataLoader(dm.test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# ------------------- Transform -------------------\n",
    "crop = CenterCrop(patch_size)\n",
    "\n",
    "# ------------------- Save samples -------------------\n",
    "for i, sample in enumerate(loader):\n",
    "    sample = {k: v.squeeze(0) for k, v in sample.items()}\n",
    "\n",
    "    # Apply the datamodule's augmentation (normalization, etc.)\n",
    "    sample = dm.aug(sample)\n",
    "\n",
    "    image = sample[\"image\"]  # (C,H,W)\n",
    "    mask = sample[\"mask\"]    # (H,W)\n",
    "\n",
    "    image_np = image.detach().cpu().numpy()\n",
    "    channel_names = [\"RED\", \"GREEN\", \"BLUE\", \"NIR\"]\n",
    "    for c in range(image_np.shape[0]):\n",
    "        ch_data = image_np[c].flatten()\n",
    "        hist, bin_edges = np.histogram(ch_data, bins=12)\n",
    "        print(f\"Sample {i}, Channel {channel_names[c]} histogram:\")\n",
    "        for edge_start, edge_end, count in zip(bin_edges[:-1], bin_edges[1:], hist):\n",
    "            print(f\"  {edge_start:.3f} - {edge_end:.3f}: {count}\")\n",
    "\n",
    "    # ---- Run model and save computed mask ----\n",
    "    x = image\n",
    "    if x.ndim == 3:          # [C,H,W] → add batch and temporal\n",
    "        x = x.unsqueeze(0).unsqueeze(2)\n",
    "    elif x.ndim == 4:        # [B,C,H,W] → add temporal\n",
    "        x = x.unsqueeze(2)\n",
    "    print(x.shape)           # should be [B,C,T,H,W]\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "        pred_mask = output.output.argmax(dim=1).squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "\n",
    "        import numpy as np\n",
    "        from PIL import Image\n",
    "\n",
    "        # Example class colors\n",
    "        class_colors = {\n",
    "            0: (0, 0, 255),\n",
    "            1: (34, 139, 34),\n",
    "            2: (124, 252, 0),\n",
    "            3: (210, 180, 140),\n",
    "            4: (128, 128, 128),\n",
    "            5: (0, 255, 255),\n",
    "            6: (255, 255, 0),\n",
    "        }\n",
    "\n",
    "        # Create RGB mask\n",
    "        color_mask = np.zeros((pred_mask.shape[0], pred_mask.shape[1], 3), dtype=np.uint8)\n",
    "        for cls_id, color in class_colors.items():\n",
    "            color_mask[pred_mask == cls_id] = color\n",
    "\n",
    "        # Save as PNG\n",
    "        Image.fromarray(color_mask).save(out_dir / f\"sample_{i}_pred_mask.png\")\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Save 4-channel input as raw .bin ----\n",
    "    img_bin_path = out_dir / f\"sample_{i}_x.bin\"\n",
    "    image.detach().cpu().numpy().astype(np.float32).tofile(img_bin_path)\n",
    "\n",
    "    # ---- Save RGB preview for browser/grid ----\n",
    "    rgb_sample = sample\n",
    "    rgb_sample[\"image\"] = rgb_sample[\"image\"].squeeze(0)\n",
    "    fig = dm.test_dataset.plot_rgb(rgb_sample)\n",
    "    fig.savefig(out_dir / f\"sample_{i}_rgb.png\", dpi=150, bbox_inches='tight')\n",
    "    fig.clf()  # clear the figure to free memory\n",
    "\n",
    "    # ---- Save mask with colors ----\n",
    "    mask_np = mask.detach().cpu().numpy()\n",
    "    if mask_np.ndim == 4:\n",
    "        mask_np = mask_np.squeeze(0).squeeze(0)\n",
    "    elif mask_np.ndim == 3:\n",
    "        mask_np = mask_np.squeeze(0)\n",
    "\n",
    "    color_mask = np.zeros((mask_np.shape[0], mask_np.shape[1], 3), dtype=np.uint8)\n",
    "    for cls_id, color in class_colors.items():\n",
    "        color_mask[mask_np == cls_id] = color\n",
    "    Image.fromarray(color_mask).save(out_dir / f\"sample_{i}_y.png\")\n",
    "\n",
    "    # Print info\n",
    "    unique_classes = torch.unique(mask).tolist()\n",
    "    class_names = [class_map[c] for c in unique_classes]\n",
    "    print(f\"Saved sample {i}: classes {unique_classes} → {class_names}\")\n",
    "\n",
    "    if i + 1 >= num_samples:\n",
    "        break\n",
    "\n",
    "print(f\"Samples stored in: {out_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d78ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
