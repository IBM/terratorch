{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Wrapper Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Using `TemporalWrapper` with a TerraTorch backbone (e.g., TerraMind) to process a temporal stack of inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.registry import BACKBONE_REGISTRY\n",
    "\n",
    "backbone = BACKBONE_REGISTRY.build(\n",
    "    \"terramind_v1_base\",\n",
    "    modalities=[\"S2L2A\"],\n",
    "    pretrained=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Temporal Input Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S2L2A': torch.Size([1, 12, 4, 224, 224])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(1, 12, 4, 224, 224) # Shape [B, C, T, H, W]\n",
    "x_dict = {\"S2L2A\": x} # Wrap into modality dict for TerraMind\n",
    "\n",
    "print({k: v.shape for k, v in x_dict.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap backbone with TemporalWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load `TemporalWrapper` with `pooling='mean'`, which averages the latent representations across the 4 input timesteps after they have been processed by the backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.models.utils import TemporalWrapper\n",
    "\n",
    "temporal_backbone = TemporalWrapper(backbone, pooling=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using wrapped backbone in an EncoderDecoder Task Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the wrapped backbone as a backbone when building an EncoderDecoder Model for a segmentation downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Decoder UNetDecoder does not have an `includes_head` attribute. Falling back to the value of the registry.\n"
     ]
    }
   ],
   "source": [
    "from terratorch.tasks import SemanticSegmentationTask\n",
    "\n",
    "model = SemanticSegmentationTask(\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    "    model_args={\n",
    "            \"backbone\": temporal_backbone,\n",
    "            # Necks\n",
    "            \"necks\": [\n",
    "                {\"name\": \"SelectIndices\",\n",
    "                \"indices\": [2, 5, 8, 11]\n",
    "                },\n",
    "                {\"name\": \"ReshapeTokensToImage\",\n",
    "                \"remove_cls_token\": False},  # TerraMind is trained without CLS token\n",
    "                {\"name\": \"LearnedInterpolateToPyramidal\"}  # Some decoders like UNet or UperNet expect hierarchical features.\n",
    "            ],\n",
    "\n",
    "            # Decoder\n",
    "            \"decoder\": \"UNetDecoder\",\n",
    "            \"decoder_channels\": [512, 256, 128, 64],\n",
    "            \n",
    "            # Head\n",
    "            \"head_dropout\": 0.1,\n",
    "            \"num_classes\": 2,\n",
    "        }, \n",
    "    loss=\"dice\",\n",
    "    optimizer=\"AdamW\",\n",
    "    lr=2e-5, \n",
    "    ignore_index=-1,\n",
    "    freeze_backbone=False, \n",
    "    freeze_decoder=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample forward pass shows that the 4-timestep input produces a single segmentation mask. The temporal dimension is averaged after the backbone, before the features are passed through the neck, decoder, and head of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'S2L2A': torch.Size([1, 12, 4, 224, 224])}\n",
      "Output: torch.Size([1, 2, 224, 224])\n",
      "\n",
      "Encoder output: 12 features of shape torch.Size([1, 196, 768])\n",
      "After decoder: torch.Size([1, 64, 112, 112])\n",
      "Final head output: torch.Size([1, 2, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model = model.to(device).eval()\n",
    "x_dict = {k: v.to(device) for k, v in x_dict.items()}\n",
    "\n",
    "print(\"Input:\", {k: v.shape for k, v in x_dict.items()})\n",
    "print(\"Output:\", model(x_dict).output.shape)\n",
    "print()\n",
    "\n",
    "# Forward through temporal encoder (backbone)\n",
    "feats = model.model.encoder(x_dict)\n",
    "print(\"Encoder output:\", len(feats), \"features of shape\", feats[0].shape)\n",
    "\n",
    "# Forward through necks\n",
    "necks_out = model.model.neck(feats)\n",
    "\n",
    "# Forward through decoder\n",
    "decoder_out = model.model.decoder(necks_out)\n",
    "print(\"After decoder:\", decoder_out.shape)\n",
    "\n",
    "# Forward through head\n",
    "head_out = model.model.head(decoder_out)\n",
    "print(\"Final head output:\", head_out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
