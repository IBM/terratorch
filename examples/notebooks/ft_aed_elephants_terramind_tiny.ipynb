{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15dec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.datasets.od_aed_elephant import ElephantCocoDataset\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c6e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hostname = socket.gethostname()\n",
    "print(f\"The hostname is: {hostname}\")\n",
    "\n",
    "# Output will be similar to:\n",
    "# The hostname is: my-computer-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "if socket.gethostname() == \"li-dee0034c-2e23-11b2-a85c-bbf4df0b1948\": # romeo\n",
    "    img_folder_train='/home/romeokienzler/Downloads/AED/training_images'\n",
    "    ann_file_train='/home/romeokienzler/Downloads/AED/annotations_elephants_training.json'\n",
    "    img_folder_test='/home/romeokienzler/Downloads/AED/test_images'\n",
    "    ann_file_test='/home/romeokienzler/Downloads/AED/annotations_elephants_test.json'\n",
    "else:\n",
    "    img_folder_train='/dccstor/terratorch/shared/datasets/aed-elephant/AED/training_images'\n",
    "    ann_file_train='/dccstor/terratorch/shared/datasets/aed-elephant/annotations_elephants_train.json'\n",
    "    img_folder_test='/dccstor/terratorch/shared/datasets/aed-elephant/AED/test_images'\n",
    "    ann_file_test='/dccstor/terratorch/shared/datasets/aed-elephant/annotations_elephants_test.json'\n",
    "\n",
    "ds = ElephantCocoDataset(\n",
    "    img_folder=img_folder_train,\n",
    "    ann_file=ann_file_train,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd46acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66254ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = e['image'].shape[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.datamodules.od_aed_elephant import ElephantDataModule\n",
    "dm = ElephantDataModule(img_folder_train=img_folder_train, ann_file_train=ann_file_train, img_folder_val=img_folder_test, ann_file_val=ann_file_test, min_size=(3648, 5472), tile_size=(512,512), overlap=128, batch_size=8, num_workers=0)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da16e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbce25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tdl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl.dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "\n",
    "def show_from_datamodule(dm, n=3, split=\"train\"):\n",
    "    \"\"\"Show exactly n samples with bounding boxes from a DataModule.\"\"\"\n",
    "    # pick the right loader\n",
    "    if split == \"train\":\n",
    "        loader = dm.train_dataloader()\n",
    "    elif split == \"val\":\n",
    "        loader = dm.val_dataloader()\n",
    "    elif split == \"test\":\n",
    "        loader = dm.test_dataloader()\n",
    "    else:\n",
    "        raise ValueError(\"split must be 'train', 'val', or 'test'\")\n",
    "    \n",
    "    shown = 0\n",
    "    for batch in loader:  # loop until enough samples found\n",
    "        images = batch[\"image\"]     # (B, C, H, W)\n",
    "        boxes  = batch[\"boxes\"]     # list[Tensor] or padded tensor\n",
    "\n",
    "        for img, b in zip(images, boxes):\n",
    "            # skip samples with no boxes\n",
    "            if b is None or (isinstance(b, torch.Tensor) and b.numel() == 0) or len(b) == 0:\n",
    "                continue  \n",
    "\n",
    "            if torch.is_tensor(img):\n",
    "                img = F.to_pil_image(img)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "            ax.imshow(img)\n",
    "\n",
    "            # convert to tensor if needed\n",
    "            if isinstance(b, torch.Tensor):\n",
    "                b = b.cpu()\n",
    "            for box in b:\n",
    "                x1, y1, x2, y2 = box.tolist()\n",
    "                rect = patches.Rectangle(\n",
    "                    (x1, y1), x2 - x1, y2 - y1,\n",
    "                    linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "            ax.set_title(f\"Sample {shown}, {len(b)} objects\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "            shown += 1\n",
    "            if shown >= n:\n",
    "                return  # stop once we have shown n samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e81256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#m.setup(\"fit\")\n",
    "#show_from_datamodule(dm, n=30, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a862ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" train_loader = dm.train_dataloader()\n",
    "\n",
    "# Iterate and count\n",
    "total_elements = 0\n",
    "for batch in train_loader:\n",
    "    # 'batch' is a tensor or list of tensors depending on your dataset\n",
    "    batch_size = len(batch)\n",
    "    print(f\"Processing a batch of size: {batch_size}\")\n",
    "    total_elements += batch_size\n",
    "\n",
    "print(f\"All elements read. Total count: {total_elements}\") \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.tasks.object_detection_task import ObjectDetectionTask\n",
    "\n",
    "model = ObjectDetectionTask.load_from_checkpoint(\"./best-epoch=18.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# load png → tensor\n",
    "path = \"tile_cache/tile_101_3072_384.png\"\n",
    "img = Image.open(path).convert(\"RGB\")\n",
    "tensor = F.to_tensor(img).unsqueeze(0)  # shape (1, C, H, W) for model input\n",
    "\n",
    "# send to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tensor = tensor.to(device)\n",
    "\n",
    "# example model inference\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb25df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detection(img: torch.Tensor, output, suptitle: str | None = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot image with object detection predictions from ModelOutputObjectDetection.\n",
    "\n",
    "    Args:\n",
    "        img: torch.Tensor in (C,H,W) or (B,C,H,W), range [0,1]\n",
    "        output: ModelOutputObjectDetection object\n",
    "        suptitle: Optional title for figure\n",
    "    \"\"\"\n",
    "    # handle batch dimension\n",
    "    if img.ndim == 4:\n",
    "        if img.shape[0] != 1:\n",
    "            raise ValueError(f\"Expected batch of size 1, got {img.shape[0]}\")\n",
    "        img = img[0]\n",
    "\n",
    "    # CHW → HWC for matplotlib\n",
    "    img_np = img.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(img_np)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    if output is not None and output.output is not None:\n",
    "        for pred in output.output:\n",
    "            boxes = pred.get(\"boxes\", [])\n",
    "            labels = pred.get(\"labels\", [])\n",
    "            scores = pred.get(\"scores\", [])\n",
    "\n",
    "            for box, label, score in zip(boxes, labels, scores):\n",
    "                x1, y1, x2, y2 = box.tolist()\n",
    "                w, h = x2 - x1, y2 - y1\n",
    "                rect = patches.Rectangle(\n",
    "                    (x1, y1), w, h,\n",
    "                    linewidth=2, edgecolor=\"lime\", facecolor=\"none\"\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(\n",
    "                    x1, y1,\n",
    "                    f\"{label.item()} ({score.item():.2f})\",\n",
    "                    fontsize=8,\n",
    "                    color=\"black\",\n",
    "                    bbox=dict(facecolor=\"lime\", alpha=0.5, edgecolor=\"none\", pad=1)\n",
    "                )\n",
    "\n",
    "    if suptitle is not None:\n",
    "        fig.suptitle(suptitle)\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4bf4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detection(tensor, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ecbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from terratorch.models.object_detection_model_factory import ModelOutputObjectDetection\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "class ONNXWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        out = self.model(*args, **kwargs)\n",
    "\n",
    "        # --- Handle ModelOutputObjectDetection ---\n",
    "        if isinstance(out, ModelOutputObjectDetection):\n",
    "            if isinstance(out.output, dict):\n",
    "                # Single image\n",
    "                boxes = out.output.get(\"boxes\", torch.empty(0, 4))\n",
    "                scores = out.output.get(\"scores\", torch.empty(0))\n",
    "                labels = out.output.get(\"labels\", torch.empty(0, dtype=torch.long))\n",
    "                return boxes, scores, labels\n",
    "\n",
    "            elif isinstance(out.output, list):\n",
    "                # Batch: flatten all boxes/scores/labels into single tensors\n",
    "                all_boxes, all_scores, all_labels = [], [], []\n",
    "                for item in out.output:\n",
    "                    all_boxes.append(item.get(\"boxes\", torch.empty(0, 4)))\n",
    "                    all_scores.append(item.get(\"scores\", torch.empty(0)))\n",
    "                    all_labels.append(item.get(\"labels\", torch.empty(0, dtype=torch.long)))\n",
    "\n",
    "                boxes = torch.cat(all_boxes, dim=0) if all_boxes else torch.empty(0, 4)\n",
    "                scores = torch.cat(all_scores, dim=0) if all_scores else torch.empty(0)\n",
    "                labels = torch.cat(all_labels, dim=0) if all_labels else torch.empty(0, dtype=torch.long)\n",
    "                return boxes, scores, labels\n",
    "\n",
    "        # --- If it's already a tensor ---\n",
    "        if isinstance(out, Tensor):\n",
    "            return out\n",
    "\n",
    "        # --- Fallback ---\n",
    "        raise TypeError(f\"Unsupported model output type: {type(out)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = ONNXWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65177e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 512,512)  # batch size 1, 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    onnx_model,              # the PyTorch model\n",
    "    dummy_input,              # dummy input\n",
    "    \"model_tm_tiny_aed_elephants.onnx\",             # output file     \n",
    "    export_params=True,       # store trained weights\n",
    "    opset_version=17,         # recommended ONNX opset\n",
    "    do_constant_folding=True, # optimize constants\n",
    "    input_names=['input'],    # model input name\n",
    "    output_names=['output'],  # model output name\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # allow variable batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d84e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
