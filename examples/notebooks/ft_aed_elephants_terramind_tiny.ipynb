{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15dec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.datasets.od_aed_elephant import ElephantCocoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5003e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = ElephantCocoDataset(\n",
    "#    img_folder='/home/romeokienzler/Downloads/AED/training_images',\n",
    "#    ann_file='/home/romeokienzler/Downloads/AED/annotations_elephants_training.json',\n",
    "#)\n",
    "img_folder='/dccstor/terratorch/shared/datasets/aed-elephant/AED/training_images'\n",
    "ann_file='/dccstor/terratorch/shared/datasets/aed-elephant/annotations_elephants_training.json'\n",
    "\n",
    "ds = ElephantCocoDataset(\n",
    "    img_folder=img_folder,\n",
    "    ann_file=ann_file,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db6c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd46acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66254ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = e['image'].shape[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce87622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.datamodules.od_aed_elephant import ElephantDataModule\n",
    "dm = ElephantDataModule(img_folder=img_folder, ann_file=ann_file, min_size=(3648, 5472), tile_size=(512,512), overlap=128, batch_size=8, num_workers=8, save_png=True)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da16e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbce25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tdl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl.dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "\n",
    "def show_from_datamodule(dm, n=3, split=\"train\"):\n",
    "    \"\"\"Show exactly n samples with bounding boxes from a DataModule.\"\"\"\n",
    "    # pick the right loader\n",
    "    if split == \"train\":\n",
    "        loader = dm.train_dataloader()\n",
    "    elif split == \"val\":\n",
    "        loader = dm.val_dataloader()\n",
    "    elif split == \"test\":\n",
    "        loader = dm.test_dataloader()\n",
    "    else:\n",
    "        raise ValueError(\"split must be 'train', 'val', or 'test'\")\n",
    "    \n",
    "    shown = 0\n",
    "    for batch in loader:  # loop until enough samples found\n",
    "        images = batch[\"image\"]     # (B, C, H, W)\n",
    "        boxes  = batch[\"boxes\"]     # list[Tensor] or padded tensor\n",
    "\n",
    "        for img, b in zip(images, boxes):\n",
    "            # skip samples with no boxes\n",
    "            if b is None or (isinstance(b, torch.Tensor) and b.numel() == 0) or len(b) == 0:\n",
    "                continue  \n",
    "\n",
    "            if torch.is_tensor(img):\n",
    "                img = F.to_pil_image(img)\n",
    "\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "            ax.imshow(img)\n",
    "\n",
    "            # convert to tensor if needed\n",
    "            if isinstance(b, torch.Tensor):\n",
    "                b = b.cpu()\n",
    "            for box in b:\n",
    "                x1, y1, x2, y2 = box.tolist()\n",
    "                rect = patches.Rectangle(\n",
    "                    (x1, y1), x2 - x1, y2 - y1,\n",
    "                    linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "            ax.set_title(f\"Sample {shown}, {len(b)} objects\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "            shown += 1\n",
    "            if shown >= n:\n",
    "                return  # stop once we have shown n samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e81256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup(\"fit\")\n",
    "#show_from_datamodule(dm, n=30, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a862ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dm.train_dataloader()\n",
    "\n",
    "# Iterate and count\n",
    "total_elements = 0\n",
    "for batch in train_loader:\n",
    "    # 'batch' is a tensor or list of tensors depending on your dataset\n",
    "    batch_size = len(batch)\n",
    "    print(f\"Processing a batch of size: {batch_size}\")\n",
    "    total_elements += batch_size\n",
    "\n",
    "print(f\"All elements read. Total count: {total_elements}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
