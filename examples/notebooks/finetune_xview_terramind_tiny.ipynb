{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e196a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -e ../../\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f89725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging import: 3.24249267578125e-05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectDetectionTask import: 55.79015898704529 seconds\n",
      "XviewDataModule import: 0.27486085891723633 seconds\n",
      "Trainer import: 0.00011301040649414062 seconds\n",
      "torchvision.transforms import: 7.224082946777344e-05 seconds\n",
      "Total import time: 56.06584310531616 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "t0 = time.time()\n",
    "import logging\n",
    "print(\"logging import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "from terratorch.tasks import ObjectDetectionTask\n",
    "print(\"ObjectDetectionTask import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "from terratorch.datamodules.xview import XviewDataModule\n",
    "print(\"XviewDataModule import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "from lightning.pytorch import Trainer\n",
    "print(\"Trainer import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "from torchvision import transforms\n",
    "print(\"torchvision.transforms import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "print(\"Total import time:\", time.time() - start_total, \"seconds\")\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ded29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/akihironitta/xView-PyTorch/refs/heads/master/datasets.py\n",
    "#!mv datasets.py ../../terratorch/datasets/xview.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d1fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dm = XviewDataModule(data_dir = '/dccstor/terratorch/users/rkie/data/xview/train_images', ann_file = '/dccstor/terratorch/users/rkie/data/xview/xView_train.geojson', img_transform = img_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2e14a",
   "metadata": {},
   "source": [
    "import json\n",
    "with open('/dccstor/terratorch/users/rkie/data/xview/xView_train.geojson') as f:\n",
    "    anns = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ba3dc",
   "metadata": {},
   "source": [
    "s = set()\n",
    "for i, f in enumerate(anns['features']):\n",
    "    s.add(f['properties']['image_id'])\n",
    "\n",
    "for setitem in s:\n",
    "    print(setitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dm.setup()\n",
    "\n",
    "#dl = dm.train_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99246bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a55e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d01e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item['boxes_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1802f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"framework\": \"faster-rcnn\",\n",
    "    \"backbone\": \"terramind_v1_tiny\",\n",
    "    \"backbone_pretrained\": True,\n",
    "    \"num_classes\": 100,\n",
    "    \"framework_min_size\": 512,\n",
    "    \"framework_max_size\": 512,\n",
    "    \"backbone_modalities\": [\"RGB\"],\n",
    "    \"in_channels\": 3,\n",
    "    \"necks\": [\n",
    "        {\n",
    "            \"name\": \"SelectIndices\",\n",
    "            \"indices\": [2, 5, 8, 11]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ReshapeTokensToImage\",\n",
    "            \"remove_cls_token\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"LearnedInterpolateToPyramidal\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"FeaturePyramidNetworkNeck\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "m = ObjectDetectionTask(model_factory='ObjectDetectionModelFactory', model_args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db545147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import torch\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"odet-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caaa211",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    max_epochs=1,\n",
    "    default_root_dir='output',\n",
    "    log_every_n_steps=1,\n",
    "    check_val_every_n_epoch=1,\n",
    "    callbacks=[checkpoint_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=m, datamodule=dm)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d601d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ObjectDetectionTask.load_from_checkpoint(\"./checkpoints/last.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aec701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d1252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.models.object_detection_model_factory import ModelOutputObjectDetection\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "class ONNXWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        out = self.model(*args, **kwargs)\n",
    "\n",
    "        # --- Handle ModelOutputObjectDetection ---\n",
    "        if isinstance(out, ModelOutputObjectDetection):\n",
    "            if isinstance(out.output, dict):\n",
    "                # Single image\n",
    "                boxes = out.output.get(\"boxes\", torch.empty(0, 4))\n",
    "                scores = out.output.get(\"scores\", torch.empty(0))\n",
    "                labels = out.output.get(\"labels\", torch.empty(0, dtype=torch.long))\n",
    "                return boxes, scores, labels\n",
    "\n",
    "            elif isinstance(out.output, list):\n",
    "                # Batch: flatten all boxes/scores/labels into single tensors\n",
    "                all_boxes, all_scores, all_labels = [], [], []\n",
    "                for item in out.output:\n",
    "                    all_boxes.append(item.get(\"boxes\", torch.empty(0, 4)))\n",
    "                    all_scores.append(item.get(\"scores\", torch.empty(0)))\n",
    "                    all_labels.append(item.get(\"labels\", torch.empty(0, dtype=torch.long)))\n",
    "\n",
    "                boxes = torch.cat(all_boxes, dim=0) if all_boxes else torch.empty(0, 4)\n",
    "                scores = torch.cat(all_scores, dim=0) if all_scores else torch.empty(0)\n",
    "                labels = torch.cat(all_labels, dim=0) if all_labels else torch.empty(0, dtype=torch.long)\n",
    "                return boxes, scores, labels\n",
    "\n",
    "        # --- If it's already a tensor ---\n",
    "        if isinstance(out, Tensor):\n",
    "            return out\n",
    "\n",
    "        # --- Fallback ---\n",
    "        raise TypeError(f\"Unsupported model output type: {type(out)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7596987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = ONNXWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d5dbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ONNXWrapper(\n",
       "  (model): ObjectDetectionTask(\n",
       "    (model): ObjectDetectionModel(\n",
       "      (torchvision_model): FasterRCNN(\n",
       "        (transform): GeneralizedRCNNTransform(\n",
       "            Normalize(mean=[0 0 0], std=[1 1 1])\n",
       "            Resize(min_size=(512,), max_size=512, mode='bilinear')\n",
       "        )\n",
       "        (backbone): BackboneWrapper(\n",
       "          (backbone): TerraMindViT(\n",
       "            (encoder_embeddings): ModuleDict(\n",
       "              (untok_sen2rgb@224): ImageEncoderEmbedding(\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "            )\n",
       "            (encoder): ModuleList(\n",
       "              (0-11): 12 x Block(\n",
       "                (norm1): LayerNorm()\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): LayerNorm()\n",
       "                (mlp): GatedMlp(\n",
       "                  (fc1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                  (act): SiLU()\n",
       "                  (fc2): Linear(in_features=2048, out_features=768, bias=False)\n",
       "                  (fc3): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (encoder_norm): LayerNorm()\n",
       "            (tokenizer): ModuleDict()\n",
       "          )\n",
       "          (necks): Sequential(\n",
       "            (0): SelectIndices()\n",
       "            (1): ReshapeTokensToImage()\n",
       "            (2): LearnedInterpolateToPyramidal(\n",
       "              (fpn1): Sequential(\n",
       "                (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "                (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): ConvTranspose2d(384, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "              )\n",
       "              (fpn2): Sequential(\n",
       "                (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "              )\n",
       "              (fpn3): Sequential(\n",
       "                (0): Identity()\n",
       "              )\n",
       "              (fpn4): Sequential(\n",
       "                (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "              )\n",
       "            )\n",
       "            (3): FeaturePyramidNetworkNeck(\n",
       "              (fpn): FeaturePyramidNetwork(\n",
       "                (inner_blocks): ModuleList(\n",
       "                  (0): Conv2dNormActivation(\n",
       "                    (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  )\n",
       "                  (1): Conv2dNormActivation(\n",
       "                    (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  )\n",
       "                  (2-3): 2 x Conv2dNormActivation(\n",
       "                    (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  )\n",
       "                )\n",
       "                (layer_blocks): ModuleList(\n",
       "                  (0-3): 4 x Conv2dNormActivation(\n",
       "                    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (rpn): RegionProposalNetwork(\n",
       "          (anchor_generator): AnchorGenerator()\n",
       "          (head): RPNHead(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (1): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (roi_heads): RoIHeads(\n",
       "          (box_roi_pool): MultiScaleRoIAlign(featmap_names=['feat0', 'feat1', 'feat2', 'feat3'], output_size=(7, 7), sampling_ratio=2)\n",
       "          (box_head): TwoMLPHead(\n",
       "            (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "            (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (box_predictor): FastRCNNPredictor(\n",
       "            (cls_score): Linear(in_features=1024, out_features=100, bias=True)\n",
       "            (bbox_pred): Linear(in_features=1024, out_features=400, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (train_metrics): MetricCollection(\n",
       "      (mAP): MeanAveragePrecision(),\n",
       "      prefix=train_\n",
       "    )\n",
       "    (val_metrics): MetricCollection(\n",
       "      (mAP): MeanAveragePrecision(),\n",
       "      prefix=val_\n",
       "    )\n",
       "    (test_metrics): MetricCollection(\n",
       "      (mAP): MeanAveragePrecision(),\n",
       "      prefix=test_\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9d2751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy_input = torch.randn(1, 3, 512,512)  # batch size 1, 10 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53844fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7804f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "BackboneWrapper(\n",
      "  (backbone): TerraMindViT(\n",
      "    (encoder_embeddings): ModuleDict(\n",
      "      (untok_sen2rgb@224): ImageEncoderEmbedding(\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm()\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm()\n",
      "        (mlp): GatedMlp(\n",
      "          (fc1): Linear(in_features=768, out_features=2048, bias=False)\n",
      "          (act): SiLU()\n",
      "          (fc2): Linear(in_features=2048, out_features=768, bias=False)\n",
      "          (fc3): Linear(in_features=768, out_features=2048, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (encoder_norm): LayerNorm()\n",
      "    (tokenizer): ModuleDict()\n",
      "  )\n",
      "  (necks): Sequential(\n",
      "    (0): SelectIndices()\n",
      "    (1): ReshapeTokensToImage()\n",
      "    (2): LearnedInterpolateToPyramidal(\n",
      "      (fpn1): Sequential(\n",
      "        (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): ConvTranspose2d(384, 192, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (fpn2): Sequential(\n",
      "        (0): ConvTranspose2d(768, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (fpn3): Sequential(\n",
      "        (0): Identity()\n",
      "      )\n",
      "      (fpn4): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (3): FeaturePyramidNetworkNeck(\n",
      "      (fpn): FeaturePyramidNetwork(\n",
      "        (inner_blocks): ModuleList(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2-3): 2 x Conv2dNormActivation(\n",
      "            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (layer_blocks): ModuleList(\n",
      "          (0-3): 4 x Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    onnx_model,              # the PyTorch model\n",
    "    dummy_input,              # dummy input\n",
    "    \"model.onnx\",             # output file     \n",
    "    export_params=True,       # store trained weights\n",
    "    opset_version=17,         # recommended ONNX opset\n",
    "    do_constant_folding=True, # optimize constants\n",
    "    input_names=['input'],    # model input name\n",
    "    output_names=['output'],  # model output name\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # allow variable batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample[0])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628830ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample[1])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de4fd612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input name: input\n",
      "Expected input shape: ['batch_size', 3, 512, 512] → resolved: [1, 3, 512, 512]\n",
      "Runtime output 'output': shape=(0, 4), dtype=float32\n",
      "Runtime output '3185': shape=(0,), dtype=float32\n",
      "Runtime output '3186': shape=(0,), dtype=int64\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Path to your ONNX model\n",
    "onnx_path = \"model.onnx\"\n",
    "\n",
    "# --- Load the model to inspect input/output metadata ---\n",
    "model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Create inference session\n",
    "session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# Get input metadata\n",
    "input_meta = session.get_inputs()[0]\n",
    "input_name = input_meta.name\n",
    "input_shape = input_meta.shape  # may have symbolic dimensions\n",
    "\n",
    "# Replace symbolic dimensions with actual values where possible\n",
    "resolved_shape = [\n",
    "    dim if isinstance(dim, int) else 1 for dim in input_shape\n",
    "]\n",
    "\n",
    "# If the model expects fixed spatial dimensions, use them\n",
    "# Example: input_shape could be ['batch_size', 3, 512, 512]\n",
    "height = resolved_shape[2]\n",
    "width = resolved_shape[3]\n",
    "\n",
    "print(f\"Input name: {input_name}\")\n",
    "print(f\"Expected input shape: {input_shape} → resolved: {resolved_shape}\")\n",
    "\n",
    "# Create a dummy input with the correct shape\n",
    "dummy_input = np.random.randn(\n",
    "    resolved_shape[0],  # batch\n",
    "    resolved_shape[1],  # channels\n",
    "    height,\n",
    "    width\n",
    ").astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "outputs = session.run(None, {input_name: dummy_input})\n",
    "\n",
    "# Print output names and shapes\n",
    "for name, arr in zip([o.name for o in session.get_outputs()], outputs):\n",
    "    print(f\"Runtime output '{name}': shape={arr.shape}, dtype={arr.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7b8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2502c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42905344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
