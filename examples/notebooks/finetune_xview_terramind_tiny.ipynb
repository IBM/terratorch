{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e196a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -e ../../\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4263d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2020 Akihiro Nitta\n",
    "# All rights reserved.\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import json\n",
    "import collections\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "LABEL_TO_STRING = {\n",
    "    11: \"Fixed-wing Aircraft\",\n",
    "    12: \"Small Aircraft\",\n",
    "    13: \"Passenger/Cargo Plane\",\n",
    "    15: \"Helicopter\",\n",
    "    17: \"Passenger Vehicle\",\n",
    "    18: \"Small Car\",\n",
    "    19: \"Bus\",\n",
    "    20: \"Pickup Truck\",\n",
    "    21: \"Utility Truck\",\n",
    "    23: \"Truck\",\n",
    "    24: \"Cargo Truck\",\n",
    "    25: \"Truck Tractor w/ Box Trailer\",\n",
    "    26: \"Truck Tractor\",\n",
    "    27: \"Trailer\",\n",
    "    28: \"Truck Tractor w/ Flatbed Trailer\",\n",
    "    29: \"Truck Tractor w/ Liquid Tank\",\n",
    "    32: \"Crane Truck\",\n",
    "    33: \"Railway Vehicle\",\n",
    "    34: \"Passenger Car\",\n",
    "    35: \"Cargo/Container Car\",\n",
    "    36: \"Flat Car\",\n",
    "    37: \"Tank car\",\n",
    "    38: \"Locomotive\",\n",
    "    40: \"Maritime Vessel\",\n",
    "    41: \"Motorboat\",\n",
    "    42: \"Sailboat\",\n",
    "    44: \"Tugboat\",\n",
    "    45: \"Barge\",\n",
    "    47: \"Fishing Vessel\",\n",
    "    49: \"Ferry\",\n",
    "    50: \"Yacht\",\n",
    "    51: \"Container Ship\",\n",
    "    52: \"Oil Tanker\",\n",
    "    53: \"Engineering Vehicle\",\n",
    "    54: \"Tower crane\",\n",
    "    55: \"Container Crane\",\n",
    "    56: \"Reach Stacker\",\n",
    "    57: \"Straddle Carrier\",\n",
    "    59: \"Mobile Crane\",\n",
    "    60: \"Dump Truck\",\n",
    "    61: \"Haul Truck\",\n",
    "    62: \"Scraper/Tractor\",\n",
    "    63: \"Front loader/Bulldozer\",\n",
    "    64: \"Excavator\",\n",
    "    65: \"Cement Mixer\",\n",
    "    66: \"Ground Grader\",\n",
    "    71: \"Hut/Tent\",\n",
    "    72: \"Shed\",\n",
    "    73: \"Building\",\n",
    "    74: \"Aircraft Hangar\",\n",
    "    76: \"Damaged Building\",\n",
    "    77: \"Facility\",\n",
    "    79: \"Construction Site\",\n",
    "    83: \"Vehicle Lot\",\n",
    "    84: \"Helipad\",\n",
    "    86: \"Storage Tank\",\n",
    "    89: \"Shipping container lot\",\n",
    "    91: \"Shipping Container\",\n",
    "    93: \"Pylon\",\n",
    "    94: \"Tower\",\n",
    "}\n",
    "\n",
    "class XviewDataset(VisionDataset):\n",
    "    \"\"\"xView object detection dataset.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (string): Directory with all the images.\n",
    "        annFile (string): Path to json annotation file.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
    "            and returns a transformed version.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root, annFile, transform=None):\n",
    "        import os, json, collections\n",
    "\n",
    "        self.root = root\n",
    "        self.annFile = annFile\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load annotations\n",
    "        with open(self.annFile) as f:\n",
    "            anns = json.load(f)\n",
    "\n",
    "        self.objects = {}  # object_id -> object\n",
    "        self.ids = set()\n",
    "\n",
    "        for idx, feat in enumerate(anns['features']):\n",
    "            obj_id = idx\n",
    "            image_id = feat['properties']['image_id']\n",
    "\n",
    "            base, ext = os.path.splitext(image_id)\n",
    "            img_path = os.path.join(self.root, f\"{base}_1_1{ext}\") # check if name_1_1.tif is there, therefore we assume name.tif was \n",
    "            # present in the original dataset and also name_6_6.tif should be there\n",
    "            if not os.path.exists(img_path):\n",
    "                logging.info(f'{img_path} skipped because it was not found')\n",
    "                continue\n",
    "\n",
    "            self.ids.add(image_id)\n",
    "            self.objects[obj_id] = {\n",
    "                \"image_id\": image_id,\n",
    "                \"bbox\": feat['properties']['bounds_imcoords'],\n",
    "                \"type_id\": feat['properties']['type_id']\n",
    "            }\n",
    "\n",
    "        # Create mapping from image_id to all object_ids\n",
    "        self.image_id_to_object_ids = collections.defaultdict(list)\n",
    "        for obj_id, obj in self.objects.items():\n",
    "            self.image_id_to_object_ids[obj[\"image_id\"]].append(obj_id)\n",
    "\n",
    "        # Keep ids only for images that still exist\n",
    "        self.ids = list(self.ids)\n",
    "\n",
    "\n",
    "    def filter_invalid_boxes(self, target):\n",
    "        boxes = target[\"boxes\"]\n",
    "        # keep only boxes with strictly positive width & height\n",
    "        keep = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n",
    "        target[\"boxes\"] = boxes[keep]\n",
    "        if \"labels\" in target:\n",
    "            target[\"labels\"] = target[\"labels\"][keep]\n",
    "        return target\n",
    "\n",
    "    def adjust_boxes_for_tile(self, target, grid_coordinates, tile_size=512):\n",
    "        \"\"\"\n",
    "        Adjust bounding boxes to a tile's local coordinate system.\n",
    "\n",
    "        Args:\n",
    "            target (dict): Must contain 'boxes' (list of [x_min, y_min, x_max, y_max]) and 'labels'.\n",
    "            grid_coordinates (tuple): (row, col) 0-based or 1-based (assume 1-based here).\n",
    "            tile_size (int): Size of each tile in pixels.\n",
    "\n",
    "        Returns:\n",
    "            dict: target with boxes adjusted to tile-local coordinates.\n",
    "        \"\"\"\n",
    "        if not isinstance(target, dict) or \"boxes\" not in target:\n",
    "            raise RuntimeError(\"Target must contain 'boxes'.\")\n",
    "\n",
    "        row, col = grid_coordinates\n",
    "        x_offset = (col - 1) * tile_size\n",
    "        y_offset = (row - 1) * tile_size\n",
    "\n",
    "        new_boxes = []\n",
    "        new_labels = []\n",
    "\n",
    "        for box, label in zip(target[\"boxes\"], target[\"labels\"]):\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "\n",
    "            # Shift box to tile-local coordinates\n",
    "            x_min_tile = x_min - x_offset\n",
    "            y_min_tile = y_min - y_offset\n",
    "            x_max_tile = x_max - x_offset\n",
    "            y_max_tile = y_max - y_offset\n",
    "\n",
    "            # Clip box to tile boundaries\n",
    "            x_min_tile = max(0, x_min_tile)\n",
    "            y_min_tile = max(0, y_min_tile)\n",
    "            x_max_tile = min(tile_size, x_max_tile)\n",
    "            y_max_tile = min(tile_size, y_max_tile)\n",
    "\n",
    "            def ensure_tensor(x, dtype=torch.float32):\n",
    "                if not isinstance(x, torch.Tensor):\n",
    "                    return torch.tensor(x, dtype=dtype)\n",
    "                return x\n",
    "\n",
    "            x_min_tile = ensure_tensor(x_min_tile)\n",
    "            y_min_tile = ensure_tensor(y_min_tile)\n",
    "            x_max_tile = ensure_tensor(x_max_tile)\n",
    "            y_max_tile = ensure_tensor(y_max_tile)\n",
    "\n",
    "            # Only keep boxes that are at least partially inside the tile\n",
    "            if x_max_tile > x_min_tile and y_max_tile > y_min_tile:\n",
    "                new_box = torch.stack([x_min_tile, y_min_tile, x_max_tile, y_max_tile])\n",
    "                if not isinstance(new_box, torch.Tensor):\n",
    "                    raise RuntimeError()\n",
    "                new_boxes.append(new_box)\n",
    "                new_labels.append(torch.Tensor(label))\n",
    "\n",
    "        if len(new_boxes) > 0:\n",
    "            target[\"boxes\"] = torch.stack(new_boxes).float()\n",
    "        else:\n",
    "            target[\"boxes\"] = torch.empty((0, 4), dtype=torch.float32)\n",
    "\n",
    "        if len(new_labels) > 0:\n",
    "            target[\"labels\"] = torch.stack(new_labels).to(torch.int64)\n",
    "        else:\n",
    "            target[\"labels\"] = torch.empty((0,), dtype=torch.int64)\n",
    "\n",
    "\n",
    "        labels = target.get(\"labels\")\n",
    "        if isinstance(labels, list):\n",
    "            if len(labels) == 1 and torch.is_tensor(labels[0]):\n",
    "                target[\"labels\"] = labels[0]\n",
    "            else:\n",
    "                raise TypeError(\n",
    "                    f\"Expected target['labels'] to be a list with exactly one Tensor element, \"\n",
    "                    f\"but got list of length {len(labels)} with element types {[type(l) for l in labels]} \"\n",
    "                    f\"and values {labels}\"\n",
    "                )\n",
    "        elif not torch.is_tensor(labels):\n",
    "            raise TypeError(\n",
    "                f\"Expected target['labels'] to be either a list with one Tensor or a Tensor, \"\n",
    "                f\"but got type {type(labels)} with value {labels}\"\n",
    "            )\n",
    "\n",
    "        return target\n",
    "\n",
    "\n",
    "    def target_list_to_dict(self, target_list):\n",
    "        if len(target_list) == 0:\n",
    "            return {\n",
    "                \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n",
    "                \"labels\": torch.zeros((0,), dtype=torch.int64)\n",
    "            }\n",
    "        boxes = torch.tensor([list(map(float, t['bbox'].split(','))) for t in target_list], dtype=torch.float32)\n",
    "        labels = torch.tensor([t['type_id'] for t in target_list], dtype=torch.int64)\n",
    "        return {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "\n",
    "    def get_grid_coordinates(self, index, grid_size=6):\n",
    "        \"\"\"\n",
    "        Maps a sequential index to (row, col) coordinates for a square grid.\n",
    "        \n",
    "        Args:\n",
    "            index (int): The sequential index (0 to 35).\n",
    "            grid_size (int): The size of the grid (e.g., 6 for 6x6).\n",
    "            \n",
    "        Returns:\n",
    "            tuple: A tuple (row_id, col_id) where IDs are 1-based.\n",
    "        \"\"\"\n",
    "        if not 0 <= index < grid_size * grid_size:\n",
    "            raise ValueError(f\"Index must be between 0 and {grid_size*grid_size - 1}.\")\n",
    "        \n",
    "        row_id = index // grid_size + 1\n",
    "        col_id = index % grid_size + 1\n",
    "        \n",
    "        return row_id, col_id\n",
    "\n",
    "    def generate_cropped_filename(self, original_img_id, grid_coords):\n",
    "        \"\"\"\n",
    "        Generates a new filename for a cropped tile based on the original image ID\n",
    "        and the grid coordinates.\n",
    "\n",
    "        Args:\n",
    "            original_img_id (str): The original image filename (e.g., 'xyz.tif').\n",
    "            grid_coords (tuple): A tuple (row_id, col_id) where IDs are 1-based.\n",
    "\n",
    "        Returns:\n",
    "            str: The new filename (e.g., 'xyz_1_1.tif').\n",
    "        \"\"\"\n",
    "        # Split the filename into the base name and extension\n",
    "        base_name, ext = os.path.splitext(original_img_id)\n",
    "\n",
    "        # Unpack the row and column IDs from the tuple\n",
    "        row_id, col_id = grid_coords\n",
    "\n",
    "        # Create the new filename with the specified pattern\n",
    "        new_filename = f\"{base_name}_{row_id}_{col_id}{ext}\"\n",
    "\n",
    "        return new_filename\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        print(f'Getting item: {index}')\n",
    "        img_id = self.ids[math.floor(index/36)]\n",
    "        ann_ids = self.image_id_to_object_ids[img_id]\n",
    "        target = [self.objects[ann_id] for ann_id in ann_ids]\n",
    "\n",
    "        grid_coordinates = self.get_grid_coordinates(index%36)\n",
    "\n",
    "        target = self.target_list_to_dict(target)\n",
    "        target = self.filter_invalid_boxes(target)\n",
    "        target = self.adjust_boxes_for_tile(target, grid_coordinates)\n",
    "\n",
    "        img_id = self.generate_cropped_filename(img_id, grid_coordinates)\n",
    "        fname = os.path.join(self.root, img_id)  # don't append .tif if it's already in img_id\n",
    "\n",
    "        print(f'opening image : {img_id}')\n",
    "\n",
    "        img = Image.open(fname).convert(\"RGB\")\n",
    "\n",
    "\n",
    "        if self.transform is not None :\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids) * 36 # (because we have 6x6 tiles per image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch import LightningDataModule\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class XviewDataModule(LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir, ann_file, batch_size=8, num_workers=4, img_transform=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.ann_file = ann_file\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.img_transform = img_transform or transforms.ToTensor()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise FileNotFoundError(f\"Data directory not found: {self.data_dir}\")\n",
    "        if not os.path.isfile(self.ann_file):\n",
    "            raise FileNotFoundError(f\"Annotation file not found: {self.ann_file}\")\n",
    "\n",
    "        self.xv_dataset = XviewDataset(\n",
    "            root=self.data_dir,\n",
    "            annFile=self.ann_file,\n",
    "            transform=self.img_transform\n",
    "        )\n",
    "\n",
    "    def detection_collate(self, batch):\n",
    "        \"\"\"\n",
    "        Collate function for TorchVision object detection datasets.\n",
    "        Returns:\n",
    "            images: Tensor [B,C,Hmax,Wmax]\n",
    "            targets: List[dict], each dict has 'boxes' [N,4] and 'labels' [N]\n",
    "        \"\"\"\n",
    "        images = [item[0] for item in batch]\n",
    "        annots_batch = [item[1] for item in batch]\n",
    "\n",
    "        \"\"\"         # pad images to max H,W\n",
    "        max_h = max(img.shape[1] for img in images)\n",
    "        max_w = max(img.shape[2] for img in images)\n",
    "        padded_images = []\n",
    "        for img in images:\n",
    "            c, h, w = img.shape\n",
    "            pad = (0, max_w - w, 0, max_h - h)  # left, right, top, bottom\n",
    "            padded_images.append(F.pad(img, pad, value=0.0))\n",
    "        images_tensor = torch.stack(padded_images, dim=0)  # [B,C,Hmax,Wmax] \"\"\"\n",
    "\n",
    "        images_tensor = torch.stack(images, dim=0)\n",
    "\n",
    "        # prepare targets\n",
    "        targets = []\n",
    "        #for annots in annots_batch:\n",
    "        #    boxes = torch.tensor([\n",
    "        #        list(map(float, obj['bbox'].split(','))) for obj in annots\n",
    "        #    ], dtype=torch.float32) if annots else torch.zeros((0, 4), dtype=torch.float32)\n",
    "#\n",
    " #           labels = torch.tensor([\n",
    "  #              int(obj['type_id']) for obj in annots\n",
    "   #         ], dtype=torch.int64) if annots else torch.zeros((0,), dtype=torch.int64)\n",
    "#\n",
    " #           targets.append({\n",
    "  #              'boxes': boxes,\n",
    "   #             'labels': labels\n",
    "    #        })\n",
    "\n",
    "\n",
    "        return { 'image': images_tensor, 'boxes': [d[\"boxes\"] for d in annots_batch], 'labels': [d[\"labels\"] for d in annots_batch] }\n",
    "\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.xv_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.detection_collate,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.xv_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.detection_collate,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.xv_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.detection_collate,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f89725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging import: 3.4332275390625e-05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectDetectionTask import: 117.55496048927307 seconds\n",
      "XviewDataModule import: 2.6702880859375e-05 seconds\n",
      "Trainer import: 4.410743713378906e-05 seconds\n",
      "torchvision.transforms import: 3.600120544433594e-05 seconds\n",
      "Total import time: 117.55548739433289 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "t0 = time.time()\n",
    "import logging\n",
    "print(\"logging import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "from terratorch.tasks import ObjectDetectionTask\n",
    "print(\"ObjectDetectionTask import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "#from terratorch.datamodules.xview import XviewDataModule\n",
    "print(\"XviewDataModule import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "from lightning.pytorch import Trainer\n",
    "print(\"Trainer import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "from torchvision import transforms\n",
    "print(\"torchvision.transforms import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "print(\"Total import time:\", time.time() - start_total, \"seconds\")\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ded29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/akihironitta/xView-PyTorch/refs/heads/master/datasets.py\n",
    "#!mv datasets.py ../../terratorch/datasets/xview.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d1fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dm = XviewDataModule(data_dir = '/dccstor/terratorch/users/rkie/data/xview/train_images_cropped', ann_file = '/dccstor/terratorch/users/rkie/data/xview/xView_train.geojson', img_transform = img_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2e14a",
   "metadata": {},
   "source": [
    "import json\n",
    "with open('/dccstor/terratorch/users/rkie/data/xview/xView_train.geojson') as f:\n",
    "    anns = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ba3dc",
   "metadata": {},
   "source": [
    "s = set()\n",
    "for i, f in enumerate(anns['features']):\n",
    "    s.add(f['properties']['image_id'])\n",
    "\n",
    "for setitem in s:\n",
    "    print(setitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dm.setup()\n",
    "\n",
    "#dl = dm.train_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99246bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a55e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d01e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item['boxes_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3879a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1802f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"framework\": \"faster-rcnn\",\n",
    "    \"backbone\": \"terramind_v1_tiny\",\n",
    "    \"backbone_pretrained\": True,\n",
    "    \"num_classes\": 100,\n",
    "    \"framework_min_size\": 512,\n",
    "    \"framework_max_size\": 512,\n",
    "    \"backbone_modalities\": [\"RGB\"],\n",
    "    \"in_channels\": 3,\n",
    "    \"necks\": [\n",
    "        {\n",
    "            \"name\": \"SelectIndices\",\n",
    "            \"indices\": [2, 5, 8, 11]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ReshapeTokensToImage\",\n",
    "            \"remove_cls_token\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"LearnedInterpolateToPyramidal\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"FeaturePyramidNetworkNeck\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "m = ObjectDetectionTask(model_factory='ObjectDetectionModelFactory', model_args=model_args,\n",
    "    lr=1e-4,\n",
    "    optimizer=\"AdamW\",\n",
    "    optimizer_hparams={\"weight_decay\": 0.05},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2fe55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db545147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import torch\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"odet-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caaa211",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=10,\n",
    "    default_root_dir='output',\n",
    "    log_every_n_steps=1,\n",
    "    check_val_every_n_epoch=1,\n",
    "    callbacks=[checkpoint_cb],\n",
    "    logger=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=m, datamodule=dm)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d601d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/terratorch/tasks/object_detection_task.py:86: UserWarning: The Object Detection Task has to be considered experimental. This is less mature than the other tasks and being further improved.\n",
      "  warnings.warn(\"The Object Detection Task has to be considered experimental. This is less mature than the other tasks and being further improved.\")\n"
     ]
    }
   ],
   "source": [
    "model = ObjectDetectionTask.load_from_checkpoint(\"./checkpoints/last-v6.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aec701",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://challenge.xviewdataset.org/static/example_unlabeled.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db694b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('../../../../data/xview/train_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc17752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from terratorch.models.object_detection_model_factory import ModelOutputObjectDetection\n",
    "import math\n",
    "\n",
    "def run_tiled_inference(model, img, tile_size=504, pad=21, score_thresh=0.5, device=\"cpu\", align_to_multiple=21):\n",
    "    \"\"\"Run inference on large images using tiling approach with proper alignment\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    img_tensor = F.to_tensor(img).to(device)\n",
    "    _, H_padded, W_padded = img_tensor.shape\n",
    "\n",
    "    all_boxes, all_scores, all_labels = [], [], []\n",
    "\n",
    "    # Calculate steps ensuring tiles overlap properly with padding\n",
    "    y_steps = list(range(0, 3072, 512))\n",
    "    x_steps = list(range(0, 3072, 512))\n",
    "\n",
    "    for y in y_steps:\n",
    "        for x in x_steps:\n",
    "           \n",
    "            # Extract tile\n",
    "            tile_tensor = img_tensor[:, y:y+512, x:x+512].unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output: ModelOutputObjectDetection = model(tile_tensor)\n",
    "\n",
    "            out_dict = output.output[0] if isinstance(output.output, list) else output.output\n",
    "            boxes = out_dict.get(\"boxes\", torch.empty(0, 4, device=device))\n",
    "            scores = out_dict.get(\"scores\", torch.empty(0, device=device))\n",
    "            labels = out_dict.get(\"labels\", torch.empty(0, dtype=torch.long, device=device))\n",
    "\n",
    "            if boxes.numel() > 0:\n",
    "                # Adjust box coordinates back to the original image space\n",
    "                boxes[:, [0, 2]] += x1\n",
    "                boxes[:, [1, 3]] += y1\n",
    "\n",
    "            all_boxes.append(boxes)\n",
    "            all_scores.append(scores)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    if all_boxes:\n",
    "        all_boxes = torch.cat(all_boxes)\n",
    "        all_scores = torch.cat(all_scores)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "    else:\n",
    "        all_boxes = torch.empty(0, 4)\n",
    "        all_scores = torch.empty(0)\n",
    "        all_labels = torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    keep = all_scores >= score_thresh\n",
    "    filtered_boxes = all_boxes[keep]\n",
    "    filtered_scores = all_scores[keep]\n",
    "    filtered_labels = all_labels[keep]\n",
    "    \n",
    "    return filtered_boxes, filtered_scores, filtered_labels\n",
    "\n",
    "# Load and process image\n",
    "image_name = files.pop()\n",
    "img_path = f\"../../../../data/xview/train_images/{image_name}\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img = img.crop((0, 0, 3072, 3072))\n",
    "\n",
    "# Run inference\n",
    "boxes, scores, labels = run_tiled_inference(\n",
    "    model, \n",
    "    img, \n",
    "    tile_size=512,\n",
    "    pad=6,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Print detected boxes\n",
    "print(\"\\nDetected Objects:\")\n",
    "print(\"----------------\")\n",
    "for i, (box, score, label) in enumerate(zip(boxes, scores, labels)):\n",
    "    x1, y1, x2, y2 = box.tolist()\n",
    "    print(f\"Object {i+1}:\")\n",
    "    print(f\"  Label: {label.item()}\")\n",
    "    print(f\"  Confidence: {score.item():.4f}\")\n",
    "    print(f\"  Bounding Box: [{x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f}]\")\n",
    "    print(f\"  Width: {x2-x1:.1f}px, Height: {y2-y1:.1f}px\")\n",
    "    print(\"----------------\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "ax.imshow(img)\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    x1, y1, x2, y2 = box.tolist()\n",
    "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x1, y1-5, f\"{label.item()}:{score:.2f}\", color='yellow', fontsize=12)\n",
    "plt.title(f\"Detected Objects: {len(boxes)}\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"../../../../data/xview/train_images/10.tif\"\n",
    "img = Image.open(img_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_img = img.crop((0, 0, 3072, 3072))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602aa8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d1252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.models.object_detection_model_factory import ModelOutputObjectDetection\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "class ONNXWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        out = self.model(*args, **kwargs)\n",
    "\n",
    "        # --- Handle ModelOutputObjectDetection ---\n",
    "        if isinstance(out, ModelOutputObjectDetection):\n",
    "            if isinstance(out.output, dict):\n",
    "                # Single image\n",
    "                boxes = out.output.get(\"boxes\", torch.empty(0, 4))\n",
    "                scores = out.output.get(\"scores\", torch.empty(0))\n",
    "                labels = out.output.get(\"labels\", torch.empty(0, dtype=torch.long))\n",
    "                return boxes, scores, labels\n",
    "\n",
    "            elif isinstance(out.output, list):\n",
    "                # Batch: flatten all boxes/scores/labels into single tensors\n",
    "                all_boxes, all_scores, all_labels = [], [], []\n",
    "                for item in out.output:\n",
    "                    all_boxes.append(item.get(\"boxes\", torch.empty(0, 4)))\n",
    "                    all_scores.append(item.get(\"scores\", torch.empty(0)))\n",
    "                    all_labels.append(item.get(\"labels\", torch.empty(0, dtype=torch.long)))\n",
    "\n",
    "                boxes = torch.cat(all_boxes, dim=0) if all_boxes else torch.empty(0, 4)\n",
    "                scores = torch.cat(all_scores, dim=0) if all_scores else torch.empty(0)\n",
    "                labels = torch.cat(all_labels, dim=0) if all_labels else torch.empty(0, dtype=torch.long)\n",
    "                return boxes, scores, labels\n",
    "\n",
    "        # --- If it's already a tensor ---\n",
    "        if isinstance(out, Tensor):\n",
    "            return out\n",
    "\n",
    "        # --- Fallback ---\n",
    "        raise TypeError(f\"Unsupported model output type: {type(out)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7596987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = ONNXWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61d5dbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ONNXWrapper(\n",
       "  (model): ObjectDetectionTask(\n",
       "    (model): ObjectDetectionModel(\n",
       "      (torchvision_model): FasterRCNN(\n",
       "        (transform): GeneralizedRCNNTransform(\n",
       "            Normalize(mean=[0 0 0], std=[1 1 1])\n",
       "            Resize(min_size=(512,), max_size=512, mode='bilinear')\n",
       "        )\n",
       "        (backbone): BackboneWrapper(\n",
       "          (backbone): TerraMindViT(\n",
       "            (encoder_embeddings): ModuleDict(\n",
       "              (untok_sen2rgb@224): ImageEncoderEmbedding(\n",
       "                (proj): Linear(in_features=768, out_features=192, bias=False)\n",
       "              )\n",
       "            )\n",
       "            (encoder): ModuleList(\n",
       "              (0-11): 12 x Block(\n",
       "                (norm1): LayerNorm()\n",
       "                (attn): Attention(\n",
       "                  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): LayerNorm()\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (encoder_norm): LayerNorm()\n",
       "          )\n",
       "          (necks): Sequential(\n",
       "            (0): SelectIndices()\n",
       "            (1): ReshapeTokensToImage()\n",
       "            (2): LearnedInterpolateToPyramidal(\n",
       "              (fpn1): Sequential(\n",
       "                (0): ConvTranspose2d(192, 96, kernel_size=(2, 2), stride=(2, 2))\n",
       "                (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): ConvTranspose2d(96, 48, kernel_size=(2, 2), stride=(2, 2))\n",
       "              )\n",
       "              (fpn2): Sequential(\n",
       "                (0): ConvTranspose2d(192, 96, kernel_size=(2, 2), stride=(2, 2))\n",
       "              )\n",
       "              (fpn3): Sequential(\n",
       "                (0): Identity()\n",
       "              )\n",
       "              (fpn4): Sequential(\n",
       "                (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "              )\n",
       "            )\n",
       "            (3): FeaturePyramidNetworkNeck(\n",
       "              (fpn): FeaturePyramidNetwork(\n",
       "                (inner_blocks): ModuleList(\n",
       "                  (0): Conv2dNormActivation(\n",
       "                    (0): Conv2d(48, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  )\n",
       "                  (1): Conv2dNormActivation(\n",
       "                    (0): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  )\n",
       "                  (2-3): 2 x Conv2dNormActivation(\n",
       "                    (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  )\n",
       "                )\n",
       "                (layer_blocks): ModuleList(\n",
       "                  (0-3): 4 x Conv2dNormActivation(\n",
       "                    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (rpn): RegionProposalNetwork(\n",
       "          (anchor_generator): AnchorGenerator()\n",
       "          (head): RPNHead(\n",
       "            (conv): Sequential(\n",
       "              (0): Conv2dNormActivation(\n",
       "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (1): ReLU(inplace=True)\n",
       "              )\n",
       "            )\n",
       "            (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (roi_heads): RoIHeads(\n",
       "          (box_roi_pool): MultiScaleRoIAlign(featmap_names=['feat0', 'feat1', 'feat2', 'feat3'], output_size=(7, 7), sampling_ratio=2)\n",
       "          (box_head): TwoMLPHead(\n",
       "            (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "            (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (box_predictor): FastRCNNPredictor(\n",
       "            (cls_score): Linear(in_features=1024, out_features=100, bias=True)\n",
       "            (bbox_pred): Linear(in_features=1024, out_features=400, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (train_metrics): MetricCollection(\n",
       "      (mAP): MeanAveragePrecision(),\n",
       "      prefix=train_\n",
       "    )\n",
       "    (val_metrics): MetricCollection(\n",
       "      (mAP): MeanAveragePrecision(),\n",
       "      prefix=val_\n",
       "    )\n",
       "    (test_metrics): MetricCollection(\n",
       "      (mAP): MeanAveragePrecision(),\n",
       "      prefix=test_\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9d2751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy_input = torch.randn(1, 3, 512,512)  # batch size 1, 10 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53844fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7804f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75658/2518839109.py:1: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/torch/nn/functional.py:4705: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  * torch.tensor(scale_factors[i], dtype=torch.float32)\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/terratorch/models/backbones/terramind/model/encoder_embeddings.py:327: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (H % self.patch_size[0] == 0) and (W % self.patch_size[1] == 0), f'Image sizes {H}x{W} must be divisible by patch sizes {self.patch_size[0]}x{self.patch_size[1]}'\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/terratorch/models/backbones/terramind/model/encoder_embeddings.py:332: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (H, W) != self.image_size:\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/terratorch/models/backbones/terramind/model/tm_utils.py:408: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  sqrt_num_positions = int(num_positions**0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "BackboneWrapper(\n",
      "  (backbone): TerraMindViT(\n",
      "    (encoder_embeddings): ModuleDict(\n",
      "      (untok_sen2rgb@224): ImageEncoderEmbedding(\n",
      "        (proj): Linear(in_features=768, out_features=192, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (encoder): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm()\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm()\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (encoder_norm): LayerNorm()\n",
      "  )\n",
      "  (necks): Sequential(\n",
      "    (0): SelectIndices()\n",
      "    (1): ReshapeTokensToImage()\n",
      "    (2): LearnedInterpolateToPyramidal(\n",
      "      (fpn1): Sequential(\n",
      "        (0): ConvTranspose2d(192, 96, kernel_size=(2, 2), stride=(2, 2))\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): ConvTranspose2d(96, 48, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (fpn2): Sequential(\n",
      "        (0): ConvTranspose2d(192, 96, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (fpn3): Sequential(\n",
      "        (0): Identity()\n",
      "      )\n",
      "      (fpn4): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "    )\n",
      "    (3): FeaturePyramidNetworkNeck(\n",
      "      (fpn): FeaturePyramidNetwork(\n",
      "        (inner_blocks): ModuleList(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(48, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2-3): 2 x Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (layer_blocks): ModuleList(\n",
      "          (0-3): 4 x Conv2dNormActivation(\n",
      "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/terratorch/models/necks.py:171: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  h = int(math.sqrt(tokens_per_timestep))\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/torchvision/ops/boxes.py:174: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_x = torch.min(boxes_x, torch.tensor(width, dtype=boxes.dtype, device=boxes.device))\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/torchvision/ops/boxes.py:176: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes_y = torch.min(boxes_y, torch.tensor(height, dtype=boxes.dtype, device=boxes.device))\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/torch/__init__.py:2174: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert condition, message\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/torchvision/models/detection/transform.py:308: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(s, dtype=torch.float32, device=boxes.device)\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/torchvision/models/detection/transform.py:309: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  / torch.tensor(s_orig, dtype=torch.float32, device=boxes.device)\n",
      "/dccstor/terratorch/users/rkie/gitco/terratorch/.venv/lib/python3.12/site-packages/torch/onnx/symbolic_opset9.py:5350: UserWarning: Exporting aten::index operator of advanced indexing in opset 17 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    onnx_model,              # the PyTorch model\n",
    "    dummy_input,              # dummy input\n",
    "    \"model.onnx\",             # output file     \n",
    "    export_params=True,       # store trained weights\n",
    "    opset_version=17,         # recommended ONNX opset\n",
    "    do_constant_folding=True, # optimize constants\n",
    "    input_names=['input'],    # model input name\n",
    "    output_names=['output'],  # model output name\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # allow variable batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample[0])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628830ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample[1])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4fd612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Path to your ONNX model\n",
    "onnx_path = \"model.onnx\"\n",
    "\n",
    "# --- Load the model to inspect input/output metadata ---\n",
    "model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Create inference session\n",
    "session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# Get input metadata\n",
    "input_meta = session.get_inputs()[0]\n",
    "input_name = input_meta.name\n",
    "input_shape = input_meta.shape  # may have symbolic dimensions\n",
    "\n",
    "# Replace symbolic dimensions with actual values where possible\n",
    "resolved_shape = [\n",
    "    dim if isinstance(dim, int) else 1 for dim in input_shape\n",
    "]\n",
    "\n",
    "# If the model expects fixed spatial dimensions, use them\n",
    "# Example: input_shape could be ['batch_size', 3, 512, 512]\n",
    "height = resolved_shape[2]\n",
    "width = resolved_shape[3]\n",
    "\n",
    "print(f\"Input name: {input_name}\")\n",
    "print(f\"Expected input shape: {input_shape} → resolved: {resolved_shape}\")\n",
    "\n",
    "# Create a dummy input with the correct shape\n",
    "dummy_input = np.random.randn(\n",
    "    resolved_shape[0],  # batch\n",
    "    resolved_shape[1],  # channels\n",
    "    height,\n",
    "    width\n",
    ").astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "outputs = session.run(None, {input_name: dummy_input})\n",
    "\n",
    "# Print output names and shapes\n",
    "for name, arr in zip([o.name for o in session.get_outputs()], outputs):\n",
    "    print(f\"Runtime output '{name}': shape={arr.shape}, dtype={arr.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7b8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2502c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42905344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
