{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e196a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -e ../../\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4263d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2020 Akihiro Nitta\n",
    "# All rights reserved.\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import json\n",
    "import collections\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "LABEL_TO_STRING = {\n",
    "    11: \"Fixed-wing Aircraft\",\n",
    "    12: \"Small Aircraft\",\n",
    "    13: \"Passenger/Cargo Plane\",\n",
    "    15: \"Helicopter\",\n",
    "    17: \"Passenger Vehicle\",\n",
    "    18: \"Small Car\",\n",
    "    19: \"Bus\",\n",
    "    20: \"Pickup Truck\",\n",
    "    21: \"Utility Truck\",\n",
    "    23: \"Truck\",\n",
    "    24: \"Cargo Truck\",\n",
    "    25: \"Truck Tractor w/ Box Trailer\",\n",
    "    26: \"Truck Tractor\",\n",
    "    27: \"Trailer\",\n",
    "    28: \"Truck Tractor w/ Flatbed Trailer\",\n",
    "    29: \"Truck Tractor w/ Liquid Tank\",\n",
    "    32: \"Crane Truck\",\n",
    "    33: \"Railway Vehicle\",\n",
    "    34: \"Passenger Car\",\n",
    "    35: \"Cargo/Container Car\",\n",
    "    36: \"Flat Car\",\n",
    "    37: \"Tank car\",\n",
    "    38: \"Locomotive\",\n",
    "    40: \"Maritime Vessel\",\n",
    "    41: \"Motorboat\",\n",
    "    42: \"Sailboat\",\n",
    "    44: \"Tugboat\",\n",
    "    45: \"Barge\",\n",
    "    47: \"Fishing Vessel\",\n",
    "    49: \"Ferry\",\n",
    "    50: \"Yacht\",\n",
    "    51: \"Container Ship\",\n",
    "    52: \"Oil Tanker\",\n",
    "    53: \"Engineering Vehicle\",\n",
    "    54: \"Tower crane\",\n",
    "    55: \"Container Crane\",\n",
    "    56: \"Reach Stacker\",\n",
    "    57: \"Straddle Carrier\",\n",
    "    59: \"Mobile Crane\",\n",
    "    60: \"Dump Truck\",\n",
    "    61: \"Haul Truck\",\n",
    "    62: \"Scraper/Tractor\",\n",
    "    63: \"Front loader/Bulldozer\",\n",
    "    64: \"Excavator\",\n",
    "    65: \"Cement Mixer\",\n",
    "    66: \"Ground Grader\",\n",
    "    71: \"Hut/Tent\",\n",
    "    72: \"Shed\",\n",
    "    73: \"Building\",\n",
    "    74: \"Aircraft Hangar\",\n",
    "    76: \"Damaged Building\",\n",
    "    77: \"Facility\",\n",
    "    79: \"Construction Site\",\n",
    "    83: \"Vehicle Lot\",\n",
    "    84: \"Helipad\",\n",
    "    86: \"Storage Tank\",\n",
    "    89: \"Shipping container lot\",\n",
    "    91: \"Shipping Container\",\n",
    "    93: \"Pylon\",\n",
    "    94: \"Tower\",\n",
    "}\n",
    "\n",
    "class XviewDataset(VisionDataset):\n",
    "    \"\"\"xView object detection dataset.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (string): Directory with all the images.\n",
    "        annFile (string): Path to json annotation file.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
    "            and returns a transformed version.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root, annFile, transform=None):\n",
    "        import os, json, collections\n",
    "\n",
    "        self.root = root\n",
    "        self.annFile = annFile\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load annotations\n",
    "        with open(self.annFile) as f:\n",
    "            anns = json.load(f)\n",
    "\n",
    "        self.objects = {}  # object_id -> object\n",
    "        self.ids = set()\n",
    "\n",
    "        for idx, feat in enumerate(anns['features']):\n",
    "            obj_id = idx\n",
    "            image_id = feat['properties']['image_id']\n",
    "\n",
    "            base, ext = os.path.splitext(image_id)\n",
    "            img_path = os.path.join(self.root, f\"{base}_1_1{ext}\") # check if name_1_1.tif is there, therefore we assume name.tif was \n",
    "            # present in the original dataset and also name_6_6.tif should be there\n",
    "            if not os.path.exists(img_path):\n",
    "                logging.info(f'{img_path} skipped because it was not found')\n",
    "                continue\n",
    "\n",
    "            self.ids.add(image_id)\n",
    "            self.objects[obj_id] = {\n",
    "                \"image_id\": image_id,\n",
    "                \"bbox\": feat['properties']['bounds_imcoords'],\n",
    "                \"type_id\": feat['properties']['type_id']\n",
    "            }\n",
    "\n",
    "        # Create mapping from image_id to all object_ids\n",
    "        self.image_id_to_object_ids = collections.defaultdict(list)\n",
    "        for obj_id, obj in self.objects.items():\n",
    "            self.image_id_to_object_ids[obj[\"image_id\"]].append(obj_id)\n",
    "\n",
    "        # Keep ids only for images that still exist\n",
    "        self.ids = list(self.ids)\n",
    "\n",
    "\n",
    "    def filter_invalid_boxes(self, target):\n",
    "        boxes = target[\"boxes\"]\n",
    "        # keep only boxes with strictly positive width & height\n",
    "        keep = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n",
    "        target[\"boxes\"] = boxes[keep]\n",
    "        if \"labels\" in target:\n",
    "            target[\"labels\"] = target[\"labels\"][keep]\n",
    "        return target\n",
    "\n",
    "    def adjust_boxes_for_tile(self, target, grid_coordinates, tile_size=512):\n",
    "        \"\"\"\n",
    "        Adjusts bounding box coordinates to a tile's local coordinate system.\n",
    "\n",
    "        Args:\n",
    "            target (dict): A dictionary containing 'boxes' and 'labels' tensors.\n",
    "            row (int): The 1-based row index of the tile (e.g., 1 to 6).\n",
    "            col (int): The 1-based column index of the tile (e.g., 1 to 6).\n",
    "            tile_size (int): The size of each tile in pixels (default is 512).\n",
    "\n",
    "        Returns:\n",
    "            dict: The updated target dictionary with boxes adjusted for the tile.\n",
    "        \"\"\"\n",
    "        if not isinstance(target, dict) or \"boxes\" not in target:\n",
    "            raise RuntimeError()\n",
    "\n",
    "        boxes = target[\"boxes\"]\n",
    "\n",
    "        # Calculate the pixel offset for the current grid cell\n",
    "        x_offset = (grid_coordinates[1] - 1) * tile_size\n",
    "        y_offset = (grid_coordinates[0] - 1) * tile_size\n",
    "        \n",
    "        # Create the offset tensor\n",
    "        offset_tensor = torch.tensor([x_offset, y_offset, x_offset, y_offset], dtype=boxes.dtype, device=boxes.device)\n",
    "        \n",
    "        # Subtract the offset from the bounding box coordinates\n",
    "        target[\"boxes\"] = boxes - offset_tensor\n",
    "        \n",
    "        return target\n",
    "\n",
    "\n",
    "    def target_list_to_dict(self, target_list):\n",
    "        if len(target_list) == 0:\n",
    "            return {\n",
    "                \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n",
    "                \"labels\": torch.zeros((0,), dtype=torch.int64)\n",
    "            }\n",
    "        boxes = torch.tensor([list(map(float, t['bbox'].split(','))) for t in target_list], dtype=torch.float32)\n",
    "        labels = torch.tensor([t['type_id'] for t in target_list], dtype=torch.int64)\n",
    "        return {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "\n",
    "    def get_grid_coordinates(self, index, grid_size=6):\n",
    "        \"\"\"\n",
    "        Maps a sequential index to (row, col) coordinates for a square grid.\n",
    "        \n",
    "        Args:\n",
    "            index (int): The sequential index (0 to 35).\n",
    "            grid_size (int): The size of the grid (e.g., 6 for 6x6).\n",
    "            \n",
    "        Returns:\n",
    "            tuple: A tuple (row_id, col_id) where IDs are 1-based.\n",
    "        \"\"\"\n",
    "        if not 0 <= index < grid_size * grid_size:\n",
    "            raise ValueError(f\"Index must be between 0 and {grid_size*grid_size - 1}.\")\n",
    "        \n",
    "        row_id = index // grid_size + 1\n",
    "        col_id = index % grid_size + 1\n",
    "        \n",
    "        return row_id, col_id\n",
    "\n",
    "    def generate_cropped_filename(self, original_img_id, grid_coords):\n",
    "        \"\"\"\n",
    "        Generates a new filename for a cropped tile based on the original image ID\n",
    "        and the grid coordinates.\n",
    "\n",
    "        Args:\n",
    "            original_img_id (str): The original image filename (e.g., 'xyz.tif').\n",
    "            grid_coords (tuple): A tuple (row_id, col_id) where IDs are 1-based.\n",
    "\n",
    "        Returns:\n",
    "            str: The new filename (e.g., 'xyz_1_1.tif').\n",
    "        \"\"\"\n",
    "        # Split the filename into the base name and extension\n",
    "        base_name, ext = os.path.splitext(original_img_id)\n",
    "\n",
    "        # Unpack the row and column IDs from the tuple\n",
    "        row_id, col_id = grid_coords\n",
    "\n",
    "        # Create the new filename with the specified pattern\n",
    "        new_filename = f\"{base_name}_{row_id}_{col_id}{ext}\"\n",
    "\n",
    "        return new_filename\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[math.floor(index/36)]\n",
    "        ann_ids = self.image_id_to_object_ids[img_id]\n",
    "        target = [self.objects[ann_id] for ann_id in ann_ids]\n",
    "\n",
    "        grid_coordinates = self.get_grid_coordinates(math.floor(index/36)%36)\n",
    "\n",
    "        target = self.target_list_to_dict(target)\n",
    "        target = self.filter_invalid_boxes(target)\n",
    "        target = self.adjust_boxes_for_tile(target, grid_coordinates)\n",
    "\n",
    "        img_id = self.generate_cropped_filename(img_id, grid_coordinates)\n",
    "        fname = os.path.join(self.root, img_id)  # don't append .tif if it's already in img_id\n",
    "        img = Image.open(fname).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None :\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids) * 36 # (because we have 6x6 tiles per image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch import LightningDataModule\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class XviewDataModule(LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir, ann_file, batch_size=8, num_workers=4, img_transform=None):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.ann_file = ann_file\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.img_transform = img_transform or transforms.ToTensor()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise FileNotFoundError(f\"Data directory not found: {self.data_dir}\")\n",
    "        if not os.path.isfile(self.ann_file):\n",
    "            raise FileNotFoundError(f\"Annotation file not found: {self.ann_file}\")\n",
    "\n",
    "        self.xv_dataset = XviewDataset(\n",
    "            root=self.data_dir,\n",
    "            annFile=self.ann_file,\n",
    "            transform=self.img_transform\n",
    "        )\n",
    "\n",
    "    def detection_collate(self, batch):\n",
    "        \"\"\"\n",
    "        Collate function for TorchVision object detection datasets.\n",
    "        Returns:\n",
    "            images: Tensor [B,C,Hmax,Wmax]\n",
    "            targets: List[dict], each dict has 'boxes' [N,4] and 'labels' [N]\n",
    "        \"\"\"\n",
    "        images = [item[0] for item in batch]\n",
    "        annots_batch = [item[1] for item in batch]\n",
    "\n",
    "        \"\"\"         # pad images to max H,W\n",
    "        max_h = max(img.shape[1] for img in images)\n",
    "        max_w = max(img.shape[2] for img in images)\n",
    "        padded_images = []\n",
    "        for img in images:\n",
    "            c, h, w = img.shape\n",
    "            pad = (0, max_w - w, 0, max_h - h)  # left, right, top, bottom\n",
    "            padded_images.append(F.pad(img, pad, value=0.0))\n",
    "        images_tensor = torch.stack(padded_images, dim=0)  # [B,C,Hmax,Wmax] \"\"\"\n",
    "\n",
    "        images_tensor = torch.stack(images, dim=0)\n",
    "\n",
    "        # prepare targets\n",
    "        targets = []\n",
    "        #for annots in annots_batch:\n",
    "        #    boxes = torch.tensor([\n",
    "        #        list(map(float, obj['bbox'].split(','))) for obj in annots\n",
    "        #    ], dtype=torch.float32) if annots else torch.zeros((0, 4), dtype=torch.float32)\n",
    "#\n",
    " #           labels = torch.tensor([\n",
    "  #              int(obj['type_id']) for obj in annots\n",
    "   #         ], dtype=torch.int64) if annots else torch.zeros((0,), dtype=torch.int64)\n",
    "#\n",
    " #           targets.append({\n",
    "  #              'boxes': boxes,\n",
    "   #             'labels': labels\n",
    "    #        })\n",
    "\n",
    "\n",
    "        return { 'image': images_tensor, 'boxes': [d[\"boxes\"] for d in annots_batch], 'labels': [d[\"labels\"] for d in annots_batch] }\n",
    "\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.xv_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.detection_collate,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.xv_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.detection_collate,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.xv_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.detection_collate,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f89725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging import: 3.314018249511719e-05 seconds\n",
      "ObjectDetectionTask import: 33.487263917922974 seconds\n",
      "XviewDataModule import: 2.6941299438476562e-05 seconds\n",
      "Trainer import: 4.482269287109375e-05 seconds\n",
      "torchvision.transforms import: 3.790855407714844e-05 seconds\n",
      "Total import time: 33.48777937889099 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "t0 = time.time()\n",
    "import logging\n",
    "print(\"logging import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "from terratorch.tasks import ObjectDetectionTask\n",
    "print(\"ObjectDetectionTask import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "#from terratorch.datamodules.xview import XviewDataModule\n",
    "print(\"XviewDataModule import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "from lightning.pytorch import Trainer\n",
    "print(\"Trainer import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "t0 = time.time()\n",
    "from torchvision import transforms\n",
    "print(\"torchvision.transforms import:\", time.time() - t0, \"seconds\")\n",
    "\n",
    "print(\"Total import time:\", time.time() - start_total, \"seconds\")\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ded29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/akihironitta/xView-PyTorch/refs/heads/master/datasets.py\n",
    "#!mv datasets.py ../../terratorch/datasets/xview.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d1fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dm = XviewDataModule(data_dir = '/dccstor/terratorch/users/rkie/data/xview/train_images_cropped', ann_file = '/dccstor/terratorch/users/rkie/data/xview/xView_train.geojson', img_transform = img_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2e14a",
   "metadata": {},
   "source": [
    "import json\n",
    "with open('/dccstor/terratorch/users/rkie/data/xview/xView_train.geojson') as f:\n",
    "    anns = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ba3dc",
   "metadata": {},
   "source": [
    "s = set()\n",
    "for i, f in enumerate(anns['features']):\n",
    "    s.add(f['properties']['image_id'])\n",
    "\n",
    "for setitem in s:\n",
    "    print(setitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dm.setup()\n",
    "\n",
    "#dl = dm.train_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99246bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a55e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d01e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#item['boxes_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3879a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1802f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"framework\": \"faster-rcnn\",\n",
    "    \"backbone\": \"terramind_v1_tiny\",\n",
    "    \"backbone_pretrained\": True,\n",
    "    \"num_classes\": 100,\n",
    "    \"framework_min_size\": 512,\n",
    "    \"framework_max_size\": 512,\n",
    "    \"backbone_modalities\": [\"RGB\"],\n",
    "    \"in_channels\": 3,\n",
    "    \"necks\": [\n",
    "        {\n",
    "            \"name\": \"SelectIndices\",\n",
    "            \"indices\": [2, 5, 8, 11]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ReshapeTokensToImage\",\n",
    "            \"remove_cls_token\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"LearnedInterpolateToPyramidal\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"FeaturePyramidNetworkNeck\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "m = ObjectDetectionTask(model_factory='ObjectDetectionModelFactory', model_args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2fe55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db545147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import torch\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"odet-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caaa211",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    devices=1,\n",
    "    max_epochs=100,\n",
    "    default_root_dir='output',\n",
    "    log_every_n_steps=1,\n",
    "    check_val_every_n_epoch=1,\n",
    "    callbacks=[checkpoint_cb],\n",
    "    logger=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model=m, datamodule=dm)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d601d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ObjectDetectionTask.load_from_checkpoint(\"./checkpoints/last-v3.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aec701",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://challenge.xviewdataset.org/static/example_unlabeled.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db694b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('../../../../data/xview/train_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc17752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from terratorch.models.object_detection_model_factory import ModelOutputObjectDetection\n",
    "import math\n",
    "\n",
    "def run_tiled_inference(model, img, tile_size=504, pad=21, score_thresh=0.5, device=\"cpu\", align_to_multiple=21):\n",
    "    \"\"\"Run inference on large images using tiling approach with proper alignment\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    img_tensor = F.to_tensor(img).to(device)\n",
    "    _, H_padded, W_padded = img_tensor.shape\n",
    "\n",
    "    all_boxes, all_scores, all_labels = [], [], []\n",
    "\n",
    "    # Calculate steps ensuring tiles overlap properly with padding\n",
    "    y_steps = list(range(0, 3072, 512))\n",
    "    x_steps = list(range(0, 3072, 512))\n",
    "\n",
    "    for y in y_steps:\n",
    "        for x in x_steps:\n",
    "           \n",
    "            # Extract tile\n",
    "            tile_tensor = img_tensor[:, y:y+512, x:x+512].unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output: ModelOutputObjectDetection = model(tile_tensor)\n",
    "\n",
    "            out_dict = output.output[0] if isinstance(output.output, list) else output.output\n",
    "            boxes = out_dict.get(\"boxes\", torch.empty(0, 4, device=device))\n",
    "            scores = out_dict.get(\"scores\", torch.empty(0, device=device))\n",
    "            labels = out_dict.get(\"labels\", torch.empty(0, dtype=torch.long, device=device))\n",
    "\n",
    "            if boxes.numel() > 0:\n",
    "                # Adjust box coordinates back to the original image space\n",
    "                boxes[:, [0, 2]] += x1\n",
    "                boxes[:, [1, 3]] += y1\n",
    "\n",
    "            all_boxes.append(boxes)\n",
    "            all_scores.append(scores)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    if all_boxes:\n",
    "        all_boxes = torch.cat(all_boxes)\n",
    "        all_scores = torch.cat(all_scores)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "    else:\n",
    "        all_boxes = torch.empty(0, 4)\n",
    "        all_scores = torch.empty(0)\n",
    "        all_labels = torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    keep = all_scores >= score_thresh\n",
    "    filtered_boxes = all_boxes[keep]\n",
    "    filtered_scores = all_scores[keep]\n",
    "    filtered_labels = all_labels[keep]\n",
    "    \n",
    "    return filtered_boxes, filtered_scores, filtered_labels\n",
    "\n",
    "# Load and process image\n",
    "image_name = files.pop()\n",
    "img_path = f\"../../../../data/xview/train_images/{image_name}\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img = img.crop((0, 0, 3072, 3072))\n",
    "\n",
    "# Run inference\n",
    "boxes, scores, labels = run_tiled_inference(\n",
    "    model, \n",
    "    img, \n",
    "    tile_size=512,\n",
    "    pad=6,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Print detected boxes\n",
    "print(\"\\nDetected Objects:\")\n",
    "print(\"----------------\")\n",
    "for i, (box, score, label) in enumerate(zip(boxes, scores, labels)):\n",
    "    x1, y1, x2, y2 = box.tolist()\n",
    "    print(f\"Object {i+1}:\")\n",
    "    print(f\"  Label: {label.item()}\")\n",
    "    print(f\"  Confidence: {score.item():.4f}\")\n",
    "    print(f\"  Bounding Box: [{x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f}]\")\n",
    "    print(f\"  Width: {x2-x1:.1f}px, Height: {y2-y1:.1f}px\")\n",
    "    print(\"----------------\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "ax.imshow(img)\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    x1, y1, x2, y2 = box.tolist()\n",
    "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x1, y1-5, f\"{label.item()}:{score:.2f}\", color='yellow', fontsize=12)\n",
    "plt.title(f\"Detected Objects: {len(boxes)}\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7095ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"../../../../data/xview/train_images/10.tif\"\n",
    "img = Image.open(img_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_img = img.crop((0, 0, 3072, 3072))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602aa8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from terratorch.models.object_detection_model_factory import ModelOutputObjectDetection\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "class ONNXWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        out = self.model(*args, **kwargs)\n",
    "\n",
    "        # --- Handle ModelOutputObjectDetection ---\n",
    "        if isinstance(out, ModelOutputObjectDetection):\n",
    "            if isinstance(out.output, dict):\n",
    "                # Single image\n",
    "                boxes = out.output.get(\"boxes\", torch.empty(0, 4))\n",
    "                scores = out.output.get(\"scores\", torch.empty(0))\n",
    "                labels = out.output.get(\"labels\", torch.empty(0, dtype=torch.long))\n",
    "                return boxes, scores, labels\n",
    "\n",
    "            elif isinstance(out.output, list):\n",
    "                # Batch: flatten all boxes/scores/labels into single tensors\n",
    "                all_boxes, all_scores, all_labels = [], [], []\n",
    "                for item in out.output:\n",
    "                    all_boxes.append(item.get(\"boxes\", torch.empty(0, 4)))\n",
    "                    all_scores.append(item.get(\"scores\", torch.empty(0)))\n",
    "                    all_labels.append(item.get(\"labels\", torch.empty(0, dtype=torch.long)))\n",
    "\n",
    "                boxes = torch.cat(all_boxes, dim=0) if all_boxes else torch.empty(0, 4)\n",
    "                scores = torch.cat(all_scores, dim=0) if all_scores else torch.empty(0)\n",
    "                labels = torch.cat(all_labels, dim=0) if all_labels else torch.empty(0, dtype=torch.long)\n",
    "                return boxes, scores, labels\n",
    "\n",
    "        # --- If it's already a tensor ---\n",
    "        if isinstance(out, Tensor):\n",
    "            return out\n",
    "\n",
    "        # --- Fallback ---\n",
    "        raise TypeError(f\"Unsupported model output type: {type(out)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = ONNXWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d5dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy_input = torch.randn(1, 3, 512,512)  # batch size 1, 10 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53844fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7804f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    onnx_model,              # the PyTorch model\n",
    "    dummy_input,              # dummy input\n",
    "    \"model.onnx\",             # output file     \n",
    "    export_params=True,       # store trained weights\n",
    "    opset_version=17,         # recommended ONNX opset\n",
    "    do_constant_folding=True, # optimize constants\n",
    "    input_names=['input'],    # model input name\n",
    "    output_names=['output'],  # model output name\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # allow variable batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f4f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample[0])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628830ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample[1])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4fd612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Path to your ONNX model\n",
    "onnx_path = \"model.onnx\"\n",
    "\n",
    "# --- Load the model to inspect input/output metadata ---\n",
    "model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Create inference session\n",
    "session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# Get input metadata\n",
    "input_meta = session.get_inputs()[0]\n",
    "input_name = input_meta.name\n",
    "input_shape = input_meta.shape  # may have symbolic dimensions\n",
    "\n",
    "# Replace symbolic dimensions with actual values where possible\n",
    "resolved_shape = [\n",
    "    dim if isinstance(dim, int) else 1 for dim in input_shape\n",
    "]\n",
    "\n",
    "# If the model expects fixed spatial dimensions, use them\n",
    "# Example: input_shape could be ['batch_size', 3, 512, 512]\n",
    "height = resolved_shape[2]\n",
    "width = resolved_shape[3]\n",
    "\n",
    "print(f\"Input name: {input_name}\")\n",
    "print(f\"Expected input shape: {input_shape} â†’ resolved: {resolved_shape}\")\n",
    "\n",
    "# Create a dummy input with the correct shape\n",
    "dummy_input = np.random.randn(\n",
    "    resolved_shape[0],  # batch\n",
    "    resolved_shape[1],  # channels\n",
    "    height,\n",
    "    width\n",
    ").astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "outputs = session.run(None, {input_name: dummy_input})\n",
    "\n",
    "# Print output names and shapes\n",
    "for name, arr in zip([o.name for o in session.get_outputs()], outputs):\n",
    "    print(f\"Runtime output '{name}': shape={arr.shape}, dtype={arr.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7b8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2502c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42905344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
