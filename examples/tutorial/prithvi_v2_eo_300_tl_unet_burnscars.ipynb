{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "1. In colab: Go to \"Runtime\" -> \"Change runtime type\" -> Select \"T4 GPU\"\n",
    "2. Install TerraTorch"
   ],
   "id": "b4bacc318390456b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install terratorch==0.99.8 gdown tensorboard",
   "id": "W_4z81Fn9RET",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a",
   "metadata": {
    "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a"
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import gdown\n",
    "import terratorch\n",
    "import albumentations\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from terratorch.datamodules import GenericNonGeoSegmentationDataModule"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. Download the dataset from Google Drive",
   "id": "917b65b8e7cd7d65"
  },
  {
   "cell_type": "code",
   "source": [
    "if not os.path.isfile('hls_burn_scars.tar.gz'):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1qKpDZoY_-8RA6sm6uM6xi5x6ogkyCXki\")\n",
    "    !tar -xzvf hls_burn_scars.tar.gz"
   ],
   "metadata": {
    "id": "dw5-9A4A4OmI",
    "collapsed": true
   },
   "id": "dw5-9A4A4OmI",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494",
   "metadata": {
    "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494"
   },
   "source": [
    "## HLS Burn Scars Dataset\n",
    "\n",
    "Lets start with analyzing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "e3854bdb-17a4-43c8-bfa8-822b44fd59c3",
   "metadata": {
    "id": "e3854bdb-17a4-43c8-bfa8-822b44fd59c3"
   },
   "source": [
    "dataset_path = Path('hls_burn_scars')\n",
    "!ls \"hls_burn_scars/\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!ls \"hls_burn_scars/data/\" | head",
   "id": "c84969a1f8bcae68",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc",
   "metadata": {
    "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc"
   },
   "source": [
    "datamodule = terratorch.datamodules.GenericNonGeoSegmentationDataModule(\n",
    "    batch_size=8,\n",
    "    num_workers=2,\n",
    "    num_classes=2,\n",
    "\n",
    "    # Define dataset paths \n",
    "    train_data_root=dataset_path / 'data/',\n",
    "    train_label_data_root=dataset_path / 'data/',\n",
    "    val_data_root=dataset_path / 'data/',\n",
    "    val_label_data_root=dataset_path / 'data/',\n",
    "    test_data_root=dataset_path / 'data/',\n",
    "    test_label_data_root=dataset_path / 'data/',\n",
    "\n",
    "    # Define splits\n",
    "    train_split=dataset_path / 'splits/train.txt',\n",
    "    val_split=dataset_path / 'splits/val.txt',\n",
    "    test_split=dataset_path / 'splits/test.txt',\n",
    "    \n",
    "    img_grep='*_merged.tif',\n",
    "    label_grep='*.mask.tif',\n",
    "    \n",
    "    train_transform=[\n",
    "        albumentations.D4(), # Random flips and rotation\n",
    "        albumentations.pytorch.transforms.ToTensorV2(),\n",
    "    ],\n",
    "    val_transform=None,  # Using ToTensor() by default\n",
    "    test_transform=None,\n",
    "        \n",
    "    # Define standardization values\n",
    "    means=[\n",
    "      0.0333497067415863,\n",
    "      0.0570118552053618,\n",
    "      0.0588974813200132,\n",
    "      0.2323245113436119,\n",
    "      0.1972854853760658,\n",
    "      0.1194491422518656,\n",
    "    ],\n",
    "    stds=[\n",
    "      0.0226913556882377,\n",
    "      0.0268075602230702,\n",
    "      0.0400410984436278,\n",
    "      0.0779173242367269,\n",
    "      0.0870873883814014,\n",
    "      0.0724197947743781,\n",
    "    ],\n",
    "    # We use all six bands of the data, so we don't need to define dataset_bands and output_bands.\n",
    ")\n",
    "\n",
    "# Setup train and val datasets\n",
    "datamodule.setup(\"fit\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# checking datasets train split size\n",
    "train_dataset = datamodule.train_dataset\n",
    "len(train_dataset)"
   ],
   "id": "08644e71-d82f-426c-b0c1-79026fccb578",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# checking datasets validation split size\n",
    "val_dataset = datamodule.val_dataset\n",
    "len(val_dataset)"
   ],
   "id": "b7062ddc-a3b7-4378-898c-41abcdf2ee3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plotting a few samples\n",
    "val_dataset.plot(val_dataset[0])\n",
    "val_dataset.plot(val_dataset[6])\n",
    "val_dataset.plot(val_dataset[10])"
   ],
   "id": "5bc29b1698dc4149",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79",
   "metadata": {
    "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79"
   },
   "source": [
    "# checking datasets testing split size\n",
    "datamodule.setup(\"test\")\n",
    "test_dataset = datamodule.test_dataset\n",
    "len(test_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-tune Prithvi",
   "id": "654a30ddef8ed5a"
  },
  {
   "cell_type": "code",
   "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
   "metadata": {
    "scrolled": true,
    "id": "ae69d39a-857a-4392-b058-0f4b518edf6e"
   },
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=\"output/burnscars/checkpoints/\",\n",
    "    mode=\"max\",\n",
    "    monitor=\"val/Multiclass_Jaccard_Index\", # Variable to monitor\n",
    "    filename=\"best-{epoch:02d}\",\n",
    ")\n",
    "\n",
    "# Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=1, # Deactivate multi-gpu because it often fails in notebooks\n",
    "    precision='16-mixed',  # Speed up training\n",
    "    num_nodes=1,\n",
    "    logger=True,  # Uses TensorBoard by default\n",
    "    max_epochs=5, # For demos\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n",
    "    default_root_dir=\"output/burnscars\",\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = terratorch.tasks.SemanticSegmentationTask(\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    "    model_args={\n",
    "        # Backbone\n",
    "        \"backbone\": \"prithvi_eo_v2_300_tl\", # Model can be either prithvi_eo_v1_100, prithvi_eo_v2_300, prithvi_eo_v2_300_tl, prithvi_eo_v2_600, prithvi_eo_v2_600_tl\n",
    "        \"backbone_pretrained\": True,\n",
    "        \"backbone_num_frames\": 1, # 1 is the default value\n",
    "        \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_NARROW\", \"SWIR_1\", \"SWIR_2\"],\n",
    "        \"backbone_coords_encoding\": [], # use [\"time\", \"location\"] for time and location metadata\n",
    "        \n",
    "        # Necks \n",
    "        \"necks\": [\n",
    "            {\n",
    "                \"name\": \"SelectIndices\",\n",
    "                # \"indices\": [2, 5, 8, 11] # indices for prithvi_eo_v1_100\n",
    "                \"indices\": [5, 11, 17, 23] # indices for prithvi_eo_v2_300\n",
    "                # \"indices\": [7, 15, 23, 31] # indices for prithvi_eo_v2_600\n",
    "            },\n",
    "            {\"name\": \"ReshapeTokensToImage\",},\n",
    "            {\"name\": \"LearnedInterpolateToPyramidal\"}            \n",
    "        ],\n",
    "        \n",
    "        # Decoder\n",
    "        \"decoder\": \"UNetDecoder\",\n",
    "        \"decoder_channels\": [512, 256, 128, 64],\n",
    "        \n",
    "        # Head\n",
    "        \"head_dropout\": 0.1,\n",
    "        \"num_classes\": 2,\n",
    "    },\n",
    "    \n",
    "    loss=\"dice\",\n",
    "    optimizer=\"AdamW\",\n",
    "    lr=1e-4,\n",
    "    ignore_index=-1,\n",
    "    freeze_backbone=True, # Only to speed up fine-tuning\n",
    "    freeze_decoder=False,\n",
    "    plot_on_val=True,\n",
    "    class_names=['no burned', 'burned']  # optionally define class names\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output"
   ],
   "id": "dbe0ec00057c56dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training\n",
    "trainer.fit(model, datamodule=datamodule)"
   ],
   "id": "ff284062edfce308",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_ckpt_path = \"output/burnscars/checkpoints/best-epoch=01.ckpt\"",
   "id": "a0d51d08802df9e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35a77263-5308-4781-a17f-a35e62ca1875",
   "metadata": {
    "scrolled": true,
    "id": "35a77263-5308-4781-a17f-a35e62ca1875"
   },
   "source": "trainer.test(model, datamodule=datamodule, ckpt_path=best_ckpt_path)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e015fe0-88ee-46cf-b972-f8cb9d361536",
   "metadata": {
    "id": "1e015fe0-88ee-46cf-b972-f8cb9d361536",
    "outputId": "c7c06228-e634-4608-cb0e-617d003fea46",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# now we can use the model for predictions and plotting!\n",
    "model = terratorch.tasks.SemanticSegmentationTask.load_from_checkpoint(\n",
    "    best_ckpt_path,\n",
    "    model_factory=model.hparams.model_factory,\n",
    "    model_args=model.hparams.model_args,\n",
    ")\n",
    "\n",
    "test_loader = datamodule.test_dataloader()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(test_loader))\n",
    "    images = batch[\"image\"].to(model.device)\n",
    "    masks = batch[\"mask\"].numpy()\n",
    "\n",
    "    outputs = model(images)\n",
    "    preds = torch.argmax(outputs.output, dim=1).cpu().numpy()\n",
    "\n",
    "for i in range(5):\n",
    "    sample = {key: batch[key][i] for key in batch}\n",
    "    sample[\"prediction\"] = preds[i]\n",
    "    test_dataset.plot(sample)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning via CLI\n",
    "\n",
    "You might want to restart the session to free up GPU memory."
   ],
   "id": "54779ae510fefc8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download config\n",
    "!wget https://raw.githubusercontent.com/ibm/TerraTorch/refs/heads/main/examples/tutorial/configs/prithvi_v2_eo_300_tl_unet_burnscars.yaml"
   ],
   "id": "ef479ca99891c730",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run fine-tuning\n",
    "!terratorch fit -c prithvi_v2_eo_300_tl_unet_burnscars.yaml"
   ],
   "id": "25e8e91384bb8ebe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
