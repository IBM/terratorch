{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "1. In colab: Go to \"Runtime\" -> \"Change runtime type\" -> Select \"T4 GPU\"\n",
    "2. Install TerraTorch"
   ],
   "id": "b4bacc318390456b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install terratorch==0.99.8 gdown tensorboard",
   "id": "W_4z81Fn9RET",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fix needed for temporal tasks \n",
    "!pip uninstall -y terratorch\n",
    "!pip install git+https://github.com/IBM/terratorch.git"
   ],
   "id": "8276ac502d23c51d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a",
   "metadata": {
    "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a"
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import gdown\n",
    "import terratorch\n",
    "import albumentations\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from terratorch.datamodules import MultiTemporalCropClassificationDataModule"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. Download the dataset from Google Drive",
   "id": "917b65b8e7cd7d65"
  },
  {
   "cell_type": "code",
   "source": [
    "# Download the full dataset (~24 GB)\n",
    "if False and not os.path.isfile('multi-temporal-crop-classification.zip'):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1Di_vj2mDuFaHnIUpneHlfOm7mMLekEyk\")\n",
    "    !unzip multi-temporal-crop-classification.zip    \n",
    "    !tar -xvzf datasets--ibm-nasa-geospatial--multi-temporal-crop-classification/snapshots/04b439f179e52a7b144f69676210eecd30c39cfc/training_chips.tgz -C datasets--ibm-nasa-geospatial--multi-temporal-crop-classification/snapshots/04b439f179e52a7b144f69676210eecd30c39cfc/ && tar -xvzf datasets--ibm-nasa-geospatial--multi-temporal-crop-classification/snapshots/04b439f179e52a7b144f69676210eecd30c39cfc/validation_chips.tgz -C datasets--ibm-nasa-geospatial--multi-temporal-crop-classification/snapshots/04b439f179e52a7b144f69676210eecd30c39cfc/ && mv datasets--ibm-nasa-geospatial--multi-temporal-crop-classification/snapshots/04b439f179e52a7b144f69676210eecd30c39cfc/training_data.txt datasets--ibm-nasa-geospatial--multi-temporal-crop-classification/snapshots/04b439f179e52a7b144f69676210eecd30c39cfc/training_chips/ && mv datasets--ibm-nasa-geospatial--multi-temporal-crop-classification/snapshots/04b439f179e52a7b144f69676210eecd30c39cfc/validation_data.txt datasets--ibm-nasa-geospatial--multi-temporal-crop-classification/snapshots/04b439f179e52a7b144f69676210eecd30c39cfc/validation_chips/\n",
    "\n",
    "dataset_path = 'datasets--ibm-nasa-geospatial--multi-temporal-crop-classification/snapshots/04b439f179e52a7b144f69676210eecd30c39cfc/'"
   ],
   "metadata": {
    "id": "dw5-9A4A4OmI",
    "collapsed": true
   },
   "id": "dw5-9A4A4OmI",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Download a random subset for demos (~1 GB)\n",
    "if not os.path.isfile('multi-temporal-crop-classification-subset.tar.gz'):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1A0cmT-Ye4X4QjETTmsmOh392vaKp7YhB\")\n",
    "    !tar -xzvf multi-temporal-crop-classification-subset.tar.gz\n",
    "\n",
    "dataset_path = \"multi-temporal-crop-classification-subset\""
   ],
   "metadata": {
    "collapsed": true,
    "id": "3t-YKKUztjXn"
   },
   "id": "3t-YKKUztjXn",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494",
   "metadata": {
    "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494"
   },
   "source": [
    "## Multi-temporal Crop Dataset\n",
    "\n",
    "Lets start with analyzing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e3854bdb-17a4-43c8-bfa8-822b44fd59c3",
   "metadata": {
    "id": "e3854bdb-17a4-43c8-bfa8-822b44fd59c3"
   },
   "source": "!ls \"{dataset_path}\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Each merged sample includes the stacked bands of three time steps\n",
    "!ls \"{dataset_path}/training_chips\" | head"
   ],
   "id": "ddd7d83440895e87",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc",
   "metadata": {
    "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc"
   },
   "source": [
    "# Adjusted dataset class for this dataset (general dataset could be used as well)\n",
    "datamodule = MultiTemporalCropClassificationDataModule(\n",
    "    batch_size=8,\n",
    "    num_workers=2,\n",
    "    data_root=dataset_path,\n",
    "    train_transform=[\n",
    "        terratorch.datasets.transforms.FlattenTemporalIntoChannels(),  # Required for temporal data\n",
    "        albumentations.D4(), # Random flips and rotation\n",
    "        albumentations.pytorch.transforms.ToTensorV2(),\n",
    "        terratorch.datasets.transforms.UnflattenTemporalFromChannels(n_timesteps=3),\n",
    "    ],\n",
    "    val_transform=None,  # Using ToTensor() by default\n",
    "    test_transform=None,\n",
    "    expand_temporal_dimension=True,\n",
    "    use_metadata=False, # The crop dataset has metadata for location and time\n",
    "    reduce_zero_label=True,\n",
    ")\n",
    "\n",
    "# Setup train and val datasets\n",
    "datamodule.setup(\"fit\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a87ed3b7-f7dc-486d-ac59-cd781a070925",
   "metadata": {
    "id": "a87ed3b7-f7dc-486d-ac59-cd781a070925"
   },
   "source": [
    "# checking for the dataset means and stds\n",
    "datamodule.means, datamodule.stds"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08644e71-d82f-426c-b0c1-79026fccb578",
   "metadata": {
    "id": "08644e71-d82f-426c-b0c1-79026fccb578"
   },
   "source": [
    "# checking datasets train split size\n",
    "train_dataset = datamodule.train_dataset\n",
    "len(train_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "88b86821-3481-4d92-bdba-246568c66c48",
   "metadata": {
    "id": "88b86821-3481-4d92-bdba-246568c66c48"
   },
   "source": [
    "# checking datasets available bands\n",
    "train_dataset.all_band_names"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9264de41-ab16-43cc-9ea2-ee51b0969624",
   "metadata": {
    "id": "9264de41-ab16-43cc-9ea2-ee51b0969624"
   },
   "source": [
    "# checking datasets classes\n",
    "train_dataset.class_names"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3a1da2ad-a797-4f4a-ad1a-cd10f9addb01",
   "metadata": {
    "id": "3a1da2ad-a797-4f4a-ad1a-cd10f9addb01",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "9c948b7c-e02b-4980-a142-b36bcb51a8e4"
   },
   "source": [
    "# plotting a few samples\n",
    "for i in range(5):\n",
    "    train_dataset.plot(train_dataset[i])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7062ddc-a3b7-4378-898c-41abcdf2ee3b",
   "metadata": {
    "id": "b7062ddc-a3b7-4378-898c-41abcdf2ee3b"
   },
   "source": [
    "# checking datasets validation split size\n",
    "val_dataset = datamodule.val_dataset\n",
    "len(val_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79",
   "metadata": {
    "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79"
   },
   "source": [
    "# checking datasets testing split size\n",
    "datamodule.setup(\"test\")\n",
    "test_dataset = datamodule.test_dataset\n",
    "len(test_dataset)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-tune Prithvi",
   "id": "4072e2f849c0df2d"
  },
  {
   "cell_type": "code",
   "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
   "metadata": {
    "scrolled": true,
    "id": "ae69d39a-857a-4392-b058-0f4b518edf6e"
   },
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=\"output/multicrop/checkpoints/\",\n",
    "    mode=\"max\",\n",
    "    monitor=\"val/Multiclass_Jaccard_Index\", # Variable to monitor\n",
    "    filename=\"best-{epoch:02d}\",\n",
    ")\n",
    "\n",
    "# Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=1, # Lightning multi-gpu often fails in notebooks\n",
    "    precision='16-mixed',  # Speed up training\n",
    "    num_nodes=1,\n",
    "    logger=True, # Uses TensorBoard by default\n",
    "    max_epochs=2, # For demos\n",
    "    log_every_n_steps=5,\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n",
    "    default_root_dir=\"output/multicrop\",\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = terratorch.tasks.SemanticSegmentationTask(\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    "    model_args={\n",
    "        # Backbone\n",
    "        \"backbone\": \"prithvi_eo_v2_300_tl\", # Model can be either prithvi_eo_v1_100, prithvi_eo_v2_300, prithvi_eo_v2_300_tl, prithvi_eo_v2_600, prithvi_eo_v2_600_tl\n",
    "        \"backbone_pretrained\": True,\n",
    "        \"backbone_num_frames\": 3,\n",
    "        \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_NARROW\", \"SWIR_1\", \"SWIR_2\"],\n",
    "        \"backbone_coords_encoding\": [], # use [\"time\", \"location\"] for time and location metadata\n",
    "        \n",
    "        # Necks \n",
    "        \"necks\": [\n",
    "            {\n",
    "                \"name\": \"SelectIndices\",\n",
    "                # \"indices\": [2, 5, 8, 11] # indices for prithvi_eo_v1_100\n",
    "                \"indices\": [5, 11, 17, 23] # indices for prithvi_eo_v2_300\n",
    "                # \"indices\": [7, 15, 23, 31] # indices for prithvi_eo_v2_600\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"ReshapeTokensToImage\",\n",
    "                \"effective_time_dim\": 3\n",
    "            },\n",
    "            {\"name\": \"LearnedInterpolateToPyramidal\"},            \n",
    "        ],\n",
    "        \n",
    "        # Decoder\n",
    "        \"decoder\": \"UNetDecoder\",\n",
    "        \"decoder_channels\": [512, 256, 128, 64],\n",
    "        \n",
    "        # Head\n",
    "        \"head_dropout\": 0.1,\n",
    "        \"num_classes\": 13,\n",
    "    },\n",
    "    \n",
    "    loss=\"ce\",\n",
    "    lr=1e-4,\n",
    "    optimizer=\"AdamW\",\n",
    "    ignore_index=-1,\n",
    "    freeze_backbone=True,  # Speeds up fine-tuning\n",
    "    freeze_decoder=False,\n",
    "    plot_on_val=True,\n",
    "    \n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output"
   ],
   "id": "c468e22795657205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training\n",
    "trainer.fit(model, datamodule=datamodule)"
   ],
   "id": "27fee1e72be7349",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_ckpt_path = \"output/multicrop/checkpoints/best-epoch=01.ckpt\"",
   "id": "388aa3db0dc07460",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35a77263-5308-4781-a17f-a35e62ca1875",
   "metadata": {
    "scrolled": true,
    "id": "35a77263-5308-4781-a17f-a35e62ca1875"
   },
   "source": "trainer.test(model, datamodule=datamodule, ckpt_path=best_ckpt_path)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e015fe0-88ee-46cf-b972-f8cb9d361536",
   "metadata": {
    "id": "1e015fe0-88ee-46cf-b972-f8cb9d361536",
    "outputId": "c7c06228-e634-4608-cb0e-617d003fea46",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# now we can use the model for predictions and plotting!\n",
    "model = terratorch.tasks.SemanticSegmentationTask.load_from_checkpoint(\n",
    "    best_ckpt_path,\n",
    "    model_factory=model.hparams.model_factory,\n",
    "    model_args=model.hparams.model_args,\n",
    ")\n",
    "\n",
    "test_loader = datamodule.test_dataloader()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(test_loader))\n",
    "    images = batch[\"image\"].to(model.device)\n",
    "    masks = batch[\"mask\"].numpy()\n",
    "\n",
    "    outputs = model(images)\n",
    "    preds = torch.argmax(outputs.output, dim=1).cpu().numpy()\n",
    "\n",
    "for i in range(5):\n",
    "    sample = {key: batch[key][i] for key in batch}\n",
    "    sample[\"prediction\"] = preds[i]\n",
    "    test_dataset.plot(sample)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning via CLI\n",
    "\n",
    "You might want to restart the session to free up GPU memory."
   ],
   "id": "a0c88e2d5ab78020"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Download config\n",
    "!wget wget https://raw.githubusercontent.com/ibm/TerraTorch/refs/heads/main/examples/tutorial/configs/prithvi_v2_eo_300_tl_unet_multitemporal_crop.yaml"
   ],
   "id": "31bbce2eeffef594"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run fine-tuning\n",
    "!terratorch fit -c prithvi_v2_eo_300_tl_unet_multitemporal_crop.yaml"
   ],
   "id": "bdbf05ebc81b9998"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
