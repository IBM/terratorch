{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a61638-d211-4818-a619-afd6096c606b",
   "metadata": {},
   "source": [
    "Please run prithvi_v2_eo_300_tl_unet_burnscars_tensorrt.ipynb first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W_4z81Fn9RET",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install terratorch==1.0.1 gdown tensorrt onnx onnxruntime polygraphy numpy pycuda numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a",
   "metadata": {
    "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import gdown\n",
    "import terratorch\n",
    "import albumentations\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from terratorch.datamodules import GenericNonGeoSegmentationDataModule\n",
    "import warnings\n",
    "import time\n",
    "import tensorrt as trt\n",
    "from pytorch_lightning import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc",
   "metadata": {
    "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc"
   },
   "outputs": [],
   "source": [
    "dataset_path = Path('hls_burn_scars')\n",
    "datamodule = terratorch.datamodules.GenericNonGeoSegmentationDataModule(\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    num_classes=2,\n",
    "\n",
    "    # Define dataset paths \n",
    "    train_data_root=dataset_path / 'data/',\n",
    "    train_label_data_root=dataset_path / 'data/',\n",
    "    val_data_root=dataset_path / 'data/',\n",
    "    val_label_data_root=dataset_path / 'data/',\n",
    "    test_data_root=dataset_path / 'data/',\n",
    "    test_label_data_root=dataset_path / 'data/',\n",
    "\n",
    "    # Define splits\n",
    "    train_split=dataset_path / 'splits/train.txt',\n",
    "    val_split=dataset_path / 'splits/val.txt',\n",
    "    test_split=dataset_path / 'splits/test.txt',\n",
    "    \n",
    "    img_grep='*_merged.tif',\n",
    "    label_grep='*.mask.tif',\n",
    "    \n",
    "    train_transform=[\n",
    "        albumentations.D4(), # Random flips and rotation\n",
    "        albumentations.pytorch.transforms.ToTensorV2(),\n",
    "    ],\n",
    "    val_transform=None,  # Using ToTensor() by default\n",
    "    test_transform=None,\n",
    "        \n",
    "    # Define standardization values\n",
    "    means=[\n",
    "      0.0333497067415863,\n",
    "      0.0570118552053618,\n",
    "      0.0588974813200132,\n",
    "      0.2323245113436119,\n",
    "      0.1972854853760658,\n",
    "      0.1194491422518656,\n",
    "    ],\n",
    "    stds=[\n",
    "      0.0226913556882377,\n",
    "      0.0268075602230702,\n",
    "      0.0400410984436278,\n",
    "      0.0779173242367269,\n",
    "      0.0870873883814014,\n",
    "      0.0724197947743781,\n",
    "    ],\n",
    "    no_data_replace=0,\n",
    "    no_label_replace=-1,\n",
    "    # We use all six bands of the data, so we don't need to define dataset_bands and output_bands.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79",
   "metadata": {
    "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79"
   },
   "outputs": [],
   "source": [
    "datamodule.setup(\"test\")\n",
    "test_dataset = datamodule.test_dataset\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d5c8b-9086-410f-bff8-0e72cc1e2ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return terratorch.tasks.SemanticSegmentationTask(\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    "    model_args={\n",
    "        # Backbone\n",
    "        \"backbone\": \"prithvi_eo_v2_300\", # Model can be either prithvi_eo_v1_100, prithvi_eo_v2_300, prithvi_eo_v2_300_tl, prithvi_eo_v2_600, prithvi_eo_v2_600_tl\n",
    "        \"backbone_pretrained\": True,\n",
    "        \"backbone_num_frames\": 1, # 1 is the default value,\n",
    "        \"backbone_img_size\": 512,\n",
    "        \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_NARROW\", \"SWIR_1\", \"SWIR_2\"],\n",
    "        # \"backbone_coords_encoding\": [], # use [\"time\", \"location\"] for time and location metadata\n",
    "        \n",
    "        # Necks \n",
    "        \"necks\": [\n",
    "            {\n",
    "                \"name\": \"SelectIndices\",\n",
    "                # \"indices\": [2, 5, 8, 11] # indices for prithvi_eo_v1_100\n",
    "                \"indices\": [5, 11, 17, 23] # indices for prithvi_eo_v2_300\n",
    "                # \"indices\": [7, 15, 23, 31] # indices for prithvi_eo_v2_600\n",
    "            },\n",
    "            {\"name\": \"ReshapeTokensToImage\",},\n",
    "            {\"name\": \"LearnedInterpolateToPyramidal\"}            \n",
    "        ],\n",
    "        \n",
    "        # Decoder\n",
    "        \"decoder\": \"UNetDecoder\",\n",
    "        \"decoder_channels\": [512, 256, 128, 64],\n",
    "        \n",
    "        # Head\n",
    "        \"head_dropout\": 0.1,\n",
    "        \"num_classes\": 2,\n",
    "    },\n",
    "    \n",
    "    loss=\"ce\",\n",
    "    optimizer=\"AdamW\",\n",
    "    lr=1e-4,\n",
    "    ignore_index=-1,\n",
    "    freeze_backbone=True, # Only to speed up fine-tuning\n",
    "    freeze_decoder=False,\n",
    "    plot_on_val=True,\n",
    "    class_names=['no burned', 'burned']  # optionally define class names\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
   "metadata": {
    "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.seed_everything(0)\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a48c81-da6c-4f58-b3a1-5d63331ebfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"checkpoint.pt\", map_location=torch.device('cuda')), strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b4a90f-d464-40ae-9d9f-3d8fc3e7d96d",
   "metadata": {},
   "source": [
    "# Compute pytorch inference metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc5edb6-6dd1-41fc-b8a5-37a78f551c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "\n",
    "class ClassifierWrapper(LightningModule):\n",
    "    def __init__(self, base_model: LightningModule):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model.eval()\n",
    "        self.accuracy = BinaryAccuracy(ignore_index=-1)\n",
    "        self.precision = BinaryPrecision(ignore_index=-1)\n",
    "        self.recall = BinaryRecall(ignore_index=-1)\n",
    "        self.f1 = BinaryF1Score(ignore_index=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch[\"image\"]  # (B, C, H, W)\n",
    "        y = batch[\"mask\"]   # (B, H, W)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x)\n",
    "            probs = output.output  # (B, 1, H, W)\n",
    "            preds = torch.argmax(probs, dim=1)  # (B, H, W) as torch.long\n",
    "\n",
    "        preds_flat = preds.reshape(-1)\n",
    "        y_flat = y.reshape(-1)\n",
    "\n",
    "        \n",
    "        self.accuracy(preds_flat, y_flat)\n",
    "        self.precision(preds_flat, y_flat)\n",
    "        self.recall(preds_flat, y_flat)\n",
    "        self.f1(preds_flat, y_flat)\n",
    "\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log(\"test/accuracy\", self.accuracy.compute(), prog_bar=True)\n",
    "        self.log(\"test/precision\", self.precision.compute(), prog_bar=True)\n",
    "        self.log(\"test/recall\", self.recall.compute(), prog_bar=True)\n",
    "        self.log(\"test/f1\", self.f1.compute(), prog_bar=True)\n",
    "\n",
    "        self.accuracy.reset()\n",
    "        self.precision.reset()\n",
    "        self.recall.reset()\n",
    "        self.f1.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899efeb5-f474-4d17-a503-342573b4db02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = ClassifierWrapper(model)\n",
    "\n",
    "trainer = Trainer(accelerator=\"auto\", devices=1 if torch.cuda.is_available() else None)\n",
    "trainer.test(classifier, dataloaders=datamodule.test_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d118fb81-44ea-4693-8c82-c82a9b0ca6cb",
   "metadata": {},
   "source": [
    "# Compute TensorRT inference metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad080c-8272-432f-92d4-ef8965c48233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import tensorrt as trt\n",
    "import time\n",
    "\n",
    "def run_tensorrt_evaluation(datamodule, engine_path='model.trt'):\n",
    "    # Initialize metrics\n",
    "    accuracy = BinaryAccuracy(ignore_index=-1).cuda()\n",
    "    precision = BinaryPrecision(ignore_index=-1).cuda()\n",
    "    recall = BinaryRecall(ignore_index=-1).cuda()\n",
    "    f1 = BinaryF1Score(ignore_index=-1).cuda()\n",
    "\n",
    "    test_loader = datamodule.test_dataloader()\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "    # Load engine\n",
    "    with open(engine_path, 'rb') as f:\n",
    "        engine_data = f.read()\n",
    "    runtime = trt.Runtime(TRT_LOGGER)\n",
    "    engine = runtime.deserialize_cuda_engine(engine_data)\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # Assume fixed output shape (binary class logits: 2 classes)\n",
    "    output_shape = (1, 2, 512, 512)\n",
    "    output_size_bytes = int(np.prod(output_shape)) * np.float32().itemsize\n",
    "    d_output = cuda.mem_alloc(output_size_bytes)\n",
    "\n",
    "    for batch in test_loader:\n",
    "        # Ensure batch size 1\n",
    "        for key in batch:\n",
    "            batch[key] = batch[key][:1]\n",
    "\n",
    "        batch = datamodule.aug(batch)\n",
    "        images = batch[\"image\"].to('cuda')  # (1, C, H, W)\n",
    "        masks = batch[\"mask\"].to('cuda')    # (1, H, W)\n",
    "\n",
    "        input_data = images.cpu().numpy().astype(np.float32)\n",
    "        input_shape = input_data.shape\n",
    "        input_size_bytes = int(np.prod(input_shape)) * np.float32().itemsize\n",
    "        d_input = cuda.mem_alloc(input_size_bytes)\n",
    "\n",
    "        # Transfer input\n",
    "        cuda.memcpy_htod(d_input, input_data)\n",
    "\n",
    "        context.execute_v2([int(d_input), int(d_output)])\n",
    "        cuda.Context.synchronize()\n",
    "\n",
    "        # Get output\n",
    "        output_data = np.empty(output_shape, dtype=np.float32)\n",
    "        cuda.memcpy_dtoh(output_data, d_output)\n",
    "\n",
    "        # Process output\n",
    "        preds = torch.argmax(torch.from_numpy(output_data), dim=1).to('cuda')  # (1, H, W)\n",
    "\n",
    "        # Flatten for metrics\n",
    "        preds_flat = preds.view(-1)\n",
    "        masks_flat = masks.view(-1)\n",
    "\n",
    "        # Apply metrics\n",
    "        accuracy(preds_flat, masks_flat)\n",
    "        precision(preds_flat, masks_flat)\n",
    "        recall(preds_flat, masks_flat)\n",
    "        f1(preds_flat, masks_flat)\n",
    "\n",
    "    # Final metrics\n",
    "    print(\"\\n=== TensorRT Evaluation Metrics ===\")\n",
    "    print(f\"Accuracy:  {accuracy.compute().item():.4f}\")\n",
    "    print(f\"Precision: {precision.compute().item():.4f}\")\n",
    "    print(f\"Recall:    {recall.compute().item():.4f}\")\n",
    "    print(f\"F1 Score:  {f1.compute().item():.4f}\")\n",
    "\n",
    "    # Reset metrics\n",
    "    accuracy.reset()\n",
    "    precision.reset()\n",
    "    recall.reset()\n",
    "    f1.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da708f-5cf8-43f7-8ec6-347b5efa349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tensorrt_evaluation(datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c12f7-7261-4443-b0ac-6f8b1168a863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a191b-0164-432c-b6ca-cc8ba29ce20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838cce3f-a8f3-475c-9b60-6ee303873081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
