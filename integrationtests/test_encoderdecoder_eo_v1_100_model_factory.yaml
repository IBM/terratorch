# lightning.pytorch==2.1.1
seed_everything: 0
trainer:
  accelerator: auto
  strategy: auto
  devices: auto
  num_nodes: 1
  precision: 16-mixed
  logger: true             
  callbacks:
    - class_path: RichProgressBar
    - class_path: LearningRateMonitor
      init_args:
        logging_interval: epoch
    # ---- Early stop if ----
    - class_path: EarlyStopping
      init_args:
        monitor: val/loss
        patience: 20
     # ---- Early stop endif ----
    - class_path: ModelCheckpoint
      init_args:
        dirpath: /dccstor/terratorch/tmp/
        mode: min
        monitor: val/loss
        filename: best-state_dict-{epoch:02d}
        save_weights_only: True
  max_epochs: 2
  check_val_every_n_epoch: 1
  log_every_n_steps: 50
  enable_checkpointing: true
  default_root_dir: /dccstor/terratorch/tmp/

data:
  class_path: terratorch.datamodules.GenericNonGeoSegmentationDataModule
  init_args:
    batch_size: 4
    num_workers: 2
    no_label_replace: -1
    no_data_replace: 0
    constant_scale: 1.0
    dataset_bands:
      - '0'
      - '1'
      - '2'
      - '3'
      - '4'
      - '5'

    output_bands:
      - '0'
      - '1'
      - '2'
      - '3'
      - '4'
      - '5'

    rgb_indices:
      - 0
      - 1
      - 2

    train_data_root: /dccstor/geofm-finetuning/fire-scars/finetune-data/6_bands_no_replant_extended/training
    train_label_data_root: /dccstor/geofm-finetuning/fire-scars/finetune-data/6_bands_no_replant_extended/training
    val_data_root: /dccstor/geofm-finetuning/fire-scars/finetune-data/6_bands_no_replant_extended/validation
    val_label_data_root: /dccstor/geofm-finetuning/fire-scars/finetune-data/6_bands_no_replant_extended/validation
    test_data_root: /dccstor/geofm-finetuning/fire-scars/finetune-data/6_bands_no_replant_extended/validation
    test_label_data_root: /dccstor/geofm-finetuning/fire-scars/finetune-data/6_bands_no_replant_extended/validation
    # Splits not available in ccc for burnscars data
    # train_split: /data//geodata-060bbc44822a11efb3260a580a830dad/split_files/train_data.txt
    # test_split: /data//geodata-060bbc44822a11efb3260a580a830dad/split_files/test_data.txt
    # val_split: /data//geodata-060bbc44822a11efb3260a580a830dad/split_files/val_data.txt
    ignore_split_file_extensions: true
    allow_substring_split_file: true
    img_grep: "*_merged.tif"
    label_grep: "*.mask.tif"
    means: 
      - 0.052829564761523104
      - 0.07822514779700994
      - 0.09545302348640401
      - 0.2128596444116123
      - 0.2363016737011897
      - 0.17234100022878698

    stds: 
      - 0.028757146620143812
      - 0.03540772770593507
      - 0.05291947163682527
      - 0.06949186937256507
      - 0.08958868240264736
      - 0.08198354165348874

    num_classes: 2
    # ---- train_transform if ----
    # ---- train_transform endif ----

    # if backbone is prithvi-EO-v2
    test_transform:
      - class_path: ToTensorV2
model:
  class_path: terratorch.tasks.SemanticSegmentationTask
  init_args:
    model_args:
      backbone_pretrained: true 
      backbone: prithvi_eo_v1_100
      # backbone_ckpt_path: /terratorch/gfm_models/prithvi_eo_v1_100/Prithvi_EO_V1_100M.pt 
      backbone_drop_path: 0.1 
      backbone_bands:
        - '0'
        - '1'
        - '2'
        - '3'
        - '4'
        - '5'

      necks: 
        - name: SelectIndices
          indices: [2, 5, 8, 11] # 100M models
        - name: ReshapeTokensToImage # required
        - name: LearnedInterpolateToPyramidal 
      decoder: UNetDecoder
      #TODO user provided channels
      decoder_channels: [512, 256, 128, 64]
      num_classes: 2
      head_dropout: 0.1
    model_factory: EncoderDecoderFactory 
    loss: ce
    plot_on_val: 2
    ignore_index: -1
    freeze_backbone: false
    freeze_decoder: false

    # ---- optimizer start ----
    # ---- optimizer end ----
    
    tiled_inference_parameters: 
      h_crop: 512
      h_stride: 448
      w_crop: 512
      w_stride: 448
      average_patches: True
    
optimizer:
  class_path: torch.optim.Adam
  init_args:
    # ---- Optimizer start if ----
    lr: 6e-05
    
    weight_decay: 0.05
    # ---- Optimizer stop if ----
lr_scheduler:
  class_path: ReduceLROnPlateau
  init_args:
    monitor: val/loss